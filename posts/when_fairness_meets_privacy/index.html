<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://responsible-ai-datascience-ipParis.github.io/"><title>When Fairness Meets Privacy | Bloggin on Responsible AI</title>
<meta name=description content="Bloggin on Responsible AI"><meta property="og:url" content="https://responsible-ai-datascience-ipParis.github.io/posts/when_fairness_meets_privacy/"><meta property="og:site_name" content="Bloggin on Responsible AI"><meta property="og:title" content="When Fairness Meets Privacy"><meta property="og:description" content="When Fairness Meets Privacy: A Double-Edged Sword in Machine Learning This blog is based on and aims to present the key insights from the paper: When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks by Tian et al. (2023) 1. The study investigates how fairness-aware models can introduce new privacy risks, specifically through membership inference attacks. By summarizing the main findings and implications, this blog provides an accessible overview of the paper’s contributions and their significance for machine learning security and ethical AI development. For full details, refer to the original publication here."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-10T12:22:37+01:00"><meta property="article:modified_time" content="2025-03-10T12:22:37+01:00"><meta itemprop=name content="When Fairness Meets Privacy"><meta itemprop=description content="When Fairness Meets Privacy: A Double-Edged Sword in Machine Learning This blog is based on and aims to present the key insights from the paper: When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks by Tian et al. (2023) 1. The study investigates how fairness-aware models can introduce new privacy risks, specifically through membership inference attacks. By summarizing the main findings and implications, this blog provides an accessible overview of the paper’s contributions and their significance for machine learning security and ethical AI development. For full details, refer to the original publication here."><meta itemprop=datePublished content="2025-03-10T12:22:37+01:00"><meta itemprop=dateModified content="2025-03-10T12:22:37+01:00"><meta itemprop=wordCount content="2411"><link rel=canonical href=https://responsible-ai-datascience-ipParis.github.io/posts/when_fairness_meets_privacy/><link rel=icon href=https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//atom.xml><link rel=alternate type=application/json title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"When Fairness Meets Privacy","headline":"When Fairness Meets Privacy","alternativeHeadline":"","description":"\u003cstyle TYPE=\u0022text\/css\u0022\u003e\n code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n \u003c\/style\u003e\n \u003cscript type=\u0022text\/x-mathjax-config\u0022\u003e\n MathJax.Hub.Config({\n     tex2jax: {\n         inlineMath: [[\u0027$\u0027,\u0027$\u0027], [\u0027\\\\(\u0027,\u0027\\\\)\u0027]],\n         displayMath: [[\u0027$$\u0027,\u0027$$\u0027], [\u0027\\\\[\u0027,\u0027\\\\]\u0027]],\n         skipTags: [\u0027script\u0027, \u0027noscript\u0027, \u0027style\u0027, \u0027textarea\u0027, \u0027pre\u0027] \/\/ removed \u0027code\u0027 entry\n     }\n });\n MathJax.Hub.Queue(function() {\n     var all = MathJax.Hub.getAllJax(), i;\n     for(i = 0; i \u003c all.length; i \u002b= 1) {\n         all[i].SourceElement().parentNode.className \u002b= \u0027 has-jax\u0027;\n     }\n });\n \u003c\/script\u003e\n \u003cscript type=\u0022text\/javascript\u0022 src=\u0022https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.4\/MathJax.js?config=TeX-AMS_HTML-full\u0022\u003e\u003c\/script\u003e\n\u003ch1 style=\u0022font-size: 36px;\u0022\u003eWhen Fairness Meets Privacy: A Double-Edged Sword in Machine Learning\u003c\/h1\u003e\n\u003cp\u003e\u003cem\u003eThis blog is based on and aims to present the key insights from the paper: \u003cstrong\u003eWhen Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks\u003c\/strong\u003e by Tian et al. (2023) \u003csup id=\u0022fnref:1\u0022\u003e\u003ca href=\u0022#fn:1\u0022 class=\u0022footnote-ref\u0022 role=\u0022doc-noteref\u0022\u003e1\u003c\/a\u003e\u003c\/sup\u003e. The study investigates how fairness-aware models can introduce new privacy risks, specifically through membership inference attacks. By summarizing the main findings and implications, this blog provides an accessible overview of the paper’s contributions and their significance for machine learning security and ethical AI development. For full details, refer to the original publication \u003ca href=\u0022https:\/\/arxiv.org\/pdf\/2311.03865\u0022\u003ehere\u003c\/a\u003e.\u003c\/em\u003e\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/when_fairness_meets_privacy\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Bloggin on Responsible AI","copyrightYear":"2025","dateCreated":"2025-03-10T12:22:37.00Z","datePublished":"2025-03-10T12:22:37.00Z","dateModified":"2025-03-10T12:22:37.00Z","publisher":{"@type":"Organization","name":"Bloggin on Responsible AI","url":"https://responsible-ai-datascience-ipParis.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://responsible-ai-datascience-ipParis.github.io/assets/favicon.ico","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/when_fairness_meets_privacy\/","wordCount":"2411","genre":[],"keywords":[]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=../../><b>Bloggin on Responsible AI</b>
<span class="text-stone-500 animate-blink">▮</span></a></p><ul style=padding:0;margin:0><li><a href=../../posts/><span>Post</span></a><li><a href=../../tutorial/><span>Tutorial</span></a><li><a href=../../about/><span>About</span></a><li><a href=../../articles/><span>Articles</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>When Fairness Meets Privacy</h2><style type=text/css>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit}</style><script type=text/x-mathjax-config>
 MathJax.Hub.Config({
     tex2jax: {
         inlineMath: [['$','$'], ['\\(','\\)']],
         displayMath: [['$$','$$'], ['\\[','\\]']],
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
     }
 });
 MathJax.Hub.Queue(function() {
     var all = MathJax.Hub.getAllJax(), i;
     for(i = 0; i < all.length; i += 1) {
         all[i].SourceElement().parentNode.className += ' has-jax';
     }
 });
 </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script><h1 style=font-size:36px>When Fairness Meets Privacy: A Double-Edged Sword in Machine Learning</h1><p><em>This blog is based on and aims to present the key insights from the paper: <strong>When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks</strong> by Tian et al. (2023) <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. The study investigates how fairness-aware models can introduce new privacy risks, specifically through membership inference attacks. By summarizing the main findings and implications, this blog provides an accessible overview of the paper’s contributions and their significance for machine learning security and ethical AI development. For full details, refer to the original publication <a href=https://arxiv.org/pdf/2311.03865>here</a>.</em></p><h1 style=font-size:24px>Authors: Lagarde Vincent, Boyenval Thibaut, Leurquin Daniel</h1><div style=text-align:center><img src=../../images/image_fairness_privacy/figure2.webp alt="FD-MIA: Prediction Difference Distribution" style=width:80%;display:block;margin:auto><p style=font-style:italic;font-size:14px>This image was generated using artificial intelligence.</p></div><h2 id=table-of-contents>Table of Contents</h2><ol><li><a href=#1-introduction-fairness-or-privacy-pick-your-poison>Introduction: Fairness or Privacy—Pick Your Poison?</a></li><li><a href=#2-algorithmic-fairness-a-noble-goal-that-cuts-both-ways>Algorithmic Fairness: A Noble Goal That Cuts Both Ways</a></li><li><a href=#3-membership-inference-attacks-the-silent-thief-of-privacy>Membership Inference Attacks: The Silent Thief of Privacy</a></li><li><a href=#4-the-birth-of-a-new-threat-fairness-discrepancy-membership-inference-attacks-fd-mia>The Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA)</a></li><li><a href=#5-reproducible-code-experiments-illustrating-fd-mia>Reproducible Code Experiments: Illustrating FD-MIA</a></li><li><a href=#6-experimental-findings-how-fairness-opens-the-door-to-attackers>Experimental Findings: How Fairness Opens the Door to Attackers</a></li><li><a href=#7-the-future-of-fairness-and-privacy-can-we-have-both>The Future of Fairness and Privacy: Can We Have Both?</a></li></ol><hr><h2 id=1-introduction-fairness-or-privacy-pick-your-poison>1. Introduction: Fairness or Privacy Pick Your Poison</h2><blockquote><p><em>It is double pleasure to deceive the deceiver.</em> — Niccolò Machiavelli.</p></blockquote><p>This paradox of attack and defense perfectly applies to the interplay between fairness and privacy in machine learning.</p><p>Imagine stepping into a high-tech courtroom. The AI judge, designed to be perfectly fair, renders unbiased decisions. But then, a hacker in the back row smirks—because that same fairness-enhancing mechanism just leaked private data about every case it trained on.</p><p>Fairness and privacy in AI are like the two ends of a seesaw: push too hard on one side, and the other rises uncontrollably. <strong>Recent research reveals a disturbing paradox: making a model fairer can also make it leak more private information.</strong></p><p>This blog explores how fairness in machine learning, despite its good intentions, can introduce <strong>Membership Inference Attacks (MIAs)</strong>. Worse still, it uncovers a devastating new attack—<strong>Fairness Discrepancy Membership Inference Attack (FD-MIA)</strong>—that exploits fairness interventions to <strong>make privacy breaches even more effective</strong>.</p><hr><h2 id=2-algorithmic-fairness-a-noble-goal-that-cuts-both-ways>2. Algorithmic Fairness: A Noble Goal That Cuts Both Ways</h2><p>Fairness in AI is like forging a perfect sword—it must be balanced, precise, and just. Researchers have developed <strong>in-processing fairness interventions</strong>, which modify the training process to remove biases in model predictions. These methods act like master swordsmiths, hammering out the unwanted imperfections in AI decision-making.</p><p>However, every sword has two edges. These fairness techniques do not just eliminate biases—they also alter how models respond to data. This change in behavior can create exploitable patterns that adversaries can use to infer whether a specific individual was part of the training data. In short, while fairness dulls one blade (bias), it sharpens another (privacy risk).</p><p>Mathematically, fairness interventions often involve introducing constraints into the loss function:</p><p>$$L_\text{fair} = L_\text{orig} + \lambda \cdot \mathcal{L}_{\text{fairness}}$$</p><p>where</p><ul><li>$\mathcal{L}_{\text{orig}}$ is the original loss function (e.g., cross-entropy loss for classification tasks).</li><li>$\mathcal{L}_{\text{fairness}}$ is a fairness penalty term, which ensures that predictions are balanced across different demographic groups.</li><li>$\lambda$ is a hyperparameter controlling the trade-off between accuracy and fairness.</li></ul><p>Common fairness constraints include <strong>Equalized Odds</strong>, which ensures that true positive and false positive rates are equal across groups:</p><p>$$P(\hat{Y} = 1 | Y = 1, S = s_0) = P(\hat{Y} = 1 | Y = 1, S = s_1)$$</p><p>where $S$ represents a sensitive attribute (e.g., gender or race).</p><p>The fairness penalty can be incorporated during model training by adding it to the loss function, as shown in this training function. A penalty coefficient can be specified to control the impact of the fairness term. Setting the coefficient to 0 results in no penalty being applied to the loss:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>train_model</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>data_loader</span><span style=color:#111>,</span> <span style=color:#111>optimizer</span><span style=color:#111>,</span> <span style=color:#111>criterion</span><span style=color:#111>,</span> <span style=color:#111>fairness_weight</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>train</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#111>total_loss</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>X_batch</span><span style=color:#111>,</span> <span style=color:#111>y_batch</span><span style=color:#111>,</span> <span style=color:#111>sensitive_batch</span> <span style=color:#f92672>in</span> <span style=color:#111>data_loader</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>        <span style=color:#111>optimizer</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>logits</span> <span style=color:#f92672>=</span> <span style=color:#111>model</span><span style=color:#111>(</span><span style=color:#111>X_batch</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>criterion</span><span style=color:#111>(</span><span style=color:#111>logits</span><span style=color:#111>,</span> <span style=color:#111>y_batch</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Fairness regularization: penalize different confidence (softmax probabilities)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>if</span> <span style=color:#111>fairness_weight</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>probs</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>functional</span><span style=color:#f92672>.</span><span style=color:#111>softmax</span><span style=color:#111>(</span><span style=color:#111>logits</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Compute average probability for each sensitive group</span>
</span></span><span style=display:flex><span>            <span style=color:#111>group0_mask</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#111>sensitive_batch</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>group1_mask</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#111>sensitive_batch</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>if</span> <span style=color:#111>group0_mask</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>()</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> <span style=color:#111>group1_mask</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>()</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>                <span style=color:#111>avg_prob0</span> <span style=color:#f92672>=</span> <span style=color:#111>probs</span><span style=color:#111>[</span><span style=color:#111>group0_mask</span><span style=color:#111>]</span><span style=color:#f92672>.</span><span style=color:#111>mean</span><span style=color:#111>(</span><span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>                <span style=color:#111>avg_prob1</span> <span style=color:#f92672>=</span> <span style=color:#111>probs</span><span style=color:#111>[</span><span style=color:#111>group1_mask</span><span style=color:#111>]</span><span style=color:#f92672>.</span><span style=color:#111>mean</span><span style=color:#111>(</span><span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># L2 difference between average prediction distributions</span>
</span></span><span style=display:flex><span>                <span style=color:#111>fairness_penalty</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>norm</span><span style=color:#111>(</span><span style=color:#111>avg_prob0</span> <span style=color:#f92672>-</span> <span style=color:#111>avg_prob1</span><span style=color:#111>,</span> <span style=color:#111>p</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>                <span style=color:#111>loss</span> <span style=color:#f92672>+=</span> <span style=color:#111>fairness_weight</span> <span style=color:#f92672>*</span> <span style=color:#111>fairness_penalty</span>
</span></span><span style=display:flex><span>        <span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>optimizer</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>total_loss</span> <span style=color:#f92672>+=</span> <span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>item</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#111>total_loss</span> <span style=color:#f92672>/</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>data_loader</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>While these interventions improve fairness, they also alter the confidence distribution of model predictions—<strong>a fact that attackers can exploit</strong>.</p><hr><h2 id=3-membership-inference-attacks-the-silent-thief-of-privacy>3. Membership Inference Attacks: The Silent Thief of Privacy</h2><p>🔐 A Parallel with Cryptography
<strong>Membership inference attacks (MIAs)</strong> are to privacy what brute-force attacks are to passwords. Instead of guessing a password, they test thousands of combinations to see which one is good.</p><p>MIAs work the same way: they analyze a model’s outputs to determine if a given data point was part of its training set.</p><p>A traditional MIA exploits <strong>confidence scores</strong>—the probabilities that a model assigns to different predictions. The intuition is simple: models tend to be more confident on data they have seen during training. Given a target model $T$ and a queried sample $x$, an attacker computes:</p><p>$$M(x) = 1 \text{ if } A(T(x)) > \tau$$</p><p>where:</p><ul><li>$A(T(x))$ is a decision function (often a threshold on the confidence score).</li><li>$\tau$ is a predefined threshold.</li></ul><h3 id=why-traditional-mias-fail-on-fair-models>Why Traditional MIAs Fail on Fair Models</h3><p>Fairness interventions introduce <strong>more uncertainty</strong> into the model’s predictions. This causes:</p><ul><li><strong>Lower confidence scores overall</strong>, making it harder for attackers to distinguish between training and non-training samples.</li><li><strong>More uniform confidence distributions</strong>, which means attackers lose their key signal.</li></ul><p>Thus, fairness-enhanced models resist traditional MIAs. But this protection is not foolproof—<strong>a new, more dangerous attack lurks in the shadows</strong>.</p><hr><h2 id=4-the-birth-of-a-new-threat-fairness-discrepancy-membership-inference-attacks-fd-mia>4. The Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA)</h2><p>If traditional MIAs are blunt weapons, <strong>FD-MIA is a scalpel.</strong> It exploits the discrepancies between a biased model and a fairness-enhanced one.</p><h3 id=how-does-fd-mia-work>How does FD-MIA work?</h3><p>Fairness interventions shift model predictions differently for training and non-training data. This creates a <strong>gap</strong> between how biased and fair models behave for the same inputs. An attacker, armed with knowledge of both models, can exploit this difference to infer membership with high accuracy.</p><p>Mathematically, FD-MIA extends membership prediction by comparing prediction shifts between biased and fair models:</p><p>$$M(x) = 1 \text{ if } |T_{\text{bias}}(x) - T_{\text{fair}}(x)| > \tau$$</p><p>where:</p><ul><li>$T_{\text{bias}}(x)$ and $T_{\text{fair}}(x)$ are the predictions from the biased and fair models, respectively.</li><li>$\tau$ is a threshold chosen by the attacker.</li></ul><div style=text-align:center><img src=../../images/image_fairness_privacy/figure3.png alt="FD-MIA: Prediction Difference Distribution" style=width:80%;display:block;margin:auto><p style=font-style:italic;font-size:14px>Figure 1: FD-MIA exploits the predictions from both models to achieve efficient attacks. From original paper.</p></div><p>The key insight is that <strong>fairness interventions cause systematic shifts</strong> in model confidence, creating a measurable pattern that attackers can exploit.</p><p>Here is an example implementation of the FD-MIA attack using a function that compares the prediction difference between two models against a user-defined threshold:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>fd_mia_attack</span><span style=color:#111>(</span><span style=color:#111>sample</span><span style=color:#111>,</span> <span style=color:#111>biased_model</span><span style=color:#111>,</span> <span style=color:#111>fair_model</span><span style=color:#111>,</span> <span style=color:#111>threshold</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#d88200>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d88200>    Given a sample, compute the absolute difference between the
</span></span></span><span style=display:flex><span><span style=color:#d88200>    biased and fair model&#39;s softmax outputs for the positive class.
</span></span></span><span style=display:flex><span><span style=color:#d88200>    If the difference exceeds the threshold, predict membership.
</span></span></span><span style=display:flex><span><span style=color:#d88200>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#111>biased_model</span><span style=color:#f92672>.</span><span style=color:#111>eval</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#111>fair_model</span><span style=color:#f92672>.</span><span style=color:#111>eval</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>with</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>no_grad</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>        <span style=color:#111>logits_b</span> <span style=color:#f92672>=</span> <span style=color:#111>biased_model</span><span style=color:#111>(</span><span style=color:#111>sample</span><span style=color:#f92672>.</span><span style=color:#111>unsqueeze</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>logits_f</span> <span style=color:#f92672>=</span> <span style=color:#111>fair_model</span><span style=color:#111>(</span><span style=color:#111>sample</span><span style=color:#f92672>.</span><span style=color:#111>unsqueeze</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>prob_b</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>functional</span><span style=color:#f92672>.</span><span style=color:#111>softmax</span><span style=color:#111>(</span><span style=color:#111>logits_b</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)[</span><span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>prob_f</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>functional</span><span style=color:#f92672>.</span><span style=color:#111>softmax</span><span style=color:#111>(</span><span style=color:#111>logits_f</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)[</span><span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>diff</span> <span style=color:#f92672>=</span> <span style=color:#111>abs</span><span style=color:#111>(</span><span style=color:#111>prob_b</span> <span style=color:#f92672>-</span> <span style=color:#111>prob_f</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># In practice, the threshold can be tuned via shadow models or validation</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#ae81ff>1</span> <span style=color:#00a8c8>if</span> <span style=color:#111>diff</span> <span style=color:#f92672>&gt;</span> <span style=color:#111>threshold</span> <span style=color:#00a8c8>else</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>diff</span><span style=color:#f92672>.</span><span style=color:#111>item</span><span style=color:#111>()</span>
</span></span></code></pre></div><hr><h2 id=5-reproducible-code-experiments-illustrating-fd-mia>5. Reproducible Code Experiments: Illustrating FD-MIA</h2><p>In this section, we reproduce the FD-MIA attack described in the original paper:</p><p><strong>Data Generation and Splitting</strong><br>The process starts with generating a synthetic dataset where a binary sensitive attribute influences the feature distribution. The target labels are computed using a logistic function, and the dataset is then split into training (member) and testing (non-member) sets to simulate membership inference scenarios.</p><p><strong>Classifier Architectures</strong><br>Two identical neural network architectures are defined:</p><ul><li>A <strong>biased baseline model</strong> trained without fairness constraints.</li><li>A <strong>fairness-enhanced model</strong>, which incorporates a fairness penalty to balance prediction distributions across sensitive groups.</li></ul><p><strong>Training with Fairness Regularization</strong><br>The function <code>train_model</code> allows the inclusion of a fairness penalty during training. For the fair model, this penalty—weighted by <code>fairness_weight</code>—is added to the standard cross-entropy loss to encourage prediction consistency across sensitive groups.</p><p><strong>FD-MIA Attack Implementation</strong><br>The attack, implemented in <code>fd_mia_attack</code>, exploits the absolute difference in predicted probabilities (for the positive class) between the biased and fair models. If this difference exceeds a given threshold, the sample is inferred as a training member. This approach leverages the core principle of FD-MIA: fairness interventions create prediction discrepancies that can be used for membership inference.</p><p><strong>Evaluation and Visualization</strong><br>The attack is evaluated by comparing prediction differences between member and non-member data. We visualize the prediction difference distributions to highlight how fairness-driven adjustments can unintentionally expose membership information.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.optim</span> <span style=color:#00a8c8>as</span> <span style=color:#111>optim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torch.utils.data</span> <span style=color:#f92672>import</span> <span style=color:#111>Dataset</span><span style=color:#111>,</span> <span style=color:#111>DataLoader</span><span style=color:#111>,</span> <span style=color:#111>random_split</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>numpy</span> <span style=color:#00a8c8>as</span> <span style=color:#111>np</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>matplotlib.pyplot</span> <span style=color:#00a8c8>as</span> <span style=color:#111>plt</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.metrics</span> <span style=color:#f92672>import</span> <span style=color:#111>roc_auc_score</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set random seeds for reproducibility</span>
</span></span><span style=display:flex><span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>manual_seed</span><span style=color:#111>(</span><span style=color:#ae81ff>42</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>random</span><span style=color:#f92672>.</span><span style=color:#111>seed</span><span style=color:#111>(</span><span style=color:#ae81ff>42</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1. Create a synthetic dataset with a sensitive attribute</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>SyntheticFairDataset</span><span style=color:#111>(</span><span style=color:#111>Dataset</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>n_samples</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Features: two-dimensional points drawn from different distributions</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>n_samples</span> <span style=color:#f92672>=</span> <span style=color:#111>n_samples</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>X</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>y</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>sensitive</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>  <span style=color:#75715e># sensitive attribute: 0 or 1</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>i</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>n_samples</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Randomly assign a sensitive group (imbalance can be introduced here)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>s</span> <span style=color:#f92672>=</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>random</span><span style=color:#f92672>.</span><span style=color:#111>choice</span><span style=color:#111>([</span><span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>],</span> <span style=color:#111>p</span><span style=color:#f92672>=</span><span style=color:#111>[</span><span style=color:#ae81ff>0.7</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.3</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Generate features from group-dependent distributions</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>if</span> <span style=color:#111>s</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>                <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>random</span><span style=color:#f92672>.</span><span style=color:#111>normal</span><span style=color:#111>(</span><span style=color:#111>loc</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>scale</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>else</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>                <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>random</span><span style=color:#f92672>.</span><span style=color:#111>normal</span><span style=color:#111>(</span><span style=color:#111>loc</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1.5</span><span style=color:#111>,</span> <span style=color:#111>scale</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span><span style=color:#111>,</span> <span style=color:#111>size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Label is determined by a linear rule (with some noise)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>y_prob</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> <span style=color:#111>(</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>exp</span><span style=color:#111>(</span><span style=color:#f92672>-</span> <span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]</span> <span style=color:#f92672>+</span> <span style=color:#111>x</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>]</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.5</span><span style=color:#111>)))</span>
</span></span><span style=display:flex><span>            <span style=color:#111>y_label</span> <span style=color:#f92672>=</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>random</span><span style=color:#f92672>.</span><span style=color:#111>binomial</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>y_prob</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>X</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>y</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>y_label</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>sensitive</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>s</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>X</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>X</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>float32</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>y</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>y</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>long</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>sensitive</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>sensitive</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>long</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__len__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>n_samples</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__getitem__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>idx</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>X</span><span style=color:#111>[</span><span style=color:#111>idx</span><span style=color:#111>],</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>y</span><span style=color:#111>[</span><span style=color:#111>idx</span><span style=color:#111>],</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>sensitive</span><span style=color:#111>[</span><span style=color:#111>idx</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>dataset</span> <span style=color:#f92672>=</span> <span style=color:#111>SyntheticFairDataset</span><span style=color:#111>(</span><span style=color:#111>n_samples</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2000</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Split into training (for model training) and attack evaluation (simulate member vs. non-member)</span>
</span></span><span style=display:flex><span><span style=color:#111>train_size</span> <span style=color:#f92672>=</span> <span style=color:#111>int</span><span style=color:#111>(</span><span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span><span style=color:#111>test_size</span> <span style=color:#f92672>=</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>)</span> <span style=color:#f92672>-</span> <span style=color:#111>train_size</span>
</span></span><span style=display:flex><span><span style=color:#111>train_dataset</span><span style=color:#111>,</span> <span style=color:#111>test_dataset</span> <span style=color:#f92672>=</span> <span style=color:#111>random_split</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>,</span> <span style=color:#111>[</span><span style=color:#111>train_size</span><span style=color:#111>,</span> <span style=color:#111>test_size</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>train_loader</span> <span style=color:#f92672>=</span> <span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>train_dataset</span><span style=color:#111>,</span> <span style=color:#111>batch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>64</span><span style=color:#111>,</span> <span style=color:#111>shuffle</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>test_loader</span>  <span style=color:#f92672>=</span> <span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>test_dataset</span><span style=color:#111>,</span> <span style=color:#111>batch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>64</span><span style=color:#111>,</span> <span style=color:#111>shuffle</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. Define the classifier architectures</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>SimpleClassifier</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>input_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#111>output_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>(</span><span style=color:#111>SimpleClassifier</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>input_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>relu</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ReLU</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>output_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>out</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc2</span><span style=color:#111>(</span><span style=color:#111>out</span><span style=color:#111>)</span>  <span style=color:#75715e># raw logits</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Baseline (biased) model</span>
</span></span><span style=display:flex><span><span style=color:#111>biased_model</span> <span style=color:#f92672>=</span> <span style=color:#111>SimpleClassifier</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Fairness-enhanced model: we add a fairness penalty to the loss (for demonstration)</span>
</span></span><span style=display:flex><span><span style=color:#111>fair_model</span> <span style=color:#f92672>=</span> <span style=color:#111>SimpleClassifier</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 3. Train both models</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>criterion</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>CrossEntropyLoss</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Train biased model without fairness penalty</span>
</span></span><span style=display:flex><span><span style=color:#111>optimizer_biased</span> <span style=color:#f92672>=</span> <span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>Adam</span><span style=color:#111>(</span><span style=color:#111>biased_model</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>epoch</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>train_model</span><span style=color:#111>(</span><span style=color:#111>biased_model</span><span style=color:#111>,</span> <span style=color:#111>train_loader</span><span style=color:#111>,</span> <span style=color:#111>optimizer_biased</span><span style=color:#111>,</span> <span style=color:#111>criterion</span><span style=color:#111>,</span> <span style=color:#111>fairness_weight</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Uncomment to print loss: print(f&#34;Biased Model Epoch {epoch+1}: Loss {loss:.4f}&#34;)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Train fair model with a fairness penalty (fairness_weight &gt; 0)</span>
</span></span><span style=display:flex><span><span style=color:#111>optimizer_fair</span> <span style=color:#f92672>=</span> <span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>Adam</span><span style=color:#111>(</span><span style=color:#111>fair_model</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>fairness_weight</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>  <span style=color:#75715e># adjust to control trade-off</span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>epoch</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>train_model</span><span style=color:#111>(</span><span style=color:#111>fair_model</span><span style=color:#111>,</span> <span style=color:#111>train_loader</span><span style=color:#111>,</span> <span style=color:#111>optimizer_fair</span><span style=color:#111>,</span> <span style=color:#111>criterion</span><span style=color:#111>,</span> <span style=color:#111>fairness_weight</span><span style=color:#f92672>=</span><span style=color:#111>fairness_weight</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Uncomment to print loss: print(f&#34;Fair Model Epoch {epoch+1}: Loss {loss:.4f}&#34;)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 4. Membership Inference Attack using FD-MIA</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -------------------------------</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Evaluate the attack on both training (members) and test (non-members) samples</span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>evaluate_attack</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>,</span> <span style=color:#111>biased_model</span><span style=color:#111>,</span> <span style=color:#111>fair_model</span><span style=color:#111>,</span> <span style=color:#111>threshold</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>attack_labels</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>
</span></span><span style=display:flex><span>    <span style=color:#111>attack_scores</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Assuming samples in train_dataset are members and test_dataset non-members</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>sample</span><span style=color:#111>,</span> <span style=color:#111>_</span><span style=color:#111>,</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>,</span> <span style=color:#111>batch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>shuffle</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>pred</span><span style=color:#111>,</span> <span style=color:#111>diff</span> <span style=color:#f92672>=</span> <span style=color:#111>fd_mia_attack</span><span style=color:#111>(</span><span style=color:#111>sample</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>],</span> <span style=color:#111>biased_model</span><span style=color:#111>,</span> <span style=color:#111>fair_model</span><span style=color:#111>,</span> <span style=color:#111>threshold</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attack_labels</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>pred</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attack_scores</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>diff</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>array</span><span style=color:#111>(</span><span style=color:#111>attack_labels</span><span style=color:#111>),</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>array</span><span style=color:#111>(</span><span style=color:#111>attack_scores</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># For demonstration, we use the entire training set as member data and test set as non-member data.</span>
</span></span><span style=display:flex><span><span style=color:#111>member_labels</span><span style=color:#111>,</span> <span style=color:#111>member_diffs</span> <span style=color:#f92672>=</span> <span style=color:#111>evaluate_attack</span><span style=color:#111>(</span><span style=color:#111>train_dataset</span><span style=color:#111>,</span> <span style=color:#111>biased_model</span><span style=color:#111>,</span> <span style=color:#111>fair_model</span><span style=color:#111>,</span> <span style=color:#111>threshold</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>nonmember_labels</span><span style=color:#111>,</span> <span style=color:#111>nonmember_diffs</span> <span style=color:#f92672>=</span> <span style=color:#111>evaluate_attack</span><span style=color:#111>(</span><span style=color:#111>test_dataset</span><span style=color:#111>,</span> <span style=color:#111>biased_model</span><span style=color:#111>,</span> <span style=color:#111>fair_model</span><span style=color:#111>,</span> <span style=color:#111>threshold</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span><span style=color:#111>)</span>
</span></span></code></pre></div><hr><h3 id=6-experimental-findings-how-fairness-opens-the-door-to-attackers><strong>6. Experimental Findings: How Fairness Opens the Door to Attackers</strong></h3><p>The study conducted extensive experiments across <strong>six datasets, three attack methods, and five fairness approaches</strong>, testing over <strong>160 models</strong>.
To do this, they performed a comprehensive set of experiments involving:</p><ul><li><p>Multiple datasets with different sensitive attributes (like gender or race),</p></li><li><p>Biased vs. fair model variants,</p></li><li><p>Three types of MIAs (including their novel FD-MIA),</p></li><li><p>Multiple fairness interventions</p></li></ul><table><thead><tr><th>Dataset</th><th>Number of Fairness Settings</th><th>Target Tasks</th><th>FD-MIA Effectiveness</th></tr></thead><tbody><tr><td>CelebA</td><td>3 settings (smile, hair, makeup)</td><td>3 binary targets × 2 sensitive attributes</td><td>FD-MIA consistently outperformed other methods.</td></tr><tr><td>UTKFace</td><td>2 settings (race prediction, gender prediction)</td><td>Race or gender, sensitive to the other</td><td>FD-MIA revealed privacy leaks even with balanced groups.</td></tr><tr><td>FairFace</td><td>2 settings (race prediction, gender prediction)</td><td>Same as UTKFace</td><td>Most vulnerable dataset—biggest fairness shift = biggest privacy leak.</td></tr></tbody></table><p>The results were shocking:</p><ul><li><strong>Fair models were significantly harder to attack using traditional MIAs.</strong></li><li><strong>FD-MIA, however, dramatically increased attack success rates—fairness actually made models more vulnerable!</strong></li><li>The greater the fairness intervention, the wider the discrepancy between biased and fair models, making FD-MIA even more effective.</li></ul><p>Our synthetic experiment further supports these findings. The histogram below illustrates the absolute difference between the predictions of the biased and fair models for both <em>members</em> (training data) and <em>non-members</em> (test data).</p><div style=text-align:center><img src=../../images/image_fairness_privacy/figure1.png alt="FD-MIA: Prediction Difference Distribution"><p style=font-style:italic;font-size:14px>Figure 2: FD-MIA: Prediction Difference Distribution.</p></div><p>As expected, members exhibit a significantly higher prediction discrepancy compared to non-members. This clear separation highlights how fairness constraints alter model confidence differently for training and test samples—providing an exploitable signal for membership inference. <strong>In simple terms: making a model fairer may paradoxically make it leak more private information.</strong> A cruel irony for those trying to do the right thing.</p><hr><h2 id=7-the-future-of-fairness-and-privacy-can-we-have-both>7. The Future of Fairness and Privacy: Can We Have Both?</h2><p>The million-dollar question: <strong>Can we balance fairness and privacy without sacrificing one for the other?</strong></p><blockquote><p><em>If you know the enemy and know yourself, you need not fear the result of a hundred battles.</em> — Sun Tzu.</p></blockquote><p>A better understanding of the link between fairness and privacy, as well as the potential and new attacks introduced by unbiased models, is already a solid step toward defending against threats. By understanding the underlying mechanisms, it becomes possible to counteract them.</p><p>The researchers propose two key defenses:</p><ol><li><p><strong>Restricting Information Access</strong></p><ul><li>Limiting confidence score outputs reduces the information available to attackers.</li></ul></li><li><p><strong>Differential Privacy (DP)</strong></p><ul><li><p>By injecting noise into model training, DP-SGD (Differentially Private Stochastic Gradient Descent) helps obscure membership information:</p><p>$$\tilde{g}_t = g_t + \mathcal{N}(0, \sigma^2 I)$$</p><p>where $g_t$ is the original gradient and $\mathcal{N}(0, \sigma^2 I)$ is Gaussian noise added to prevent membership inference.</p></li></ul></li></ol><p>While these methods help, they come with trade-offs: <strong>too much privacy protection can lower fairness and accuracy, while too little leaves models vulnerable.</strong> The challenge ahead is designing AI systems that can balance both.</p><p>💡 Alternative approach: Fairness-Aware Differential Privacy (FADP) adapts noise levels based on protected groups, balancing privacy and fairness.</p><hr><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>H. Tian, G. Zhang, B. Liu, T. Zhu, M. Ding, and W. Zhou, &ldquo;When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks&rdquo;, <em>arXiv e-prints</em>, Art. no. <a href=https://arxiv.org/pdf/2311.03865>arXiv.2311.03865</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><div class=post-date><span class="g time">March 10, 2025 </span>&#8729;</div></section><div id=comments><script src=https://utteranc.es/client.js repo=ZgotmplZ issue-term=pathname theme=ZgotmplZ crossorigin=anonymous async></script></div></div></main></body></html>