<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://responsible-ai-datascience-ipParis.github.io/"><title>SelfIE: When Large Language Models Explain Their Own Thoughts | Bloggin on Responsible AI</title><meta name=description content="Bloggin on Responsible AI"><meta property="og:url" content="https://responsible-ai-datascience-ipParis.github.io/posts/selfie_with_me/"><meta property="og:site_name" content="Bloggin on Responsible AI"><meta property="og:title" content="SelfIE: When Large Language Models Explain Their Own Thoughts"><meta property="og:description" content="Authors: Adrian Durando, Clément Ferrere, Romain Torris
The original paper was authored by Haozhe Chen (Columbia), Carl Vondrick (Columbia), Chengzhi Mao (Mila/McGill)
Original Paper: arXiv:2403.10949
-
Imagine asking a chef how they cooked a dish, only to receive the reply: “It’s a secret.” That’s the frustration researchers face with today’s large language models (LLMs). Despite their impressive abilities (writing code, diagnosing illnesses, or debating ethics), their decision-making remains opaque. This isn’t just an academic concern. When an LLM recommends a medical treatment or evaluates a legal contract, we need to know: How did it arrive at that answer?"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-05T14:08:30+01:00"><meta property="article:modified_time" content="2025-02-05T14:08:30+01:00"><meta itemprop=name content="SelfIE: When Large Language Models Explain Their Own Thoughts"><meta itemprop=description content="Authors: Adrian Durando, Clément Ferrere, Romain Torris
The original paper was authored by Haozhe Chen (Columbia), Carl Vondrick (Columbia), Chengzhi Mao (Mila/McGill)
Original Paper: arXiv:2403.10949
-
Imagine asking a chef how they cooked a dish, only to receive the reply: “It’s a secret.” That’s the frustration researchers face with today’s large language models (LLMs). Despite their impressive abilities (writing code, diagnosing illnesses, or debating ethics), their decision-making remains opaque. This isn’t just an academic concern. When an LLM recommends a medical treatment or evaluates a legal contract, we need to know: How did it arrive at that answer?"><meta itemprop=datePublished content="2025-02-05T14:08:30+01:00"><meta itemprop=dateModified content="2025-02-05T14:08:30+01:00"><meta itemprop=wordCount content="1239"><link rel=canonical href=https://responsible-ai-datascience-ipParis.github.io/posts/selfie_with_me/><link rel=icon href=https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//atom.xml><link rel=alternate type=application/json title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"SelfIE: When Large Language Models Explain Their Own Thoughts","headline":"SelfIE: When Large Language Models Explain Their Own Thoughts","alternativeHeadline":"","description":"\u003cp\u003e\u003cstrong\u003eAuthors\u003c\/strong\u003e: Adrian Durando, Clément Ferrere, Romain Torris\u003c\/p\u003e\n\u003cp\u003e\u003cem\u003eThe original paper was authored by Haozhe Chen (Columbia), Carl Vondrick (Columbia), Chengzhi Mao (Mila\/McGill)\u003c\/em\u003e\u003c\/p\u003e\n\u003cp\u003e\u003cstrong\u003eOriginal Paper\u003c\/strong\u003e: \u003ca href=\u0022https:\/\/arxiv.org\/pdf\/2403.10949\u0022\u003earXiv:2403.10949\u003c\/a\u003e\u003c\/p\u003e\n\u003cp\u003e-\u003c\/p\u003e\n\u003cp\u003eImagine asking a chef how they cooked a dish, only to receive the reply: “It’s a secret.” That’s the frustration researchers face with today’s large language models (LLMs). Despite their impressive abilities (writing code, diagnosing illnesses, or debating ethics), their decision-making remains opaque. This isn’t just an academic concern. When an LLM recommends a medical treatment or evaluates a legal contract, we need to know: \u003cem\u003eHow did it arrive at that answer?\u003c\/em\u003e\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/selfie_with_me\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Bloggin on Responsible AI","copyrightYear":"2025","dateCreated":"2025-02-05T14:08:30.00Z","datePublished":"2025-02-05T14:08:30.00Z","dateModified":"2025-02-05T14:08:30.00Z","publisher":{"@type":"Organization","name":"Bloggin on Responsible AI","url":"https://responsible-ai-datascience-ipParis.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://responsible-ai-datascience-ipParis.github.io/assets/favicon.ico","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/selfie_with_me\/","wordCount":"1239","genre":[],"keywords":[]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=../../><b>Bloggin on Responsible AI</b>
<span class="text-stone-500 animate-blink">▮</span></a></p><ul style=padding:0;margin:0><li><a href=../../posts/><span>Post</span></a><li><a href=../../tutorial/><span>Tutorial</span></a><li><a href=../../about/><span>About</span></a><li><a href=../../articles/><span>Articles</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>SelfIE: When Large Language Models Explain Their Own Thoughts</h2><p><strong>Authors</strong>: Adrian Durando, Clément Ferrere, Romain Torris</p><p><em>The original paper was authored by Haozhe Chen (Columbia), Carl Vondrick (Columbia), Chengzhi Mao (Mila/McGill)</em></p><p><strong>Original Paper</strong>: <a href=https://arxiv.org/pdf/2403.10949>arXiv:2403.10949</a></p><p>-</p><p>Imagine asking a chef how they cooked a dish, only to receive the reply: “It’s a secret.” That’s the frustration researchers face with today’s large language models (LLMs). Despite their impressive abilities (writing code, diagnosing illnesses, or debating ethics), their decision-making remains opaque. This isn’t just an academic concern. When an LLM recommends a medical treatment or evaluates a legal contract, we need to know: <em>How did it arrive at that answer?</em></p><p>A team from Columbia University and Mila might have cracked this open. Their framework, <strong>SelfIE</strong>, forces LLMs to do something unprecedented: <em>interpret their own internal embeddings in plain English</em>. No training required, no pre-defined concepts, just the model translating its hidden states into human-readable explanations.</p><p>-</p><h3 id=table-of-contents>Table of contents</h3><ol><li><a href=#why-this-matters>Why This Matters</a></li><li><a href=#the-mechanics-of-selfie>The Mechanics of SelfIE</a><ul><li><a href=#inserting-embeddings-in-interpretation-forward-pass>Inserting Embeddings in Interpretation Forward Pass</a></li><li><a href=#relevancy-score>Relevancy Score</a></li></ul></li><li><a href=#supervised-control-editing-the-models-thoughts>Supervised Control: Editing the Model&rsquo;s Thoughts</a></li><li><a href=#reinforcement-control-guiding-the-models-behavior>Reinforcement Control: Guiding the Model&rsquo;s Behavior</a></li><li><a href=#results>Results</a></li><li><a href=#limitations>Limitations</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#key-references>Key References</a></li></ol><p>-</p><h3 id=1-why-this-matters>1. Why This Matters</h3><p>Understanding the internal reasoning of LLMs is crucial for building trustworthy AI systems. SelfIE isn’t just a diagnostic tool; it reveals unsettling truths about how these models operate:</p><ul><li><strong>Harmful Knowledge</strong>: LLMs internally store dangerous knowledge (like building weapons) even after safety training.</li><li><strong>Prompt Injections</strong>: Simple prompt injections (like adding “!!!”) can hijack reasoning by exploiting embedded notions of “urgency.”</li><li><strong>Ethical Decisions</strong>: Ethical choices can flip based on whether the model is asked to explain itself.</li></ul><p><img src=../../images/ARC/fig1e.PNG alt loading=lazy decoding=async class=full-width></p><p><em>Figure 1 : Flipped choices</em></p><p>For the first time, we can peer into the model’s <em>chain of thought</em> as it unfolds, layer by layer. Let&rsquo;s dive into how SelfIE achieves this.</p><h3 id=2-the-mechanics-of-selfie>2. The Mechanics of SelfIE</h3><p>To understand how SelfIE works, we need to explore how it leverages the LLM&rsquo;s existing capabilities. SelfIE builds on the observation that LLMs can summarize or repeat information provided in context. By inserting hidden embeddings into a forward pass, SelfIE prompts the model to explain what those embeddings represent.</p><p><img src=../../images/ARC/fig2e.jpg alt loading=lazy decoding=async class=full-width>
<em>Figure 2 : Interpretation forward pass</em></p><h4 id=inserting-embeddings-in-interpretation-forward-pass>Inserting Embeddings in Interpretation Forward Pass</h4><p>Consider a transformer-based LLM. It processes input text through multiple layers, each transforming the input embeddings. SelfIE interprets these embeddings by extracting the embedding of interest and injecting it into a separate forward pass of the same LLM. This is called the interpretation forward pass, which uses an interpretation prompt to summarize the embedding.</p><p><img src=../../images/ARC/maths5.jpg alt loading=lazy decoding=async class=full-width></p><h3 id=3-supervised-control-editing-the-models-thoughts>3. Supervised Control: Editing the Model&rsquo;s Thoughts</h3><p>One of the most exciting applications of SelfIE is supervised control. This technique allows researchers to edit the model&rsquo;s internal representations to align with desired outcomes. For example, if an LLM contains harmful knowledge, supervised control can rewrite the relevant embeddings to remove or neutralize that information.</p><p>The process involves minimizing the Mean Squared Error (MSE) between the current embedding and a target embedding that represents the desired outcome. This is done through gradient descent, a common optimization technique in machine learning.</p><p>The loss function for supervised control is:</p><p><img src=../../images/ARC/maths2.PNG alt loading=lazy decoding=async class=full-width></p><h3 id=4-reinforcement-control-guiding-the-models-behavior>4. Reinforcement Control: Guiding the Model&rsquo;s Behavior</h3><p>Reinforcement control extends the principles of reinforcement learning to the level of embeddings. Instead of training the model on external rewards, reinforcement control uses internal embeddings to guide behavior.</p><p><img src=../../images/ARC/maths4.PNG alt loading=lazy decoding=async class=full-width></p><p>By using reinforcement control, we can steer the model&rsquo;s behavior without needing explicit supervision, making it a powerful tool for aligning AI with human values.</p><h3 id=5-results>5. Results</h3><p>The results obtained with SelfIE are both promising and insightful, demonstrating the framework&rsquo;s effectiveness in interpreting and controlling the internal reasoning of LLMs. Here are the key findings:</p><h4 id=eliciting-implicit-world-states>Eliciting Implicit World States</h4><p>SelfIE successfully recovered 60%-80% of the information about entity states in the TextWorld dataset, matching the performance of a 100-shot supervised classification model. This indicates that SelfIE can faithfully interpret the information represented in embeddings without extensive training.</p><p><img src=../../images/ARC/fig6.PNG alt loading=lazy decoding=async class=full-width></p><p><em>Figure 3: Classification accuracy on TextWorld dataset compared with Linear Probing and random guess.</em></p><p>The TextWorld Dataset provides a platform for generating synthetic worlds for text-based games, used to test reinforcement learning agents. The red line represents the zero-shot SelfIE method, while gray lines show k-shot supervised models (k ranging from 1 to 100). The dashed line indicates supervised learning results on the entire dataset. For these results, for each sample, we pass through the context of the original direct pass and extract the embedding of the last mention of the entity on different layers, which we interpret with SelfIE.</p><h4 id=detecting-harmful-knowledge>Detecting Harmful Knowledge</h4><p>SelfIE revealed that LLMs can internally store harmful knowledge, even after safety alignment. By applying reinforcement control, the framework significantly reduced the presence of harmful knowledge, decreasing the success rate of prompt injections eliciting harmful responses by 84.66%.</p><p><img src=../../images/ARC/fig7.PNG alt loading=lazy decoding=async class=full-width></p><p><em>Figure 4: Detecting harmful knowledge in LLM. (a) LLM contains harmful knowledge despite safety alignment. (b) Harmful knowledge is mostly removed after Reinforcement Control.</em></p><p>These results underscore the potential of SelfIE to enhance the transparency and controllability of LLMs, paving the way for more trustworthy AI systems.</p><h3 id=6-limitations>6. Limitations</h3><p>While SelfIE represents a significant advancement in interpreting and controlling LLMs, it is not without limitations:</p><ul><li><strong>Complexity</strong>: The method requires a deep understanding of the model&rsquo;s architecture and embeddings, which may be challenging for non-experts. Implementing SelfIE effectively necessitates familiarity with the intricacies of transformer models and how embeddings are manipulated within them.</li><li><strong>Scalability</strong>: Applying SelfIE to very large models or real-time applications may face computational constraints. The process of inserting and interpreting embeddings across multiple layers can be resource-intensive, potentially limiting its use in scenarios where rapid processing is essential.</li><li><strong>Generalization</strong>: Although SelfIE can interpret a wide range of concepts, it may struggle with extremely abstract or nuanced ideas that are difficult to encode in embeddings. Certain complex or highly context-dependent thoughts might not be fully captured by the embeddings, limiting the framework&rsquo;s ability to provide comprehensive interpretations in such cases.</li></ul><h3 id=7-conclusion>7. Conclusion</h3><p>SelfIE represents a groundbreaking advancement in the field of large language models, offering insights into their internal reasoning processes. By enabling LLMs to interpret their own embeddings, SelfIE provides a window into the decision-making mechanisms of these complex systems, allowing researchers and developers to understand how these models arrive at specific conclusions. This capability not only enhances the transparency of AI systems but also paves the way for more accountable and trustworthy applications, particularly in critical domains such as healthcare, law, and education.</p><p>By identifying and mitigating harmful knowledge and ensuring that ethical considerations are embedded in the reasoning process, SelfIE addresses some of the most pressing concerns surrounding AI deployment. As AI continues to integrate more deeply into society, tools like SelfIE will be instrumental in building systems that are not only powerful but also responsible and aligned with our collective values.</p><h3 id=8-key-references>8. Key References</h3><ol><li><p><strong>Locating and Editing Factual Associations in GPT</strong><br>Meng et al. (2022)<br><em>The foundational work on model editing that SelfIE builds upon, introducing causal tracing for knowledge localization.</em></p></li><li><p><strong>Implicit Representations of Meaning in Neural Language Models</strong><br>Li et al. (2021)<br><em>Demonstrates how LLMs encode world states in embeddings, directly informing SelfIE&rsquo;s TextWorld experiments.</em></p></li><li><p><strong>A Mathematical Framework for Transformer Circuits</strong><br>Elhage et al. (2021)<br><em>Provides the theoretical basis for SelfIE&rsquo;s layer-wise embedding interpretation via transformer residual streams.</em></p></li><li><p><strong>Training Language Models to Follow Instructions with Human Feedback</strong><br>Ouyang et al. (2022)<br><em>The RLHF methodology that SelfIE extends to embedding-level control.</em></p></li><li><p><strong>Patchscopes: A Unifying Framework for Inspecting Hidden Representations</strong><br>Ghandeharioun et al. (2024)<br><em>Concurrent work on LLM self-interpretation, offering complementary approaches to embedding analysis.</em></p></li></ol><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script><div class=post-date><span class="g time">February 5, 2025 </span>&#8729;</div></section><div id=comments><script src=https://utteranc.es/client.js repo=ZgotmplZ issue-term=pathname theme=ZgotmplZ crossorigin=anonymous async></script></div></div></main></body></html>