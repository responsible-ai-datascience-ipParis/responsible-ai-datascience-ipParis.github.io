<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://responsible-ai-datascience-ipParis.github.io/"><title>BitFit: BIas-Term FIne-Tuning | Bloggin on Responsible AI</title>
<meta name=description content="Bloggin on Responsible AI"><meta property="og:url" content="https://responsible-ai-datascience-ipParis.github.io/posts/bitfit/"><meta property="og:site_name" content="Bloggin on Responsible AI"><meta property="og:title" content="BitFit: BIas-Term FIne-Tuning"><meta property="og:description" content="BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers Authors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich 1. Introduction Fine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-19T15:20:48+01:00"><meta property="article:modified_time" content="2025-02-19T15:20:48+01:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Fine-Tuning"><meta property="article:tag" content="BitFit"><meta property="article:tag" content="BERT"><meta itemprop=name content="BitFit: BIas-Term FIne-Tuning"><meta itemprop=description content="BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers Authors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich 1. Introduction Fine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?"><meta itemprop=datePublished content="2025-02-19T15:20:48+01:00"><meta itemprop=dateModified content="2025-02-19T15:20:48+01:00"><meta itemprop=wordCount content="1709"><meta itemprop=keywords content="NLP,Machine Learning,Fine-Tuning,BitFit,BERT"><link rel=canonical href=https://responsible-ai-datascience-ipParis.github.io/posts/bitfit/><link rel=icon href=https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//atom.xml><link rel=alternate type=application/json title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"BitFit: BIas-Term FIne-Tuning","headline":"BitFit: BIas-Term FIne-Tuning","alternativeHeadline":"","description":"\u003cstyle\nTYPE=\u0022text\/css\u0022\u003e\n\ncode.has-jax {font:\ninherit;\nfont-size:\n100%; \nbackground: \ninherit; \nborder: \ninherit;}\n\n\u003c\/style\u003e\n\u003cscript\ntype=\u0022text\/x-mathjax-config\u0022\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [[\u0027$\u0027,\u0027$\u0027], [\u0027\\\\(\u0027,\u0027\\\\)\u0027]],\n\n        skipTags: [\u0027script\u0027, \u0027noscript\u0027, \u0027style\u0027, \u0027textarea\u0027, \u0027pre\u0027] \/\/ removed \u0027code\u0027 entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i \u002b= 1) {\n\n        all[i].SourceElement().parentNode.className \u002b= \u0027 has-jax\u0027;\n\n    }\n\n});\n\n\u003c\/script\u003e\n\u003cscript\ntype=\u0022text\/javascript\u0022\nsrc=\u0022https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.4\/MathJax.js?config=TeX-AMS_HTML-full\u0022\u003e\u003c\/script\u003e\n\u003ch1 style=\u0022font-size: 24px;\u0022\u003eBitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers\u003c\/h1\u003e\n\u003ch3 id=\u0022authors--abdoul-r-zeba-nour-yahya-nourelhouda-klich\u0022\u003eAuthors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich\u003c\/h3\u003e\n\u003ch2 style=\u0022font-size: 20px;\u0022\u003e 1. Introduction \u003c\/h2\u003e\n\u003cp\u003eFine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/bitfit\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Bloggin on Responsible AI","copyrightYear":"2025","dateCreated":"2025-02-19T15:20:48.00Z","datePublished":"2025-02-19T15:20:48.00Z","dateModified":"2025-02-19T15:20:48.00Z","publisher":{"@type":"Organization","name":"Bloggin on Responsible AI","url":"https://responsible-ai-datascience-ipParis.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://responsible-ai-datascience-ipParis.github.io/assets/favicon.ico","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/bitfit\/","wordCount":"1709","genre":["NLP","Machine Learning","Fine-tuning","BitFit","BERT"],"keywords":["NLP","Machine Learning","Fine-tuning","BitFit","BERT"]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=../../><b>Bloggin on Responsible AI</b>
<span class="text-stone-500 animate-blink">â–®</span></a></p><ul style=padding:0;margin:0><li><a href=../../posts/><span>Post</span></a><li><a href=../../tutorial/><span>Tutorial</span></a><li><a href=../../about/><span>About</span></a><li><a href=../../articles/><span>Articles</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>BitFit: BIas-Term FIne-Tuning</h2><style type=text/css>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit}</style><script type=text/x-mathjax-config>

MathJax.Hub.Config({

    tex2jax: {

        inlineMath: [['$','$'], ['\\(','\\)']],

        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry

    }

});

MathJax.Hub.Queue(function() {

    var all = MathJax.Hub.getAllJax(), i;

    for(i = 0; i < all.length; i += 1) {

        all[i].SourceElement().parentNode.className += ' has-jax';

    }

});

</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script><h1 style=font-size:24px>BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers</h1><h3 id=authors--abdoul-r-zeba-nour-yahya-nourelhouda-klich>Authors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich</h3><h2 style=font-size:20px>1. Introduction</h2><p>Fine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?</p><p>Through this blog post, we will discuss <i><strong>BitFit</strong></i> â€” a novel parameter-efficient fine-tuning technique proposed in the paper BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers (<a href=#benzaken>Ben Zaken et al., 2022</a>).</p><h2 style=font-size:20px>2. Why Fine-tuning Needs Optimization</h2><p>Fine-tuning NLP models typically involves updating all model parameters, but this poses some major challenges:</p><ol><li>Computational Cost: Training a full BERT-large model requires high-end GPUs and significant memory.</li><li>Deployment Complexity: Every task-specific fine-tuned model requires a separate copy of the large model.</li><li>Transfer Learning Issues: Modifying too many parameters can lead to overfitting on small datasets.</li></ol><p>Wouldn&rsquo;t it be great if we could fine-tune just a small subset of the parameters and get similar results? This is exactly where <i><strong>BitFit</strong></i> comes in.</p><h2 style=font-size:20px>3. How BitFit Works</h2><p>Traditional fine-tuning updates <strong>all</strong> the parameters in a Transformer model, which is computationally expensive. BitFit, on the other hand, <strong>only updates the bias terms</strong> in the model while keeping all other weights <strong>frozen</strong>.</p><blockquote><p><strong>Bias terms</strong> are small but essential parameters in neural networks. They adjust activations before applying transformations, helping models adapt to new tasks with minimal updates.</p></blockquote><h3 style=font-size:18px>3.1 Why Focus on Bias Terms ?</h3><p>Bias terms <strong><i>b</i></strong> play a crucial role in neural networks because:</p><ul><li>They allow neurons to fire (activate) even when inputs are zero.</li><li>Adjusting bias values can shift outputs without requiring full retraining.</li><li>Updating biases is a lightweight operation, meaning less memory and faster adaptation.</li></ul><p>A key finding in the BitFit paper is that bias terms contribute uniquely to fine-tuning. When researchers randomly selected the <strong>same number</strong> of non-bias parameters for fine-tuning, the model performed significantly worse than BitFit â†’ This suggests that bias parameters are not just a small subset but play an important role in model adaptation.</p><h3 style=font-size:18px>3.2 What Layers Does BitFit Modify ?</h3><p>BitFit updates the bias terms in <strong>key layers</strong> of Transformer models like BERT:</p><ul><li><strong>Self-Attention Layers</strong></li></ul><p>Transformers use self-attention to focus on important words in a sentence. Each attention head contains three transformations: <strong>Query</strong> (<code>Q</code>), <strong>Key</strong> (<code>K</code>), <strong>Value</strong> (<code>V</code>). And each transformation has its own bias term (<code>bQ</code>, <code>bK</code>, <code>bV</code>).</p><p>â†’ BitFit <strong>updates only these biases</strong> (<code>bQ</code>, <code>bK</code>, <code>bV</code>), while the main attention weights (<code>WQ</code>, <code>WK</code>, <code>WV</code>) remain <strong>unchanged</strong>.</p><ul><li><strong>Feedforward Layers (MLPs)</strong></li></ul><p>Transformers contain fully connected layers that transform intermediate representations. These layers consist of two main <strong>weight matrices</strong> (<code>W1</code>, <code>W2</code>) and <strong>bias terms</strong> (<code>b1</code>, <code>b2</code>).</p><p>â†’ BitFit <strong>updates only the bias terms</strong> (<code>b1</code>, <code>b2</code>), keeping the weight matrices (<code>W1</code>, <code>W2</code>) <strong>frozen</strong>.</p><ul><li><strong>Layer Normalization (LN) Layers</strong></li></ul><p>Transformers use Layer Normalization to stabilize training and prevent exploding gradients. Each LN layer has two learnable parameters: the <strong>scale factor</strong> <code>$\gamma$</code> to control the output scaling and the <strong>bias</strong> <code>$\beta$</code> to adjust the mean shift.</p><p>â†’ BitFit <strong>modifies only the bias term</strong> <code>$\beta$</code>, while keeping the scale factor <code>$\gamma$</code> <strong>fixed</strong>.</p><h3 style=font-size:18px>3.3 Mathematical Explanation</h3><h4 style=font-size:16px>3.3.1 Standard Fine-Tuning</h4><p>In traditional fine-tuning, we update <strong>both</strong> the weights ($W$) and bias terms ($b$):</p><p>$$
W_{\text{fine-tuned}} = W_{\text{pretrained}} + \Delta W
$$</p><p>$$
b_{\text{fine-tuned}} = b_{\text{pretrained}} + \Delta b
$$</p><p>where:</p><ul><li>$W_{\text{pretrained}}$ is the original weight matrix from the pre-trained model.</li><li>$\Delta W$ is the learned update during fine-tuning.</li><li>$b_{\text{pretrained}}$ is the original bias vector.</li><li>$\Delta b$ is the learned bias update.</li></ul><h4 style=font-size:16px>3.3.2 BitFit: BIas-Term FIne-Tuning</h4><p>Instead of updating all weights, BitFit <strong>freezes</strong> $W$ and <strong>only updates</strong> $b$:</p><p>$$
W_{\text{fine-tuned}} = W_{\text{pretrained}}
$$</p><p>$$
b_{\text{fine-tuned}} = b_{\text{pretrained}} + \Delta b
$$</p><p>Here, $\Delta b$ represents the learned adjustments needed for the new task.</p><h3 style=font-size:18px>3.4 Are all Bias Terms Equal ?</h3><p>Not all bias parameters contribute equally to fine-tuning. Researchers found that two types of bias terms are especially important:</p><ul><li><strong>Query Biases</strong> <code>bQ</code>: Found in self-attention layers, responsible for selecting relevant words.</li><li><strong>Middle-Layer MLP Biases</strong> <code>b2</code>: Found in feedforward layers, responsible for transforming representations.</li></ul><p>By only fine-tuning these two subsets, performance remained almost identical to full BitFit while updating half the parameters.</p><blockquote><p>BitFit typically fine-tunes <strong>0.08%</strong> of model parameters, but using only <code>bQ</code> and <code>b2</code>, this number drops to <strong>0.04%</strong> with no major accuracy loss!</p></blockquote><p>This means fine-tuning can be made even more efficient by selecting only the most impactful bias terms.</p><h2 style=font-size:20px>4. How Well Does BitFit Perform?</h2><p>Compared to other parameter-efficient fine-tuning techniques such as Diff-Pruning and Adapters, BitFit achieves competitive performance with significantly fewer trainable parameters.</p><p>BitFit outperforms Diff-Pruning on 4 of the 9 tasks of the GLUE benchmark using the BERTLARGE model and with 6 times fewer trainable parameters. On the test set, BitFit decisively beats Diff-Pruning over two tasks and Adapters over four tasks with 45 times fewer trainable parameters.</p><p>The performance trends of BitFit remain consistent across different base models, e.g., BERTBASE and RoBERTaBASE. The performance of BitFit is not simply due to its adaptation of a collection of parameters, but rather the specific choice of bias parameters. Random selection of an identical number of parameters yields significantly poorer performance, which means that bias parameters have a unique critical contribution to fine-tuning.
Moreover, further analysis reveals that not all bias parameters are equally important as some of them contribute more to the model&rsquo;s performance than others.</p><p>BitFit also demonstrates a smaller generalization gap compared to full fine-tuning, suggesting better generalization capabilities. In token-level tasks such as POS-tagging, BitFit achieves comparable results to full fine-tuning.</p><p>Finally, BitFit&rsquo;s performance also appears to rely on training set size. In experiment with the Stanford Question Answering Dataset, BitFit outperforms full fine-tuning in small-data regimes, but the trend reverses as the training set size increases. What that means is that BitFit is particularly useful when it comes to targeted fine-tuning under small-to-mid-sized data conditions.</p><h2 style=font-size:20px>5. Why Does BitFit Work?</h2><p>BitFit&rsquo;s success can be attributed to several key factors that challenge traditional assumptions about fine-tuning large language models. Rather than retraining all parameters, BitFit selectively updates only the bias terms, leading to efficient adaptation without sacrificing performance. But why is this approach effective?</p><h3 style=font-size:18px>5.1 Fine-Tuning as Knowledge Exposure, Not Learning</h3><p>A crucial insight is that fine-tuning large pre-trained transformers is often less about &ldquo;learning new knowledge&rdquo; and more about &ldquo;exposing&rdquo; the knowledge already embedded in the model. Since transformer-based models like BERT have already learned a vast range of linguistic patterns during their unsupervised pre-training phase, adjusting a small number of parametersâ€”specifically the bias termsâ€”can be enough to bring out task-specific information without reworking the entire model.</p><h3 style=font-size:18px>5.2 Bias Terms and Their Unique Role</h3><p>Bias terms in neural networks serve as offset values, allowing neurons to activate even when input features are zero. Unlike weights, which define relationships between features, bias terms shift outputs in a task-specific manner.</p><p>BitFit leverages the fact that bias terms interact across layers in a way that can subtly adjust how information flows through the model without needing to modify the main weight matrices. This enables significant changes in task-specific performance with minimal modifications to the model structure.</p><h3 style=font-size:18px>5.3 A Targeted and Structured Approach</h3><p>Not all bias parameters contribute equally to model adaptation. The study found that:</p><ul><li><p><strong>Query Biases (bQ)</strong>: Found in self-attention layers, crucial for determining which words receive attention.</p></li><li><p><strong>Middle-Layer MLP Biases (b2)</strong>: Found in feedforward layers, responsible for transforming hidden representations.</p></li></ul><p>By focusing updates on these specific biases, BitFit achieves near full fine-tuning performance while modifying only 0.04% of model parameters.</p><h3 style=font-size:18px>5.4 The Generalization Advantage</h3><p>Another reason BitFit works well is its effect on generalization. Traditional fine-tuning tends to overfit on small datasets because it updates many parameters, potentially memorizing noise rather than learning transferable patterns. BitFit, by contrast, updates only a fraction of the parameters, leading to a smaller generalization gap.</p><p>In fact, experiments show that BitFit performs better than full fine-tuning in small-data regimes. This suggests that limiting parameter updates can sometimes lead to better robustness and generalization.</p><h2 style=font-size:20px>6. Implications and Future Directions</h2><p>BitFit opens new avenues for efficient fine-tuning, making it particularly relevant in scenarios where computational resources are limited, such as:</p><h3 style=font-size:18px>6.1 Efficient Deployment in Real-World Applications</h3><ul><li><p><strong>Low-Resource AI Systems</strong>: BitFitâ€™s lightweight approach is ideal for deploying NLP models in mobile applications, IoT devices, and embedded AI systems where computational efficiency is critical.</p></li><li><p><strong>Multi-Task Learning</strong>: Since only bias terms need updating, multiple tasks can share the same base model, reducing memory overhead and increasing flexibility in production systems.</p></li><li><p><strong>Scalable NLP Services</strong>: Cloud-based NLP services that handle multiple tasks (e.g., chatbots, automated translations) can benefit from BitFit by reducing the need to store and load separate models for each task.</p></li></ul><h3 style=font-size:18px>6.2 Understanding Model Adaptation</h3><p>The success of BitFit raises deeper questions about the nature of transfer learning and fine-tuning:</p><ul><li><p>Do large transformers truly need full fine-tuning, or is most of their knowledge already latent?</p></li><li><p>Could bias-only tuning be the key to unlocking efficient continual learning strategies?</p></li><li><p>Are there other small but critical subsets of parameters that can be updated to achieve similar efficiency gains?</p></li></ul><h3 style=font-size:18px>6.3 Potential Enhancements</h3><p>While BitFit is a promising step forward, future research could explore:</p><ul><li><p><strong>Selective bias tuning</strong>: Further analyzing which specific bias terms contribute most to adaptation and whether additional optimization can reduce the number of updates even further.</p></li><li><p><strong>Hybrid approaches</strong>: Combining BitFit with methods like adapters or LoRA (Low-Rank Adaptation) to achieve even better efficiency.</p></li><li><p><strong>Application to other architectures</strong>: Investigating whether BitFitâ€™s principles extend beyond BERT to models like GPT, T5, and multimodal transformers.</p></li></ul><h2 style=font-size:20px>7. Conclusion</h2><p>In conclusion, BitFit offers a desirable compromise between effectiveness and efficiency, making it a valuable tool for fine-tuning transformer-based models, especially in resource-constrained environments or with limited amounts of training data. Having the capability to achieve competitive performance using significantly fewer trainable parameters, coupled with its achievement in low data regimes, bodes well for other NLP tasks and applications.</p><p>BitFit defies the usual wisdom concerning universal fine-tuning by illustrating how slight tweaks in only a very small percentage of model parameters yield high-performance. This efficient approach makes AI models more accessible, scalable, and cost-effective.</p><h3 id=references>References</h3><ul><li><a id=#benzaken></a><a href=https://aclanthology.org/2022.acl-short.1/>BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</a> (Ben Zaken et al., ACL 2022)</li></ul><div class=post-date><span class="g time">February 19, 2025 </span>&#8729;
<a href=https://responsible-ai-datascience-ipParis.github.io/tags/nlp/>nlp</a> <a href=https://responsible-ai-datascience-ipParis.github.io/tags/machine-learning/>machine-learning</a> <a href=https://responsible-ai-datascience-ipParis.github.io/tags/fine-tuning/>fine-tuning</a> <a href=https://responsible-ai-datascience-ipParis.github.io/tags/bitfit/>bitfit</a> <a href=https://responsible-ai-datascience-ipParis.github.io/tags/bert/>bert</a></div></section><div id=comments><script src=https://utteranc.es/client.js repo=ZgotmplZ issue-term=pathname theme=ZgotmplZ crossorigin=anonymous async></script></div></div></main></body></html>