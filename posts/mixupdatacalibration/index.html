<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://responsible-ai-datascience-ipParis.github.io/"><title>Get a calibrated and efficient model with tailored data augmentation. | Bloggin on Responsible AI</title>
<meta name=description content="Bloggin on Responsible AI"><meta property="og:url" content="https://responsible-ai-datascience-ipParis.github.io/posts/mixupdatacalibration/"><meta property="og:site_name" content="Bloggin on Responsible AI"><meta property="og:title" content="Get a calibrated and efficient model with tailored data augmentation."><meta property="og:description" content="Authors : Tristan Waddington, Fabien Lagnieu & Dimitri Henrard-Iratchet Comment on the research paper: Tailoring Mixup to Data for Calibration, written by Quentin Bouniot, Pavlo Mozharovskyi & Florence d’Alché-Buc, from LTCI, Télécom Paris, Institut Polytechnique de Paris, France Table of contents Existing Data Augmentation Methods Understanding Calibration Best of both worlds: Tailoring Mixup to Data for Calibration Introduction “But it works well on the training set!” is the machine learning equivalent to the classic “But it works on my computer!”"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-09T21:03:13+01:00"><meta property="article:modified_time" content="2025-03-09T21:03:13+01:00"><meta itemprop=name content="Get a calibrated and efficient model with tailored data augmentation."><meta itemprop=description content="Authors : Tristan Waddington, Fabien Lagnieu & Dimitri Henrard-Iratchet Comment on the research paper: Tailoring Mixup to Data for Calibration, written by Quentin Bouniot, Pavlo Mozharovskyi & Florence d’Alché-Buc, from LTCI, Télécom Paris, Institut Polytechnique de Paris, France Table of contents Existing Data Augmentation Methods Understanding Calibration Best of both worlds: Tailoring Mixup to Data for Calibration Introduction “But it works well on the training set!” is the machine learning equivalent to the classic “But it works on my computer!”"><meta itemprop=datePublished content="2025-03-09T21:03:13+01:00"><meta itemprop=dateModified content="2025-03-09T21:03:13+01:00"><meta itemprop=wordCount content="3985"><link rel=canonical href=https://responsible-ai-datascience-ipParis.github.io/posts/mixupdatacalibration/><link rel=icon href=https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//atom.xml><link rel=alternate type=application/json title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Get a calibrated and efficient model with tailored data augmentation.","headline":"Get a calibrated and efficient model with tailored data augmentation.","alternativeHeadline":"","description":"\u003cstyle TYPE=\u0022text\/css\u0022\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c\/style\u003e\n\u003cscript type=\u0022text\/x-mathjax-config\u0022\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [[\u0027$\u0027,\u0027$\u0027], [\u0027\\\\(\u0027,\u0027\\\\)\u0027]],\n        displayMath: [[\u0027$$\u0027,\u0027$$\u0027], [\u0027\\\\[\u0027,\u0027\\\\]\u0027]],\n        skipTags: [\u0027script\u0027, \u0027noscript\u0027, \u0027style\u0027, \u0027textarea\u0027, \u0027pre\u0027] \/\/ removed \u0027code\u0027 entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i \u002b= 1) {\n        all[i].SourceElement().parentNode.className \u002b= \u0027 has-jax\u0027;\n    }\n});\n\u003c\/script\u003e\n\u003cscript type=\u0022text\/javascript\u0022 src=\u0022https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.4\/MathJax.js?config=TeX-AMS_HTML-full\u0022\u003e\u003c\/script\u003e\n\u003ch2 id=\u0022authors--tristan-waddington-fabien-lagnieu--dimitri-henrard-iratchet\u0022\u003eAuthors : \u003cem\u003eTristan Waddington, Fabien Lagnieu \u0026amp; Dimitri Henrard-Iratchet\u003c\/em\u003e\u003c\/h2\u003e\n\u003ch2 id=\u0022comment-on-the-research-paper-tailoring-mixup-to-data-for-calibration-written-by-quentin-bouniot-pavlo-mozharovskyi--florence-dalché-buc-from-ltci-télécom-paris-institut-polytechnique-de-paris-france\u0022\u003eComment on the research paper: \u003ca href=\u0022https:\/\/arxiv.org\/abs\/2311.01434\u0022\u003e\u003cstrong\u003eTailoring Mixup to Data for Calibration\u003c\/strong\u003e\u003c\/a\u003e, written by \u003cem\u003eQuentin Bouniot, Pavlo Mozharovskyi \u0026amp; Florence d’Alché-Buc\u003c\/em\u003e, from LTCI, Télécom Paris, Institut Polytechnique de Paris, France\u003c\/h2\u003e\n\u003ch1 id=\u0022table-of-contents\u0022\u003eTable of contents\u003c\/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\u0022#1-existing-data-augmentation-methods\u0022\u003eExisting Data Augmentation Methods\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#2-understanding-calibration\u0022\u003eUnderstanding Calibration\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#3-best-of-both-worlds-tailoring-mixup-to-data-for-calibration\u0022\u003eBest of both worlds: Tailoring Mixup to Data for Calibration\u003c\/a\u003e\u003c\/li\u003e\n\u003c\/ol\u003e\n\u003ch1 id=\u0022introduction\u0022\u003eIntroduction\u003c\/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;But it works well on the training set!\u0026rdquo; is the machine learning equivalent to the classic \u0026ldquo;But it works on my computer!\u0026rdquo;\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/mixupdatacalibration\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Bloggin on Responsible AI","copyrightYear":"2025","dateCreated":"2025-03-09T21:03:13.00Z","datePublished":"2025-03-09T21:03:13.00Z","dateModified":"2025-03-09T21:03:13.00Z","publisher":{"@type":"Organization","name":"Bloggin on Responsible AI","url":"https://responsible-ai-datascience-ipParis.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://responsible-ai-datascience-ipParis.github.io/assets/favicon.ico","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/mixupdatacalibration\/","wordCount":"3985","genre":[],"keywords":[]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=../../><b>Bloggin on Responsible AI</b>
<span class="text-stone-500 animate-blink">▮</span></a></p><ul style=padding:0;margin:0><li><a href=../../posts/><span>Post</span></a><li><a href=../../tutorial/><span>Tutorial</span></a><li><a href=../../about/><span>About</span></a><li><a href=../../articles/><span>Articles</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>Get a calibrated and efficient model with tailored data augmentation.</h2><style type=text/css>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit}</style><script type=text/x-mathjax-config>
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script><h2 id=authors--tristan-waddington-fabien-lagnieu--dimitri-henrard-iratchet>Authors : <em>Tristan Waddington, Fabien Lagnieu & Dimitri Henrard-Iratchet</em></h2><h2 id=comment-on-the-research-paper-tailoring-mixup-to-data-for-calibration-written-by-quentin-bouniot-pavlo-mozharovskyi--florence-dalché-buc-from-ltci-télécom-paris-institut-polytechnique-de-paris-france>Comment on the research paper: <a href=https://arxiv.org/abs/2311.01434><strong>Tailoring Mixup to Data for Calibration</strong></a>, written by <em>Quentin Bouniot, Pavlo Mozharovskyi & Florence d’Alché-Buc</em>, from LTCI, Télécom Paris, Institut Polytechnique de Paris, France</h2><h1 id=table-of-contents>Table of contents</h1><ol><li><a href=#1-existing-data-augmentation-methods>Existing Data Augmentation Methods</a></li><li><a href=#2-understanding-calibration>Understanding Calibration</a></li><li><a href=#3-best-of-both-worlds-tailoring-mixup-to-data-for-calibration>Best of both worlds: Tailoring Mixup to Data for Calibration</a></li></ol><h1 id=introduction>Introduction</h1><blockquote><p>&ldquo;But it works well on the training set!&rdquo; is the machine learning equivalent to the classic &ldquo;But it works on my computer!&rdquo;</p></blockquote><p>The basic workflow of machine learning has two steps:</p><ul><li>First, <strong>train</strong> your model to perform a task from an available dataset.</li><li>Second, <strong>generalize</strong> and predict the results from unseen data.</li></ul><p>How can data scientists be sure and <em>confident</em> that their model will infer a correct result on this new data?</p><p align=center><figure><img src=../../images/MixUpDataCalibration/WoMM.jpg alt="Does not know why it (don't) works." width=300></img></figure></p><p>We know that deep learning models need vast amounts of data to be efficient.
So, when there is not enough, researchers simply… create more data:
this is the concept of <strong>data augmentation</strong>.</p><p>However, this technique tends to exarcerbate the models&rsquo; <strong>overconfidence</strong> in their
predictions.
Discrepancies between confidence and prediction accuracy are acceptable in domains such as e-commerce recommendations,
but high stake applications such as medical diagnosis or nuclear safety require an accurate <strong>confidence score</strong>, where robustness and reliability are critical.</p><p><strong>This is the idea behind calibration: the model&rsquo;s confidence in its prediction must truly reflect its own prediction accuracy.</strong>
Well-calibrated models do not just inspire trust; they also contribute to <strong>fairer decision-making</strong>, by ensuring that predictions are accompanied by reliable confidence estimates, which can help reduce biased or unjust outcomes.</p><p align=center><figure><img src=../../images/MixUpDataCalibration/CalibratedClassifier.png alt="Difference between a calibrated classifier (right) and a bad one (left)." width=600></img></figure></p><p><em>Figure: Illustration of the confidence of a calibrated classifier (right) with
accurate prediction probabilities and a more brutal and under-confident one (left).</em></p><p>Merging data augmentation and calibration is challenging. The first is prone to
create <strong>manifold intrusion</strong>, where synthetic data with a given label conflicts
with original data of another class. The rise of the size of the dataset also
increases the computational cost of the training. This contradicts the potential objective of frugality. The second is known to <strong>constrain the accuracy</strong>.</p><p>To handle these challenges, Quentin Bouniot and Pavlo Mozharovskyi have conducted
under the direction of Florence d&rsquo;Alché-Buc an extensive study on one of the
technique of data augmentation, the <strong>linear interpolation of training samples</strong>,
also called <strong>Mixup</strong>. They have found an efficient way to tune this process to
both improve the performance <strong>and</strong> the calibration of models, while being
much more efficient than previous methods.</p><p>Let&rsquo;s dig step by step into it.</p><hr><h1 id=1-existing-data-augmentation-methods>1. Existing Data Augmentation Methods</h1><p>Deep learning methods rely on vast amounts of data, so if you do not have enough, make
it yourself. This is the first conclusion of the
study of a Microsoft Research team, lead by Patrice Simard in 2003 aimed to list the
current best practices of neural networks training:</p><blockquote><p><em>&ldquo;The most important practice is getting a training set as large as possible: we
expand the training set by adding a new form of distorted data.&rdquo;</em> [Simard et al. 2003] <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p></blockquote><p>The good results of subsequent models have proved them right. And numerous
techniques have been developed since. Let&rsquo;s review some of them.</p><h2 id=11-create-new-images-for-classification>1.1. Create new images for classification</h2><p>The most visual example of data augmentation is the way image classifiers are trained.
To make them more robust and efficient, scientists have transformed the input images
to drastically increase the size of the training set (by up to 2048 times).</p><p>The most commonly used transformations
are random cropping and resizing, flipping, and color distortion.
This is now so common that it can be done in a few lines in <code>pytorch</code> (see next code
snippet), and automatic recipes
such as <code>AutoAugment</code><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> are readily available to augment common datasets.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torchvision</span> <span style=color:#f92672>import</span> <span style=color:#111>transforms</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Definition of transformations for an image dataset</span>
</span></span><span style=display:flex><span><span style=color:#111>transformTrain</span> <span style=color:#f92672>=</span> <span style=color:#111>transforms</span><span style=color:#f92672>.</span><span style=color:#111>Compose</span><span style=color:#111>([</span>
</span></span><span style=display:flex><span>         <span style=color:#111>transforms</span><span style=color:#f92672>.</span><span style=color:#111>RandomResizedCrop</span><span style=color:#111>(</span><span style=color:#ae81ff>224</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>         <span style=color:#111>transforms</span><span style=color:#f92672>.</span><span style=color:#111>RandomHorizontalFlip</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span>         <span style=color:#111>transforms</span><span style=color:#f92672>.</span><span style=color:#111>ColorJitter</span><span style=color:#111>(</span><span style=color:#111>brightness</span><span style=color:#f92672>=</span><span style=color:#ae81ff>.5</span><span style=color:#111>,</span> <span style=color:#111>hue</span><span style=color:#f92672>=</span><span style=color:#ae81ff>.3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>         <span style=color:#111>])</span>
</span></span></code></pre></div><p>Additional transformations for images are illustrated
on the next figure. We expect the neural networks to &ldquo;see&rdquo; these 10 new images
as being close in their latent space. With this example, one original labeled image
is processed 10 times in different versions during the training of the model.</p><p align=center><figure><img src=../../images/MixUpDataCalibration/Data-Augmentation.png alt="Different methods of data augmentation operators." width=600></img></figure></p><p><em>Figure: Illustration of different data augmentation operators, taken from the paper of Chen 2020 <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></em></p><h2 id=12-linear-interpolation-or-mixup>1.2. Linear interpolation or Mixup</h2><p>Another idea is to create a virtual sample from a <strong>vicinity</strong> around the true
training data—like we did in high school when we added epsilon to a number to see in witch direction
the function is moving. This principle has been demonstrated to help models
generalize. However, the method of creation is often hand-crafted and only mimic
natural perturbations.</p><p>To scale up this process, [Zhang et al., 2018]<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> imagined the <strong>Mixup</strong> process,
which is a linear interpolation, or mixing, of two or more training datapoints.</p><p align=center><figure><img src=../../images/MixUpDataCalibration/mixup_figures-vanilla.png alt="Example of vanilla Mixup" width=300></img></figure></p><p><em>Figure: Illustration of a linear interpolation of Mixup. New points $\tilde{\mathrm{x}}_1$ and $\tilde{\mathrm{x}}_2$ are respectively drawn from the segment $[\mathrm{x}_1, \mathrm{x}_2]$ and $[\mathrm{x}_1, \mathrm{x}_3]$</em></p><p>The process of data augmentation during training with Mixup consists of three phases:</p><ol><li><strong>selecting tuples</strong> (most often pairs) of points to mix together,</li><li><strong>sampling coefficients</strong> that will govern the interpolation to generate synthetic points,</li><li>applying a specific <strong>interpolation procedure</strong> between the points weighted by the coefficients sampled.</li></ol><p>However, the literature explores the drawbacks of this process:</p><ul><li>Mixing carelessly different points can result in
<strong>incorrect labels and hurt generalization</strong> [Guo et al., 2019]<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>, while
mixing similar points helps in diversity [Dablain et al., 2022].</li><li>Furthermore, several previous work have highlighted a <strong>trade-off between performance and calibration</strong> in Mixup [Wang et al., 2023].</li></ul><p>Before digging further into the Mixup process, it is time to understand what
exactly is the calibration of a model and why it can be worth of a trade-off with performance.</p><hr><h1 id=2-understanding-calibration>2. Understanding Calibration</h1><p>Modern (post 2016) neural networks have a high accuracy but are overconfident
in their predictions, outputting softmax scores of
above 99.9% for the dominant class, hence misleading the user into a false sense of confidence.
This is why we need <strong>calibration</strong>.</p><p align=center><figure><img src=../../images/MixUpDataCalibration/Over_confident_IA.jpg alt="Failed object detection with high confidence" width=400></img></figure></p><p><em>Figure: Meme about the overconfidence of an AI agent (obviously uncalibrated) over a failed prediction.</em></p><blockquote><p><em>Calibration</em> is a metric to quantify uncertainty, measuring the difference between a model’s confidence
in its predictions and the actual probability of those predictions being correct.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></p></blockquote><p>In other words, if a <em>calibrated model</em> predicts the image as a cat with a confidence
of 0.3, this prediction has a 30% chance of being correct.
<strong>The actual aim is not exactly to explain the results, but confidence calibration prevents
some mistakes by associating a prediction with its confidence score.</strong></p><p>Let&rsquo;s explore further the motivations of calibration and the way to measure
it and the potential draw backs.</p><h3 id=21-importance-of-calibration>2.1. Importance of calibration</h3><p>The gap in confidence calibration has been spotted by [Guo et al. (2017)]<sup id=fnref1:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>,
and is linked to the actual use cases of neural networks, where the calibration is not crucial.
In LLMs or online recommender systems, a 90% quality of predictions is enough and occasional mistakes are acceptable. For further use however, like in medical diagnosis prediction or
in defense systems, an overconfident model can lead to tragic consequences.</p><p>What would be the benefits of a well calibrated model ?</p><ul><li>It can <strong>filter out the poor predictions</strong>, and not provide a wrong prediction to the user.</li><li>It can <strong>reinforce the continuous training</strong>, by asking for the actual label of the low confidence prediction.</li><li>It can <strong>detect outliers</strong> and warn the user that something strange is happening.</li><li>It can improve the <strong>robustness of the model</strong> by ensuring that prediction confidence accurately reflects the underlying uncertainty, leading to more reliable decisions in critical situations.</li></ul><p><strong>To sum it up, a well calibrated model is a reliable coworker aware of its own capacities.</strong></p><h3 id=22-calibration-metrics>2.2. Calibration Metrics</h3><p>To compare the calibration of models, specific metrics are required. Unlike simple accuracy on a dataset, various metrics have been proposed in the literature, each focusing on different characteristics.</p><h4 id=221-the-brier-score>2.2.1. The Brier Score</h4><p>The Brier score [Brier, 1050] is the mean square error between predicted confidence and target.
Here the target has the form of a one-hot encoded vector.</p><p align=center><figure><img src=../../images/MixUpDataCalibration/BrierScore_Wolfe.png alt="Brier Score illustration" width=600></img></figure></p><p><em>Figure: Computing the Brier Score on classification task (image by Wolfe)<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>.</em></p><p>Intuitively, the <strong>Brier Score measures the accuracy of predicted probabilities</strong>. It can be decomposed into three components — uncertainty (marginal uncertainty over labels), resolution (deviations of individual predictions against the marginal), and reliability (average violation of true label frequencies)</p><blockquote><p><strong>Brier Score = uncertainty - resolution + reliability</strong></p></blockquote><p>The Brier Score is insensitive to the low frequencies events, hence it can be used in combination
with one of the other following metrics to provide useful insights.
Basically, the score is low when the predictions reflect the confidence, i.e. when the model is calibrated.</p><p><em>The following code is a dummy example of Brier score computation of a single classification probabilities over 3 classes. The same probabilities will be used on different metrics.</em></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.metrics</span> <span style=color:#f92672>import</span> <span style=color:#111>brier_score_loss</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example of prediction outputs</span>
</span></span><span style=display:flex><span><span style=color:#111>prob_u</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>0.34</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.33</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.33</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>prob_l</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>0.5</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.25</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.25</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>prob_h</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>0.9</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.07</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.03</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>target</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Compute brier score</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>f</span><span style=color:#d88200>&#34;Brier score for uniform:</span><span style=color:#8045ff>\t</span><span style=color:#d88200> </span><span style=color:#d88200>{</span><span style=color:#111>brier_score_loss</span><span style=color:#111>(</span><span style=color:#111>target</span><span style=color:#111>,</span> <span style=color:#111>prob_u</span><span style=color:#111>)</span><span style=color:#d88200>:</span><span style=color:#d88200>.4f</span><span style=color:#d88200>}</span><span style=color:#d88200>&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>f</span><span style=color:#d88200>&#34;Brier score for low confidence:</span><span style=color:#8045ff>\t</span><span style=color:#d88200> </span><span style=color:#d88200>{</span><span style=color:#111>brier_score</span><span style=color:#111>(</span><span style=color:#111>target</span><span style=color:#111>,</span> <span style=color:#111>prob_l</span><span style=color:#111>)</span><span style=color:#d88200>:</span><span style=color:#d88200>.4f</span><span style=color:#d88200>}</span><span style=color:#d88200>&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>f</span><span style=color:#d88200>&#34;Brier score for high confidence: </span><span style=color:#d88200>{</span><span style=color:#111>brier_score</span><span style=color:#111>(</span><span style=color:#111>target</span><span style=color:#111>,</span> <span style=color:#111>prob_h</span><span style=color:#111>)</span><span style=color:#d88200>:</span><span style=color:#d88200>.4f</span><span style=color:#d88200>}</span><span style=color:#d88200>&#34;</span><span style=color:#111>)</span>
</span></span></code></pre></div><p><em>Output</em></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Brier score <span style=color:#00a8c8>for</span> uniform:	      	0.2178
</span></span><span style=display:flex><span>Brier score <span style=color:#00a8c8>for</span> low confidence:		0.1250
</span></span><span style=display:flex><span>Brier score <span style=color:#00a8c8>for</span> high confidence:	0.0053
</span></span></code></pre></div><h4 id=222-the-expected-calibration-error-ece>2.2.2. The Expected Calibration Error (ECE)</h4><p>The Expected Calibration Error [Guo et al, 2017] approximates the difference between <strong>accuracy</strong> and <strong>confidence</strong> by grouping samples into equally spaced <strong>bins</strong> with respect to their confidence scores.
Because it is both simple and interpretable, ECE is a popular metric to evaluate calibration on classification tasks in practice.
ECE computes the difference between average confidence and accuracy within each
bin, then takes a <strong>weighted average of these values based upon the relative
size of each bin.</strong></p><p align=center><figure><img src=../../images/MixUpDataCalibration/ECE_Wolfe.png alt="ECE illustration " width=600></img><figcaption></figcaption></figure></p><p><em>Figure: Computing ECE over a group of prediction, (image by Wolfe)<sup id=fnref1:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>.</em></p><p>ECE measures how well a model’s estimated &ldquo;probabilities&rdquo; match the observed
probabilities by taking a weighted average over the absolute difference between
accuracy and estimated probabilities (confidence). This measure involves splitting
the predictions into $M$ equally spaced bins.</p><p>$$ECE = \sum_{bins}^M \frac{\text{bin size}}{\text{nb samples}} | \text{accuracy per bin} - \text{average bin probability}| $$</p><p>A very good example on how to compute ECE by hand can be found in the article
of <a href=https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/>Maja Pavlovic</a>
on the blog TowardsDataScience<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>.</p><p><strong>Variants</strong>: <em>Adaptative ECE</em> (AECE) is simmilar to ECE, but with each bin having the same number of samples. Other extensions of ECE can
be used to estimate the variance over the bins, the <em>Uncertainty Calibration Error</em>
(UCE) or the <em>Expected Normalize Calibration Error</em> (ENCE). They will not be
detailed further here.</p><h4 id=223-the-negative-log-likelihood-nll>2.2.3. The Negative Log Likelihood (NLL)</h4><p>The Negative Log Likelihood (NLL) is the typical objective function for training neural networks in multi-class classification. It characterizes the disparity
between the predicted and the actual confidence for the true label.
It reaches a perfect score of $0$ when all data is correctly predicted with 100% confidence,
and rises as soon as some are misclassified. Hence lower scores correspond to better calibration.</p><p><em>Dummy example of NLL computation of a single prediction over 3 classes</em></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>log_softmax</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LogSoftmax</span><span style=color:#111>(</span><span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>loss_fn</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>NLLLoss</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># input to NLLLoss is of size (batch_size x nb_classes) = 1 x 3</span>
</span></span><span style=display:flex><span><span style=color:#111>target</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>0</span><span style=color:#111>])</span><span style=color:#f92672>.</span><span style=color:#111>long</span><span style=color:#111>()</span>  <span style=color:#75715e># correct class is at index O</span>
</span></span><span style=display:flex><span><span style=color:#75715e># different examples of logits from a classifier</span>
</span></span><span style=display:flex><span><span style=color:#111>logits_u</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([[</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>]])</span>      <span style=color:#75715e># uniform prediction</span>
</span></span><span style=display:flex><span><span style=color:#111>logits_l</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([[</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.2</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.1</span><span style=color:#111>]])</span>  <span style=color:#75715e># low confidence prediction</span>
</span></span><span style=display:flex><span><span style=color:#111>logits_h</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>Tensor</span><span style=color:#111>([[</span><span style=color:#ae81ff>10</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.1</span><span style=color:#111>]])</span> <span style=color:#75715e># high confidence prediction</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>f</span><span style=color:#d88200>&#34;nll uniform: </span><span style=color:#8045ff>\t\t</span><span style=color:#d88200>{</span><span style=color:#111>loss_fn</span><span style=color:#111>(</span><span style=color:#111>log_softmax</span><span style=color:#111>(</span><span style=color:#111>logits_u</span><span style=color:#111>),</span> <span style=color:#111>target</span><span style=color:#111>)</span><span style=color:#d88200>:</span><span style=color:#d88200>.4f</span><span style=color:#d88200>}</span><span style=color:#d88200>&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>f</span><span style=color:#d88200>&#34;nll low confidence: </span><span style=color:#8045ff>\t</span><span style=color:#d88200>{</span><span style=color:#111>loss_fn</span><span style=color:#111>(</span><span style=color:#111>log_softmax</span><span style=color:#111>(</span><span style=color:#111>logits_l</span><span style=color:#111>),</span> <span style=color:#111>target</span><span style=color:#111>)</span><span style=color:#d88200>:</span><span style=color:#d88200>.4f</span><span style=color:#d88200>}</span><span style=color:#d88200>&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>f</span><span style=color:#d88200>&#34;nll high confidence: </span><span style=color:#8045ff>\t</span><span style=color:#d88200>{</span><span style=color:#111>loss_fn</span><span style=color:#111>(</span><span style=color:#111>log_softmax</span><span style=color:#111>(</span><span style=color:#111>logits_h</span><span style=color:#111>),</span> <span style=color:#111>target</span><span style=color:#111>)</span><span style=color:#d88200>:</span><span style=color:#d88200>.4f</span><span style=color:#d88200>}</span><span style=color:#d88200>&#34;</span><span style=color:#111>)</span>
</span></span></code></pre></div><p><em>Output</em></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>nll uniform: 		  1.0986
</span></span><span style=display:flex><span>nll low confidence:   0.6184
</span></span><span style=display:flex><span>nll high confidence:  0.0001
</span></span></code></pre></div><p><strong>However, NNL also causes overconfidence in modern neural networks.</strong>
They are purposely trained to minimize it by making high confidence predictions, which actually lowers the exponential sum of the soft max, as in our
high_confidence example above.</p><p>This kind of behavior can be exhibited by drawing the calibration curve of the predictor.</p><h4 id=23-calibration-curves---reliability-diagrams>2.3. Calibration Curves - Reliability diagrams</h4><p>The calibration curves [Wilks, 1995] compare how well the probabilistic predictions of a
binary classifier are calibrated. It shows the frequency of the predicted label against the
predicted probability. It is easily drawn with the method <code>model.predict_proba()</code> of scikit-learn.</p><p><a href=https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html#calibration-curves>Scikit-Learn&rsquo;s documentation</a>
provides a very insightful illustration to better understand these curves. They
have fitted 4 different classifiers on a very small training set and plot the
calibration curve along with the histogram showing the related distribution of
the predicted probabilities on each of the 10 bins. On this specific example,
we can observe the following behaviors:</p><ul><li><strong>Logistic Regression</strong>: not perfect, but well calibrated because the
optimized log loss is also the scoring rule (as seen in previous section).</li><li><strong>Gaussian Naive Bayes</strong>: its tendency to push probabilities to 0 or 1 is well
shown on histogram orange. This means an overconfident model.</li><li><strong>Support vector Classifier</strong> displays a typical sigmoid calibration curve.
This under-confident result is typical of maximum-margin methods.</li><li><strong>Random Forest</strong> averages the predictions over a set of models, meaning exact predictions of 0 or 1 are rare, hence the shift towards 0.2 and 0.9. The whole
model seems under-confident, but since each tree is fitted in minimizing a scoring
rule (Brier score of log-loss) the pink calibration curve is pretty close to the dot line.</li></ul><p align=center><figure><img src=../../images/MixUpDataCalibration/sphx_glr_plot_compare_calibration_001.png alt="Calibration plot comparison" width=600></img><figcaption></figcaption></figure></p><p><em>Figure: Behavior of regular classifier on a standard task.
Upper: Calibration curves with <code>sklearn.calibration.CalibrationDisplay.from_estimator</code>.
Bottom: Histograms of the number of samples per bins of <code>predict_proba</code> values.</em></p><p>Remember: a <strong>perfectly calibrated estimator</strong> will get the doted diagonal line
and its histogram will be flat.</p><h3 id=24-drawbacks>2.4 Drawbacks</h3><p>To provide an insight of the side effects of the calibration, we will study the
impact of the provided method <code>CalibratedClassifierCV</code> in Scikit-Learn.
It uses cross-validation to obtain unbiased predictions, which are then used for calibration.
The sigmoid method here is a simple logistic regression model. We experiment
the effect of the calibration on the accuracy and the Brier Score of 4 classifiers
fitted on the titanic dataset. We will only display the code to instantiate and
calibrate the models.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.linear_model</span> <span style=color:#f92672>import</span> <span style=color:#111>LogisticRegression</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.naive_bayes</span> <span style=color:#f92672>import</span> <span style=color:#111>GaussianNB</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.svm</span> <span style=color:#f92672>import</span> <span style=color:#111>SVC</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.ensemble</span> <span style=color:#f92672>import</span> <span style=color:#111>RandomForestClassifier</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.calibration</span> <span style=color:#f92672>import</span> <span style=color:#111>CalibratedClassifierCV</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [...] load titanic dataset, split data, Skub automatically the data</span>
</span></span><span style=display:flex><span><span style=color:#111>X_train</span><span style=color:#111>,</span> <span style=color:#111>X_test</span><span style=color:#111>,</span> <span style=color:#111>y_train</span><span style=color:#111>,</span> <span style=color:#111>y_test</span> <span style=color:#f92672>=</span> <span style=color:#111>train_test_split</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>)</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize classifiers</span>
</span></span><span style=display:flex><span><span style=color:#111>classifiers</span> <span style=color:#f92672>=</span> <span style=color:#111>{</span>
</span></span><span style=display:flex><span>    <span style=color:#d88200>&#34;Logistic Regression&#34;</span><span style=color:#111>:</span> <span style=color:#111>LogisticRegression</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span>    <span style=color:#d88200>&#34;Naive Bayes&#34;</span><span style=color:#111>:</span> <span style=color:#111>GaussianNB</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span>    <span style=color:#d88200>&#34;Support Vector Classifier&#34;</span><span style=color:#111>:</span> <span style=color:#111>SVC</span><span style=color:#111>(</span><span style=color:#111>probability</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>    <span style=color:#d88200>&#34;Random Forest&#34;</span><span style=color:#111>:</span> <span style=color:#111>RandomForestClassifier</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span><span style=color:#111>}</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Calibrate with the simgmoid regression</span>
</span></span><span style=display:flex><span><span style=color:#111>calibrated_classifiers</span> <span style=color:#f92672>=</span> <span style=color:#111>{}</span>
</span></span><span style=display:flex><span><span style=color:#111>calibrated_proba</span> <span style=color:#f92672>=</span> <span style=color:#111>{}</span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>name</span><span style=color:#111>,</span> <span style=color:#111>classifier</span> <span style=color:#f92672>in</span> <span style=color:#111>classifiers</span><span style=color:#f92672>.</span><span style=color:#111>items</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#111>calibrated_classifiers</span><span style=color:#111>[</span><span style=color:#111>name</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>CalibratedClassifierCV</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>        <span style=color:#111>classifier</span><span style=color:#111>,</span> <span style=color:#111>cv</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>method</span><span style=color:#f92672>=</span><span style=color:#d88200>&#34;sigmoid&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>fit</span><span style=color:#111>(</span><span style=color:#111>X_train</span><span style=color:#111>,</span> <span style=color:#111>y_train</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>calibrated_proba</span><span style=color:#111>[</span><span style=color:#111>name</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>classifier</span><span style=color:#f92672>.</span><span style=color:#111>predict_proba</span><span style=color:#111>(</span><span style=color:#111>X_test</span><span style=color:#111>)[:,</span> <span style=color:#ae81ff>1</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [...] Compute metrics and display as bar plots</span>
</span></span></code></pre></div><p align=center><figure><img src=../../images/MixUpDataCalibration/Calibration_side_effect.png alt="Calibration effect on metrics" width=600></img></figure></p><p><em>Figure: effect of calibration on classifiers&rsquo; metrics. We expect a small drop in
accuracy and a reduction (improvement) of the Brier Score. But our method is not
efficient here.</em></p><p>We can see that this method has moderate and sometimes counterintuitive effects.
<strong>This suggests that the training set is not sufficient to
fit a calibrated estimator.</strong></p><hr><h1 id=3-best-of-both-worlds-tailoring-mixup-to-data-for-calibration>3. Best of Both Worlds: Tailoring Mixup to Data for Calibration</h1><p>We have reached the core of the paper of Bouniot et al. They show that, by
<strong>taking the distance of points into account when sampling the coefficients</strong> in the
second phase of Mixup, we can (i) avoid a loss in diversity, and (ii) reduce manifold intrusion and label noise.</p><p align=center><figure><img src=../../images/MixUpDataCalibration/Mixup-ours.png alt="Similarity Mixup" width=300></img></figure></p><p><em>Figure: Illustration of a Blouniot et al.&rsquo;s process of <strong>similarity</strong> in interpolation. New point $\tilde{\mathrm{x}}_2$ have to be closer to $\mathrm{x}_1$ because $\mathrm{x}_3$ has a different label but still preserve diversity.</em></p><p>If we had only used a selection of samples with similar labels, we would have
lost the possible exploration of new directions of the latent space.
<strong>With this similarity process, at the end of the day, we have avoided restricting</strong>
<strong>possible direction of mixing while staying in the vicinity of original points,</strong>
<strong>hence preventing manifold intrusion.</strong></p><h2 id=31-linear-interpolation-of-training-samples-mixup>3.1 Linear interpolation of training samples: Mixup</h2><p>Mixing samples through linear interpolation is the easiest and most efficient way
to create new data from a computational point of view. Combining data from the same
batch also avoids additional sampling during training.</p><p>Specific techniques have been proposed since 2018 to compute linear interpolation
but often at the cost of more complex training or loss of diversity.
The selection process of samples to interpolate from may be computationally
expensive.
Furthermore
such studies have been conducted with the aim of improving models&rsquo; generalization, not their calibration, and will not solve our issue.</p><p>In the original mixup method of [Zhang et al. 2018]<sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, at each training iteration
of the model, each input is mixed with another input randomly selected from the
same batch, with a random strength drawn form a Beta law.</p><p>But how can we be sure of the label of these new datapoints?</p><h2 id=32-weighting-to-prevent-manifold-intrusion>3.2 Weighting to prevent manifold intrusion</h2><p>The real danger of mixup is <strong>manifold intrusion</strong>, where the interpolated
sample between two identical label points falls into an other class.</p><blockquote><p>The likelihood of conflict in the synthetic label
increases with the distance between the two points. As
data live in manifolds of the representation space, the linear combination of
two points far from each other can lie in a different manifold than the linear
combination of the labels. The further away the
points are, the more manifolds can exist in between.</p></blockquote><p align=center><figure><img src=../../images/MixUpDataCalibration/Manifold_mixup.png alt="Risk of manifold intrusion when mixing samples" width=600></img></figure></p><p><em>Figure: Illustration from [Baena, 2022]<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> of a <strong>manifold intrusion</strong> (right) when drawing samples as mixup existing points. The linear interpolation (red line) crosses the blue classe leading to conflict.</em></p><p>Bouniot et al. have conduct extensive experiments to show that there is a
<strong>trade-off between adding diversity</strong> by increasing the proportion
of elements to mix, <strong>and uncertainty</strong> by mixing elements far from each other.
Furthermore, it shows that we cannot restrict pairs to mix by selecting data
solely based on distance, as <strong>it can degrade performance by reducing diversity of synthetic samples</strong>.</p><p>To better control this trade-off with Mixup, they suggest to tailor interpolation
coefficients based on the distance of training data. The final part will detail this process.</p><h2 id=33-the-power-of-the-similarity-kernel>3.3 The power of the similarity kernel</h2><p>Bouniot et al. <strong>used a similarity kernel to mix more strongly
similar data and avoid mixing less similar ones</strong>, to preserve label
quality and confidence of the network.</p><p>To do so, <strong>they needed to change the interpolation coefficient depending on the
similarity between the points</strong>. They have found a way to preserve the type of
distribution of samples by warping these coefficients at every iteration to
govern the strength and direction of the mixup. Curious readers can refer
to section 3.2 of <sup id=fnref1:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> for technical details. In summary, they only
need the parameter $\tau$ of a Beta distribution $B(\tau, \tau)$ that behaves
logarithmically with this parameter. Hence, $\tau$ should be exponentially
correlated with the distance of points to finally obtain a linear interpolation.</p><p><strong>To this end, they define a class of similarity kernels, based on a normalized and
centered Gaussian kernel, that outputs the correct warping parameter $\tau$ for the given pair of points.</strong>
This similarity kernel is defined by the amplitude and the standard derivation
of the Gaussian, two additional parameters to tune separately. The computation
of $\tau$ also depends on the average distance of samples in the same batch.
More specifically, for classification
tasks, they use the $L_2$ distance between embeddings, while for regression
tasks, they use the distance between labels.</p><p>The algorithm to compute this parameter is described bellow in pseudo-code:</p><pre tabindex=0><code class=language-code data-lang=code>Input: (Batch (x_i, y_i) of size n, kernel similarity parameters, current model parameters)
Sample random permutation sigma
For i in [1, n], do
  # Compute the interpolated points from x_i and x_sigma(i):
  Compute warping parameter tau using a Beta coefficient and the similarity kernel
  Generate new point x_tilde as a linear interpolation of x_i and x_sigma(i), weighted by tau
  Generate new label y_tilde as a linear interpolation of y_i and y_sigma(i), weighted by tau
  Aggregate new data to batch
Optimize loss over this augmented batch
Output: the updated model parameters
</code></pre><p align=center><figure><img src=../../images/MixUpDataCalibration/heatmap_density_dist_inverse_warp_v2.png alt="Evolution of the similarity kernel" width=300></img></figure></p><p><em>Figure: Illustration of the effect of the similarity kernel on two points $x_i$ and $x_{\sigma{(i)}}$, additional description bellow. Figure from [Bouniot, 2024]<sup id=fnref2:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</em></p><blockquote><p>The motivation behind this kernel is to have $\tau >1$ when the two points to mix are similar, i.e., the
distance is lower than average, to increase the mixing effect, and $\tau &lt; 1$ otherwise, to reduce the
mixing. Above Figure illustrates the evolution of the density of warped interpolation coefficients $ω_τ(λ)$,
depending on the distance between the points to mix. Close distances (left part of the heatmap)
induce strong interpolations, while far distances (right part of the heatmap) reduce interpolation.
<strong>Using this similarity kernel to find the correct τ to parameterize the Beta distribution defines our full</strong>
<strong>Similarity Kernel Mixup framework.</strong></p></blockquote><h2 id=34-going-further>3.4 Going further</h2><p>Extensive experiments have been conducted by the authors on image classification and regression tasks.
They have reproduced the protocol of the literature and <strong>their framework displays an improvement in
both accuracy and calibration across the 3 metrics described above (ECE, Brier and NLL)</strong>.</p><p>It is important to note, however, that the hyper-parameters have been tuned, and
the best results across different metrics do not share the same values for the kernel standard deviation.</p><p>During the experiment process, the authors have compared the final results after
<em>temperature scaling</em>, following [Guo, 2017]<sup id=fnref2:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> process. This temperature
is also a learnable parameter that have been optimized during
the training of the models.</p><p>In addition to improving calibration and performance, this approach demonstrates <strong>greater frugality</strong> by reducing computational overhead compared to other calibration-driven data augmentation methods. By efficiently tailoring the interpolation process, it lowers the number of unnecessary computations and memory usage, contributing to <strong>more sustainable and energy-efficient</strong> machine learning practices.</p><h1 id=conclusion>Conclusion</h1><p>With similarity kernel, we get a more accurate and better calibrated model because
the coefficients governing the interpolation are warped to change their underlying distribution
depending on the similarity between the points to mix, so that
<strong>similar datapoints are mixed more strongly than less similar ones</strong>,
<strong>preserving calibration by avoiding manifold intrusion and label noise</strong>.</p><p>As seen in the pseudo-code, this provides a more efficient data augmentation
approach than Calibration-driven Mixup methods, both in
terms of time and memory, offering a <strong>better trade-off between performance and calibration improvement</strong>, while promoting frugality by reducing unnecessary computational resources.</p><p>Concurrently, [Verma et al. 2018]<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> have proposed the <strong>Manifold Mixup</strong>
framework that encourages neural networks to predict less confidently on
interpolations of hidden representation via a simple regularizer. This
training method leads to class-representations with fewer directions of variance.
But even with the actual purpose the reduce the over confidence, the word
&ldquo;Calibration&rdquo; never occurs in the paper&mldr;</p><p><strong>This highlights the necessity of raising awareness about calibration and establishing a standard process for evaluating models.</strong></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to
visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis
and Recognition, volume 2, pages 958–962, 2003.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Ekin D. Cubuk, Barret Zoph , Dandelion Mané, Vijay Vasudevan, Quoc V. Le, Google Brain (2018).
AutoAugment: Learning Augmentation Strategies from Data <a href=https://arxiv.org/abs/1805.09501>arXiv</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, (2020).
A Simple Framework for Contrastive Learning of Visual Representations <a href=https://arxiv.org/abs/2002.05709>arXiv</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2018). mixup: Beyond empirical risk
minimization. In International Conference on Learning Representations.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Guo, Chuan, et al. “On calibration of modern neural networks.” International Conference on Machine Learning. PMLR, 2017.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Bouniot, Q., Mozharovskyi P., d&rsquo;Alché-Buc, F. (2023).
Tailoring Mixup to Data for Calibration <a href=https://arxiv.org/abs/2311.01434>arXiv</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Cameron R. Wolfe, Ph.D. Confidence Calibration for Deep Networks: Why and How? <a href=https://medium.com/towards-data-science/confidence-calibration-for-deep-networks-why-and-how-e2cd4fe4a086>medium/TowardsDataScience blogpost</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Maja Pavlovicic, Expected Calibration Error (ECE): A Step-by-Step Visual Explanation <a href=https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/>link</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Raphael Baena, Lucas Drumetz, Vincent Gripon (2022)
Preventing Manifold Intrusion with Locality: Local Mixup <a href=https://arxiv.org/abs/2201.04368>arXiv</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D., and Bengio, Y. (2019).
Manifold mixup: Better representations by interpolating hidden states. In Chaudhuri, K. and
Salakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pages 6438–6447. PMLR.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><div class=post-date><span class="g time">March 9, 2025 </span>&#8729;</div></section><div id=comments><script src=https://utteranc.es/client.js repo=ZgotmplZ issue-term=pathname theme=ZgotmplZ crossorigin=anonymous async></script></div></div></main></body></html>