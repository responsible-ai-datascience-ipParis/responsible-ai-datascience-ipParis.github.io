<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://responsible-ai-datascience-ipParis.github.io/"><title>Knowledge Distillation: Boosting Interpretability in Deep Learning Models | Bloggin on Responsible AI</title><meta name=description content="Bloggin on Responsible AI"><meta property="og:url" content="https://responsible-ai-datascience-ipParis.github.io/posts/impact-knowledge-distillation-model-interpretability/"><meta property="og:site_name" content="Bloggin on Responsible AI"><meta property="og:title" content="Knowledge Distillation:  Boosting Interpretability in Deep Learning Models"><meta property="og:description" content="<!DOCTYPE html> Interpretability, the hidden power of knowledge distillation Published March 15, 2025
Update on GitHub bryanbradfo Bryan Chen"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-15T12:16:21+01:00"><meta property="article:modified_time" content="2025-03-15T12:16:21+01:00"><meta itemprop=name content="Knowledge Distillation:  Boosting Interpretability in Deep Learning Models"><meta itemprop=description content="<!DOCTYPE html> Interpretability, the hidden power of knowledge distillation Published March 15, 2025
Update on GitHub bryanbradfo Bryan Chen"><meta itemprop=datePublished content="2025-03-15T12:16:21+01:00"><meta itemprop=dateModified content="2025-03-15T12:16:21+01:00"><meta itemprop=wordCount content="2547"><link rel=canonical href=https://responsible-ai-datascience-ipParis.github.io/posts/impact-knowledge-distillation-model-interpretability/><link rel=icon href=https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//atom.xml><link rel=alternate type=application/json title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Knowledge Distillation:  Boosting Interpretability in Deep Learning Models","headline":"Knowledge Distillation:  Boosting Interpretability in Deep Learning Models","alternativeHeadline":"","description":"\u003cstyle TYPE=\u0022text\/css\u0022\u003e\n   code.has-jax {font:inherit;\n                  font-size:100%;\n                  background: inherit;\n                  border: inherit;}\n\u003c\/style\u003e\n\u003cscript TYPE=\u0022text\/x-mathjax-config\u0022\u003e\n   MathJax.Hub.Config({\n      tex2jax: {\n         inlineMath: [[\u0027$\u0027,\u0027$\u0027], [\u0027\\$\u0027,\u0027\\$\u0027]],\n         skipTags: [\u0027script\u0027,\u0027noscript\u0027, \u0027style\u0027, \u0027textarea\u0027, \u0027pre\u0027] \/\/ removed \u0027code\u0027 entry\n      }\n   });\n\n   MathJax.Hub.Queue(function() {\n      var all = MathJax.Hub.getAllJax(), i;\n      for(i = 0; i \u003c all.length; i \u002b= 1) {\n         all[i].SourceElement().parentNode.className \u002b= \u0027 has-jax\u0027;\n      }\n   });\n\u003c\/script\u003e\n\u003cscript TYPE=\u0022text\/javascript\u0022 src=\u0022https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.4\/MathJax.js?config=TeX-AMS_HTML-full\u0022\u003e\n\u003c\/script\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\u0022fr\u0022\u003e\n\u003chead\u003e\n   \u003cmeta charset=\u0022UTF-8\u0022\u003e\n   \u003cmeta name=\u0022viewport\u0022 content=\u0022width=device-width, initial-scale=1.0\u0022\u003e\n\u003c\/head\u003e\n\u003ch1 style=\u0022font-size: 28px;\u0022\u003eInterpretability, the hidden power of knowledge distillation\u003c\/h1\u003e\n\u003cp\u003ePublished March 15, 2025\u003c\/p\u003e\n\u003cdiv\u003e\n  \u003ca href=\u0022https:\/\/github.com\/BryanBradfo\/responsible-ai-datascience-ipParis.github.io\u0022 class=\u0022btn\u0022 style=\u0022text-decoration: none; display: inline-block; padding: 8px 16px; background-color: #f1f1f1; border: 1px solid #ddd; border-radius: 4px; color: black;\u0022\u003eUpdate on GitHub\u003c\/a\u003e\n\u003c\/div\u003e\n\u003cdiv style=\u0022display: flex; margin-top: 20px;\u0022\u003e\n   \u003cdiv style=\u0022display: flex; align-items: center; margin-right: 20px;\u0022\u003e\n      \u003cimg src=\u0022\/images\/Bryan_Remi\/bryan.jpeg\u0022 alt=\u0022Bryan Chen\u0022 style=\u0022width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\u0022\u003e\n      \u003cdiv\u003e\n         \u003ca href=\u0022https:\/\/github.com\/BryanBradfo\u0022 style=\u0022text-decoration: none; color: #0366d6;\u0022\u003ebryanbradfo\u003c\/a\u003e\n         \u003cp style=\u0022margin: 0;\u0022\u003eBryan Chen\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/impact-knowledge-distillation-model-interpretability\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Bloggin on Responsible AI","copyrightYear":"2025","dateCreated":"2025-03-15T12:16:21.00Z","datePublished":"2025-03-15T12:16:21.00Z","dateModified":"2025-03-15T12:16:21.00Z","publisher":{"@type":"Organization","name":"Bloggin on Responsible AI","url":"https://responsible-ai-datascience-ipParis.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://responsible-ai-datascience-ipParis.github.io/assets/favicon.ico","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/impact-knowledge-distillation-model-interpretability\/","wordCount":"2547","genre":[],"keywords":[]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=../../><b>Bloggin on Responsible AI</b>
<span class="text-stone-500 animate-blink">▮</span></a></p><ul style=padding:0;margin:0><li><a href=../../posts/><span>Post</span></a><li><a href=../../tutorial/><span>Tutorial</span></a><li><a href=../../about/><span>About</span></a><li><a href=../../articles/><span>Articles</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>Knowledge Distillation: Boosting Interpretability in Deep Learning Models</h2><style type=text/css>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit}</style><script type=text/x-mathjax-config>
   MathJax.Hub.Config({
      tex2jax: {
         inlineMath: [['$','$'], ['\$','\$']],
         skipTags: ['script','noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
   });

   MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
         all[i].SourceElement().parentNode.className += ' has-jax';
      }
   });
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script><!doctype html><html lang=fr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"></head><h1 style=font-size:28px>Interpretability, the hidden power of knowledge distillation</h1><p>Published March 15, 2025</p><div><a href=https://github.com/BryanBradfo/responsible-ai-datascience-ipParis.github.io class=btn style="text-decoration:none;display:inline-block;padding:8px 16px;background-color:#f1f1f1;border:1px solid #ddd;border-radius:4px;color:#000">Update on GitHub</a></div><div style=display:flex;margin-top:20px><div style=display:flex;align-items:center;margin-right:20px><img src=../../images/Bryan_Remi/bryan.jpeg alt="Bryan Chen" style=width:40px;height:40px;border-radius:50%;margin-right:10px><div><a href=https://github.com/BryanBradfo style=text-decoration:none;color:#0366d6>bryanbradfo</a><p style=margin:0>Bryan Chen</p></div></div><div style=display:flex;align-items:center;margin-right:20px><img src=../../images/Bryan_Remi/remi.jpg alt="Rémi Calvet" style=width:40px;height:40px;border-radius:50%;margin-right:10px><div><a href=https://github.com/RemiCSK style=text-decoration:none;color:#0366d6>remicsk</a><p style=margin:0>Rémi Calvet</p></div></div></div><p>Knowledge distillation is a powerful technique to transfer the knowledge from a large &ldquo;teacher&rdquo; model to a &ldquo;student&rdquo; model. While it&rsquo;s commonly used to improve performance and reduce computational costs by compressing large models, this blog post explores a fascinating discovery: knowledge distillation can also enhance model interpretability. We&rsquo;ll dive into the paper <a href=https://arxiv.org/abs/2305.15734>On the Impact of Knowledge Distillation for Model Interpretability"</a> (ICML 2023) by H. Han et al., which sheds light on this novel perspective.</p><p align=center><img src=../../images/Bryan_Remi/better_meme.png alt=Introduction style=width:60%;max-width:500px;height:auto></p><p>Interpretability in AI allows researchers, engineers, and decision-makers to trust and control machine learning models. Recent models show impressive performance on many different tasks and often rely on deep learning models. Unfortunately, deep learning models are also know for the difficulty to interprete them and understand how they come to a result wich can be problematic in highly sensitive applications like autonomous driving or healthcare. The article we present in this blog shows that knowledge distillation can improve the interpretability of deep learning models.</p><p>When AI is a black box, you&rsquo;re just hoping for the best. But when you understand it, you become unstoppable.</p><h2 style=font-size:21px;display:flex;align-items:center>0. Table of Contents</h2><ul><li><a href=#i-crash-course-on-knowledge-distillation>I. Crash Course on Knowledge Distillation and Label Smoothing</a></li><li><a href=#ii-defining-interpretability-through-network-dissection>II. Defining Interpretability Through Network Dissection</a></li><li><a href=#iii-logit-distillation-feature-distillation>III. Logit Distillation & Feature Distillation: A Powerful Duo for Interpretability</a></li><li><a href=#iv-why-knowledge-distillation-enhances-interpretability>IV. Why Knowledge Distillation Enhances Interpretability</a></li><li><a href=#v-experimental-results-and-reproduction>V. Experimental Results and Reproduction</a></li><li><a href=#vi-beyond-network-dissection-other-interpretability-metrics>VI. Beyond Network Dissection: Other Interpretability Metrics</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#join-the-discussion>Join the Discussion</a></li></ul><h2 id=i-crash-course-on-knowledge-distillation style=font-size:21px;display:flex;align-items:center>I. Crash Course on Knowledge Distillation and Label Smoothing</h2><h3 id=what-is-knowledge-distillation>What is Knowledge Distillation?</h3><p align=center><img src=../../images/Bryan_Remi/knowledge_distillation.png alt="Knowledge Distillation Overview" style=width:70%;max-width:500px;height:auto></p><p><a href=https://arxiv.org/pdf/1503.02531>Knowledge distillation (KD)</a> is a model compression technique introduced by Hinton et al. (2015) that transfers knowledge from a complex teacher model to a simpler student model. Unlike traditional training where models learn directly from hard labels (one-hot encodings), KD allows the student to learn from the teacher&rsquo;s soft probability distributions.</p><h3 id=the-key-mechanics-of-knowledge-distillation>The Key Mechanics of Knowledge Distillation</h3><p>The standard KD loss function combines the standard cross-entropy loss with a distillation loss term:</p><p>$$\mathcal{L}_{KD}=(1-\alpha)\mathrm{CE}(y,\sigma(z_s))+\alpha T^2 \mathrm{CE}(\sigma(z_t^T),\sigma(z_s^T))$$</p><p>Where:</p><ul><li>$z_s$ and $z_t$ are the logits from the student and teacher models</li><li>$T$ is the temperature parameter that controls softening of probability distributions</li><li>$z_s^T \mathrel{:}= \frac{z_s}{T}$ and $z_t^T \mathrel{:}= \frac{z_t}{T}$</li><li>$\sigma$ is the softmax function</li><li>$\sigma(z_s^T) \mathrel{:}= \frac{\exp(z_s^T)}{\sum_j \exp(z_j^T)}$ and $\sigma(z_t^T) \mathrel{:}= \frac{\exp(z_t^T)}{\sum_j \exp(z_j^T)}$</li><li>$\mathrm{CE}$ is cross-entropy loss</li><li>$\alpha$ balances the importance of each loss component.</li></ul><p>The first part of the loss $(1-\alpha)\mathrm{CE}(y,\sigma(z_s))$ is to incitate the student model to learn from one hot encoded ground truth label.</p><p>The second part of the loss $\alpha T^2 \mathrm{CE}(\sigma(z_t^T),\sigma(z_s^T))$ is to incitate the student model to try to reproduce the ouputs of the teacher model. This is what permits the student to learn from the teacher.
The larger $\alpha$ is, the more the student will try to replicate the teacher model&rsquo;s outputs and ignore the one hot encoded groundtruth and vice versa.</p><h3 id=label-smoothing>Label Smoothing</h3><p>Label smoothing (LS) is another technique that smooths hard targets by mixing them with a uniform distribution. In the cross entropy loss we replace the one hot encoded $y$ by $y_{LS} \mathrel{:}= (1-\alpha)y + \frac{\alpha}{K}$, where $K$ is the number of classes and $\alpha$ the smoothing parameter:</p><script type="math/tex;mode=display">
\begin{align}
CE(y_{LS},\sigma(z)) &= - \sum_{i=1}^{K} \left( (1 - \alpha) y_i + \frac{\alpha}{K} \right) \log \sigma(z_i) \\
&= -(1 - \alpha) \sum_{i=1}^{K} y_i \log \sigma(z_i) - \alpha \sum_{i=1}^{K} \frac{1}{K}\log \sigma(z_i) \\
\end{align}
</script><p>We obtain a loss that is similar to knowledge diffusion but there is a key difference important for interpretability that we will discuss later.
From the equation above, we get the label smoothing loss equation:
$$L_{LS} = (1-\alpha)\mathrm{CE}(y,\sigma(z)) + \alpha\mathrm{CE}(u,\sigma(z)) $$
Where $u$ is a uniform distribution over all the possible $K$ classes.</p><p align=center><img src=../../images/Bryan_Remi/label_smoothing.jpg alt="Label Smoothing" width=700></p><h2 id=ii-defining-interpretability-through-network-dissection style=font-size:21px;display:flex;align-items:center>II. Defining Interpretability Through Network Dissection</h2><p>The first thing to know is that there are different approaches to define and measure interpretability in machine learning.</p><p>For image classification, the authors use <a href=https://arxiv.org/pdf/1711.05611v2>network dissection</a> to quantitatively measure interpretability. The idea is to compare activation maps and see if areas with high activation correspond to an object or a meaningful concept on the image.</p><p>The process can be better understood through the following illustration:</p><p align=center><img src=../../images/Bryan_Remi/network_dissection.png alt="Network Dissection Process" width=700></p><p>Feed a neural network model an image, pick a deep layer and count the number of neurons that detects a concept like &ldquo;cat&rdquo; or &ldquo;dog&rdquo;.
We call those neurons concept detectors and will define them more precisely. The <strong>number of concept detectors will be the primary metric to define the interpretability of a model</strong>, the higher the more we will consider it interpretable.</p><p><strong>The easiest way to understand what is a concept detector is to look at the following pseudo code to compute the number of concept detectors:</strong></p><style>body{font-family:arial,sans-serif;line-height:1.6}.steps-container{background:#f8f9fa;border-left:5px solid #007bff;padding:15px 20px;margin:20px 0;border-radius:5px}.step{font-weight:700;color:#007bff;margin-top:15px}.math-expression{font-family:courier new,Courier,monospace;background:#e9ecef;padding:5px;border-radius:3px}.important{background:#fff3cd;color:#856404;padding:10px;border-left:4px solid #ffc107;border-radius:3px;margin:10px 0}</style><div class=steps-container><h3 id=1-selecting-the-layer><span class=step>1. Selecting the Layer</span></h3><p>First, we need to choose a layer $\mathcal{l}$ to <strong>dissect</strong>, typically deep in the network.</p><h3 id=2-processing-each-image><span class=step>2. Processing Each Image</span></h3><p>For each image <strong>x</strong> in the dataset:</p><ol><li><p><strong>Feedforward Pass</strong>:</p><ul><li>Input an image <strong>x</strong> of shape $ (n,n) $ into the neural network.</li></ul></li><li><p><strong>Activation Extraction</strong>:</p><ul><li>For each neuron in layer $\mathcal{l}$, collect the activation maps:<div class=math-expression>\[ A_i(x) \in \mathbb{R}^{d \times d}, \quad \text{where } d < n \text{ and } i \text{ is the neuron index.} \]</div></li></ul></li></ol><h3 id=3-defining-activation-distribution><span class=step>3. Defining Activation Distribution</span></h3><p>For each neuron <strong>i</strong> in the layer $\mathcal{l}$:</p><ul><li>Define <strong>a<sub>i</sub></strong> as the empirical distribution of activation values across different images <strong>x</strong>.</li></ul><h3 id=4-computing-activation-threshold><span class=step>4. Computing Activation Threshold</span></h3><ul><li>Compute a threshold <strong>T<sub>i</sub></strong> such that:<div class=math-expression>\[ P(a_i \geq T_i) = 0.005 \]</div>- This ensures only the **top 0.5%** activations are retained.</li></ul><h3 id=5-resizing-activation-maps><span class=step>5. Resizing Activation Maps</span></h3><ul><li>Interpolate <strong>A<sub>i</sub></strong> to match the dimension $ (n,n) $ for direct comparison with input images.</li></ul><h3 id=6-creating-binary-masks><span class=step>6. Creating Binary Masks</span></h3><p>For each image <strong>x</strong>:</p><ol><li><p><strong>Generating Activation Masks</strong>:</p><ul><li>Create a <strong>binary mask</strong> $ A_i^{\text{mask}}(x) $ of shape $ (n,n) $:<div class=math-expression>\[ A_i^{\text{mask}}(x)[j,k] = \begin{cases} 1, & \text{if } A_i(x)[j,k] \geq T_i \\ 0, & \text{otherwise} \end{cases} \]</div></li><li>This retains only the highest activations.</li></ul></li><li><p><strong>Using Ground Truth Masks</strong>:</p><ul><li>Given a <strong>ground truth mask</strong> $ M_c(x) $ of shape $ (n,n) $, where:<ul><li>$ M_c(x)[j,k] = 1 $ if the pixel in <strong>x</strong> belongs to class <strong>c</strong>, otherwise <strong>0</strong>.</li></ul></li></ul></li><li><p><strong>Computing Intersection over Union (IoU)</strong>:</p><ul><li>Calculate the IoU between <strong>A<sub>i</sub><sup>mask</sup>(x)</strong> and <strong>M<sub>c</sub>(x)</strong>:<div class=math-expression>\[ \text{IoU}_{i,c} = \frac{|A_i^{\text{mask}}(x) \cap M_c(x)|}{|A_i^{\text{mask}}(x) \cup M_c(x)|} \]</div></li><li>If $\text{IoU}_{i,c} > 0.05$, the neuron <strong>i</strong> is considered a <strong>concept detector</strong> for concept <strong>c</strong>.</li></ul></li></ol></div><h3 id=if-you-prefer-to-understand-with-code-here-is-an-implementation-of-the-procedure-described-above>If you prefer to understand with code, here is an implementation of the procedure described above:</h3><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>identify_concept_detectors</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>layer_name</span><span style=color:#111>,</span> <span style=color:#111>dataset</span><span style=color:#111>,</span> <span style=color:#111>concept_masks</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#d88200>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d88200>    Identify neurons that act as concept detectors in a specific layer.
</span></span></span><span style=display:flex><span><span style=color:#d88200>
</span></span></span><span style=display:flex><span><span style=color:#d88200>    Args:
</span></span></span><span style=display:flex><span><span style=color:#d88200>        model: Neural network model
</span></span></span><span style=display:flex><span><span style=color:#d88200>        layer_name: Name of the layer to analyze
</span></span></span><span style=display:flex><span><span style=color:#d88200>        dataset: Dataset with images
</span></span></span><span style=display:flex><span><span style=color:#d88200>        concept_masks: Dictionary mapping images to concept segmentation masks
</span></span></span><span style=display:flex><span><span style=color:#d88200>
</span></span></span><span style=display:flex><span><span style=color:#d88200>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#d88200>        Dictionary mapping neurons to detected concepts
</span></span></span><span style=display:flex><span><span style=color:#d88200>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Step 1: Collect activation maps for each image</span>
</span></span><span style=display:flex><span>    <span style=color:#111>activation_maps</span> <span style=color:#f92672>=</span> <span style=color:#111>{}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>image</span> <span style=color:#f92672>in</span> <span style=color:#111>dataset</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Forward pass and extract activation at specified layer</span>
</span></span><span style=display:flex><span>        <span style=color:#111>activations</span> <span style=color:#f92672>=</span> <span style=color:#111>get_layer_activation</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>layer_name</span><span style=color:#111>,</span> <span style=color:#111>image</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>neuron_idx</span><span style=color:#111>,</span> <span style=color:#111>activation</span> <span style=color:#f92672>in</span> <span style=color:#111>enumerate</span><span style=color:#111>(</span><span style=color:#111>activations</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>if</span> <span style=color:#111>neuron_idx</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> <span style=color:#111>activation_maps</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>                <span style=color:#111>activation_maps</span><span style=color:#111>[</span><span style=color:#111>neuron_idx</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>
</span></span><span style=display:flex><span>            <span style=color:#111>activation_maps</span><span style=color:#111>[</span><span style=color:#111>neuron_idx</span><span style=color:#111>]</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>activation</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Step 2: Compute threshold for top 0.5% activations for each neuron</span>
</span></span><span style=display:flex><span>    <span style=color:#111>thresholds</span> <span style=color:#f92672>=</span> <span style=color:#111>{}</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>neuron_idx</span><span style=color:#111>,</span> <span style=color:#111>activations</span> <span style=color:#f92672>in</span> <span style=color:#111>activation_maps</span><span style=color:#f92672>.</span><span style=color:#111>items</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Flatten all activations for this neuron</span>
</span></span><span style=display:flex><span>        <span style=color:#111>all_activations</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>cat</span><span style=color:#111>([</span><span style=color:#111>act</span><span style=color:#f92672>.</span><span style=color:#111>flatten</span><span style=color:#111>()</span> <span style=color:#00a8c8>for</span> <span style=color:#111>act</span> <span style=color:#f92672>in</span> <span style=color:#111>activations</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Compute threshold for top 0.5%</span>
</span></span><span style=display:flex><span>        <span style=color:#111>threshold</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>quantile</span><span style=color:#111>(</span><span style=color:#111>all_activations</span><span style=color:#111>,</span> <span style=color:#ae81ff>0.995</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>thresholds</span><span style=color:#111>[</span><span style=color:#111>neuron_idx</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>threshold</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Step 3: Create binary masks and compute IoU with concept masks</span>
</span></span><span style=display:flex><span>    <span style=color:#111>concept_detectors</span> <span style=color:#f92672>=</span> <span style=color:#111>{}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>image_idx</span><span style=color:#111>,</span> <span style=color:#111>image</span> <span style=color:#f92672>in</span> <span style=color:#111>enumerate</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>image_concepts</span> <span style=color:#f92672>=</span> <span style=color:#111>concept_masks</span><span style=color:#111>[</span><span style=color:#111>image_idx</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>neuron_idx</span><span style=color:#111>,</span> <span style=color:#111>activations</span> <span style=color:#f92672>in</span> <span style=color:#111>activation_maps</span><span style=color:#f92672>.</span><span style=color:#111>items</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Get activation for this neuron on this image</span>
</span></span><span style=display:flex><span>            <span style=color:#111>activation</span> <span style=color:#f92672>=</span> <span style=color:#111>activations</span><span style=color:#111>[</span><span style=color:#111>image_idx</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Create binary mask using threshold</span>
</span></span><span style=display:flex><span>            <span style=color:#111>binary_mask</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#111>activation</span> <span style=color:#f92672>&gt;=</span> <span style=color:#111>thresholds</span><span style=color:#111>[</span><span style=color:#111>neuron_idx</span><span style=color:#111>])</span><span style=color:#f92672>.</span><span style=color:#111>float</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Resize to match image dimensions</span>
</span></span><span style=display:flex><span>            <span style=color:#111>binary_mask</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>interpolate</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>                <span style=color:#111>binary_mask</span><span style=color:#f92672>.</span><span style=color:#111>unsqueeze</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>unsqueeze</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>                <span style=color:#111>size</span><span style=color:#f92672>=</span><span style=color:#111>image</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>:],</span>
</span></span><span style=display:flex><span>                <span style=color:#111>mode</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;bilinear&#39;</span>
</span></span><span style=display:flex><span>            <span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>squeeze</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Compute IoU with each concept mask</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>for</span> <span style=color:#111>concept</span><span style=color:#111>,</span> <span style=color:#111>mask</span> <span style=color:#f92672>in</span> <span style=color:#111>image_concepts</span><span style=color:#f92672>.</span><span style=color:#111>items</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>                <span style=color:#111>intersection</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>(</span><span style=color:#111>binary_mask</span> <span style=color:#f92672>*</span> <span style=color:#111>mask</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>                <span style=color:#111>union</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>(</span><span style=color:#111>binary_mask</span><span style=color:#111>)</span> <span style=color:#f92672>+</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>(</span><span style=color:#111>mask</span><span style=color:#111>)</span> <span style=color:#f92672>-</span> <span style=color:#111>intersection</span>
</span></span><span style=display:flex><span>                <span style=color:#111>iou</span> <span style=color:#f92672>=</span> <span style=color:#111>intersection</span> <span style=color:#f92672>/</span> <span style=color:#111>union</span> <span style=color:#00a8c8>if</span> <span style=color:#111>union</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#00a8c8>else</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># If IoU exceeds threshold (typically 0.05), consider it a detector</span>
</span></span><span style=display:flex><span>                <span style=color:#00a8c8>if</span> <span style=color:#111>iou</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.05</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>                    <span style=color:#00a8c8>if</span> <span style=color:#111>neuron_idx</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> <span style=color:#111>concept_detectors</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>concept_detectors</span><span style=color:#111>[</span><span style=color:#111>neuron_idx</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>set</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>                    <span style=color:#111>concept_detectors</span><span style=color:#111>[</span><span style=color:#111>neuron_idx</span><span style=color:#111>]</span><span style=color:#f92672>.</span><span style=color:#111>add</span><span style=color:#111>(</span><span style=color:#111>concept</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#111>concept_detectors</span>
</span></span></code></pre></div><h2 id=iii-logit-distillation-feature-distillation style=font-size:21px;display:flex;align-items:center>III. Logit Distillation & Feature Distillation: A Powerful Duo for Interpretability</h2><p>Combining logit distillation with feature distillation not only boosts performance but also enhances the interpretability of student models. This improvement is measured by an increase in the number of concept detectors, which represent units aligned with human-interpretable concepts.</p><p align=center><img src=../../images/Bryan_Remi/feature_logit_distillation.png alt=Feature_Logit_Distillation width=700></p><p>where Attention Transfer (AT), Factor Transfer (FT), Contrastive Representation Distillation (CRD), and Self-Supervised Knowledge Distillation (SSKD) are all variations of knowledge distillation techniques, each designed to transfer knowledge from teacher models to student models in unique ways.</p><h3 id=how-they-work-together>How they work together?</h3><ol><li><strong>Logit Distillation:</strong></li></ol><ul><li>Transfers class-similarity information from the teacher to the student through softened logits.</li><li>Helps the student model understand the relationships between semantically similar classes, making activation maps more object-centric.</li></ul><ol start=2><li><strong>Feature Distillation:</strong></li></ol><ul><li>Focuses on aligning intermediate layer features between the teacher and student.</li><li>Improves the student model&rsquo;s ability to replicate the teacher’s feature representations, supporting richer internal representations.</li></ul><h2 id=iv-why-knowledge-distillation-enhances-interpretability style=font-size:21px;display:flex;align-items:center>IV. Why Knowledge Distillation Enhances Interpretability</h2><p>The key insight from the paper is that knowledge distillation transfers not just the ability to classify correctly, but also class-similarity information that makes the model focus on more interpretable features.</p><h3 id=transfer-of-class-similarities>Transfer of Class Similarities</h3><p>When a teacher model sees an image of a dog, it might assign:</p><ul><li>85% probability to &ldquo;Golden Retriever&rdquo;</li><li>10% probability to other dog breeds</li><li>5% probability to other animals and objects</li></ul><p>These &ldquo;soft targets&rdquo; (consequence of logit distillation) encode rich hierarchical information about how classes relate. The student model distilling this knowledge learns to focus on features that are common to similar classes (e.g., general &ldquo;dog&rdquo; features).</p><h3 id=label-smoothing-vs-knowledge-distillation>Label Smoothing vs. Knowledge Distillation</h3><p>By looking at the KD and label smoothing losses, we can see that they are similar. When $T=1$ they only differ in the second member where we have a $\sigma(z_t^T)$ that contains class-similarity information instead of $u$ that doesn&rsquo;t contain any information.</p><ul><li>$\mathcal{L}_{KD}=(1-\alpha)\mathrm{CE}(y,\sigma(z_s))+\alpha T^2 \mathrm{CE}(\sigma(z_t^T),\sigma(z_s^T))$</li><li>$L_{LS} = (1-\alpha)\mathrm{CE}(y,\sigma(z)) + \alpha\mathrm{CE}(u,\sigma(z)) $</li></ul><p>So, if there is a difference in interpretability, it is likely that it comes from the fact that distillation permits to get class similarity knowledge from the teacher model. This is exactly what is shown in the figure below. Knowledge distillation guides student models to focus on more object-centric features rather than background or contextual features. This results in activation maps that better align with the actual objects in images.</p><p align=center><img src=../../images/Bryan_Remi/comparisons_dog.png alt=ObjectCentricActivation width=700></p><p>The next figure also highlights the loss of interpretability (less concept detectors) when using label smoothing and the improvement of interpretability (more concept detectors) for KD:</p><p align=center><img src=../../images/Bryan_Remi/NbConceptDetDiffModels.png alt="KD vs LS Distributions" width=600></p><p>While label smoothing can improve accuracy, it often reduces interpretability by erasing valuable class relationships while KD keeps class relationship information and improves both accuracy and interpretability.</p><h2 id=v-experimental-results-and-reproduction style=font-size:21px;display:flex;align-items:center>V. Experimental Results and Reproduction</h2><p>Let&rsquo;s implement a reproduction of one of the paper&rsquo;s key experiments to see knowledge distillation&rsquo;s effect on interpretability in action.</p><h3 id=setting-up-the-experiment>Setting Up the Experiment</h3><p>We are going to replicate the experiment by using the <a href=https://github.com/Rok07/KD_XAI>GitHub repository provided by the authors</a>. The repository contains the code to train the models, compute the concept detectors, and evaluate the interpretability of the models.</p><p>As it is often the case with a machine learning paper, running the code to reproduce results requires some struggle.
To reproduce the results, you could use a virtual environment (e.g. <a href=https://datalab.sspcloud.fr/>SSP Cloud Datalab</a>) and then do the following:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/Rok07/KD_XAI.git
</span></span><span style=display:flex><span><span style=color:#111>cd</span> torchdistill
</span></span><span style=display:flex><span>pip install -e .
</span></span><span style=display:flex><span><span style=color:#111>cd</span> ..
</span></span><span style=display:flex><span>bash script/dlbroden.sh
</span></span><span style=display:flex><span>nano torchdistill/torchdistill/models/custom/bottleneck/__init__.py
</span></span><span style=display:flex><span>~ comment the first line
</span></span><span style=display:flex><span>pip install opencv-python
</span></span><span style=display:flex><span>pip install imageio
</span></span><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install -y libgl1-mesa-glx
</span></span><span style=display:flex><span>nano util/vecquantile.py
</span></span><span style=display:flex><span>~ change NaN by nan
</span></span><span style=display:flex><span>nano loader/data_loader.py
</span></span><span style=display:flex><span>~ add out<span style=color:#f92672>[</span>i<span style=color:#f92672>]</span> <span style=color:#f92672>=</span> rgb<span style=color:#f92672>[</span>:,:,0<span style=color:#f92672>]</span> + <span style=color:#f92672>(</span>rgb<span style=color:#f92672>[</span>:,:,1<span style=color:#f92672>]</span>.astype<span style=color:#f92672>(</span>np.uint16<span style=color:#f92672>)</span> * 256<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#111>cd</span> ..
</span></span><span style=display:flex><span>nano settings.py
</span></span><span style=display:flex><span>~ change <span style=color:#111>TEST_MODE</span> <span style=color:#f92672>=</span> False to True
</span></span><span style=display:flex><span><span style=color:#111>cd</span> dataset/broden1_224
</span></span><span style=display:flex><span>cp index.csv index_sm.csv
</span></span><span style=display:flex><span>~ keep the <span style=color:#ae81ff>4000</span> first lines
</span></span><span style=display:flex><span><span style=color:#111>cd</span> ../..
</span></span><span style=display:flex><span>nano visualize/bargraph.py
</span></span><span style=display:flex><span>~ change parameter threshold of bar_graph_svg<span style=color:#f92672>()</span> to 0.001
</span></span><span style=display:flex><span>python main.py
</span></span></code></pre></div><p>Network Dissection quantifies the interpretability of hidden units by measuring their alignment with human-interpretable concepts. The following results reveal several interesting findings:</p><h4 id=1-concept-distribution-from-bargraphsvg>1. Concept Distribution (from bargraph.svg):</h4><p align=center><img src=../../images/Bryan_Remi/class_distribution.png alt="Class Distribution" width=600></p><ul><li>~6 units detecting object concepts</li><li>~2 units detecting scene concepts</li><li>1 unit detecting material properties</li><li>~13 units detecting textures</li><li>~6 units detecting colors</li></ul><h4 id=2-specific-units-layer4-0xxxjpg>2. Specific Units: (layer4-0xxx.jpg)</h4><p align=center><img src=../../images/Bryan_Remi/unit_grid.png alt="Unit Grid" width=1600></p><ul><li><strong>Unit 330</strong> has specialized in detecting grid and regular pattern textures</li></ul><p align=center><img src=../../images/Bryan_Remi/unit_sky.png alt="Unit Sky" width=1600></p><ul><li><strong>Unit 202</strong> detects sky regions in images</li></ul><p>The network dissection approach reveals interpretable neurons of a distilled ResNet18.</p><h2 id=vi-beyond-network-dissection-other-interpretability-metrics style=font-size:21px;display:flex;align-items:center>VI. Beyond Network Dissection: Other Interpretability Metrics</h2><p>While the paper emphasizes the use of <strong>Network Dissection</strong> to measure model interpretability by quantifying concept detectors, it also explores several additional metrics to confirm the broader impact of <strong>Knowledge Distillation (KD)</strong> on interpretability:</p><ul><li><strong><a href=https://arxiv.org/pdf/2009.02899>Five-Band Scores</a>, proposed by Tjoah & Guan (2020):</strong> This metric assesses interpretability by evaluating pixel accuracy (accuracy of saliency maps in identifying critical features), precision (how well the saliency maps match the actual distinguishing features), recall, and false positive rates (FPR, lower FPR indicates better interpretability) using a synthesized dataset with heatmap ground truths. KD-trained models consistently show higher accuracy and lower FPR compared to other methods.</li><li><strong><a href=https://arxiv.org/pdf/2102.12781>DiffROAR Scores</a>, proposed by Shah et al. (2021):</strong> This evaluates the difference in predictive power on a model trained on a dataset and a model trained on a version of the dataset where we removed top and bottom x% of the pixel according to their importance for the task. The authors find that KD has a higher DiffROAR score than a model trained from scratch. It means that KD makes the model use more relevant features and thus more interpretable in that sense.<li><strong>Loss Gradient Alignment:</strong> This metric measures the alignment of model gradients with human-perceived important features. KD models exhibit better alignment, indicating greater interpretability as we can see on this figure:<p align=center><img src=../../images/Bryan_Remi/gradient_interpre.png alt=ObjectCentricActivation width=700></p></li></ul><p><p>These metrics collectively show that KD can enhance interpretability. The consistent results showing that knowledge distillation can enhance interpretability for different metrics of interpretability provide strong arguments to believe that KD could be broadly used for better interpretability of deep learning models.</p><h2 id=conclusion style=font-size:21px;display:flex;align-items:center>Conclusion</h2><p align=center><img src=../../images/Bryan_Remi/pinguins_studying.gif alt="Feeling strong with interpretable AI" style=width:30%;max-width:500px;height:auto></p><p>The article showed that knowledge distillation can improve both accuracy and interpretability. They attribute the improvement in interpretability to the transfer of class similarity knowledge from the teacher to the student model. They compare label smoothing (LS) that is similar to KD but LS does not benefit from class-similarity information. The empirical experiments shows better accuracy for LS and KD but the interpretability of LS decreases whereas it increases for KD confirming the hypothesis that class similarity knowledge has a role in interpretability. The authors obtain consistent results when using other metrics than the number of concept detectors for interpretability showing that their approach is robust to different definitions of interpretability.</p><p>Those encouraging results could lead to applications of knowledge distillation to improve the interpretability of deep learning models in highly sensitive areas like autonomous systems and healthcare.</p><h2 id=join-the-discussion style=font-size:21px;display:flex;align-items:center>Join the Discussion</h2><p>We’d love to hear your thoughts! What are your experiences with Knowledge Distillation (KD)? Have you found it to improve not just performance but also interpretability in your projects? Feel free to share your ideas, questions, or insights in the comments section or engage with us on <a href=https://github.com/BryanBradfo/responsible-ai-datascience-ipParis.github.io>GitHub</a>!</p><h2 id=references>References</h2><ul><li>Hinton, G., Vinyals, O., & Dean, J. (2015). <a href=https://arxiv.org/abs/1503.02531>Distilling the knowledge in a neural network.</a> arXiv:1503.02531.</li><li>Han, H., Kim, S., Choi, H.-S., & Yoon, S. (2023). <a href=https://arxiv.org/pdf/2305.15734>On the Impact of Knowledge Distillation for Model Interpretability.</a> arXiv:2305.15734.</li><li>Bau, D., Zhou, B., Khosla, A., Oliva, A., & Torralba, A. (2017). <a href=https://arxiv.org/pdf/1704.05796>Network dissection: Quantifying interpretability of deep visual representations.</a> arXiv:1704.05796.</li><li>Tjoa, E., & Guan, M. Y. (2020). <a href=https://arxiv.org/pdf/2009.02899>Quantifying explainability of saliency methods in deep neural networks.</a> arXiv:2009.02899.</li><li>Shah, H., Jain, P., & Netrapalli, P. (2021). <a href=https://arxiv.org/pdf/2102.12781>Do input gradients highlight discriminative features?</a> arXiv:2102.12781, NeurIPS 2021.</li></ul></html><div class=post-date><span class="g time">March 15, 2025 </span>&#8729;</div></section><div id=comments><script src=https://utteranc.es/client.js repo=ZgotmplZ issue-term=pathname theme=ZgotmplZ crossorigin=anonymous async></script></div></div></main></body></html>