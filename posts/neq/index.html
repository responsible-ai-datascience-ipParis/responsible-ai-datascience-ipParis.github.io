<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://responsible-ai-datascience-ipParis.github.io/"><title>To update or not to update? Neurons at equilibrium in deep models | Bloggin on Responsible AI</title><meta name=description content="Bloggin on Responsible AI"><meta property="og:url" content="https://responsible-ai-datascience-ipParis.github.io/posts/neq/"><meta property="og:site_name" content="Bloggin on Responsible AI"><meta property="og:title" content="To update or not to update? Neurons at equilibrium in deep models"><meta property="og:description" content="To update or not to update? Neurons at equilibrium in deep models Author: Alexis WINTER Augustin CREUSILLET Table of content Introduction NEq Experiments Results Reproducibility Conclusion References This is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).
Introduction Background Recent advances in deep learning have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for computational power has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both affordability and environmental sustainability, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-07T15:55:14+01:00"><meta property="article:modified_time" content="2024-02-07T15:55:14+01:00"><meta itemprop=name content="To update or not to update? Neurons at equilibrium in deep models"><meta itemprop=description content="To update or not to update? Neurons at equilibrium in deep models Author: Alexis WINTER Augustin CREUSILLET Table of content Introduction NEq Experiments Results Reproducibility Conclusion References This is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).
Introduction Background Recent advances in deep learning have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for computational power has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both affordability and environmental sustainability, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands."><meta itemprop=datePublished content="2024-02-07T15:55:14+01:00"><meta itemprop=dateModified content="2024-02-07T15:55:14+01:00"><meta itemprop=wordCount content="2345"><link rel=canonical href=https://responsible-ai-datascience-ipParis.github.io/posts/neq/><link rel=icon href=https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//atom.xml><link rel=alternate type=application/json title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"To update or not to update? Neurons at equilibrium in deep models","headline":"To update or not to update? Neurons at equilibrium in deep models","alternativeHeadline":"","description":"\u003ch1 style=\u0022font-size: 36px;\u0022\u003eTo update or not to update? Neurons at equilibrium in deep models\n\u003c\/h1\u003e\n\u003ch1 style=\u0022font-size: 24px;\u0022\u003eAuthor: Alexis WINTER Augustin CREUSILLET\u003c\/h1\u003e\n\u003ch1 id=\u0022table-of-content\u0022\u003eTable of content\u003c\/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\u0022#section-0\u0022\u003eIntroduction\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#section-1\u0022\u003eNEq\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#section-2\u0022\u003eExperiments\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#section-3\u0022\u003eResults\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#section-4\u0022\u003eReproducibility\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#section-5\u0022\u003eConclusion\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#section-6\u0022\u003eReferences\u003c\/a\u003e\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003eThis is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2022\/file\/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).\u003c\/p\u003e\n\u003ch2 id=\u0022section-0\u0022\u003eIntroduction\u003c\/h2\u003e\n\u003ch3 id=\u0022background\u0022\u003eBackground\u003c\/h3\u003e\n\u003cp\u003eRecent advances in \u003cstrong\u003edeep learning\u003c\/strong\u003e have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for \u003cstrong\u003ecomputational power\u003c\/strong\u003e has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both \u003cstrong\u003eaffordability and environmental sustainability\u003c\/strong\u003e, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/neq\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Bloggin on Responsible AI","copyrightYear":"2024","dateCreated":"2024-02-07T15:55:14.00Z","datePublished":"2024-02-07T15:55:14.00Z","dateModified":"2024-02-07T15:55:14.00Z","publisher":{"@type":"Organization","name":"Bloggin on Responsible AI","url":"https://responsible-ai-datascience-ipParis.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://responsible-ai-datascience-ipParis.github.io/assets/favicon.ico","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/neq\/","wordCount":"2345","genre":[],"keywords":[]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=../../><b>Bloggin on Responsible AI</b>
<span class="text-stone-500 animate-blink">▮</span></a></p><ul style=padding:0;margin:0><li><a href=../../posts/><span>Post</span></a><li><a href=../../tutorial/><span>Tutorial</span></a><li><a href=../../about/><span>About</span></a><li><a href=../../articles/><span>Articles</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>To update or not to update? Neurons at equilibrium in deep models</h2><h1 style=font-size:36px>To update or not to update? Neurons at equilibrium in deep models</h1><h1 style=font-size:24px>Author: Alexis WINTER Augustin CREUSILLET</h1><h1 id=table-of-content>Table of content</h1><ul><li><a href=#section-0>Introduction</a></li><li><a href=#section-1>NEq</a></li><li><a href=#section-2>Experiments</a></li><li><a href=#section-3>Results</a></li><li><a href=#section-4>Reproducibility</a></li><li><a href=#section-5>Conclusion</a></li><li><a href=#section-6>References</a></li></ul><p>This is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).</p><h2 id=section-0>Introduction</h2><h3 id=background>Background</h3><p>Recent advances in <strong>deep learning</strong> have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for <strong>computational power</strong> has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both <strong>affordability and environmental sustainability</strong>, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.</p><p>This paper tries to focus on the overall behavior of neurons, leveraging the notion of <strong>neuronal equilibrium (NEq)</strong>. When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.</p><h3 id=related-works>Related works</h3><h4 id=pruning-strategies>Pruning strategies</h4><p>Pruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to <strong>improve efficiency and reduce computational complexity</strong>. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.</p><p>Despite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically <strong>do not alleviate the computational complexity associated with training neural networks</strong>. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.</p><h4 id=lottery-ticket-hypothesis>Lottery ticket hypothesis</h4><p>The <strong>lottery ticket hypothesis</strong> is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or &ldquo;winning tickets,&rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even <strong>surpass the performance of the original dense network</strong>.</p><p>The hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.</p><p>The significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these <strong>winning tickets</strong> and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.</p><h2 id=section-1>NEq</h2><h3 id=neuronal-equilibrium>Neuronal equilibrium</h3><p>The concept of <strong>neuronal equilibrium</strong> aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.</p><p>To assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\Xi_{val}$ as follows:</p><img src=../../images/images_Winter_Creusillet/neq_formula.png width=300><p>The neuron $i$-th reaches the equilibrium when $(\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium we need to detect when :</p><p>$$\lim_{t\rightarrow \infty} \phi_{i}^t = k,$$</p><p>Since it is not trivial to assess this statment we prefer to work with variations of $(\phi_{i})_t$ that can be defined as :</p><p>\begin{equation}
v_{\Delta \phi_i}^t = \Delta \phi_i^t - \mu_{eq} v_{\Delta \phi_i}^{t-1},
\end{equation}</p><p>With $\mu_{eq}$ the momentum coefficient.</p><p>This only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\Delta \phi_i^t \rightarrow 0$$</p><p>Since we want to track the evolution of $\Delta \phi_i^t$ over time we introduce the <strong>velocity of the variations</strong>:
$$
v_{\Delta \phi_i}^t = \Delta \phi_i^t - \mu_{eq} v_{\Delta \phi_i}^{t-1},
$$</p><p>With $\mu_{eq}$ the momentum coefficient.</p><p>Rewrited:</p><p><img src=../../images/images_Winter_Creusillet/momentum_coef.png alt=creusilet/winter loading=lazy decoding=async class=full-width></p><p>We need to have $$\mu_{eq} \in [0; 0.5]$$ to prevent the velocity from exploding.</p><p>Finally we can set the condition for the neuron to be at the equilibrium as:
\begin{equation}
\left| v_{\Delta \phi}^t \right | &lt; \varepsilon,~~~~~\varepsilon \geq 0.
\end{equation}</p><p>It is important to know that this relation might not hold for all $t$ since there could be an instant $t&rsquo; &lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.</p><h3 id=training-scheme>Training scheme</h3><p>The training scheme can be presented according to this scheme:</p><p><img src=../../images/images_Winter_Creusillet/prunedbackprop-scheme_full-1.png alt=creusilet/winter loading=lazy decoding=async class=full-width></p><p>At the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:</p><ul><li>An epoch of training is made for all trainable neurons on the training set.</li><li>The training either stops due to the end of training criterion being met or continues to the next step.</li><li>The velocity of the similarities is evaluated for every neuron.</li><li>The set of trainable neurons is determined for the next step according to the equilibrium criterion.</li></ul><p>Comparing with regular training, we can see two more hyper-parameters:</p><ul><li>$\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities.</li><li>$\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities.</li></ul><h2 id=section-2>Experiments</h2><h3 id=sgd-vs-adam>SGD vs Adam</h3><p><img src=../../images/images_Winter_Creusillet/sgd_vs_adam.png alt=adam/sgd loading=lazy decoding=async class=full-width></p><p>The authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.</p><p>In the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.</p><p>As training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that <strong>Adam brings the network towards this equilibrium faster than SGD</strong>, but also note that in this specific task, <strong>SGD achieves a slightly higher final accuracy than Adam</strong>. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD&rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.</p><p>The experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.</p><h3 id=distribution-of-phi--choice-of-µ_eq>Distribution of $\phi$ & choice of $µ_{eq}$</h3><p><img src=../../images/images_Winter_Creusillet/mu-line-1.png alt=creusilet/winter loading=lazy decoding=async class=full-width></p><p>The paper also discusses the distribution of $\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.</p><p>The parameter $\phi$ measures the <strong>cosine similarity between the outputs of a particular neuron at two consecutive training epochs</strong>, over the validation set. It is used to determine if a neuron&rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\phi$ equals 1, it indicates that the neuron&rsquo;s output is stable across the epochs, signifying it has reached equilibrium.</p><p>The paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\phi$ values across epochs, and $v_{∆\phi}$, which measures the velocity of this change considering a <strong>momentum coefficient $µ_{eq}$</strong>. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.</p><p>By examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper&rsquo;s Figure 5, which shows the distribution of $\phi$, $∆\phi$, and $v_{∆\phi}$ for a ResNet-32 model trained on CIFAR-10.</p><p>In summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally &ldquo;unfreeze&rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.</p><h3 id=impact-of-the-validation-set-size-and-ε>Impact of the validation set size and ε</h3><p>The authors found that the size of the validation set <strong>does not significantly impact the performance of the model</strong>. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.</p><p>When examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, <strong>striking a balance between model performance and computational efficiency</strong>.</p><h2 id=section-3>Results</h2><h2 id=section-4>Reproducibility</h2><p>Using the author&rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from <strong>limited computational resources</strong>. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.</p><h3 id=experiment>Experiment</h3><p>This experiment aims to replicate the section 4.1.1 &ldquo;SGD vs Adam&rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 train_classification.py --amp<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> --arch<span style=color:#f92672>=</span>resnet32-cifar --batch-size<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span> --dataset<span style=color:#f92672>=</span>cifar10 --device<span style=color:#f92672>=</span>cuda --epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>250</span> --eps<span style=color:#f92672>=</span>0.001 --lr<span style=color:#f92672>=</span>0.1 --momentum<span style=color:#f92672>=</span>0.9 --optim<span style=color:#f92672>=</span>sgd --val-size<span style=color:#f92672>=</span>0.01 --velocity-mu<span style=color:#f92672>=</span>0.5 --weight-decay<span style=color:#f92672>=</span>0.0005
</span></span></code></pre></div><p>The code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named <strong>Weights & Biases (wandb)</strong> to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.</p><p><img src=../../images/images_Winter_Creusillet/frozen_sgd_vs_adam1.png alt=creusilet/winter loading=lazy decoding=async class=full-width>
<img src=../../images/images_Winter_Creusillet/accuracy_sgd_vs_adam1.png alt=creusilet/winter loading=lazy decoding=async class=full-width></p><p>As expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.</p><h2 id=section-5>Conclusion</h2><p>From the initial problem of <strong>computational resources saving</strong>, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, <strong>NEq produces a new knowledge</strong> that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and <strong>might lead to new training strategies</strong>.</p><p>One possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.</p><h2 id=section-6>References</h2><ol><li>Bragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022.</li><li>Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017.</li><li>J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019.</li></ol><style type=text/css>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit}</style><script type=text/x-mathjax-config>
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script><div class=post-date><span class="g time">February 7, 2024 </span>&#8729;</div></section><div id=comments><script src=https://utteranc.es/client.js repo=ZgotmplZ issue-term=pathname theme=ZgotmplZ crossorigin=anonymous async></script></div></div></main></body></html>