<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://responsible-ai-datascience-ipParis.github.io/"><title>A Groupe Fairness | Bloggin on Responsible AI</title><meta name=description content="Bloggin on Responsible AI"><meta property="og:url" content="https://responsible-ai-datascience-ipParis.github.io/posts/a-groupe-fairness/"><meta property="og:site_name" content="Bloggin on Responsible AI"><meta property="og:title" content="A Groupe Fairness"><meta property="og:description" content="No Retraining? No Problem! FRAPPÉ’s Bold Approach to AI Fairness Authors of the blogpost: Arij Hajji , Rouaa Blel
Table of Contents Introduction Why Fairness in AI Matters When Fairness in AI Went Wrong: The Amazon Hiring Scandal Fairness Mitigation Strategies in AI How FRAPPÉ Works Mathematical Formulation of FRAPPÉ Theoretical Justification FRAPPÉ vs. Traditional Fairness Methods Why FRAPPÉ is Important Efficiency Flexibility Privacy Real-World Example Experimenting Reproducibility and Potential Difficulties Final Thoughts What Do You Think About FRAPPÉ? References About the Authors Introduction Imagine being confident in your abilities and expertise when you apply for your ideal job, only to have your application denied—not due to your qualifications, but rather to biases an AI system learned from past data. It’s infuriating, unjust, and regrettably becoming a bigger issue as AI-driven choices in criminal justice, lending, and employment become more prevalent."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-01T22:29:01+01:00"><meta property="article:modified_time" content="2025-03-01T22:29:01+01:00"><meta itemprop=name content="A Groupe Fairness"><meta itemprop=description content="No Retraining? No Problem! FRAPPÉ’s Bold Approach to AI Fairness Authors of the blogpost: Arij Hajji , Rouaa Blel
Table of Contents Introduction Why Fairness in AI Matters When Fairness in AI Went Wrong: The Amazon Hiring Scandal Fairness Mitigation Strategies in AI How FRAPPÉ Works Mathematical Formulation of FRAPPÉ Theoretical Justification FRAPPÉ vs. Traditional Fairness Methods Why FRAPPÉ is Important Efficiency Flexibility Privacy Real-World Example Experimenting Reproducibility and Potential Difficulties Final Thoughts What Do You Think About FRAPPÉ? References About the Authors Introduction Imagine being confident in your abilities and expertise when you apply for your ideal job, only to have your application denied—not due to your qualifications, but rather to biases an AI system learned from past data. It’s infuriating, unjust, and regrettably becoming a bigger issue as AI-driven choices in criminal justice, lending, and employment become more prevalent."><meta itemprop=datePublished content="2025-03-01T22:29:01+01:00"><meta itemprop=dateModified content="2025-03-01T22:29:01+01:00"><meta itemprop=wordCount content="1371"><link rel=canonical href=https://responsible-ai-datascience-ipParis.github.io/posts/a-groupe-fairness/><link rel=icon href=https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//atom.xml><link rel=alternate type=application/json title="Bloggin on Responsible AI" href=https://responsible-ai-datascience-ipParis.github.io//feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"A Groupe Fairness","headline":"A Groupe Fairness","alternativeHeadline":"","description":"\u003ch1 id=\u0022no-retraining-no-problem-frappés-bold-approach-to-ai-fairness\u0022\u003eNo Retraining? No Problem! FRAPPÉ’s Bold Approach to AI Fairness\u003c\/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors of the blogpost:\u003c\/strong\u003e Arij Hajji , Rouaa Blel\u003c\/p\u003e\n\u003ch2 id=\u0022table-of-contents\u0022\u003eTable of Contents\u003c\/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\u0022#introduction\u0022\u003eIntroduction\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#why-fairness-in-ai-matters\u0022\u003eWhy Fairness in AI Matters\u003c\/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\u0022#when-fairness-in-ai-went-wrong-the-amazon-hiring-scandal\u0022\u003eWhen Fairness in AI Went Wrong: The Amazon Hiring Scandal\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#fairness-mitigation-strategies-in-ai\u0022\u003eFairness Mitigation Strategies in AI\u003c\/a\u003e\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#how-frapp%C3%A9-works\u0022\u003eHow FRAPPÉ Works\u003c\/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\u0022#mathematical-formulation-of-frapp%C3%A9\u0022\u003eMathematical Formulation of FRAPPÉ\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#theoretical-justification\u0022\u003eTheoretical Justification\u003c\/a\u003e\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#frapp%C3%A9-vs-traditional-fairness-methods\u0022\u003eFRAPPÉ vs. Traditional Fairness Methods\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#why-frapp%C3%A9-is-important\u0022\u003eWhy FRAPPÉ is Important\u003c\/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\u0022#efficiency\u0022\u003eEfficiency\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#flexibility\u0022\u003eFlexibility\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#privacy\u0022\u003ePrivacy\u003c\/a\u003e\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#real-world-example\u0022\u003eReal-World Example\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#experimenting\u0022\u003eExperimenting\u003c\/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\u0022#reproducibility-and-potential-difficulties\u0022\u003eReproducibility and Potential Difficulties \u003c\/a\u003e\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#final-thoughts\u0022\u003eFinal Thoughts\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#what-do-you-think-about-frapp%C3%A9\u0022\u003eWhat Do You Think About FRAPPÉ?\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#references\u0022\u003eReferences\u003c\/a\u003e\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022#about-the-authors\u0022\u003eAbout the Authors\u003c\/a\u003e\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003ch2 id=\u0022introduction\u0022\u003eIntroduction\u003c\/h2\u003e\n\u003cp\u003e\u003cimg\n  src=\u0022\/images\/Arij_Roua\/aiandjob.jpg\u0022\n  alt=\u0022Job_Application\u0022\n  loading=\u0022lazy\u0022\n  decoding=\u0022async\u0022\n  class=\u0022full-width\u0022\n\/\u003e\n\n\nImagine being confident in your abilities and expertise when you apply for your ideal job, only to have your application denied—not due to your qualifications, but rather to biases an AI system learned from past data. It\u0026rsquo;s infuriating, unjust, and regrettably becoming a bigger issue as AI-driven choices in criminal justice, lending, and employment become more prevalent.\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/a-groupe-fairness\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Bloggin on Responsible AI","copyrightYear":"2025","dateCreated":"2025-03-01T22:29:01.00Z","datePublished":"2025-03-01T22:29:01.00Z","dateModified":"2025-03-01T22:29:01.00Z","publisher":{"@type":"Organization","name":"Bloggin on Responsible AI","url":"https://responsible-ai-datascience-ipParis.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://responsible-ai-datascience-ipParis.github.io/assets/favicon.ico","url":"https:\/\/responsible-ai-datascience-ipParis.github.io\/posts\/a-groupe-fairness\/","wordCount":"1371","genre":[],"keywords":[]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=../../><b>Bloggin on Responsible AI</b>
<span class="text-stone-500 animate-blink">▮</span></a></p><ul style=padding:0;margin:0><li><a href=../../posts/><span>Post</span></a><li><a href=../../tutorial/><span>Tutorial</span></a><li><a href=../../about/><span>About</span></a><li><a href=../../articles/><span>Articles</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>A Groupe Fairness</h2><h1 id=no-retraining-no-problem-frappés-bold-approach-to-ai-fairness>No Retraining? No Problem! FRAPPÉ’s Bold Approach to AI Fairness</h1><p><strong>Authors of the blogpost:</strong> Arij Hajji , Rouaa Blel</p><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#introduction>Introduction</a></li><li><a href=#why-fairness-in-ai-matters>Why Fairness in AI Matters</a><ul><li><a href=#when-fairness-in-ai-went-wrong-the-amazon-hiring-scandal>When Fairness in AI Went Wrong: The Amazon Hiring Scandal</a></li><li><a href=#fairness-mitigation-strategies-in-ai>Fairness Mitigation Strategies in AI</a></li></ul></li><li><a href=#how-frapp%C3%A9-works>How FRAPPÉ Works</a><ul><li><a href=#mathematical-formulation-of-frapp%C3%A9>Mathematical Formulation of FRAPPÉ</a></li><li><a href=#theoretical-justification>Theoretical Justification</a></li></ul></li><li><a href=#frapp%C3%A9-vs-traditional-fairness-methods>FRAPPÉ vs. Traditional Fairness Methods</a></li><li><a href=#why-frapp%C3%A9-is-important>Why FRAPPÉ is Important</a><ul><li><a href=#efficiency>Efficiency</a></li><li><a href=#flexibility>Flexibility</a></li><li><a href=#privacy>Privacy</a></li></ul></li><li><a href=#real-world-example>Real-World Example</a></li><li><a href=#experimenting>Experimenting</a><ul><li><a href=#reproducibility-and-potential-difficulties>Reproducibility and Potential Difficulties</a></li></ul></li><li><a href=#final-thoughts>Final Thoughts</a></li><li><a href=#what-do-you-think-about-frapp%C3%A9>What Do You Think About FRAPPÉ?</a></li><li><a href=#references>References</a></li><li><a href=#about-the-authors>About the Authors</a></li></ul><h2 id=introduction>Introduction</h2><p><img src=../../images/Arij_Roua/aiandjob.jpg alt=Job_Application loading=lazy decoding=async class=full-width>
Imagine being confident in your abilities and expertise when you apply for your ideal job, only to have your application denied—not due to your qualifications, but rather to biases an AI system learned from past data. It&rsquo;s infuriating, unjust, and regrettably becoming a bigger issue as AI-driven choices in criminal justice, lending, and employment become more prevalent.</p><p>Correcting biased AI models can be a difficult task. Traditional methods often involve re-training the entire model, which can be costly, time-consuming and sometimes impossible . But can&rsquo;t we find a simpler way?</p><p>Presenting FRAPPÉ, a methodology that eliminates bias in AI without requiring retraining. A novel method called FRAPPÉ (Fairness Framework for Post-Processing Everything) transforms our understanding of fairness in machine learning.
Rather than modifying the entire model, it provides a more efficient solution by adjusting the output to ensure fairness. We will examine the main ideas, contributions, and practical applications of this ground-breaking study in this blog., you can check out the original paper, FRAPPÉ: A Group Fairness Framework for Post-Processing Everything, by Alexandru Ţifrea, Preethi Lahoti, Ben Packer, Yoni Halpern, Ahmad Beirami, and Flavien Prost.</p><h2 id=why-fairness-in-ai-matters>Why Fairness in AI Matters</h2><p>Important choices including bank approvals, employment recruiting, and medical diagnosis are increasingly being made using AI algorithms. These mechanisms can, however, perpetuate current disparities and disproportionately impact particular populations when they reflect biases.</p><p>Ensuring group fairness—the notion that demographic groupings like age, gender, and ethnicity are treated fairly and equally—is one of the primary ethical concerns in AI research. Fairness is far from easy to achieve, though. Equal opportunity, statistical parity, and equalized odds are only a few of the many ways that fairness can be interpreted, and each one has its own trade-offs between predictability and justice.</p><h3 id=when-fairness-in-ai-went-wrong-the-amazon-hiring-scandal>When Fairness in AI Went Wrong: The Amazon Hiring Scandal</h3><p>When we think about a good exemple of AI bias we can think about when Amazon created a tool to assess job appplications.
With the tech sector generally employing more men, the model was trained on previous employment data and discovered trends favoring male applicants. As a result, resumes containing terms such as “feminine” (e.g. “women&rsquo;s chess club”) and indicating a preference for environments with a higher proportion of men were devalued by AI. Amazon in 2018 put an end to the usage of this method, showing what biases could do to our society and especially in job selection. This case highlights the need for robust bias mitigation techniques and the importance of fairness in AI.</p><h3 id=fairness-mitigation-strategies-in-ai>Fairness Mitigation Strategies in AI</h3><p>To address these biases, AI fairness strategies typically fall into three main categories:</p><ol><li><p><strong>In-Processing</strong>: This strategy involves modifying the model during its training phase to directly integrate fairness constraints. While effective, it can be computationally expensive and often requires retraining the entire model, which may not be practical in all situations.</p></li><li><p><strong>Post-Processing</strong>: These methods apply adjustments to the model&rsquo;s outputs after predictions have been made. It offers greater flexibility since it doesn’t require altering the original model. However, these methods are often limited to specific fairness criteria and may not apply universally.</p></li><li><p><strong>Pre-Processing</strong>: This approach involves adjusting the training data before it’s used to train the model. By altering the data to reduce bias before training, pre-processing aims to create a fairer dataset. However, this approach can also introduce challenges, such as data distortions or loss of valuable information.</p></li></ol><p><img src=../../images/Arij_Roua/threetypes.png alt=types loading=lazy decoding=async class=full-width></p><h2 id=how-frappé-works->How FRAPPÉ works ?</h2><p>Instead of relying on explicit group-dependent transformations that depend directly on sensitive attributes (e.g., gender, race), FRAPPÉ applies an additive correction based on all covariates ( x ). This ensures fairness while maintaining predictive accuracy.</p><h3 id=mathematical-formulation-of-frappé>Mathematical Formulation of FRAPPÉ</h3><p>At its core, FRAPPÉ modifies a base model’s prediction fbase(x) by applying a fairness correction term TP P (x), leading to:</p><p>ffair (x) = fbase(x) + TP P (x)</p><p>where:</p><ul><li>fbase(x) represents the original model output.</li><li>TPP (x) is an additive term learned through post-processing to correct fairness discrepancies.</li></ul><p>This transformation enables FRAPPÉ to achieve fairness objectives without modifying the model’s internal structure.</p><h3 id=theoretical-justification>Theoretical Justification</h3><p>To establish the validity of this approach, we consider generalized linear models (GLMs), where predictions take the form:</p><p><img src=../../images/Arij_Roua/eq1.png alt=types loading=lazy decoding=async class=full-width></p><p>for model parameters θ and a link function ψ (like identity function for linear regression, sigmoid for logistic regression). In fairness-constrained optimization, we often minimize:</p><p><img src=../../images/Arij_Roua/eq2.png alt=types loading=lazy decoding=async class=full-width></p><p>where:</p><ul><li>Lpred is the predictive loss (like mean squared error, logistic loss).</li><li>Lfair is a fairness penalty.</li><li>λ controls the fairness-accuracy trade-off.</li></ul><p>FRAPPÉ recasts this as a bi-level optimization problem:</p><p><img src=../../images/Arij_Roua/eq3.png alt=types loading=lazy decoding=async class=full-width></p><p>where:</p><ul><li>θbase is the solution to the unconstrained prediction problem.</li><li>DF(θ, θbase) is a Bregman divergence term ensuring predictions remain close to the original model.</li></ul><p>The key insight is that minimizing OPTIP and OPTPP leads to identical fairness-error trade-offs, proving that FRAPPÉ&rsquo;s post-processing approach is theoretically equivalent to traditional in-processing fairness methods.</p><h2 id=frappé-vs-traditional-fairness-methods>FRAPPÉ vs. Traditional Fairness Methods</h2><table><thead><tr><th><strong>Feature</strong></th><th><strong>FRAPPÉ</strong></th><th><strong>Traditional Fairness Techniques</strong></th></tr></thead><tbody><tr><td><strong>Model Agnosticism</strong></td><td>Works with any machine learning model, whether black-box or interpretable.</td><td>Typically tied to specific model architectures or requires model modifications.</td></tr><tr><td><strong>Fairness Flexibility</strong></td><td>Can adapt to various fairness criteria (equal opportunity, statistical parity, etc.).</td><td>Often focuses on a single fairness definition, limiting flexibility.</td></tr><tr><td><strong>Sensitive Data Usage</strong></td><td>Does not require sensitive data (e.g., race, gender) during prediction.</td><td>Many methods rely on sensitive data during prediction, which can raise ethical concerns.</td></tr><tr><td><strong>Efficiency</strong></td><td>Post-processing adjustment makes it faster and less resource-intensive.</td><td>Often requires full model retraining, which is computationally expensive.</td></tr></tbody></table><h2 id=why-frappé-is-important>Why FRAPPÉ is important</h2><h3 id=efficiency>Efficiency:</h3><p>Using FRAPPÉ the training cost can be reduced upto <strong>90%</strong> compared to in-processing approaches, since FRAPPÉ only learns a small layer of correction instead of the entire model.</p><p>-<strong>Flexibility:</strong>
In the case where <strong>statistical parity</strong> is mandatoryr instead of <strong>equal opportunity</strong> FRAPPÉ is the solution.</p><p>-<strong>Privacy:</strong>
For FRAPPÉ the storage or extraction of sensitive demographic data is not crucial or obigatory, as it works without group labels at the time of prediction.</p><h2 id=real-world-example>Real-World Example</h2><p>Let&rsquo;s take in mind a hiring algorithm ,it basically scores applicants based on their resumes. An exemple of historical bias means female applicants womm receive lower scores on average. Traditionally, we’d need to retrain the whole model to correct this.</p><p>Using FRAPPÉ, the original model can remain untouched — we just add a <strong>fairness correction layer</strong> that adapts scores to ensure fairness across genders. This is faster, cheaper, and works even if we didn’t train the original model.</p><h2 id=experimenting>Experimenting</h2><p><a href=../../assets/a_groupe_fairness.html>View the notebook</a></p><h3 id=reproducibility-and-potential-difficulties>Reproducibility and Potential Difficulties</h3><p>Careful consideration of model setup, hyperparameter selection, and dataset preprocessing are necessary to replicate these findings. Potential bias is indicated by the baseline model&rsquo;s notable differences in classification accuracy and error rates between Sex1 (likely male) and Sex0 (likely female). By equating false positive and false negative rates across groups, the fairness-aware model effectively reduces these discrepancies utilizing the FRAPPÉ framework. However, ensuring consistent replication of these findings poses challenges.</p><h2 id=final-thoughts>Final Thoughts</h2><p>When retraining a model isn&rsquo;t feasible, <strong>FRAPPÉ</strong> offers a practical, adaptable, and efficient solution to ensure fairness in AI systems. By <strong>decoupling model learning from fairness adjustments</strong>, it makes it easier to incorporate fairness without overhauling existing models.</p><p>For anyone working in <strong>ethical AI</strong>, <strong>data science</strong>, or <strong>policy-making</strong>, FRAPPÉ stands out as a powerful tool. It allows for the development of fairer technologies that are not only <strong>faster</strong> but also <strong>more scalable</strong>.</p><h2 id=what-do-you-think-about-frappé>What Do You Think About FRAPPÉ?</h2><p>Could this innovative framework be the key to making fairness mitigation in AI more <strong>practical</strong> and <strong>scalable</strong>? We’d love to hear your thoughts—feel free to share your opinions!</p><h2 id=references>References</h2><ul><li><p>Alexandru Țifrea, Preethi Lahoti, Ben Packer, Yoni Halpern, Ahmad Beirami, Flavien Prost. <em>FRAPPÉ: A Group Fairness Framework for Post-Processing Everything</em>. ICML 2024. <a href=https://arxiv.org/abs/2312.02592>arXiv Link</a></p></li><li><p><a href=https://responsible-ai-datascience-ipparis.github.io/tutorial/>Responsible AI Blog Guidelines - Télécom Paris</a></p></li></ul><h2 id=about-the-authors>About the Authors</h2><p><strong>Arij Hajji</strong></p><p><em>M2 Data Science, Institut Polytechnique de Paris</em><br><em><a href=mailto:arij.hajji@telecom-paris.fr>arij.hajji@telecom-paris.fr</a></em></p><p><strong>Rouaa Blel</strong></p><p><em>M2 Data Science, Institut Polytechnique de Paris</em><br><em><a href=mailto:rouaa.blel@ensae.fr>rouaa.blel@ensae.fr</a></em></p><div class=post-date><span class="g time">March 1, 2025 </span>&#8729;</div></section><div id=comments><script src=https://utteranc.es/client.js repo=ZgotmplZ issue-term=pathname theme=ZgotmplZ crossorigin=anonymous async></script></div></div></main></body></html>