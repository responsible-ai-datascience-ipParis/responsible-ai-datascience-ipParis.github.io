{
  "version": "https://jsonfeed.org/version/1",
  "title": "Bloggin on Responsible AI",
  "home_page_url": "http://localhost:1313/",
  "feed_url": "http://localhost:1313/feed.json",
  "description": "Bloggin on Responsible AI",
  "favicon": "http://localhost:1313//assets/favicon.ico",
  "expired": false,
  "author": {
    "name": "Students from M2 Data Science IP Paris",
    "url": "http://localhost:1313/"
  },
  "items": [
    
    

    
    {
      "id": "ae2f64cac6ac9f6d89cf77b7ba0c9541c49ea196",
      "title": "Axiomatic Explanations for Visual Search, Retrieval and Similarity Learning",
      "summary": "",
      "content_text": " \u003c!DOCTYPE html\u003e Styled Table AXIOMATIC EXPlanATIONS FOR VISUAL SEARCh, RETRIEVAL, AND SIMILARITY LEARNING Authors:Mark Hamilton ${ }^{1,2}$, Scott Lundberg ${ }^{2}$, Stephanie Fu ${ }^{1}$, Lei Zhang ${ }^{2}$, William T. Freeman ${ }^{1,3}$\n${ }^{1}$ MIT, ${ }^{2}$ Microsoft, ${ }^{3}$ Google\nmarkth@mit.edu **Authors of the blogpost**: Yassine Beniguemim and Noureddine BOULLAM. Table of Contents Abstract Introduction Exploring Visual Search Algorithm Explanations First-Order Explanations Unifying First-Order Search Interpretation Techniques Second-Order Explanations A Fast Shapley-Taylor Approximation Kernel Second-Order Search Activation Maps Implementing Second-Order Explanations in Practice Conclusion Abstract Visual search, recommendation, and contrastive similarity learning are pivotal technologies shaping user experiences in the digital age. However, the complexity of modern model architectures often obscures their inner workings, making them challenging to interpret. In our blog, we delve into a groundbreaking paper titled \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al. This paper introduces a novel framework grounded in the theory of fair credit assignment, providing axiomatic solutions that generalize existing explanation techniques and address fairness concerns in recommendation systems. Through our exploration, we aim to demystify the complexities of visual search algorithms, offering readers insights into their operation and implications for various domains.\nFigure 1: Architectures for search engine interpretability. Like classifier explanations, First-order search explanations yield heatmaps of important pixels for similarity (bottom row third column). Second order search interpretation methods yield a dense correspondence between image locations (last two columns). CAM (second column) is a particular case of Shapley value approximation, and we generalize it to yield dense correspondences (last column).\nIntroduction Welcome to our blog, where we embark on a journey to demystify the intricate world of visual search technology. In today\u0026rsquo;s digital age, recommendation systems play a pivotal role in guiding users through a vast sea of information, aiding in everything from online shopping to content discovery.\nYet, behind the scenes, these recommendation engines operate using sophisticated algorithms that can seem like a black box to many users. How do they decide which products to suggest, or which images are most similar to a given query? These questions lie at the heart of our exploration.\nInspired by the groundbreaking paper \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al., we embark on a quest to unravel the inner workings of these recommendation systems. By delving into the concepts of fair credit assignment, Shapley values, and Harsanyi Dividends, we aim to shed light on the underlying principles that govern visual search algorithms.\n1. Exploring Visual Search Algorithm Explanations In our exploration of visual search algorithm explanations, we delve into the fundamental concepts introduced in the paper by Mark Hamilton et al. Our journey begins with an examination of the two distinct classes of explanation methods: \u0026ldquo;first order\u0026rdquo; and \u0026ldquo;second order.\u0026rdquo; First-order approaches focus on highlighting important pixels contributing to object similarity, while second-order explanations provide a comprehensive correspondence between query and retrieved images.\n1.1 First-Order Explanations First-order interpretations are rooted in classifier explainability theory, offering insights into the importance of individual pixels or features in determining object similarity. We explore the theoretical underpinnings of these explanations, drawing parallels to existing techniques such as Class Activation Maps (CAM), GradCAM, and LIME.\nFormalizing First-Order Interpretations The core of first-order explanations lies in the formalization of the value function, typically represented as $v_1(S)$, where $S$ represents subsets of features or pixels. This function allows us to quantify the contribution of each subset to the overall similarity score between query and retrieved images.\n$$ v_1(S): 2^N \\rightarrow \\mathbb{R} := d(x, \\text{mask}(y, S)) $$\n1.2 Unifying First-Order Search Interpretation Techniques Building upon existing classifier explainability methods, we introduce an approach to transform opaque and grey-box classification explainers into search engine explainers. By formalizing the value function and leveraging concepts like Shapley values, we extend existing approaches such as LIME and SHAP to the realm of visual search.\nLeveraging Shapley Values Shapley values provide a principled way to assign credit to individual features or pixels based on their contribution to the similarity function. By applying Shapley values to the search engine context, we can identify the most influential elements in both query and retrieved images.\n$$ \\phi_{v_1}(S) = \\sum_{T: S \\subset T} \\frac{d_v(T)}{\\binom{|T|}{|S|}} $$\n1.3 Second-Order Explanations Moving beyond pixel-level interpretations, we delve into second-order explanations that capture the interactions between areas of query and retrieved images. Drawing inspiration from Harsanyi Dividends and Shapley-Taylor indices, we explore how these concepts generalize to provide richer insights into image similarity.\nUnderstanding Second-Order Interpretations Second-order explanations go beyond individual features to capture the interaction strength between different parts of query and retrieved images. We introduce the concept of Harsanyi Dividends, which provide a detailed view of the function\u0026rsquo;s behavior at every coalition of features.\n$$ d_v(S) = \\begin{cases} v(S) \u0026amp; \\text{if } |S|=1 \\ v(S) - \\sum_{T \\subsetneq S} d_v(T) \u0026amp; \\text{if } |S| \u0026gt; 1 \\end{cases} $$\n1.4 A Fast Shapley-Taylor Approximation Kernel While Harsanyi Dividends and Shapley-Taylor indices offer robust credit assignment mechanisms, their computation can be challenging. We introduce a novel weighting kernel for second-order Shapley-Taylor indices, significantly reducing computational complexity while maintaining accuracy.\nFigure 5: Convergence of Shapley-Taylor estimation schemes with respect to the Mean Squared Error (MSE) on randomly initialized deep networks with 15 dimensional input. Our strategies (Kernel) converge with significantly fewer function evaluations.\nFigure 6: Our Second-order explanation evaluation strategy. A good method should project query objects (top left and middle) to corresponding objects in the retrieved image (bottom left and middle). When censoring all but these shared objects (right column) the search engine should view these images as similar.\nEfficient Computation with Kernel Approximation The proposed weighting kernel allows for efficient approximation of Shapley-Taylor indices, enabling faster computation without sacrificing accuracy. By sampling random coalitions and aggregating information into a weighted quadratic model, we achieve a significant reduction in computational overhead.\n1.5 Second-Order Search Activation Maps Applying the Shapley-Taylor framework, we derive second-order search activation maps, offering dense correspondences between query and retrieved image locations. These maps provide a deeper understanding of image similarity, facilitating more nuanced interpretations of visual search results.\nVisualizing Second-Order Explanations Using the derived Shapley-Taylor indices, we construct matrices representing the interaction strength between query and retrieved image locations. These matrices allow us to visualize how different parts of the query image correspond to parts of the retrieved image, providing intuitive insights into the similarity judgments made by the search algorithm.\nTable 1: Comparison of performance of first- and second-order search explanation methods.\n2. Implementing Second-Order Explanations in Practice With a solid theoretical foundation in place, we now turn our attention to practical implementation steps for incorporating second-order explanations into existing visual search systems.\n2.1 Data Preparation and Preprocessing Before integrating second-order explanations, it\u0026rsquo;s crucial to preprocess and structure the data appropriately. This involves organizing the dataset into query-image pairs, ensuring consistency in image format, resolution, and preprocessing steps such as normalization and resizing.\nData Pipeline Overview We design a robust data pipeline encompassing data loading, preprocessing, and augmentation stages. Leveraging popular libraries like TensorFlow and PyTorch, we streamline the process of preparing the dataset for training and evaluation.\n2.2 Model Modification and Integration To enable the computation of second-order explanations, we modify the existing visual search model architecture. This adaptation involves incorporating additional layers or modules to capture the interactions between query and retrieved images.\nArchitectural Adjustments We introduce novel components such as interaction modules or attention mechanisms to facilitate the computation of second-order explanations. These architectural adjustments enable the model to learn and represent the complex relationships between different regions of query and retrieved images.\n2.3 Training and Evaluation Procedures Training a visual search model with second-order explanations requires careful consideration of training objectives, loss functions, and evaluation metrics. We devise training procedures that optimize both the primary search task and the secondary objective of generating accurate explanations.\nObjective Function Formulation We define a composite objective function that combines the primary search task loss with a regularization term for encouraging meaningful second-order explanations. This formulation ensures that the model learns to balance between search accuracy and explanation fidelity during training.\n2.4 Validation and Interpretation Once trained, we validate the effectiveness of the model\u0026rsquo;s second-order explanations through comprehensive evaluation procedures. This involves qualitative analysis of explanation maps, quantitative assessment of explanation quality, and user studies to gauge the interpretability of the generated explanations.\nEvaluation Metrics We define metrics such as explanation fidelity, coherence, and relevance to quantitatively evaluate the quality of second-order explanations. By comparing against baseline methods and human annotations, we assess the model\u0026rsquo;s ability to capture meaningful interactions between query and retrieved images.\n2.5 Deployment Considerations Deploying a visual search system with second-order explanations requires careful planning and integration into existing infrastructure. We address scalability, latency, and user experience considerations to ensure seamless deployment in real-world applications.\nScalable Inference Architecture We design an inference pipeline optimized for efficient computation of second-order explanations in production environments. This involves leveraging distributed computing frameworks and model optimization techniques to minimize latency and maximize throughput.\n3. Conclusion By following these implementation steps, we bridge the gap between theoretical insights and practical deployment of second-order explanations in visual search systems. Our approach empowers users to gain deeper insights into the underlying mechanisms driving search results, paving the way for more transparent and interpretable AI systems.\nAdditional Resources Video Description: Dive deeper into the concepts with a detailed video overview available here. Code Repository: Access the training and evaluation code to explore the implementation details here. For a comprehensive exploration of the technical details and experimental results, refer to the full paper.\nREFERENCES Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels. Technical report, 2010.\nJiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation with inter-pixel relations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2209-2218, 2019.\nMarco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. In International Conference on Machine Learning, pp. 272-281. PMLR, 2019.\nRobert J Aumann and Lloyd S Shapley. Values of non-atomic games. Princeton University Press, 2015.\nSebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.\nBing. Beyond text queries: Searching with bing visual search, Jun 2017. URL https://aka. ms/AAas 7 jg.\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 12091218, 2018.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.\nHila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 782-791, 2021.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020 b.\nYun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang. Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020c.\n",
      "content_html": "\u003cstyle\nTYPE=\"text/css\"\u003e\n\ncode.has-jax {font:\ninherit;\nfont-size:\n100%; \nbackground: \ninherit; \nborder: \ninherit;}\n\n\u003c/style\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"UTF-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n\u003ctitle\u003eStyled Table\u003c/title\u003e\n\u003cstyle\u003e\n    table {\n        border-collapse: collapse;\n        width: 100%;\n    }\n    th, td {\n        padding: 8px;\n        text-align: center;\n        border-bottom: 1px solid #ddd;\n    }\n    th {\n        background-color: #f2f2f2;\n    }\n    tr:hover {\n        background-color: #f5f5f5;\n    }\n\u003c/style\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAXIOMATIC EXPlanATIONS FOR VISUAL SEARCh, RETRIEVAL, AND SIMILARITY LEARNING \u003c/h1\u003e\n\u003ch1 style=\"font-size: 13px;\"\u003eAuthors:Mark Hamilton ${ }^{1,2}$, Scott Lundberg ${ }^{2}$, Stephanie Fu ${ }^{1}$, Lei Zhang ${ }^{2}$, William T. Freeman ${ }^{1,3}$\u003cbr\u003e${ }^{1}$ MIT, ${ }^{2}$ Microsoft, ${ }^{3}$ Google\u003cbr\u003emarkth@mit.edu\n\u003cbr/\u003e\n**Authors of the blogpost**: Yassine Beniguemim and Noureddine BOULLAM.\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0.0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-0.1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eExploring Visual Search Algorithm Explanations\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1.1\"\u003eFirst-Order Explanations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.2\"\u003eUnifying First-Order Search Interpretation Techniques\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.3\"\u003eSecond-Order Explanations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.4\"\u003eA Fast Shapley-Taylor Approximation Kernel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.5\"\u003eSecond-Order Search Activation Maps\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eImplementing Second-Order Explanations in Practice\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0.0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eVisual search, recommendation, and contrastive similarity learning are pivotal technologies shaping user experiences in the digital age. However, the complexity of modern model architectures often obscures their inner workings, making them challenging to interpret. In our blog, we delve into a groundbreaking paper titled \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al. This paper introduces a novel framework grounded in the theory of fair credit assignment, providing axiomatic solutions that generalize existing explanation techniques and address fairness concerns in recommendation systems. Through our exploration, we aim to demystify the complexities of visual search algorithms, offering readers insights into their operation and implications for various domains.\u003c/p\u003e\n\u003cdiv style=\"display: inline-block; width:\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-02.jpg?height=600\u0026width=1354\u0026top_left_y=282\u0026top_left_x=382\" alt=\"Figure 5\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eFigure 1: Architectures for search engine interpretability. Like classifier explanations, First-order search explanations yield heatmaps of important pixels for similarity (bottom row third column). Second order search interpretation methods yield a dense correspondence between image locations (last two columns). CAM (second column) is a particular case of Shapley value approximation, and we generalize it to yield dense correspondences (last column).\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"section-0.1\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to our blog, where we embark on a journey to demystify the intricate world of visual search technology. In today\u0026rsquo;s digital age, recommendation systems play a pivotal role in guiding users through a vast sea of information, aiding in everything from online shopping to content discovery.\u003c/p\u003e\n\u003cp\u003eYet, behind the scenes, these recommendation engines operate using sophisticated algorithms that can seem like a black box to many users. How do they decide which products to suggest, or which images are most similar to a given query? These questions lie at the heart of our exploration.\u003c/p\u003e\n\u003cp\u003eInspired by the groundbreaking paper \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al., we embark on a quest to unravel the inner workings of these recommendation systems. By delving into the concepts of fair credit assignment, Shapley values, and Harsanyi Dividends, we aim to shed light on the underlying principles that govern visual search algorithms.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003e1. Exploring Visual Search Algorithm Explanations\u003c/h2\u003e\n\u003cp\u003eIn our exploration of visual search algorithm explanations, we delve into the fundamental concepts introduced in the paper by Mark Hamilton et al. Our journey begins with an examination of the two distinct classes of explanation methods: \u0026ldquo;first order\u0026rdquo; and \u0026ldquo;second order.\u0026rdquo; First-order approaches focus on highlighting important pixels contributing to object similarity, while second-order explanations provide a comprehensive correspondence between query and retrieved images.\u003c/p\u003e\n\u003ch3 id=\"section-1.1\"\u003e1.1 First-Order Explanations\u003c/h3\u003e\n\u003cp\u003eFirst-order interpretations are rooted in classifier explainability theory, offering insights into the importance of individual pixels or features in determining object similarity. We explore the theoretical underpinnings of these explanations, drawing parallels to existing techniques such as Class Activation Maps (CAM), GradCAM, and LIME.\u003c/p\u003e\n\u003ch4 id=\"formalizing-first-order-interpretations\"\u003eFormalizing First-Order Interpretations\u003c/h4\u003e\n\u003cp\u003eThe core of first-order explanations lies in the formalization of the value function, typically represented as $v_1(S)$, where $S$ represents subsets of features or pixels. This function allows us to quantify the contribution of each subset to the overall similarity score between query and retrieved images.\u003c/p\u003e\n\u003cp\u003e$$\nv_1(S): 2^N \\rightarrow \\mathbb{R} := d(x, \\text{mask}(y, S))\n$$\u003c/p\u003e\n\u003ch3 id=\"section-1.2\"\u003e1.2 Unifying First-Order Search Interpretation Techniques\u003c/h3\u003e\n\u003cp\u003eBuilding upon existing classifier explainability methods, we introduce an approach to transform opaque and grey-box classification explainers into search engine explainers. By formalizing the value function and leveraging concepts like Shapley values, we extend existing approaches such as LIME and SHAP to the realm of visual search.\u003c/p\u003e\n\u003ch4 id=\"leveraging-shapley-values\"\u003eLeveraging Shapley Values\u003c/h4\u003e\n\u003cp\u003eShapley values provide a principled way to assign credit to individual features or pixels based on their contribution to the similarity function. By applying Shapley values to the search engine context, we can identify the most influential elements in both query and retrieved images.\u003c/p\u003e\n\u003cp\u003e$$\n\\phi_{v_1}(S) = \\sum_{T: S \\subset T} \\frac{d_v(T)}{\\binom{|T|}{|S|}}\n$$\u003c/p\u003e\n\u003ch3 id=\"section-1.3\"\u003e1.3 Second-Order Explanations\u003c/h3\u003e\n\u003cp\u003eMoving beyond pixel-level interpretations, we delve into second-order explanations that capture the interactions between areas of query and retrieved images. Drawing inspiration from Harsanyi Dividends and Shapley-Taylor indices, we explore how these concepts generalize to provide richer insights into image similarity.\u003c/p\u003e\n\u003ch4 id=\"understanding-second-order-interpretations\"\u003eUnderstanding Second-Order Interpretations\u003c/h4\u003e\n\u003cp\u003eSecond-order explanations go beyond individual features to capture the interaction strength between different parts of query and retrieved images. We introduce the concept of Harsanyi Dividends, which provide a detailed view of the function\u0026rsquo;s behavior at every coalition of features.\u003c/p\u003e\n\u003cp\u003e$$\nd_v(S) = \\begin{cases} v(S) \u0026amp; \\text{if } |S|=1 \\\nv(S) - \\sum_{T \\subsetneq S} d_v(T) \u0026amp; \\text{if } |S| \u0026gt; 1 \\end{cases}\n$$\u003c/p\u003e\n\u003ch3 id=\"section-1.4\"\u003e1.4 A Fast Shapley-Taylor Approximation Kernel\u003c/h3\u003e\n\u003cp\u003eWhile Harsanyi Dividends and Shapley-Taylor indices offer robust credit assignment mechanisms, their computation can be challenging. We introduce a novel weighting kernel for second-order Shapley-Taylor indices, significantly reducing computational complexity while maintaining accuracy.\u003c/p\u003e\n\u003cdiv style=\"display: inline-block; width: 45%;\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-07.jpg?height=455\u0026width=610\u0026top_left_y=282\u0026top_left_x=405\" alt=\"Figure 5\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eFigure 5: Convergence of Shapley-Taylor estimation schemes with respect to the Mean Squared Error (MSE) on randomly initialized deep networks with 15 dimensional input. Our strategies (Kernel) converge with significantly fewer function evaluations.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv style=\"display: inline-block; width: 45%;\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-07.jpg?height=455\u0026width=656\u0026top_left_y=282\u0026top_left_x=1079\" alt=\"Figure 6\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eFigure 6: Our Second-order explanation evaluation strategy. A good method should project query objects (top left and middle) to corresponding objects in the retrieved image (bottom left and middle). When censoring all but these shared objects (right column) the search engine should view these images as similar.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch4 id=\"efficient-computation-with-kernel-approximation\"\u003eEfficient Computation with Kernel Approximation\u003c/h4\u003e\n\u003cp\u003eThe proposed weighting kernel allows for efficient approximation of Shapley-Taylor indices, enabling faster computation without sacrificing accuracy. By sampling random coalitions and aggregating information into a weighted quadratic model, we achieve a significant reduction in computational overhead.\u003c/p\u003e\n\u003ch3 id=\"section-1.5\"\u003e1.5 Second-Order Search Activation Maps\u003c/h3\u003e\n\u003cp\u003eApplying the Shapley-Taylor framework, we derive second-order search activation maps, offering dense correspondences between query and retrieved image locations. These maps provide a deeper understanding of image similarity, facilitating more nuanced interpretations of visual search results.\u003c/p\u003e\n\u003ch4 id=\"visualizing-second-order-explanations\"\u003eVisualizing Second-Order Explanations\u003c/h4\u003e\n\u003cp\u003eUsing the derived Shapley-Taylor indices, we construct matrices representing the interaction strength between query and retrieved image locations. These matrices allow us to visualize how different parts of the query image correspond to parts of the retrieved image, providing intuitive insights into the similarity judgments made by the search algorithm.\u003c/p\u003e\n\u003cdiv style=\"display: inline-block; width:\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-08.jpg?height=1003\u0026width=1312\u0026top_left_y=377\u0026top_left_x=404\" alt=\"Figure 5\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eTable 1: Comparison of performance of first- and second-order search explanation methods.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"section-2\"\u003e2. Implementing Second-Order Explanations in Practice\u003c/h2\u003e\n\u003cp\u003eWith a solid theoretical foundation in place, we now turn our attention to practical implementation steps for incorporating second-order explanations into existing visual search systems.\u003c/p\u003e\n\u003ch3 id=\"section-2.1\"\u003e2.1 Data Preparation and Preprocessing\u003c/h3\u003e\n\u003cp\u003eBefore integrating second-order explanations, it\u0026rsquo;s crucial to preprocess and structure the data appropriately. This involves organizing the dataset into query-image pairs, ensuring consistency in image format, resolution, and preprocessing steps such as normalization and resizing.\u003c/p\u003e\n\u003ch4 id=\"data-pipeline-overview\"\u003eData Pipeline Overview\u003c/h4\u003e\n\u003cp\u003eWe design a robust data pipeline encompassing data loading, preprocessing, and augmentation stages. Leveraging popular libraries like TensorFlow and PyTorch, we streamline the process of preparing the dataset for training and evaluation.\u003c/p\u003e\n\u003ch3 id=\"section-2.2\"\u003e2.2 Model Modification and Integration\u003c/h3\u003e\n\u003cp\u003eTo enable the computation of second-order explanations, we modify the existing visual search model architecture. This adaptation involves incorporating additional layers or modules to capture the interactions between query and retrieved images.\u003c/p\u003e\n\u003ch4 id=\"architectural-adjustments\"\u003eArchitectural Adjustments\u003c/h4\u003e\n\u003cp\u003eWe introduce novel components such as interaction modules or attention mechanisms to facilitate the computation of second-order explanations. These architectural adjustments enable the model to learn and represent the complex relationships between different regions of query and retrieved images.\u003c/p\u003e\n\u003ch3 id=\"section-2.3\"\u003e2.3 Training and Evaluation Procedures\u003c/h3\u003e\n\u003cp\u003eTraining a visual search model with second-order explanations requires careful consideration of training objectives, loss functions, and evaluation metrics. We devise training procedures that optimize both the primary search task and the secondary objective of generating accurate explanations.\u003c/p\u003e\n\u003ch4 id=\"objective-function-formulation\"\u003eObjective Function Formulation\u003c/h4\u003e\n\u003cp\u003eWe define a composite objective function that combines the primary search task loss with a regularization term for encouraging meaningful second-order explanations. This formulation ensures that the model learns to balance between search accuracy and explanation fidelity during training.\u003c/p\u003e\n\u003ch3 id=\"section-2.4\"\u003e2.4 Validation and Interpretation\u003c/h3\u003e\n\u003cp\u003eOnce trained, we validate the effectiveness of the model\u0026rsquo;s second-order explanations through comprehensive evaluation procedures. This involves qualitative analysis of explanation maps, quantitative assessment of explanation quality, and user studies to gauge the interpretability of the generated explanations.\u003c/p\u003e\n\u003ch4 id=\"evaluation-metrics\"\u003eEvaluation Metrics\u003c/h4\u003e\n\u003cp\u003eWe define metrics such as explanation fidelity, coherence, and relevance to quantitatively evaluate the quality of second-order explanations. By comparing against baseline methods and human annotations, we assess the model\u0026rsquo;s ability to capture meaningful interactions between query and retrieved images.\u003c/p\u003e\n\u003ch3 id=\"section-2.5\"\u003e2.5 Deployment Considerations\u003c/h3\u003e\n\u003cp\u003eDeploying a visual search system with second-order explanations requires careful planning and integration into existing infrastructure. We address scalability, latency, and user experience considerations to ensure seamless deployment in real-world applications.\u003c/p\u003e\n\u003ch4 id=\"scalable-inference-architecture\"\u003eScalable Inference Architecture\u003c/h4\u003e\n\u003cp\u003eWe design an inference pipeline optimized for efficient computation of second-order explanations in production environments. This involves leveraging distributed computing frameworks and model optimization techniques to minimize latency and maximize throughput.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e3. Conclusion\u003c/h2\u003e\n\u003cp\u003eBy following these implementation steps, we bridge the gap between theoretical insights and practical deployment of second-order explanations in visual search systems. Our approach empowers users to gain deeper insights into the underlying mechanisms driving search results, paving the way for more transparent and interpretable AI systems.\u003c/p\u003e\n\u003ch2 id=\"additional-resources\"\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eVideo Description\u003c/strong\u003e: Dive deeper into the concepts with a detailed video overview available \u003ca href=\"https://aka.ms/axiomatic-video\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCode Repository\u003c/strong\u003e: Access the training and evaluation code to explore the implementation details \u003ca href=\"https://aka.ms/axiomatic-code\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor a comprehensive exploration of the technical details and experimental results, refer to the \u003ca href=\"https://arxiv.org/pdf/2103.00370.pdf\"\u003efull paper\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"references\"\u003eREFERENCES\u003c/h2\u003e\n\u003cp\u003eRadhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels. Technical report, 2010.\u003c/p\u003e\n\u003cp\u003eJiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation with inter-pixel relations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2209-2218, 2019.\u003c/p\u003e\n\u003cp\u003eMarco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. In International Conference on Machine Learning, pp. 272-281. PMLR, 2019.\u003c/p\u003e\n\u003cp\u003eRobert J Aumann and Lloyd S Shapley. Values of non-atomic games. Princeton University Press, 2015.\u003c/p\u003e\n\u003cp\u003eSebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\u003c/p\u003e\n\u003cp\u003eYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.\u003c/p\u003e\n\u003cp\u003eBing. Beyond text queries: Searching with bing visual search, Jun 2017. URL https://aka. ms/AAas 7 jg.\u003c/p\u003e\n\u003cp\u003eHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 12091218, 2018.\u003c/p\u003e\n\u003cp\u003eMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.\u003c/p\u003e\n\u003cp\u003eHila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 782-791, 2021.\u003c/p\u003e\n\u003cp\u003eTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.\u003c/p\u003e\n\u003cp\u003eXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020 b.\u003c/p\u003e\n\u003cp\u003eYun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang. Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020c.\u003c/p\u003e\n\u003chr\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n",
      "url": "http://localhost:1313/posts/axiomatic_explanations/",
      "date_published": "28036-28-09T358:2828:00+01:00",
      "date_modified": "28036-28-09T358:2828:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "6f60dbb8442851a1134c3a218d91373cc4dfd1a8",
      "title": "Privacy Amplification by Decentralization",
      "summary": "",
      "content_text": "Privacy Amplification by Decentralization Author: Sarah ABBANA BENNANI Table of Contents Introduction - the challenge of data privacy Theoretical Aspects on Differential Privacy First Case: walk on a ring Generalisation: walk on a complete graph Experiments Perspectives This is a blogpost about the paper Privacy Amplification by Decentralization, published by E. Cyffers et al. in 2022 and available here.\nIntroduction - the challenge of data privacy In recent years, the concept of privacy has gained significant attention due to the proliferation of data collection practices and the need to safeguard individuals\u0026rsquo; personal information. There has been a notable shift towards implementing regulations to govern the gathering of data from individuals, underscoring the pressing demand for privacy measures that are not only effective and robust against potential attacks but also transparent and firmly grounded in logic and mathematics.\nA current way to define privacy in the context of data sharing is the promess of the dataholder (the person or entity managing the data) towards the users, that there will be no consequences (positive or negative) induced by their consent to sharing their data.\nLet us take a small example to illustrate and to understand the underlying complexity of this notion: we consider an entity that desires to conduct a study on the correlation between smoking and cancer risks. Should a smoker participate, and the study concludes that smoking indeed increases the likelihood of cancer, the repercussions for the smoker could vary.\nSome negative impacts: insurance premiums could increase\nSome positive impacts: motivation to quit smoking\nWe could therefore think that privacy in this case is broken for the participant, however there is a subtility which is one of the keys to capture the nuance between the privacy of an individiual and that of a group. In this case, crucially, we cannot say privacy is breached, as the participation of the smoker should not alter the study\u0026rsquo;s outcome, i.e. from a probabilistic standpoint, whether or not the individual participates in the study will not significantly change the likelihood of the conclusion of the study. Formally, and to introduce some probabilities, which we will delve into further later on:\n$\\mathbb{P}(result=smoking\\ bad | individual\\ participates) \\approx \\mathbb{P}(result=smoking\\ bad | individual\\ does\\ not\\ participate) $ Privacy has become a real challenge for all parties, as it is necesssary to find a balance between the utility of the data and the privacy guarantees of the users.\nFor the dataholder, the aim is to retain the wealth of data to derive useful insights. They must be able to analyse enough of the data to learn about the population without revealing any individual-specific information. For the users, they must believe that their data will be protected and that they will not be hurt by giving them. This trust in the dataholder is important to incite the users to give their data. In this paper, the aim of the authors, E. Cyffers and A. Bellet, was to show some algorithms and methods that allow to improve the privacy-utility trade-offs and therefore reinforce privacy around the data, while keeping scalability.\nThe proposed algorithms are based on full decentralization, and newtork differential privacy (DP), two notions that we will explain right below.\nTheoretical Aspects on Differential Privacy Mathematical context We must introduce some key mathematical definitions to understand the problem we want to tackle.\nUsers space We consider a set of $n$ users (e.g. a population responding to a survey), each holding a private dataset that we note $D_u$ (e.g. their answer to the questions of the survey).\nNeighboring relation We write $D=D_1 \\cup \\cdots \\cup D_n$ the union of all users datasets.\nWe can define a neighboring relation over these datasets, that we call user-level Differential Privacy: For two datasets $D$ and $D\u0026rsquo;$ of the same size, we denote by $D \\sim_u D^{\\prime}$ the fact that $D$ and $D\u0026rsquo;$ are neighbors, in the sense that they only differ on user $u$\u0026rsquo;s data.\nFor example, $D$ and $D\u0026rsquo;$ could be two datasets corresponding to the answers of a survey from 10 users. For nine of these users the answers are the same for the two datasets. But for one user $u$, the answers are different (e.g. in $D$ user $u$ smokes, in $D\u0026rsquo;$ he doesn\u0026rsquo;t smoke).\nThe inuition between this definition relatively to privacy is that compared to traditional differential privacy, which considers changes in individual data points, user-level DP provides stronger privacy guarantees. By hiding the influence of an entire user\u0026rsquo;s dataset, rather than just a single data point, it ensures that individual user contributions are not discernible, thus enhancing overall privacy protection.\nDecentralization We will set ourselves in a fully decentralized system. In this configuration, each user only communicates with a small number of other users at each step, and there is no central coordinator processing all the data. The aim of this setting is to limit the concentration of sensitive information in one place, reducing the risk of data breaches and unauthorized access.\nThe users and their communications are represented by a network (directed or undirected) graph $G = (V, E)$, where $V$ is the users ensemble defined above, and $E$ is the set of edges: $(u, v) \\in E$ indicates that user $u$ can send messages to user $v$.\nIn this case, a randomised decentralized algorithm is defined as a mapping that from a dataset, returns that transcript of all messages exchanges between the users over the network. In formal terms, $A: D \\longmapsto {(u, m, v): u \\text{ sent message with content } m \\text{ to } v }$.\nThe aim of decentralization in this representation, is to give users the fewer information possible, i.e. only the messages they are involved in, and not the full transcript $A(D)$.\nWe introduce this view of a user $u$: $\\mathcal{O}_u(\\mathcal{A}(D))=\\left(\\left(v, m, v^{\\prime}\\right) \\in \\mathcal{A}(D): v=u \\text { or } v^{\\prime}=u\\right)$\nDifferential Privacy We will take a step back on this representation to introduce in a more global way the mathematical notion of Differential Privacy (DP).\nLet us consider a randomised algorithm $M$. $M$ is said to be \u0026ldquo;$\\alpha$-differentially private\u0026rdquo; if, for any event $A$:\n$$\\mathbb{P}[M(D)\\in A]\\leq e^{\\alpha} \\cdot \\mathbb{P}[M(D\u0026rsquo;)\\in A]$$\nwhere $D$ and $D\u0026rsquo;$ are two datasets differing on a single element.\nTo make this more intuitive, a randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm must treat the data so that the output is not overly depend on the data of any one individual.\nLet\u0026rsquo;s consider the event \u0026ldquo;Smoking is correlated to cancer\u0026rdquo;, and $D$ and $D\u0026rsquo;$ differing on the user $u$\u0026rsquo;s data, whether or not that individual that has cancer smokes or not.\nWe can rewrite the definition as: $\\frac{\\mathbb{P}\\left[M\\left(D\\right) \\in A\\right]}{\\mathbb{P}\\left[M\\left(D\u0026rsquo;\\right) \\in A\\right]} \\leq e^{\\alpha}$\nWe can see that $\\alpha$, the privacy factor, represents the lost of privacy:\nWhen $\\alpha \\rightarrow 0$: the two probabilities are equal, meaning that whether user $u$ participates or not to the survey, the result is the same, i.e. privacy is at its maximum, but the statistical utility is null.\nWhen $\\alpha \\rightarrow+\\infty$: there are no constraints on the probabilities and therefore no constraints on privacy.\nThus it is the intermediate case for $\\alpha$ that is the most interesting and that can allow a good trade-off between privacy and utility.\nNetwork Differential Privacy In this paper, the definition used for Differential Privacy is a bit different, actually relaxed as the algorithm is decentralized.\nAn algorithm $A$ is said to be $(\\varepsilon, \\delta)$-network Differentially Private if for all pairs of distinct user $u, v \\in V$ and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\n$$ \\mathbb{P}\\left(\\mathcal{O}_v(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_v\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\nWe can interpret this as the need that the information gathered by $v$ during the execution of $A$ must not depend too much on $u$\u0026rsquo;s data.\nFurthermore, the definition can be extended in the case of collusion between the users, i.e. if multiple individuals collaborate or conspire to exploit or manipulate a system or process for their collective benefit.\nAn algorithm $A$ is $(c, \\varepsilon, \\delta)$-network DP if for each user $u$, all subsets $W \\subset V$ such that $\\left|W\\right| \\leq c$, and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\n$$ \\mathbb{P}\\left(\\mathcal{O}_W(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_W\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\nHere $\\mathcal{O}_W$ represents the aggregated information of the collusion: $\\mathcal{O}_W = \\cup _{w \\in W} \\mathcal{O}_w$.\nDecentralized computation model The algorithms studied in this paper are meant to perform computations by using a token that will walk through the nodes of the network graph. The purpose of the token is to facilitate sequential updates across the nodes in the network. As it traverses through the nodes following the edges of the graph, it carries information and updates its states based on local computations performed at each node from the values obtainable from the corresponding user.\nIf the token $\\tau$ resides at some node $u$, it will be:\nUpdated by: $\\tau \\leftarrow \\tau+x_u^k, \\quad$ with $x_u^k=g^k\\left(\\tau ; D_u\\right)$\nSent to another user $v$ with $(u, v) \\in E$\nHere, $x_u^k$ denotes the contribution of user $u$ to the computation. It depends both on the current value of $\\tau$ and on the number of times $k$ that the token visited $u$ so far.\nThis model of computation allows to optimize the combination of local costs within the network, which is useful for tasks like training machine learning models. The token holds the model\u0026rsquo;s parameters and is updated based on the local information at each point it visits. This decentralized approach can also be used to calculate summaries of data contributed by users, such as finding totals or averages.\nThe idea of the following parts is to study different graph achitectures and computation protocols, based on the formalization explained above, to achieve good utility-privacy trade-offs\nFirst case: walk on a ring We consider here a graph architecture of a directed ring, i.e. $E = {(u, u+1)}_{u=1}^{n-1} \\cup{(n, 1)}$, meaning that the token, starting from the first user, will travel around the ring multiple times, and more precisely go through every user $K$ times.\nThis is a simple case that is meant to show how we can achieve suitable results without relying on a centralised agregator.\nWe are going to explain how this architecture can perform for privacy guarantees on the task of Real Summation, and then on Discrete Histogram Computation.\nReal Summation Each user will contribute a value during each round of the token\u0026rsquo;s journey. The task of real summation aims to estimate the sum of all contributions made by users.\nFor example, we can imagine a scenario where users of a health monitoring app report their daily step counts. The app\u0026rsquo;s goal is to calculate the total number of steps taken by all users, without revealing individual step counts. Each user\u0026rsquo;s daily step count is considered a contribution, and the app needs to aggregate these contributions while preserving user privacy.\nIndeed to preserve privacy in this case, a common method is to add random noise, an abstract perturbation mechanism, which usually consist in a standard Gaussian or Laplace deviation to the contribution. We won\u0026rsquo;t go into further details on the perturbation, but we assume that it satisfies traditional local differntial privacy (LDP).\nFurthermore, here the decentralized protocol proposes to add this noise only once every few hops of the token, and in fact every $n-1$ hops of the token as shown in the algorithm below:\nThey prove the following theorem:\nTheorem: Let $\\varepsilon, \\delta\u0026gt;0$. Algorithm 1 outputs an unbiased estimate of $\\bar{x}$ with standard deviation $\\sqrt{\\left\\lfloor \\frac{Kn}{n - 1} \\right\\rfloor} \\sigma_{\\text{loc}}$, and is $\\sqrt{2K \\ln\\left(\\frac{1}{\\delta\u0026rsquo;}\\right)\\varepsilon}$ $+ K\\epsilon(e^\\varepsilon - 1), K\\delta + \\delta\u0026rsquo;$-network DP for any $\\delta\u0026rsquo; \u0026gt; 0$\nThe Algorithm 1 proposed actually provides a gain on the error of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ compared to a LDP achieving the same privacy guarantees. This means it achieves a similar balance between privacy and utility as a centralized aggregator would, if they itratively aggregated user contributions then perturb the results before sending it to the users, buy here without the need for this centralized party.\nDiscrete Histogram Computation Here we focus on another task that is computing histograms over a discrete domain.\nWith the same example as above, it could be such as counting the frequency of steps in different ranges for a health monitoring app.\nTraditional local differential privacy (LDP) methods use L-ary randomized response, where each user submits their true value with probability $1-\\gamma$ and a random value with probability $\\gamma$. However, in the decentralized approach with a ring network, they propose Algorithm 2. This algorithm randomizes each user\u0026rsquo;s contribution using L-ary randomized response before adding it to the token, which maintains a partial histogram representing the shuffled contributions, thus enhancing privacy through shuffling, as demonstrated in previous studies.\nAs for the case of real summation, a theorem proves that to achieve the same privacy in LDP, it would need $\\sqrt n$ times more random responses, and when achieving the same utility (meaning to fix $\\gamma$), Algorithm 2 provides a gain of privacy of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$.\nWe see that decentralized computation over a ring enables comparable utility to a trusted aggregator by sequentially hiding previous users\u0026rsquo; contributions, without relying on a central server or requiring costly multi-party computation protocols.\nHowever this simple topology presents limitations including vulnerability to collusions, which compromises differential privacy guarantees, and inadequacy for extensions to gradient descent due to the lack of privacy amplification between users with fixed positions in the ring.\nThis is why we shall now consider random walks over a complete graph.\nGeneralisation: walk on a complete graph Random walk on a complete graph assumes the token is randommly sent to a user at each step. The walk consists of fixed-length random walks, ensuring that each user\u0026rsquo;s contributions are random, and their path is concealed, allowing only the messages sent and received to be known by a user.\nReal Summation Algorithm 3 shows the protocol, naturally extended from the ring topology, where each user updates the token with its contribution and a perturbation. The secrecy of the path taken by the token and the aggregations of the contributions between two visits of the token guarantee the network DP property.\nAgain, a theorem proves that asymptotically, network DP offers a privacy amplification of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ over LDP for the same conditions, which aligns with the privacy-utility trade-off of a trusted aggregtor.\nThe same analysis can be done for the discrete histogram computation case.\nStochastic Gradient Descent In this section, we address the challenge of private convex optimization using stochastic gradient descent (SGD). We consider a convex set $(W \\subseteq \\mathbb{R}^d)$ and a collection of convex functions $(f(\\cdot; D_1), \\ldots, f(\\cdot; D_n))$, each associated with a user, being L-Lipschitz and $(\\beta)$-smooth over $(W)$. Our goal is to privately solve the optimization problem to find $(w^*)$ minimizing the average of these functions over $(W)$:\n$$w^* \\in \\arg \\min_{w \\in \\mathcal{W}} \\left( F(w):=\\frac{1}{n} \\sum_{u=1}^n f\\left(w ; D_u\\right) \\right)$$\nThis equation encapsulates various machine learning tasks, such as ridge and logistic regression, and others. This is significant because it addresses the need for private optimization in machine learning, ensuring that sensitive data remains protected while training models on distributed datasets.\nThe algorithm below proposes a method to privately approximate $w^*$, where the token represents the current iterate. At each step, the user $u$ holding the token performs a projected noisy gradient step and sends the updated token to a random user. The variance in the Gaussian mechanism of line 4 is deduced from the Lipschitz property of the functions.\nA theorem based on the evolution of the privacy loss proves the differential privacy guarantees, and again the results are satisfactory. Compared to traditional local differential privacy methods, we obtain a privacy amplification of $O\\left(\\frac{\\ln n}{\\sqrt{n}}\\right)$ for a specific number of iterations, with the same level of privacy-utility trade-off.\nWith a fixed privacy budget and a large number of iteration, the expected error of this algorithm is smaller with this network DP than with LDP.\nCompared to the ring case, this random walk approach has better robustness to collusion, as colluding users can be treated as a single node with adjusted transition probabilities, leading to equivalent privacy guarantees as for non-colluding users.\nExperiments To show the efficiency of the privacy amplification methods explained in this article, some experiments have been made on the complete graph, first for the Real Summation task, and then for Machine Learning with Stochastic Gradient Descent (SGD).\nThe code is available here: Github Link\nReal Summation We reproduced the first experiment from the paper, comparing th analytical bounds of LDP and NDP on the real summation task.\nTo do so, we only need to run the main_a.py and main_b.py files with python from the fig1 folder to display the corresponding figures (a) and (b). It works, for instance, with Python version 3.8, with the prerequisite of having installed the packages numpy and matplotlib, only taking a few seconds to execute.\nIt gives the following results:\nAs we may see from the theoretical bounds, privacy is amplified with network differential privacy over LDP when the numer of users $n$ is greater or equal to 20, with increaingly substancial improvements as $n$ grows.\nIn practice by making some simulations, the gains are even more significant and even for a smaller number of users, as we see in figure (b).\nMachine Learning with SGD For this second experiment, the task is to train a logistic regression model in this decentralized context.\nThe setting of the experiment is:\nUCI Housing dataset (binarized version) Standardized features and normalized data point (to have unit L2 norm and Lipschitz property of the logistic loss) Train/test split of 80% uniformly at random Training set split between $n = 2000$ users (each user has a local dataset of size $8$) The experiment compares three settings for Stochastic Gradient Descent with perturbation:\nCentralized DP-SGD, requiring a trusted curator Local DP-SGD, corresponding to Algorithm 4 with LDP method Network DP-SGD, corresponding to Algorithm 4 with Network DP method, the one of interest We must run the main.py file of folder fig2 with Python to display the results.\nIt is possible to use the command python main.py \u0026ndash;help to show the list of parameters that can be tuned to modify the context of the experiment (the default ones are for $\\varepsilon = 10$ and $\\varepsilon = 1$):\nI had some issues to run this program with my settings (same as for the first experiment).\nThe typer module was missing therefore I had to install it : pip install typer The _intercept_dot function from sklearn.linear_model._logistic couldn\u0026rsquo;t be found either. By checking the sklearn.linear_model.LogisticRegression (which is the public class corresponding to the import here), this function doesn\u0026rsquo;t appear. I wanted to change it with the intercept_ attribute but it didn\u0026rsquo;t fit either. Then by checking the usage of this function in the case, it seemed that it computes a dot product between the model parameters and the input data, taking into account whether an intercept term is included. Therefore I tried to manually code this functionality but unfortunately it didn\u0026rsquo;t give coherent results compared to the paper. Here are the original results from the paper:\nHere, although the number of contributions per user doesn\u0026rsquo;t align with the optimal regime for network DP, the observed privacy amplification surpasses theoretical expectations. By numerically determining the minimum noise level required for theoretical proofs, they demonstrated that Network DP-SGD achieves a privacy-utility trade-off comparable to Centralized DP-SGD across various privacy levels, showcasing significant privacy amplification benefits over Local DP-SGD, especially in scenarios with fewer iterations than typically recommended.\nPerspectives The work presented suggests numerous avenues for exploration. Generalizations to diverse graph structures, incorporating dynamic topologies to reinforce resilience against collusion, and investigating decentralized models beyond our current scope are key directions. Exploring the potential of multiple tokens traversing the graph simultaneously and delving into randomized gossip algorithms offer promising avenues for advancing privacy-preserving techniques. Finally, probing the theoretical limits of network DP and exploring scenarios where users trust nearby peers more could provide insights into refining privacy mechanisms.\n",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003ePrivacy Amplification by Decentralization\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Sarah ABBANA BENNANI \u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction - the challenge of data privacy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eTheoretical Aspects on Differential Privacy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eFirst Case: walk on a ring\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eGeneralisation: walk on a complete graph\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eExperiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003ePerspectives\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr /\u003e\n\u003cp\u003eThis is a blogpost about the paper  Privacy Amplification by Decentralization, published by E. Cyffers et al. in 2022 and available \u003ca href=\"https://proceedings.mlr.press/v151/cyffers22a/cyffers22a.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"section-1\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eIntroduction - the challenge of data privacy\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eIn recent years, the concept of privacy has gained significant attention due to the proliferation of data collection practices and the need to safeguard individuals\u0026rsquo; personal information. \u003cbr\u003e\nThere has been a notable shift towards implementing regulations to govern the gathering of data from individuals, underscoring the pressing demand for privacy measures that are not only effective and robust against potential attacks but also transparent and firmly grounded in logic and mathematics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA current way to define privacy in the context of data sharing is the promess of the dataholder (the person or entity managing the data) towards the users, that there will be no consequences (positive or negative) induced by their consent to sharing their data.\u003c/strong\u003e\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003e\u003cem\u003eLet us take a small example to illustrate and to understand the underlying complexity of this notion: we consider an entity that desires to conduct a study on the correlation between smoking and cancer risks. \u003cbr\u003e\nShould a smoker participate, and the study concludes that smoking indeed increases the likelihood of cancer, the repercussions for the smoker could vary.\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eSome negative impacts: insurance premiums could increase\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eSome positive impacts: motivation to quit smoking\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eWe could therefore think that privacy in this case is broken for the participant, however there is a subtility which is one of the keys to capture the nuance between the privacy of an individiual and that of a group. In this case, crucially, we cannot say privacy is breached, as the participation of the smoker should not alter the study\u0026rsquo;s outcome, i.e. from a probabilistic standpoint, whether or not the individual participates in the study will not significantly change the likelihood of the conclusion of the study. \u003cbr\u003e\nFormally, and to introduce some probabilities, which we will delve into further later on:\u003c/em\u003e\u003c/p\u003e\n\u003ch1 style=\"font-size: 13px;\"\u003e$\\mathbb{P}(result=smoking\\ bad | individual\\ participates) \\approx \\mathbb{P}(result=smoking\\ bad | individual\\ does\\ not\\ participate) $\u003c/h1\u003e\n\u003cbr /\u003e \n\u003cp\u003ePrivacy has become a real challenge for all parties, as \u003cstrong\u003eit is necesssary to find a balance between the utility of the data and the privacy guarantees of the users\u003c/strong\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor the dataholder, the aim is to retain the wealth of data to derive useful insights. They must be able to analyse enough of the data to learn about the population without revealing any individual-specific information.\u003c/li\u003e\n\u003cli\u003eFor the users, they must believe that their data will be protected and that they will not be hurt by giving them. This trust in the dataholder is important to incite the users to give their data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr /\u003e \n\u003cp\u003eIn this paper, the aim of the authors, E. Cyffers and A. Bellet, was to show some algorithms and methods that allow to improve the privacy-utility trade-offs and therefore reinforce privacy around the data, while keeping scalability.\u003c/p\u003e\n\u003cp\u003eThe proposed algorithms are based on \u003cstrong\u003efull decentralization\u003c/strong\u003e, and \u003cstrong\u003enewtork differential privacy (DP)\u003c/strong\u003e, two notions that we will explain right below.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch1 id=\"section-2\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eTheoretical Aspects on Differential Privacy\u003c/h1\u003e\u003c/h1\u003e\n\u003ch2 id=\"h1-stylefont-size-18pxmathematical-contexth1\"\u003e\u003ch1 style=\"font-size: 18px;\"\u003eMathematical context\u003c/h1\u003e\u003c/h2\u003e\n\u003cp\u003eWe must introduce some key mathematical definitions to understand the problem we want to tackle.\u003c/p\u003e\n\u003ch2 id=\"users-space\"\u003eUsers space\u003c/h2\u003e\n\u003cp\u003eWe consider a set of $n$ users (e.g. a population responding to a survey), each holding a private dataset that we note $D_u$ (e.g. their answer to the questions of the survey).\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/users-space.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch2 id=\"neighboring-relation\"\u003eNeighboring relation\u003c/h2\u003e\n\u003cp\u003eWe write $D=D_1 \\cup \\cdots \\cup D_n$ the union of all users datasets.\u003c/p\u003e\n\u003cp\u003eWe can define a \u003cstrong\u003eneighboring relation\u003c/strong\u003e over these datasets, that we call user-level Differential Privacy: \u003cbr\u003e\nFor two datasets $D$ and $D\u0026rsquo;$ of the same size, we denote by $D \\sim_u D^{\\prime}$ the fact that $D$ and $D\u0026rsquo;$ are neighbors, in the sense that they only differ on user $u$\u0026rsquo;s data.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFor example, $D$ and $D\u0026rsquo;$ could be two datasets corresponding to the answers of a survey from 10 users. For nine of these users the answers are the same for the two datasets. But for one user $u$, the answers are different (e.g. in $D$ user $u$ smokes, in $D\u0026rsquo;$ he doesn\u0026rsquo;t smoke).\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe inuition between this definition relatively to privacy is that compared to traditional differential privacy, which considers changes in individual data points, user-level DP provides stronger privacy guarantees. By hiding the influence of an entire user\u0026rsquo;s dataset, rather than just a single data point, it ensures that individual user contributions are not discernible, thus enhancing overall privacy protection.\u003c/p\u003e\n\u003ch2 id=\"decentralization\"\u003eDecentralization\u003c/h2\u003e\n\u003cp\u003eWe will set ourselves in a fully decentralized system. In this configuration, each user only communicates with a small number of other users at each step, and there is no central coordinator processing all the data. The aim of this setting is to limit the concentration of sensitive information in one place, reducing the risk of data breaches and unauthorized access.\u003c/p\u003e\n\u003cp\u003eThe users and their communications are represented by a network (directed or undirected) graph $G = (V, E)$, where $V$ is the users ensemble defined above, and $E$ is the set of edges: $(u, v) \\in E$ indicates that user $u$ can send messages to user $v$.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eIn this case, a randomised decentralized algorithm is defined as a mapping that from a dataset, returns that transcript of all messages exchanges between the users over the network. In formal terms, $A: D \\longmapsto {(u, m, v): u \\text{ sent message with content } m \\text{ to } v }$.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eThe aim of decentralization in this representation, is to give users the fewer information possible, i.e. only the messages they are involved in, and not the full transcript $A(D)$.\u003c/p\u003e\n\u003cp\u003eWe introduce this view of a user $u$: $\\mathcal{O}_u(\\mathcal{A}(D))=\\left(\\left(v, m, v^{\\prime}\\right) \\in \\mathcal{A}(D): v=u \\text { or } v^{\\prime}=u\\right)$\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"h1-stylefont-size-18pxdifferential-privacyh1\"\u003e\u003ch1 style=\"font-size: 18px;\"\u003eDifferential Privacy\u003c/h1\u003e\u003c/h2\u003e\n\u003cp\u003eWe will take a step back on this representation to introduce in a more global way the mathematical notion of Differential Privacy (DP).\u003c/p\u003e\n\u003cp\u003eLet us consider a randomised algorithm $M$. $M$ is said to be \u0026ldquo;$\\alpha$-differentially private\u0026rdquo; if, for any event $A$:\u003c/p\u003e\n\u003cp\u003e$$\\mathbb{P}[M(D)\\in A]\\leq e^{\\alpha} \\cdot \\mathbb{P}[M(D\u0026rsquo;)\\in A]$$\u003c/p\u003e\n\u003cp\u003ewhere $D$ and $D\u0026rsquo;$ are two datasets differing on a single element.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eTo make this more intuitive, a randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm must treat the data so that the output is not overly depend on the data of any one individual.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider the event \u0026ldquo;Smoking is correlated to cancer\u0026rdquo;, and $D$ and $D\u0026rsquo;$ differing on the user $u$\u0026rsquo;s data, whether or not that individual that has cancer smokes or not.\u003c/p\u003e\n\u003cp\u003eWe can rewrite the definition as: $\\frac{\\mathbb{P}\\left[M\\left(D\\right) \\in A\\right]}{\\mathbb{P}\\left[M\\left(D\u0026rsquo;\\right) \\in A\\right]} \\leq e^{\\alpha}$\u003c/p\u003e\n\u003cp\u003eWe can see that $\\alpha$, the privacy factor, represents the lost of privacy:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eWhen $\\alpha \\rightarrow 0$: the two probabilities are equal, meaning that whether user $u$ participates or not to the survey, the result is the same, i.e. privacy is at its maximum, but the statistical utility is null.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhen $\\alpha \\rightarrow+\\infty$: there are no constraints on the probabilities and therefore no constraints on privacy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus it is the intermediate case for $\\alpha$ that is the most interesting and that can allow a good trade-off between privacy and utility.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"network-differential-privacy\"\u003eNetwork Differential Privacy\u003c/h2\u003e\n\u003cp\u003eIn this paper, the definition used for Differential Privacy is a bit different, actually relaxed as the algorithm is decentralized.\u003c/p\u003e\n\u003cp\u003eAn algorithm $A$ is said to be $(\\varepsilon, \\delta)$-network Differentially Private if for all pairs of distinct user $u, v \\in V$ and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\u003c/p\u003e\n\u003cp\u003e$$ \\mathbb{P}\\left(\\mathcal{O}_v(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_v\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\u003c/p\u003e\n\u003cp\u003eWe can interpret this as the need that the information gathered by $v$ during the execution of $A$ must not depend too much on $u$\u0026rsquo;s data.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eFurthermore, the definition can be extended in the case of collusion between the users, i.e. if multiple individuals collaborate or conspire to exploit or manipulate a system or process for their collective benefit.\u003c/p\u003e\n\u003cp\u003eAn algorithm $A$ is $(c, \\varepsilon, \\delta)$-network DP if for each user $u$, all subsets $W \\subset V$ such that $\\left|W\\right| \\leq c$, and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\u003c/p\u003e\n\u003cp\u003e$$ \\mathbb{P}\\left(\\mathcal{O}_W(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_W\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\u003c/p\u003e\n\u003cp\u003eHere $\\mathcal{O}_W$ represents the aggregated information of the collusion: $\\mathcal{O}_W = \\cup _{w \\in W} \\mathcal{O}_w$.\u003c/p\u003e\n \u003cbr /\u003e\n\u003ch2 id=\"decentralized-computation-model\"\u003eDecentralized computation model\u003c/h2\u003e\n\u003cp\u003eThe algorithms studied in this paper are meant to perform computations by using a token that will walk through the nodes of the network graph. The purpose of the token is to facilitate sequential updates across the nodes in the network. As it traverses through the nodes following the edges of the graph, it carries information and updates its states based on local computations performed at each node from the values obtainable from the corresponding user.\u003c/p\u003e\n\u003cp\u003eIf the token $\\tau$ resides at some node $u$, it will be:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUpdated by: $\\tau \\leftarrow \\tau+x_u^k, \\quad$ with $x_u^k=g^k\\left(\\tau ; D_u\\right)$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSent to another user $v$ with $(u, v) \\in E$\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHere, $x_u^k$ denotes the contribution of user $u$ to the computation. It depends both on the current value of $\\tau$ and on the number of times $k$ that the token visited $u$ so far.\u003c/p\u003e\n\u003cp\u003eThis model of computation allows to optimize the combination of local costs within the network, which is useful for tasks like training machine learning models. The token holds the model\u0026rsquo;s parameters and is updated based on the local information at each point it visits. This decentralized approach can also be used to calculate summaries of data contributed by users, such as finding totals or averages.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003e\u003cem\u003eThe idea of the following parts is to study different graph achitectures and computation protocols, based on the formalization explained above, to achieve good utility-privacy trade-offs\u003c/em\u003e\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch1 id=\"section-3\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eFirst case: walk on a ring\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eWe consider here a graph architecture of a directed ring, i.e. $E = {(u, u+1)}_{u=1}^{n-1} \\cup{(n, 1)}$, meaning that the token, starting from the first user, will travel around the ring multiple times, and more precisely go through every user $K$ times.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/walk-on-ring.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis is a simple case that is meant to show how we can achieve suitable results without relying on a centralised agregator.\u003c/p\u003e\n\u003cp\u003eWe are going to explain how this architecture can perform for privacy guarantees on the task of \u003cem\u003eReal Summation\u003c/em\u003e, and then on \u003cem\u003eDiscrete Histogram Computation\u003c/em\u003e.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"real-summation\"\u003eReal Summation\u003c/h2\u003e\n\u003cp\u003eEach user will contribute a value during each round of the token\u0026rsquo;s journey. The task of \u003cem\u003ereal summation\u003c/em\u003e aims to estimate the sum of all contributions made by users.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFor example, we can imagine a scenario where users of a health monitoring app report their daily step counts. The app\u0026rsquo;s goal is to calculate the total number of steps taken by all users, without revealing individual step counts. Each user\u0026rsquo;s daily step count is considered a contribution, and the app needs to aggregate these contributions while preserving user privacy.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIndeed to preserve privacy in this case, a common method is to add random noise, an abstract perturbation mechanism, which usually consist in a standard Gaussian or Laplace deviation to the contribution. We won\u0026rsquo;t go into further details on the perturbation, but we assume that it satisfies traditional local differntial privacy (LDP).\u003c/p\u003e\n\u003cp\u003eFurthermore, here the decentralized protocol proposes to add this noise only once every few hops of the token, and in fact every $n-1$ hops of the token as shown in the algorithm below:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo1.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThey prove the following theorem:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem:\u003c/strong\u003e Let $\\varepsilon, \\delta\u0026gt;0$. Algorithm 1 outputs an unbiased estimate of $\\bar{x}$ with standard deviation $\\sqrt{\\left\\lfloor \\frac{Kn}{n - 1} \\right\\rfloor} \\sigma_{\\text{loc}}$, and is $\\sqrt{2K \\ln\\left(\\frac{1}{\\delta\u0026rsquo;}\\right)\\varepsilon}$ $+ K\\epsilon(e^\\varepsilon - 1), K\\delta + \\delta\u0026rsquo;$-network DP for any $\\delta\u0026rsquo; \u0026gt; 0$\u003c/p\u003e\n\u003cp\u003eThe Algorithm 1 proposed actually provides a gain on the error of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ compared to a LDP achieving the same privacy guarantees. This means it achieves a similar balance between privacy and utility as a centralized aggregator would, if they itratively aggregated user contributions then perturb the results before sending it to the users, buy here without the need for this centralized party.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"discrete-histogram-computation\"\u003eDiscrete Histogram Computation\u003c/h2\u003e\n\u003cp\u003eHere we focus on another task that is computing histograms over a discrete domain.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eWith the same example as above, it could be such as counting the frequency of steps in different ranges for a health monitoring app.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eTraditional local differential privacy (LDP) methods use L-ary randomized response, where each user submits their true value with probability $1-\\gamma$ and a random value with probability $\\gamma$. However, in the decentralized approach with a ring network, they propose Algorithm 2. This algorithm randomizes each user\u0026rsquo;s contribution using L-ary randomized response before adding it to the token, which maintains a partial histogram representing the shuffled contributions, thus enhancing privacy through shuffling, as demonstrated in previous studies.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo2.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs for the case of real summation, a theorem proves that to achieve the same privacy in LDP, it would need $\\sqrt n$ times more random responses, and when achieving the same utility (meaning to fix $\\gamma$), Algorithm 2 provides a gain of privacy of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eWe see that decentralized computation over a ring enables comparable utility to a trusted aggregator by sequentially hiding previous users\u0026rsquo; contributions, without relying on a central server or requiring costly multi-party computation protocols.\u003c/p\u003e\n\u003cp\u003eHowever this simple topology presents limitations including vulnerability to collusions, which compromises differential privacy guarantees, and inadequacy for extensions to gradient descent due to the lack of privacy amplification between users with fixed positions in the ring.\u003c/p\u003e\n\u003cp\u003eThis is why we shall now consider random walks over a complete graph.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch1 id=\"section-4\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eGeneralisation: walk on a complete graph\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eRandom walk on a complete graph assumes the token is randommly sent to a user at each step. The walk consists of fixed-length random walks, ensuring that each user\u0026rsquo;s contributions are random, and their path is concealed, allowing only the messages sent and received to be known by a user.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"real-summation-1\"\u003eReal Summation\u003c/h2\u003e\n\u003cp\u003eAlgorithm 3 shows the protocol, naturally extended from the ring topology, where each user updates the token with its contribution and a perturbation. The secrecy of the path taken by the token and the aggregations of the contributions between two visits of the token guarantee the network DP property.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo3.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAgain, a theorem proves that asymptotically, network DP offers a privacy amplification of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ over LDP for the same conditions, which aligns with the privacy-utility trade-off of a trusted aggregtor.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eThe same analysis can be done for the discrete histogram computation case.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"stochastic-gradient-descent\"\u003eStochastic Gradient Descent\u003c/h2\u003e\n\u003cp\u003eIn this section, we address the challenge of private convex optimization using stochastic gradient descent (SGD). We consider a convex set $(W \\subseteq \\mathbb{R}^d)$ and a collection of convex functions $(f(\\cdot; D_1), \\ldots, f(\\cdot; D_n))$, each associated with a user, being L-Lipschitz and $(\\beta)$-smooth over $(W)$. Our goal is to privately solve the optimization problem to find $(w^*)$ minimizing the average of these functions over $(W)$:\u003c/p\u003e\n\u003cp\u003e$$w^* \\in \\arg \\min_{w \\in \\mathcal{W}} \\left( F(w):=\\frac{1}{n} \\sum_{u=1}^n f\\left(w ; D_u\\right) \\right)$$\u003c/p\u003e\n\u003cp\u003eThis equation encapsulates various machine learning tasks, such as ridge and logistic regression, and others. This is significant because it addresses the need for private optimization in machine learning, ensuring that sensitive data remains protected while training models on distributed datasets.\u003c/p\u003e\n\u003cp\u003eThe algorithm below proposes a method to privately approximate $w^*$, where the token represents the current iterate. At each step, the user $u$ holding the token performs a projected noisy gradient step and sends the updated token to a random user. The variance in the Gaussian mechanism of line 4 is deduced from the Lipschitz property of the functions.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo4.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eA theorem based on the evolution of the privacy loss proves the differential privacy guarantees, and again the results are satisfactory. Compared to traditional local differential privacy methods, we obtain a privacy amplification of $O\\left(\\frac{\\ln n}{\\sqrt{n}}\\right)$ for a specific number of iterations, with the same level of privacy-utility trade-off.\u003c/p\u003e\n\u003cp\u003eWith a fixed privacy budget and a large number of iteration, the expected error of this algorithm is smaller with this network DP than with LDP.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eCompared to the ring case, this random walk approach has better robustness to collusion, as colluding users can be treated as a single node with adjusted transition probabilities, leading to equivalent privacy guarantees as for non-colluding users.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch1 id=\"section-5\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eExperiments\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eTo show the efficiency of the privacy amplification methods explained in this article, some experiments have been made on the complete graph, first for the Real Summation task, and then for Machine Learning with Stochastic Gradient Descent (SGD).\u003c/p\u003e\n\u003cp\u003eThe code is available here: \u003ca href=\"https://github.com/totilas/privacy-amplification-by-decentralization/tree/main\"\u003eGithub Link\u003c/a\u003e\u003c/h1\u003e\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"real-summation-2\"\u003eReal Summation\u003c/h2\u003e\n\u003cp\u003eWe reproduced the first experiment from the paper, comparing th analytical bounds of LDP and NDP on the real summation task.\u003c/p\u003e\n\u003cp\u003eTo do so, we only need to run the \u003cem\u003emain_a.py\u003c/em\u003e and \u003cem\u003emain_b.py\u003c/em\u003e files with python from the \u003cem\u003efig1\u003c/em\u003e folder to display the corresponding figures (a) and (b). It works, for instance, with Python version 3.8, with the prerequisite of having installed the packages \u003cem\u003enumpy\u003c/em\u003e and \u003cem\u003ematplotlib\u003c/em\u003e, only taking a few seconds to execute.\u003c/p\u003e\n\u003cp\u003eIt gives the following results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/results1.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs we may see from the theoretical bounds, privacy is amplified with network differential privacy over LDP when the numer of users $n$ is greater or equal to 20, with increaingly substancial improvements as $n$ grows.\u003c/p\u003e\n\u003cp\u003eIn practice by making some simulations, the gains are even more significant and even for a smaller number of users, as we see in figure (b).\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch2 id=\"machine-learning-with-sgd\"\u003eMachine Learning with SGD\u003c/h2\u003e\n\u003cp\u003eFor this second experiment, the task is to train a logistic regression model in this decentralized context.\u003c/p\u003e\n\u003cp\u003eThe setting of the experiment is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUCI Housing dataset (binarized version)\u003c/li\u003e\n\u003cli\u003eStandardized features and normalized data point (to have unit L2 norm and Lipschitz property of the logistic loss)\u003c/li\u003e\n\u003cli\u003eTrain/test split of 80% uniformly at random\u003c/li\u003e\n\u003cli\u003eTraining set split between $n = 2000$ users (each user has a local dataset of size $8$)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe experiment compares three settings for Stochastic Gradient Descent with perturbation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCentralized DP-SGD, requiring a trusted curator\u003c/li\u003e\n\u003cli\u003eLocal DP-SGD, corresponding to Algorithm 4 with LDP method\u003c/li\u003e\n\u003cli\u003eNetwork DP-SGD, corresponding to Algorithm 4 with Network DP method, the one of interest\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr/\u003e\n\u003cp\u003eWe must run the \u003cem\u003emain.py\u003c/em\u003e file of folder \u003cem\u003efig2\u003c/em\u003e with Python to display the results.\u003c/p\u003e\n\u003cp\u003eIt is possible to use the command \u003cem\u003epython main.py \u0026ndash;help\u003c/em\u003e to show the list of parameters that can be tuned to modify the context of the experiment (the default ones are for $\\varepsilon = 10$ and $\\varepsilon = 1$):\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/options.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eI had some issues to run this program with my settings (same as for the first experiment).\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe \u003cem\u003etyper\u003c/em\u003e module was missing therefore I had to install it : \u003cem\u003epip install typer\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eThe \u003cem\u003e_intercept_dot\u003c/em\u003e function from \u003cem\u003esklearn.linear_model._logistic\u003c/em\u003e couldn\u0026rsquo;t be found either. By checking the \u003cem\u003esklearn.linear_model.LogisticRegression\u003c/em\u003e (which is the public class corresponding to the import here), this function doesn\u0026rsquo;t appear. I wanted to change it with the \u003cem\u003eintercept_\u003c/em\u003e attribute but it didn\u0026rsquo;t fit either. Then by checking the usage of this function in the case, it seemed that it computes a dot product between the model parameters and the input data, taking into account whether an intercept term is included. Therefore I tried to manually code this functionality but unfortunately it didn\u0026rsquo;t give coherent results compared to the paper.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHere are the original results from the paper:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/results2.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere, although the number of contributions per user doesn\u0026rsquo;t align with the optimal regime for network DP, the observed privacy amplification surpasses theoretical expectations. By numerically determining the minimum noise level required for theoretical proofs, they demonstrated that Network DP-SGD achieves a privacy-utility trade-off comparable to Centralized DP-SGD across various privacy levels, showcasing significant privacy amplification benefits over Local DP-SGD, especially in scenarios with fewer iterations than typically recommended.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch1 id=\"section-6\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003ePerspectives\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe work presented suggests numerous avenues for exploration. Generalizations to diverse graph structures, incorporating dynamic topologies to reinforce resilience against collusion, and investigating decentralized models beyond our current scope are key directions. Exploring the potential of multiple tokens traversing the graph simultaneously and delving into randomized gossip algorithms offer promising avenues for advancing privacy-preserving techniques. Finally, probing the theoretical limits of network DP and exploring scenarios where users trust nearby peers more could provide insights into refining privacy mechanisms.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e",
      "url": "http://localhost:1313/posts/privacy-amplification/",
      "date_published": "27036-27-09T35:2727:00+01:00",
      "date_modified": "27036-27-09T35:2727:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "9f7cea94a1198e99d2efbbf0a65bb95637f7f7b0",
      "title": "Robust or Fair",
      "summary": "",
      "content_text": "To be Robust or to be Fair: Towards Fairness in Adversarial Training Authors: Maryem Hajji \u0026 Cément Teulier Table of Contents Abstract Introduction Initial Analysis Previous Studies Theoretical Demonstration Model Fairness Requirements Practical Algorithms Experimentation Conclusion References Abstract This blog post retraces the study conducted in the paper \u0026ldquo;To be Robust or to be Fair: Towards Fairness in Adversarial Training\u0026rdquo; and written by Han Xu, Xiaorui Liu, Yaxin Li, Yaxin Li, Anil K. Jain and Jiliang Tang.\nTheir study is based on a simple observation: while adversarial training has been shown to improve model\u0026rsquo;s robustness, it also introduces several performances disparities among different data groups.\nTo address this issue, the authors present the Fair-Robust-Learning (FRL) framework that aims to reduce such unfairness.\nIntroduction Nowadays, Machine Learning algorithms and Artificial Intelligence are becoming more and more omnipresent in all kinds of jobs. If many of these models are developed to replace human tasks, it is of key importance that they do not reproduce the same mistakes. In fact, human decision making can sometimes be considered \u0026ldquo;unfair\u0026rdquo;, a trait that must not be present in Machine Learning. But as we push our models to be as precise as possible, one question stands out: can we find the good balance between accuracy and equity ?\nDiving into this topic, we focus our study on adversarial training algorithms. Indeed, it has been shown that there is a significant issue in adversarial training for deep neural networks: while such training boosts the model\u0026rsquo;s defenses against adversarial attacks, it unfortunately leads to significant differences in how well the model performs across various types of data. For instance, detailed observations on CIFAR-10 dataset show a non-negligeable difference in the model\u0026rsquo;s performance between \u0026ldquo;car\u0026rdquo; and \u0026ldquo;cat\u0026rdquo; classes (details of this example in our section 1.1).\nThis phenomenon raises concern on concrete topics like the safety of autonomous driving vehicules or facial recognition while also creating ethical problems by discriminating certain classes. To put a word on it, the authors have identified this issue as the robust-fairness problem of adversarial training.\n1. Initial Analysis We recall here the previous studies conducted by the authors that allowed them to identify the existence of the robust-fairness problem.\n1.1 Previous Studies For their first analysis, the authors have decided to study algorithms like the PGD ( Projected Gradient Descent) adversarial training and TRADES ( Theoretically Principled Trade-off between Robustness and Accuracy for Deep Learning ) on the CIFAR-10 dataset. The investigation is made using a PreAct-ResNet18 model structure under specific adversarial attack constraints. The results they obtained are as follows:\nAs we can see, natural training maintains a consistent standard error across classes and a consistent robust error rate when faced with 8/255 PGD attacks. However, in the case of adversarial training, some huge disparities appear. Going back to our introduction\u0026rsquo;s example with \u0026ldquo;cats\u0026rdquo; and \u0026ldquo;cars\u0026rdquo;, we observe that the standard and robust errors for \u0026ldquo;car\u0026rdquo; class ( respectively 6% and 34% ) are significantly lower than those of the \u0026ldquo;cat\u0026rdquo; class ( respectively 33% and 82% ). The results on the TRADES, altough not depicted here, also show some great disparities between certain classes.\nTo support this graphical study, the authors also present statistical evidence of this phenomenom throughout metrics like the Standard Deviation (SD) or the Normalized SD (NSD) of class-wide error. Once again, these metrics reveal that adversarial training indeed results in greater disparities across classes in both standard and robust performance compared to natural training.\nPotential Causes While the authors succeeded in identifying the problem of fairness, they also aimed to understand where it was coming from. From what they observed, it seems that the fairness issue particularly disadvantages classes that are inherently more challenging to classify. Adversarial training in fact tends to increase the standard errors for \u0026ldquo;harder\u0026rdquo; classes (like \u0026ldquo;cat\u0026rdquo;) significantly more than for \u0026ldquo;easier\u0026rdquo; classes (such as \u0026ldquo;car\u0026rdquo;).\n1.2 Theoretical Demonstration From the experiments on the potential causes of the fairness issue, the authors made the following hypotetis: Adversarial training makes hard classes even harder to classify or classify robustly. In this section, we review the theoretical proof of this hypothesis.\nFor this analysis, we place ourselves in the case of a binary classification task, using a mixed Gaussian distribution to create two classes with distinct levels of classification difficulty. Thus, adversarial training does not notably lower the average standard error but it shifts the decision boundary in a way that favours the \u0026rsquo;easier\u0026rsquo; class at the expense of the \u0026lsquo;harder\u0026rsquo; class.\nPrerequisites The classification model, denoted $f$, is a mapping $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ from input data space $\\mathcal{X}$ and output labels $\\mathcal{Y}$ defined as $f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$ with parameters $\\mathbf{w}$ and $b$ The standard error for a classifier $f$ generally is: $R_{\\text{nat}}(f) = \\Pr(f(\\mathbf{x}) \\neq y)$ The robust error for a classifier $f$ generally is: $R_{\\text{rob}}(f) = \\Pr(\\exists \\delta, |\\delta| \\leq \\epsilon, \\text{s.t. } f(\\mathbf{x} + \\delta) \\neq y)$ (the probability of a perturbation existing that would cause the model to produce an incorrect prediction) The standard error conditional on a specific class $\\{Y = y\\}$ is represented by $R_{\\text{nat}}(f; y)$ Theoretical Experiment We generate a simple example of the binary classification task that we presented at the beginning of section 1.2. The data therefore comes from two classes $\\mathcal{Y} = { \\{-1, +1\\}}$, with each class\u0026rsquo; data following a Gaussian distribution $\\mathcal{D}$ centered on $-\\theta$ and $\\theta$ respectively. It is important to specify that there is a $K$-factor difference between the variance of the two classes defined as follows: $\\sigma_{+1} : \\sigma_{-1} = K : 1$ and $K \u0026gt; 1$.\nThe authors then use the theorem stating that:\nTheorem: In the case of a data distribution $D$ like the one above, the optimal linear classifier $f_{\\text{nat}}$ which minimizes the average standard classification error is: $$ f_{\\text{nat}} = \\arg\\min_f \\Pr(f(\\mathbf{x}) \\neq y) $$.\nWith that theorem and after computations, the authors prove that the class \u0026ldquo;$+1$\u0026rdquo; as a larger standard error than the class \u0026ldquo;$-1$\u0026rdquo;.\nOverall, this result shows well that the class \u0026ldquo;$+1$\u0026rdquo;, characterized by a larger variance, tends to be more challenging to classify than the class\u0026quot;$-1$\u0026quot;; a result confirming the hypothesis initially made.\n2. Model In this section, we present the Fair Robust Learning model (FRL).\n2.1 Fairness Requirements The authors introduced the concepts of Equalized Accuracy and Equalized Robustness, emphasizing the importance of providing equal prediction quality and resilience against adversarial attacks across different groups. To achieve this balance, the authors propose a Fair Robust Learning (FRL) strategy. This framework addresses fairness issues in adversarial training by aiming to minimize overall robust error while ensuring fairness constraints are met. They separate robust error into standard error and boundary error, allowing independent solving of the unfairness of both errors. [ref 7]\nThe training objective thus becomes minimizing the sum of standard error and boundary error while adhering to fairness constraints that ensure no significant disparities in error rates among classes. Techniques from prior research are leveraged to optimize boundary errors during training.\n2.2 Practical Algorithms This section explores effective methods to implement and address the challenges outlined in the training objective, such as the Reweight strategy. In order to implement it, Lagrange multipliers are introduced, denoted as $φ = (φ_{nat}^{\\text{i}}, φ_{bndy}^{\\text{i}})$ where each multiplier corresponds to a fairness constraint. These multipliers are non-negative and play a crucial role in the optimization process.\nThe approach involves forming a Lagrangian, represented by the function $L(f, φ)$, which combines the standard error ($R_{\\text{nat}}(f)$) and boundary error ($R_{\\text{bndy}}(f)$) terms along with the fairness constraints. The Lagrangian acts as a guide for the optimization process, helping to balance the trade-off between minimizing errors and satisfying fairness requirements.\n$$ \\scriptsize{ L(f, \\phi) = R_{\\text{nat}}(f) + R_{\\text{bndy}}(f) + \\sum_{i=1}^{Y} \\phi_{\\text{nat}}^i \\left( R_{\\text{nat}}(f, i) - R_{\\text{nat}}(f) - \\tau_1 \\right)^+ + \\sum_{i=1}^{Y} \\phi_{\\text{bndy}}^i \\left( R_{\\text{bndy}}(f, i) - R_{\\text{bndy}}(f) - \\tau_2 \\right)^+ } $$\nThe optimization problem is then framed as a max-min game between the classifier $f$ and the Lagrange multipliers $φ$. The objective is to maximize the fairness constraints while minimizing the Lagrangian function, which encapsulates both standard and boundary errors.\nOn the other hand, the Reweight strategy presents a limitation particularly in mitigating boundary errors for specific classes. While upweighting the cost for standard errors ($R_{\\text{nat}}(f, i)$) can penalize large errors and improve performance for disadvantaged groups, solely upweighting the boundary error ($R_{\\text{bndy}}(f, i)$) for a class doesn\u0026rsquo;t effectively reduce its boundary error.\nTo overcome this challenge, the Remargin strategy introduces an alternative approach by enlarging the perturbation margin ($\\epsilon$) during adversarial training. This strategy is inspired by previous research showing that increasing the margin during adversarial training can enhance a model\u0026rsquo;s robustness against attacks under the current intensity.[ref 8]\nSpecifically, the Remargin strategy involves adjusting the adversarial margin for generating adversarial examples during training, focusing on specific classes where boundary errors are significant. This adjustment aims to improve the robustness of these classes and reduce their large boundary errors ($R_{\\text{bndy}}(f, i)$).\n3. Experimentation In this section, we reproduce the experimental methodology and setup used to evaluate the effectiveness of the proposed Fair Robust Learning (FRL) framework in constructing robust deep neural network (DNN) models.\nFirstly, we train a fairly simple model on the Fashion MNIST dataset, then we test out torchattack\u0026rsquo;s PGD on our naturally trained model, Then we will adversarially train the same architecture to see if we can identify this unfairness.\nAs we can see above, the naturally trained model has low standard error, but high PGD error. The adversarially trained model, in contrast, has a much lower PGD error, but higher standard error, and higher disparity between the classes.\nSecond, we implement the FRL algorithm (Reweight strategy) which formulates the learning problem as a cost-sensitive classification that penalizes those classes which violate fairness. Essentially, we create multipliers that up or down weight the loss of classes based on how fair or unfair they are with respect to the average across all classes.\nThe following is the FRL Algorithm outlined in the paper:\nWe made a setup to run the process 3 times: once with equal alpha values, once with an alpha ratio that favors the natural error, and one with an alpha ratio that favors the boundary error.\nIn accordance with the authors of the paper, we find that the alpha ratio that favors the natural error is successful in preventing the unfairness of the standard error in the model, and does help somewhat with the unfairness of the PGD error. On the other hand, we notice that the algorithm struggles to improve the worst-case boundary error, leading to disparities in robustness performance across different classes.\nConclusion In conclusion, the studied article discusses the development and implementation of Fair Robust Learning (FRL) strategies to address fairness concerns in adversarial training of deep neural networks. The objective of these strategies is to achieve both equalized accuracy and robustness across different classes.\nThe Reweight strategy aims to minimize overall robust error while adhering to fairness constraints by adjusting training weights based on class-wise errors while the Remargin strategy enlarges the perturbation margin during adversarial training to improve robustness and reduce boundary errors.\nFinally, The FRL framework combines these strategies to mitigate fairness issues and improve model performance across various classes. These approaches represent promising steps towards achieving fairness in robust deep learning models.\nReferences [1] Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang1. To be Robust or to be Fair: Towards Fairness in Adversarial Training. 2021.\n[2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. 2014.\n[3] Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y. Fooling a real car with adversarial traffic signs. 2019.\n[4] Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pp. 1528–1540, 2016.\n[5] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n[6] He, H. and Garcia, E. A. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263–1284. 2009.\n[7] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. 2019.\n[8] Tramer, F., Behrmann, J., Carlini, N., Papernot, N., and Ja- ` cobsen, J.-H. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561–9571. PMLR. 2020.\n",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eTo be Robust or to be Fair: Towards Fairness in Adversarial Training\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Maryem Hajji \u0026 Cément Teulier\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eInitial Analysis\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-2.1\"\u003ePrevious Studies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.2\"\u003eTheoretical Demonstration\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eModel\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-3.1\"\u003eFairness Requirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3.2\"\u003ePractical Algorithms\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eExperimentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThis blog post retraces the study conducted in the \u003ca href=\"http://proceedings.mlr.press/v139/xu21b.html\"\u003epaper\u003c/a\u003e \u0026ldquo;To be Robust or to be Fair: Towards Fairness in Adversarial Training\u0026rdquo; and written by Han Xu, Xiaorui Liu, Yaxin Li, Yaxin Li, Anil K. Jain and Jiliang Tang.\u003c/p\u003e\n\u003cp\u003eTheir study is based on a simple observation: while adversarial training has been shown to improve model\u0026rsquo;s robustness, it also introduces several performances disparities among different data groups.\u003c/p\u003e\n\u003cp\u003eTo address this issue, the authors present the Fair-Robust-Learning (FRL) framework that aims to reduce such unfairness.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNowadays, Machine Learning algorithms and Artificial Intelligence are becoming more and more omnipresent in all kinds of jobs. If many of these models are developed to replace human tasks, it is of key importance that they do not reproduce the same mistakes. In fact, human decision making can sometimes be considered \u0026ldquo;unfair\u0026rdquo;, a trait that must not be present in Machine Learning. But as we push our models to be as precise as possible, one question stands out: can we find the good balance between accuracy and equity ?\u003c/p\u003e\n\u003cp\u003eDiving into this topic, we focus our study on adversarial training algorithms.\nIndeed, it has been shown that there is a significant issue in adversarial training for deep neural networks: while such training boosts the model\u0026rsquo;s defenses against adversarial attacks, it unfortunately leads to significant differences in how well the model performs across various types of data.\nFor instance, detailed observations on CIFAR-10 dataset show a non-negligeable difference in the model\u0026rsquo;s performance between \u0026ldquo;car\u0026rdquo; and \u0026ldquo;cat\u0026rdquo; classes (details of this example in our section 1.1).\u003c/p\u003e\n\u003cp\u003eThis phenomenon raises concern on concrete topics like the safety of autonomous driving vehicules or facial recognition while also creating ethical problems by discriminating certain classes.\nTo put a word on it, the authors have identified this issue as the \u003cstrong\u003erobust-fairness\u003c/strong\u003e problem of adversarial training.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003e1. Initial Analysis\u003c/h2\u003e\n\u003cp\u003eWe recall here the previous studies conducted by the authors that allowed them to identify the existence of the robust-fairness problem.\u003c/p\u003e\n\u003ch2 id=\"section-2.1\"\u003e1.1 Previous Studies\u003c/h2\u003e\n\u003cp\u003eFor their first analysis, the authors have decided to study algorithms like the PGD ( Projected Gradient Descent) adversarial training and TRADES ( Theoretically Principled Trade-off between Robustness and Accuracy for Deep Learning ) on the CIFAR-10 dataset.\nThe investigation is made using a PreAct-ResNet18 model structure under specific adversarial attack constraints.\nThe results they obtained are as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Hajji_Teulier/cat_car.png\"\n  alt=\"Paper Initial Results\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs we can see, natural training maintains a consistent standard error across classes and a consistent robust error rate when faced with 8/255 PGD attacks. However, in the case of adversarial training, some huge disparities appear. Going back to our introduction\u0026rsquo;s example with \u0026ldquo;cats\u0026rdquo; and \u0026ldquo;cars\u0026rdquo;, we observe that the standard and robust errors for \u0026ldquo;car\u0026rdquo; class ( respectively 6% and 34% ) are significantly lower than those of the \u0026ldquo;cat\u0026rdquo; class ( respectively 33% and 82% ). The results on the TRADES, altough not depicted here, also show some great disparities between certain classes.\u003c/p\u003e\n\u003cp\u003eTo support this graphical study, the authors also present statistical evidence of this phenomenom throughout metrics like the Standard Deviation (SD) or the Normalized SD (NSD) of class-wide error. Once again, these metrics reveal that adversarial training indeed results in greater disparities across classes in both standard and robust performance compared to natural training.\u003c/p\u003e\n\u003ch3 id=\"potential-causes\"\u003ePotential Causes\u003c/h3\u003e\n\u003cp\u003eWhile the authors succeeded in identifying the problem of fairness, they also aimed to understand where it was coming from. From what they observed, it seems that the fairness issue particularly disadvantages classes that are inherently more challenging to classify. Adversarial training in fact tends to increase the standard errors for \u0026ldquo;harder\u0026rdquo; classes (like \u0026ldquo;cat\u0026rdquo;) significantly more than for \u0026ldquo;easier\u0026rdquo; classes (such as \u0026ldquo;car\u0026rdquo;).\u003c/p\u003e\n\u003ch2 id=\"section-2.2\"\u003e1.2 Theoretical Demonstration\u003c/h2\u003e\n\u003cp\u003eFrom the experiments on the potential causes of the fairness issue, the authors made the following hypotetis: Adversarial training makes hard classes even harder to classify or classify robustly.\nIn this section, we review the theoretical proof of this hypothesis.\u003c/p\u003e\n\u003cp\u003eFor this analysis, we place ourselves in the case of a binary classification task, using a mixed Gaussian distribution to create two classes with distinct levels of classification difficulty. Thus, adversarial training does not notably lower the average standard error but it shifts the decision boundary in a way that favours the \u0026rsquo;easier\u0026rsquo; class at the expense of the \u0026lsquo;harder\u0026rsquo; class.\u003c/p\u003e\n\u003ch3 id=\"prerequisites\"\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003eclassification model\u003c/strong\u003e, denoted $f$, is a mapping  $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ from input data space $\\mathcal{X}$ and output labels $\\mathcal{Y}$ defined as $f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$ with parameters $\\mathbf{w}$ and $b$\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003estandard error\u003c/strong\u003e for a classifier $f$ generally is: $R_{\\text{nat}}(f) = \\Pr(f(\\mathbf{x}) \\neq y)$\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003erobust error\u003c/strong\u003e for a classifier $f$ generally is: $R_{\\text{rob}}(f) = \\Pr(\\exists \\delta, |\\delta| \\leq \\epsilon, \\text{s.t. } f(\\mathbf{x} + \\delta) \\neq y)$ (the probability of a perturbation existing that would cause the model to produce an incorrect prediction)\u003c/li\u003e\n\u003cli\u003eThe standard error \u003cstrong\u003econditional\u003c/strong\u003e on a specific class $\\{Y = y\\}$ is represented by $R_{\\text{nat}}(f; y)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"theoretical-experiment\"\u003eTheoretical Experiment\u003c/h3\u003e\n\u003cp\u003eWe generate a simple example of the binary classification task that we presented at the beginning of section 1.2.\nThe data therefore comes from two classes $\\mathcal{Y} = { \\{-1, +1\\}}$, with each class\u0026rsquo; data following a Gaussian distribution $\\mathcal{D}$ centered on $-\\theta$ and $\\theta$ respectively.\nIt is important to specify that there is a $K$-factor difference between the variance of the two classes defined as follows: $\\sigma_{+1} : \\sigma_{-1} = K : 1$ and $K \u0026gt; 1$.\u003c/p\u003e\n\u003cp\u003eThe authors then use the theorem stating that:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem:\u003c/strong\u003e In the case of a data distribution $D$ like the one above, the optimal linear classifier $f_{\\text{nat}}$ which minimizes the average standard classification error is:\n$$ f_{\\text{nat}} = \\arg\\min_f \\Pr(f(\\mathbf{x}) \\neq y) $$.\u003c/p\u003e\n\u003cp\u003eWith that theorem and after computations, the authors prove that the class \u0026ldquo;$+1$\u0026rdquo; as a larger standard error than the class \u0026ldquo;$-1$\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eOverall, this result shows well that the class \u0026ldquo;$+1$\u0026rdquo;, characterized by a larger variance, tends to be more challenging to classify than the class\u0026quot;$-1$\u0026quot;; a result confirming the hypothesis initially made.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e2. Model\u003c/h2\u003e\n\u003cp\u003eIn this section, we present the Fair Robust Learning model (FRL).\u003c/p\u003e\n\u003ch2 id=\"section-3.1\"\u003e2.1 Fairness Requirements\u003c/h2\u003e\n\u003cp\u003eThe authors introduced the concepts of Equalized Accuracy and Equalized Robustness, emphasizing the importance of providing equal prediction quality and resilience against adversarial attacks across different groups.\nTo achieve this balance, the authors propose a Fair Robust Learning (FRL) strategy.\nThis framework addresses fairness issues in adversarial training by aiming to minimize overall robust error while ensuring fairness constraints are met. They separate robust error into standard error and boundary error, allowing independent  solving of the unfairness of both errors. [ref 7]\u003c/p\u003e\n\u003cp\u003eThe training objective thus becomes minimizing the sum of standard error and boundary error while adhering to fairness constraints that ensure no significant disparities in error rates among classes. Techniques from prior research are leveraged to optimize boundary errors during training.\u003c/p\u003e\n\u003ch2 id=\"section-3.2\"\u003e2.2 Practical Algorithms\u003c/h2\u003e\n\u003cp\u003eThis section explores effective methods to implement and address the challenges outlined in the training objective, such as the Reweight strategy.\nIn order to implement it, Lagrange multipliers are introduced, denoted as $φ = (φ_{nat}^{\\text{i}}, φ_{bndy}^{\\text{i}})$ where each multiplier corresponds to a fairness constraint. These multipliers are non-negative and play a crucial role in the optimization process.\u003c/p\u003e\n\u003cp\u003eThe approach involves forming a Lagrangian, represented by the function $L(f, φ)$, which combines the standard error ($R_{\\text{nat}}(f)$) and boundary error ($R_{\\text{bndy}}(f)$) terms along with the fairness constraints. The Lagrangian acts as a guide for the optimization process, helping to balance the trade-off between minimizing errors and satisfying fairness requirements.\u003c/p\u003e\n\u003cp\u003e$$\n\\scriptsize{\nL(f, \\phi) = R_{\\text{nat}}(f) + R_{\\text{bndy}}(f) + \\sum_{i=1}^{Y} \\phi_{\\text{nat}}^i \\left( R_{\\text{nat}}(f, i) - R_{\\text{nat}}(f) - \\tau_1 \\right)^+ + \\sum_{i=1}^{Y} \\phi_{\\text{bndy}}^i \\left( R_{\\text{bndy}}(f, i) - R_{\\text{bndy}}(f) - \\tau_2 \\right)^+\n}\n$$\u003c/p\u003e\n\u003cp\u003eThe optimization problem is then framed as a max-min game between the classifier $f$ and the Lagrange multipliers $φ$. The objective is to maximize the fairness constraints while minimizing the Lagrangian function, which encapsulates both standard and boundary errors.\u003c/p\u003e\n\u003cp\u003eOn the other hand, the Reweight strategy presents a limitation particularly in mitigating boundary errors for specific classes. While upweighting the cost for standard errors ($R_{\\text{nat}}(f, i)$) can penalize large errors and improve performance for disadvantaged groups, solely upweighting the boundary error ($R_{\\text{bndy}}(f, i)$) for a class doesn\u0026rsquo;t effectively reduce its boundary error.\u003c/p\u003e\n\u003cp\u003eTo overcome this challenge, the Remargin strategy introduces an alternative approach by enlarging the perturbation margin ($\\epsilon$) during adversarial training. This strategy is inspired by previous research showing that increasing the margin during adversarial training can enhance a model\u0026rsquo;s robustness against attacks under the current intensity.[ref 8]\u003c/p\u003e\n\u003cp\u003eSpecifically, the Remargin strategy involves adjusting the adversarial margin for generating adversarial examples during training, focusing on specific classes where boundary errors are significant. This adjustment aims to improve the robustness of these classes and reduce their large boundary errors ($R_{\\text{bndy}}(f, i)$).\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e3. Experimentation\u003c/h2\u003e\n\u003cp\u003eIn this section, we reproduce the experimental methodology and setup used to evaluate the effectiveness of the proposed Fair Robust Learning (FRL) framework in constructing robust deep neural network (DNN) models.\u003c/p\u003e\n\u003cp\u003eFirstly, we train a fairly simple model on the Fashion MNIST dataset, then we test out torchattack\u0026rsquo;s PGD on our naturally trained model, Then we will adversarially train the same architecture to see if we can identify this unfairness.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Hajji_Teulier/result1.png\"\n  alt=\"Paper Initial Results\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs we can see above, the naturally trained model has low standard error, but high PGD error. The adversarially trained model, in contrast, has a much lower PGD error, but higher standard error, and higher disparity between the classes.\u003c/p\u003e\n\u003cp\u003eSecond, we implement the FRL algorithm (Reweight strategy) which formulates the learning problem as a cost-sensitive classification that penalizes those classes which violate fairness. Essentially, we create multipliers that up or down weight the loss of classes based on how fair or unfair they are with respect to the average across all classes.\u003c/p\u003e\n\u003cp\u003eThe following is the FRL Algorithm outlined in the paper:\u003c/p\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n  \u003cimg src=\"/images/Hajji_Teulier/algo1.png\" alt=\"Paper Initial Results\" width=\"400\" /\u003e\n\u003c/div\u003e\n\u003cp\u003eWe made a setup to run the process 3 times: once with equal alpha values, once with an alpha ratio that favors the natural error, and one with an alpha ratio that favors the boundary error.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Hajji_Teulier/result2.png\"\n  alt=\"Paper Initial Results\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn accordance with the authors of the paper, we find that the alpha ratio that favors the natural error is successful in preventing the unfairness of the standard error in the model, and does help somewhat with the unfairness of the PGD error. On the other hand, we notice that the algorithm struggles to improve the worst-case boundary error, leading to disparities in robustness performance across different classes.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn conclusion, the studied article discusses the development and implementation of Fair Robust Learning (FRL) strategies to address fairness concerns in adversarial training of deep neural networks. The objective of these strategies is to achieve both equalized accuracy and robustness across different classes.\u003c/p\u003e\n\u003cp\u003eThe Reweight strategy aims to minimize overall robust error while adhering to fairness constraints by adjusting training weights based on class-wise errors while the Remargin strategy enlarges the perturbation margin during adversarial training to improve robustness and reduce boundary errors.\u003c/p\u003e\n\u003cp\u003eFinally, The FRL framework combines these strategies to mitigate fairness issues and improve model performance across various classes. These approaches represent promising steps towards achieving fairness in robust deep learning models.\u003c/p\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1]  Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang1. To be Robust or to be Fair: Towards Fairness in Adversarial Training. 2021.\u003c/p\u003e\n\u003cp\u003e[2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. 2014.\u003c/p\u003e\n\u003cp\u003e[3]  Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y. Fooling a real car with adversarial traffic signs. 2019.\u003c/p\u003e\n\u003cp\u003e[4]  Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pp. 1528–1540, 2016.\u003c/p\u003e\n\u003cp\u003e[5] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\u003c/p\u003e\n\u003cp\u003e[6] He, H. and Garcia, E. A. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263–1284. 2009.\u003c/p\u003e\n\u003cp\u003e[7] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. 2019.\u003c/p\u003e\n\u003cp\u003e[8] Tramer, F., Behrmann, J., Carlini, N., Papernot, N., and Ja- ` cobsen, J.-H. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561–9571. PMLR. 2020.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n",
      "url": "http://localhost:1313/posts/robust-or-fair/",
      "date_published": "27036-27-09T337:2727:00+01:00",
      "date_modified": "27036-27-09T337:2727:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "a5cd2d184b6a0acff2b0c7ff52e9a7dbf67c5d3b",
      "title": "XCM, an explainable CNN for MTS classficiation",
      "summary": "",
      "content_text": " XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification Authors : Nicolas SAINT \u0026 Matthis Guérin Table of Contents 1. Introduction 2. Related Work 3. XCM 4. Evaluation 5. Results 6. Implementation 7. Conclusion Appendix References This is a blog post about the article \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo; published by Kevin Fauvel et al. in 2021 and available here.\n1. Introduction The classification of multivariate time series (MTS) has emerged as an increasingly important research area over the last decade, driven by the exponential growth of temporal data across various domains such as finance, healthcare, mobility, and natural disaster prediction . A time series is a sequence of real values ordered in time, and when a set of co-evolving series is recorded simultaneously by a set of sensors, it is referred to as an MTS. MTS classification, which involves learning the relationship between an MTS and its label, presents a significant challenge due to the inherent complexity of the multivariate and temporal nature of the data.\nTraditional approaches to MTS classification, while effective on large datasets, encounter significant limitations such as poor generalization on small datasets and a lack of explainability, which can limit their adoption in sensitive applications where understanding the model\u0026rsquo;s decisions is crucial . For example, the European GDPR regulation highlights the importance of providing meaningful explanations for automated decisions, emphasizing the need for approaches capable of reconciling performance and explainability .\n2. Related Work The existing literature on MTS classification can be broadly grouped into three main categories: similarity-based methods, feature-based methods, and deep learning approaches.\nSimilarity-based methods: These methods utilize similarity measures to compare two MTS. Dynamic Time Warping (DTW) combined with the nearest neighbor rule (k-NN) has shown impressive performance, although it is not without limitations, particularly in terms of computational cost and the absence of an explicit feature representation.\nFeature-based methods: Approaches such as shapelets and Bag-of-Words (BoW) models transform time series into a more manageable feature space. WEASEL+MUSE, for instance, uses a symbolic Fourier approximation to create a BoW representation of MTS, enabling efficient classification using logistic regression.\nDeep learning approaches: The advent of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks has opened new avenues for MTS classification, thanks to their ability to automatically learn complex data representations. MLSTM-FCN, combining LSTM and CNN, has been identified as one of the top-performing models, despite its complexity and difficulty in providing explanations for its decisions.\nExplainability of MTS classification models has become a major concern, particularly for critical applications. Post-hoc methods, such as LIME and SHAP, offer ways to generate explanations for black-box models, but these explanations may lack fidelity to the model\u0026rsquo;s internal workings. This underscores the need for approaches that inherently integrate explainability into the model design.\nIn this context, our work presents XCM, an innovative convolutional neural network architecture for MTS classification, that not only outperforms existing approaches in terms of performance but also provides reliable and intuitive explanations for its predictions, directly addressing the challenges of performance and explainability in MTS classification. This approach is grounded on the foundational work presented in the paper \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo;, which offers a novel solution to the pressing needs in the domain of MTS classification.\n3. XCM Architecture\nXCM\u0026rsquo;s architecture is specifically designed to efficiently address the challenge of multivariate time series (MTS) classification by simultaneously extracting relevant information about observed variables and time directly from the input data. This unique approach allows XCM to capture the complexity and inherent interactions within MTS, thereby enhancing its generalization capability across different datasets and its applicability in various application contexts.\nTo achieve this, XCM employs a combination of parallel 2D and 1D convolution filters. The 2D filters focus on extracting features related to observed variables at each time instant, while the 1D filters capture temporal dynamics across all variables.\n2D Convolution Formula for Observed Variables: $$A^{(k)} = f(W^{(k)} * X + b^{(k)})$$\n$A^{(k)}$: représente la carte des caractéristiques activées pour le k-ème filtre. $f$: denotes the activation function, often ReLU, to introduce non-linearity. $W^{(k)}$, $b^{(k)}$: weights and bias of the $k$-th 2D convolution filter. $X$: the input MTS data. $*$: the convolution operation. By extracting features in this manner, XCM is able to detect complex patterns in MTS that are crucial for precise series classification.\n1D Convolution Formula for Temporal Information: $$M^{(k)} = f(W^{(k)} \\circledast X + b^{(k)})$$\n$M^{(k)}$: the activated feature map resulting from 1D filters. $\\circledast$: the 1D convolution operation focusing on the temporal dimension. This dual convolution approach enables XCM to maintain high accuracy while offering a better understanding of the contributions of different variables and temporal dynamics to the final decision.\nExplainability\nOne of the hallmark features of the XCM architecture is its inherent capability to provide explainable predictions, leveraging the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. Grad-CAM produces heatmaps that highlight the regions of the input data that most significantly contribute to a specific class prediction. This feature is crucial for applications where understanding the model\u0026rsquo;s reasoning is as important as the prediction accuracy itself.\nGrad-CAM Calculation\nGrad-CAM utilizes the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the input for predicting the concept. This method allows the visualization of which parts of the input are considered important by the CNN for classification tasks.\nThe calculation involves the following steps:\nFeature Map Extraction: Firstly, the feature maps $A^{(k)}$ are extracted from the last convolutional layer. These feature maps are essentially the output of the convolution operations and contain the spatial information that the network has learned to identify.\nGradient Calculation: The gradients of the score for class $c$, denoted as $y^c$ , with respect to the feature map activations $A^{(k)}$ of a convolutional layer, are computed. These gradients are pooled across the width and height dimensions (indexed by $i$ and $j$) to obtain the neuron importance weights $\\alpha_k^c$.\nThe weights for the feature map activations are computed as follows:\n$$\\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^{(k)}}$$ where $Z$ is the number of pixels in the feature map, and $y^c$ is the score for class $c$, before the softmax layer.\nWeighted Combination of Feature Maps: The weighted combination of feature maps, followed by a ReLU, gives the Grad-CAM heatmap $L_{\\text{Grad-CAM}}^c$ : $$L_{\\text{Grad-CAM}}^c = \\text{ReLU}\\left(\\sum_k \\alpha_k^c A^{(k)}\\right)$$\nThis equation combines the feature maps of the last convolutional layer of the network with the neuron importance weights to produce a heatmap for each class. The ReLU function is applied to the linear combination of maps to only consider the features that have a positive influence on the class of interest, effectively highlighting the regions of the input that are important for predicting class $c$.\nThis process elucidates how certain input features contribute to the model\u0026rsquo;s predictions, offering a layer of transparency that can aid in the validation and trust-building of machine learning models in critical applications. The ability to generate such visual explanations not only helps in understanding the model\u0026rsquo;s behavior but also in identifying potential biases or errors in the learning process.\nIn summary, the explainability aspect of XCM, powered by Grad-CAM, stands out as a significant advancement in making deep learning models more interpretable and trustworthy, especially in domains where decision-making processes need to be transparent and justifiable.\n4. Evaluation The evaluation of the XCM model focuses on its performance across various datasets from the UEA multivariate time series classification archive. The datasets are diverse, spanning different types such as motion, ECG, HAR (Human Activity Recognition), AS (Audio Spectra), and EEG/MEG (Electroencephalogram/Magnetoencephalogram), with varying lengths, dimensions, and number of classes. This diversity presents a rigorous challenge and a comprehensive platform to assess the capabilities of XCM.\nHere\u0026rsquo;s an exemple of datasets table used ine the paper:\nTable: Datasets Overview from UEA Archive\nDatasets Type Train Test Length Dimensions Classes Articulary Word Recognition Motion 275 300 144 9 25 Atrial Fibrillation ECG 15 15 640 2 3 Basic Motions HAR 40 40 100 6 4 Character Trajectories Motion 1422 1436 182 3 20 Cricket HAR 108 72 1197 6 12 Duck Duck Geese AS 60 40 270 1345 5 Eigen Worms Motion 128 131 17984 6 5 Epilepsy HAR 137 138 206 3 4 Ering HAR 30 30 65 4 6 Ethanol Concentration Other 261 263 1751 3 4 Face Detection EEG/MEG 5890 3524 62 144 2 Finger Movements EEG/MEG 316 100 50 28 2 Interpretation and Results:\nEach dataset presents unique challenges for MTS classification:\nArticulary Word Recognition: With a substantial number of classes (25), the model must discern between intricate motion patterns. A high accuracy score here would indicate XCM\u0026rsquo;s ability to manage high-dimensional, complex pattern recognition tasks.\nAtrial Fibrillation: Given the high length of the time series (640) and fewer instances for training and testing, the model\u0026rsquo;s performance can signal its efficiency in overfitting prevention and extracting meaningful information from lengthy sequences with minimal data.\nBasic Motions: A dataset like this with a shorter length and moderate dimensionality can showcase XCM\u0026rsquo;s quick learning capability for simple temporal patterns and basic human activities.\nCharacter Trajectories: This dataset, with a large training set and many classes, is an excellent test of XCM\u0026rsquo;s scalability and classification robustness in handling motion data.\nCricket: Long sequences (1197) and a fair number of classes (12) make this dataset suited for evaluating XCM\u0026rsquo;s temporal pattern learning and generalization over longer periods.\nDuck Duck Geese: An Audio Spectrum dataset with a high dimensionality challenges the model to process and classify complex audio patterns, testing XCM\u0026rsquo;s ability in handling non-motion data.\nEigen Worms: With the longest sequences in the given datasets (17,984), XCM\u0026rsquo;s performance can be interpreted as its capability in modeling highly intricate temporal behaviors.\nEpilepsy: Human activity recognition data like this one requires the model to be sensitive to subtle variations, a good indicator of XCM\u0026rsquo;s precision in critical classification scenarios.\nEring: Small datasets with higher class counts test the model\u0026rsquo;s overfitting resilience and classification dexterity.\nEthanol Concentration: An \u0026lsquo;Other\u0026rsquo; type dataset with long sequences will challenge any classifier\u0026rsquo;s ability to handle diverse, non-standard data.\nFace Detection: This EEG/MEG dataset has a significant number of instances for both training and testing, focusing on XCM\u0026rsquo;s performance in biometric pattern recognition scenarios.\nFinger Movements: Another EEG/MEG dataset, but with shorter sequences and fewer dimensions, this can highlight how well XCM captures rapid, subtle changes in electrical activity related to movements.\nHyperparameters and Metrics\nIn the evaluation of XCM, a systematic approach was taken to optimize hyperparameters for each dataset. A grid search was employed, where the hyperparameters were fine-tuned to achieve the best average accuracy. This process was underpinned by a stratified 5-fold cross-validation on the training set, ensuring a robust estimation of the model\u0026rsquo;s performance.\nTo benchmark against other classifiers, the primary metric used was classification accuracy. This metric is standard for evaluating MTS classifiers on the public UEA datasets. Furthermore, classifiers were ranked based on their performance, with the number of wins or ties noted to establish a comparative landscape of classifier effectiveness.\nBeyond accuracy, a critical difference diagram was used to provide a visual statistical comparison of multiple classifiers across multiple datasets. This method uses the nonparametric Friedman test to highlight performance disparities. For the implementation of this statistical test, the R package scmamp was utilized, which is a recognized tool for such analyses in the machine learning community.\nThese rigorous evaluation methods ensure that the performance assessment of XCM is both comprehensive and reliable, offering clear insights into its classification capabilities and its standing relative to existing MTS classifiers.\nFor our research paper based on the XCM method and its performance on various datasets, here’s how we could approach Section 5, which covers the analysis and interpretation of results:\n5. Results The performance of the XCM method was rigorously evaluated across a comprehensive set of UEA datasets with a focus on multivariate time series classification. Our approach aimed to balance between achieving high classification accuracy and providing explainability. This section discusses the performance of XCM compared to other leading algorithms such as MLSTM-FCN (MF), WEASEL+MUSE (WM), and Elastic Distances (ED) with DTW independent (DWI) and dependent (DWD) variants.\nTable: Performance Comparison on UEA Datasets\nDatasets XC XC Seq MC MF WM ED (n) DWI DWD (XC Params) Batch Win % Articulary Word Recognition 98.3 92.7 92.3 98.6 99.3 97.0 98.0 98.7 32 80 Atrial Fibrillation 46.7 33.3 33.3 20.0 26.7 26.7 26.7 20.0 1 60 Basic Motions 100.0 100.0 100.0 100.0 100.0 67.6 100.0 97.5 32 20 Character Trajectories 99.5 98.8 97.4 99.3 99.0 96.4 96.9 99.0 32 80 Cricket 100.0 93.1 90.3 98.6 98.6 98.6 100.0 94.4 32 100 Duck Duck Geese 70.0 52.5 65.0 67.5 57.5 27.5 55.0 60.0 8 80 Eigen Worms 43.5 45.0 41.9 80.9 89.0 55.0 60.3 61.8 32 40 Epilepsy 99.3 93.5 94.9 96.4 99.3 66.7 97.8 96.4 32 20 Ering 13.3 13.3 13.3 13.3 13.3 13.3 13.3 13.3 32 80 Ethanol Concentration 34.6 31.6 30.8 31.6 29.3 29.3 30.4 32.3 32 80 Face Detection 63.9 63.8 50.0 57.4 54.5 51.9 51.3 52.9 32 60 Finger Movements 60.0 60.0 49.0 61.0 54.0 55.0 52.0 53.0 32 40 (Note: \u0026ldquo;XC\u0026rdquo; denotes the accuracy of XCM, \u0026ldquo;XC Seq\u0026rdquo; denotes the accuracy of XCM with sequential layers, \u0026ldquo;MC\u0026rdquo; represents MTEX-CNN, \u0026ldquo;MF\u0026rdquo; denotes MLSTM-FCN, \u0026ldquo;WM\u0026rdquo; stands for WEASEL+MUSE, \u0026ldquo;ED (n)\u0026rdquo; represents Elastic Distance (normalized), \u0026ldquo;DWI\u0026rdquo; and \u0026ldquo;DWD\u0026rdquo; refer to Dynamic Time Warping independent and dependent, respectively. \u0026ldquo;Win %\u0026rdquo; indicates the percentage of times XCM achieved the highest accuracy across all folds.)\nInterpretation of Results\nArticulary Word Recognition: XCM achieved a high accuracy of 98.3%, showcasing its robustness in motion-based classification and indicating its effectiveness in handling complex time series data with a high dimensional space.\nAtrial Fibrillation: This dataset posed a challenge with lower accuracy across all methods. XCM\u0026rsquo;s performance at 46.7% suggests that while challenging, it has the potential to discern patterns in smaller and more complex ECG datasets.\nBasic Motions: XCM perfected the score, highlighting its proficiency in recognizing basic human activity patterns, a crucial capability for HAR applications.\nCharacter Trajectories: The high score of 99.5% reflects XCM\u0026rsquo;s strength in managing datasets with numerous classes, reinforcing its scalability for extensive data.\nCricket: A perfect score of 100.0% emphasizes XCM\u0026rsquo;s ability to capture intricate temporal patterns, suggesting its suitability for complex HAR scenarios.\nDuck Duck Geese: XCM\u0026rsquo;s performance at 70.0% accuracy indicates a significant capability in audio spectrum data classification, a testament to its adaptability to different data types.\nEigen Worms: Despite the lower score, XCM\u0026rsquo;s handling of the longest sequences among the datasets indicates its potential to model complex temporal behaviors in motion data.\nEpilepsy: An accuracy of 99.3% portrays XCM\u0026rsquo;s precision and reliability in critical classification scenarios, essential for medical applications.\nEring: The universally low scores across methods reflect the dataset\u0026rsquo;s complexity, underscoring a need for specialized approaches or additional features to aid classification.\nEthanol Concentration: Although challenging, XCM\u0026rsquo;s relatively higher score suggests its capacity to filter meaningful information from noisy data.\nFace Detection: XCM\u0026rsquo;s ability to handle biometric patterns is evidenced by its performance, indicating its utility in EEG/MEG data interpretation.\nFinger Movements: The moderate score reflects the complexity of the task but also suggests XCM\u0026rsquo;s capability to capture rapid changes in EEG/MEG datasets associated with movements.\nThe \u0026ldquo;Win %\u0026rdquo; column indicates the superiority of XCM in most datasets, which combined with its explainability features, positions it as a preferred choice for MTS classification in practical applications. This comprehensive analysis not only confirms the effectiveness of the XCM approach but also guides future advancements and potential improvements.\nDiscussion\nThe results underscore the effectiveness of XCM in multivariate time series classification across a variety of domains, highlighting its capability to maintain high accuracy even in datasets with challenging characteristics. Moreover, the high win percentage indicates XCM\u0026rsquo;s robustness as it frequently outperforms other methods. It is crucial to note that beyond accuracy, XCM\u0026rsquo;s design enables it to offer a layer of explainability which is not captured by accuracy metrics alone but is invaluable in practical applications.\n6. Implementation We decided to implement ourselves the XCM model using this GitHub Repository on a dataset used in the original paper : BasiMotions.\nThe code of the XCM model is shown in the Appendix.\nHere are the results we obtained for a 5 fold training :\nDataset Model_Name Batch_Size Window_Size Fold Accuracy_Train Accuracy_Validation Accuracy_Test Accuracy_Test_Full_Train BasicMotions XCM 32 20 1 0.90625 0.75 0.825 1.0 BasicMotions XCM 32 20 2 1.0 1.0 0.925 1.0 BasicMotions XCM 32 20 3 1.0 1.0 0.925 1.0 BasicMotions XCM 32 20 4 1.0 0.875 0.9 1.0 BasicMotions XCM 32 20 5 0.78125 0.875 0.825 1.0 We then analyzed with a graph the evolution of both accuaries with regard to the epochs. The model is thus perfoming really well as explained in the paper.\nOne of the main improvment of XCM is his explainalibily of the features which can be explicitly shown with layer activations features map. Here is the one we extracted from the model we trained on BasicMotions dataset.\n7. Conclusion The XCM approach signifies a substantial step forward in MTS classification, achieving high accuracy while providing explainability of features which is indispensable for applications demanding transparency in AI decision-making. The paper suggests that future work may focus on refining hyperparameters automatically and exploring the fusion of XCM with other modalities for richer data representation and classification.\nAppendix Implementation of the XCM model with Keras\nReferences Fauvel, K.; Lin, T.; Masson, V.; Fromont, É.; Termier, A. XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification. Mathematics 2021, 9, 3137. DOI: 10.3390/math9233137\nLi, J.; Rong, Y.; Meng, H.; Lu, Z.; Kwok, T.; Cheng, H. TATC: Predicting Alzheimer’s Disease with Actigraphy Data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, London, UK, 19–23 August 2018.\nJiang, R.; Song, X.; Huang, D.; Song, X.; Xia, T.; Cai, Z.; Wang, Z.; Kim, K.; Shibasaki, R. DeepUrbanEvent: A System for Predicting Citywide Crowd Dynamics at Big Events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Anchorage, AK, USA, 4–8 August 2019.\nFauvel, K.; Balouek-Thomert, D.; Melgar, D.; Silva, P.; Simonet, A.; Antoniu, G.; Costan, A.; Masson, V.; Parashar, M.; Rodero, I.; et al. A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, NY, USA, 7–12 February 2020.\nKarim, F.; Majumdar, S.; Darabi, H.; Harford, S. Multivariate LSTM-FCNs for Time Series Classification. Neural Netw. 2019, 116, 237–245. [CrossRef] [PubMed]\nSchäfer, P.; Leser, U. Multivariate Time Series Classification with WEASEL+MUSE. arXiv 2017, arXiv:1711.11343.\nBagnall, A.; Lines, J.; Keogh, E. The UEA Multivariate Time Series Classification Archive, 2018. arXiv 2018, arXiv:1811.00075.\n",
      "content_html": "\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eXCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u003c/h1\u003e\n\u003ch3 style=\"font-size: 24px;\"\u003eAuthors : Nicolas SAINT \u0026 Matthis Guérin\u003c/h3\u003e\n\u003ch4 style=\"font-size: 22px;\"\u003eTable of Contents\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#1-introduction\"\u003e1. Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#2-related-work\"\u003e2. Related Work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#3-xcm\"\u003e3. XCM\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#4-evaluation\"\u003e4. Evaluation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#5-results\"\u003e5. Results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#6-implementation\"\u003e6. Implementation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#7-conclusion\"\u003e7. Conclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#appendix\"\u003eAppendix\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the article \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo; published by Kevin Fauvel et al. in 2021 and available \u003ca href=\"https://www.mdpi.com/2227-7390/9/23/3137\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"1-introduction\"\u003e1. Introduction\u003c/h3\u003e\n\u003cp\u003eThe classification of multivariate time series (MTS) has emerged as an increasingly important research area over the last decade, driven by the exponential growth of temporal data across various domains such as finance, healthcare, mobility, and natural disaster prediction . A time series is a sequence of real values ordered in time, and when a set of co-evolving series is recorded simultaneously by a set of sensors, it is referred to as an MTS. MTS classification, which involves learning the relationship between an MTS and its label, presents a significant challenge due to the inherent complexity of the multivariate and temporal nature of the data.\u003c/p\u003e\n\u003cp\u003eTraditional approaches to MTS classification, while effective on large datasets, encounter significant limitations such as poor generalization on small datasets and a lack of explainability, which can limit their adoption in sensitive applications where understanding the model\u0026rsquo;s decisions is crucial . For example, the European GDPR regulation highlights the importance of providing meaningful explanations for automated decisions, emphasizing the need for approaches capable of reconciling performance and explainability .\u003c/p\u003e\n\u003ch3 id=\"2-related-work\"\u003e2. Related Work\u003c/h3\u003e\n\u003cp\u003eThe existing literature on MTS classification can be broadly grouped into three main categories: similarity-based methods, feature-based methods, and deep learning approaches.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSimilarity-based methods\u003c/strong\u003e: These methods utilize similarity measures to compare two MTS. Dynamic Time Warping (DTW) combined with the nearest neighbor rule (k-NN) has shown impressive performance, although it is not without limitations, particularly in terms of computational cost and the absence of an explicit feature representation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFeature-based methods\u003c/strong\u003e: Approaches such as shapelets and Bag-of-Words (BoW) models transform time series into a more manageable feature space. WEASEL+MUSE, for instance, uses a symbolic Fourier approximation to create a BoW representation of MTS, enabling efficient classification using logistic regression.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDeep learning approaches\u003c/strong\u003e: The advent of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks has opened new avenues for MTS classification, thanks to their ability to automatically learn complex data representations. MLSTM-FCN, combining LSTM and CNN, has been identified as one of the top-performing models, despite its complexity and difficulty in providing explanations for its decisions.\u003c/p\u003e\n\u003cp\u003eExplainability of MTS classification models has become a major concern, particularly for critical applications. Post-hoc methods, such as LIME and SHAP, offer ways to generate explanations for black-box models, but these explanations may lack fidelity to the model\u0026rsquo;s internal workings. This underscores the need for approaches that inherently integrate explainability into the model design.\u003c/p\u003e\n\u003cp\u003eIn this context, our work presents XCM, an innovative convolutional neural network architecture for MTS classification, that not only outperforms existing approaches in terms of performance but also provides reliable and intuitive explanations for its predictions, directly addressing the challenges of performance and explainability in MTS classification. This approach is grounded on the foundational work presented in the paper \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo;, which offers a novel solution to the pressing needs in the domain of MTS classification.\u003c/p\u003e\n\u003ch3 id=\"3-xcm\"\u003e3. XCM\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eXCM\u0026rsquo;s architecture is specifically designed to efficiently address the challenge of multivariate time series (MTS) classification by simultaneously extracting relevant information about observed variables and time directly from the input data. This unique approach allows XCM to capture the complexity and inherent interactions within MTS, thereby enhancing its generalization capability across different datasets and its applicability in various application contexts.\u003c/p\u003e\n\u003cp\u003eTo achieve this, XCM employs a combination of parallel 2D and 1D convolution filters. The 2D filters focus on extracting features related to observed variables at each time instant, while the 1D filters capture temporal dynamics across all variables.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2D Convolution Formula for Observed Variables\u003c/strong\u003e: $$A^{(k)} = f(W^{(k)} * X + b^{(k)})$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$A^{(k)}$: représente la carte des caractéristiques activées pour le k-ème filtre.\u003c/li\u003e\n\u003cli\u003e$f$: denotes the activation function, often ReLU, to introduce non-linearity.\u003c/li\u003e\n\u003cli\u003e$W^{(k)}$, $b^{(k)}$: weights and bias of the $k$-th 2D convolution filter.\u003c/li\u003e\n\u003cli\u003e$X$: the input MTS data.\u003c/li\u003e\n\u003cli\u003e$*$: the convolution operation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy extracting features in this manner, XCM is able to detect complex patterns in MTS that are crucial for precise series classification.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1D Convolution Formula for Temporal Information\u003c/strong\u003e: $$M^{(k)} = f(W^{(k)} \\circledast X + b^{(k)})$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$M^{(k)}$: the activated feature map resulting from 1D filters.\u003c/li\u003e\n\u003cli\u003e$\\circledast$: the 1D convolution operation focusing on the temporal dimension.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis dual convolution approach enables XCM to maintain high accuracy while offering a better understanding of the contributions of different variables and temporal dynamics to the final decision.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/Architecture_XCM.png\"\n  alt=\"alt text\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplainability\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOne of the hallmark features of the XCM architecture is its inherent capability to provide explainable predictions, leveraging the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. Grad-CAM produces heatmaps that highlight the regions of the input data that most significantly contribute to a specific class prediction. This feature is crucial for applications where understanding the model\u0026rsquo;s reasoning is as important as the prediction accuracy itself.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGrad-CAM Calculation\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGrad-CAM utilizes the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the input for predicting the concept. This method allows the visualization of which parts of the input are considered important by the CNN for classification tasks.\u003c/p\u003e\n\u003cp\u003eThe calculation involves the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFeature Map Extraction\u003c/strong\u003e: Firstly, the feature maps $A^{(k)}$ are extracted from the last convolutional layer. These feature maps are essentially the output of the convolution operations and contain the spatial information that the network has learned to identify.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGradient Calculation\u003c/strong\u003e: The gradients of the score for class $c$, denoted as $y^c$\n, with respect to the feature map activations $A^{(k)}$ of a convolutional layer, are computed. These gradients are pooled across the width and height dimensions (indexed by $i$ and $j$) to obtain the neuron importance weights $\\alpha_k^c$.\u003c/p\u003e\n\u003cp\u003eThe weights for the feature map activations are computed as follows:\u003c/p\u003e\n\u003cp\u003e$$\\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^{(k)}}$$ where $Z$ is the number of pixels in the feature map, and $y^c$ is the score for class $c$, before the softmax layer.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWeighted Combination of Feature Maps\u003c/strong\u003e: The weighted combination of feature maps, followed by a ReLU, gives the Grad-CAM heatmap $L_{\\text{Grad-CAM}}^c$ :\n$$L_{\\text{Grad-CAM}}^c = \\text{ReLU}\\left(\\sum_k \\alpha_k^c A^{(k)}\\right)$$\u003c/p\u003e\n\u003cp\u003eThis equation combines the feature maps of the last convolutional layer of the network with the neuron importance weights to produce a heatmap for each class. The ReLU function is applied to the linear combination of maps to only consider the features that have a positive influence on the class of interest, effectively highlighting the regions of the input that are important for predicting class $c$.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis process elucidates how certain input features contribute to the model\u0026rsquo;s predictions, offering a layer of transparency that can aid in the validation and trust-building of machine learning models in critical applications. The ability to generate such visual explanations not only helps in understanding the model\u0026rsquo;s behavior but also in identifying potential biases or errors in the learning process.\u003c/p\u003e\n\u003cp\u003eIn summary, the explainability aspect of XCM, powered by Grad-CAM, stands out as a significant advancement in making deep learning models more interpretable and trustworthy, especially in domains where decision-making processes need to be transparent and justifiable.\u003c/p\u003e\n\u003ch3 id=\"4-evaluation\"\u003e4. Evaluation\u003c/h3\u003e\n\u003cp\u003eThe evaluation of the XCM model focuses on its performance across various datasets from the UEA multivariate time series classification archive. The datasets are diverse, spanning different types such as motion, ECG, HAR (Human Activity Recognition), AS (Audio Spectra), and EEG/MEG (Electroencephalogram/Magnetoencephalogram), with varying lengths, dimensions, and number of classes. This diversity presents a rigorous challenge and a comprehensive platform to assess the capabilities of XCM.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s an exemple of datasets table used ine the paper:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTable: Datasets Overview from UEA Archive\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDatasets\u003c/th\u003e\n          \u003cth\u003eType\u003c/th\u003e\n          \u003cth\u003eTrain\u003c/th\u003e\n          \u003cth\u003eTest\u003c/th\u003e\n          \u003cth\u003eLength\u003c/th\u003e\n          \u003cth\u003eDimensions\u003c/th\u003e\n          \u003cth\u003eClasses\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eArticulary Word Recognition\u003c/td\u003e\n          \u003ctd\u003eMotion\u003c/td\u003e\n          \u003ctd\u003e275\u003c/td\u003e\n          \u003ctd\u003e300\u003c/td\u003e\n          \u003ctd\u003e144\u003c/td\u003e\n          \u003ctd\u003e9\u003c/td\u003e\n          \u003ctd\u003e25\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAtrial Fibrillation\u003c/td\u003e\n          \u003ctd\u003eECG\u003c/td\u003e\n          \u003ctd\u003e15\u003c/td\u003e\n          \u003ctd\u003e15\u003c/td\u003e\n          \u003ctd\u003e640\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasic Motions\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCharacter Trajectories\u003c/td\u003e\n          \u003ctd\u003eMotion\u003c/td\u003e\n          \u003ctd\u003e1422\u003c/td\u003e\n          \u003ctd\u003e1436\u003c/td\u003e\n          \u003ctd\u003e182\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCricket\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e108\u003c/td\u003e\n          \u003ctd\u003e72\u003c/td\u003e\n          \u003ctd\u003e1197\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003e12\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDuck Duck Geese\u003c/td\u003e\n          \u003ctd\u003eAS\u003c/td\u003e\n          \u003ctd\u003e60\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n          \u003ctd\u003e270\u003c/td\u003e\n          \u003ctd\u003e1345\u003c/td\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEigen Worms\u003c/td\u003e\n          \u003ctd\u003eMotion\u003c/td\u003e\n          \u003ctd\u003e128\u003c/td\u003e\n          \u003ctd\u003e131\u003c/td\u003e\n          \u003ctd\u003e17984\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEpilepsy\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e137\u003c/td\u003e\n          \u003ctd\u003e138\u003c/td\u003e\n          \u003ctd\u003e206\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEring\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e30\u003c/td\u003e\n          \u003ctd\u003e30\u003c/td\u003e\n          \u003ctd\u003e65\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEthanol Concentration\u003c/td\u003e\n          \u003ctd\u003eOther\u003c/td\u003e\n          \u003ctd\u003e261\u003c/td\u003e\n          \u003ctd\u003e263\u003c/td\u003e\n          \u003ctd\u003e1751\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFace Detection\u003c/td\u003e\n          \u003ctd\u003eEEG/MEG\u003c/td\u003e\n          \u003ctd\u003e5890\u003c/td\u003e\n          \u003ctd\u003e3524\u003c/td\u003e\n          \u003ctd\u003e62\u003c/td\u003e\n          \u003ctd\u003e144\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinger Movements\u003c/td\u003e\n          \u003ctd\u003eEEG/MEG\u003c/td\u003e\n          \u003ctd\u003e316\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n          \u003ctd\u003e50\u003c/td\u003e\n          \u003ctd\u003e28\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eInterpretation and Results:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eEach dataset presents unique challenges for MTS classification:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eArticulary Word Recognition\u003c/strong\u003e: With a substantial number of classes (25), the model must discern between intricate motion patterns. A high accuracy score here would indicate XCM\u0026rsquo;s ability to manage high-dimensional, complex pattern recognition tasks.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAtrial Fibrillation\u003c/strong\u003e: Given the high length of the time series (640) and fewer instances for training and testing, the model\u0026rsquo;s performance can signal its efficiency in overfitting prevention and extracting meaningful information from lengthy sequences with minimal data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBasic Motions\u003c/strong\u003e: A dataset like this with a shorter length and moderate dimensionality can showcase XCM\u0026rsquo;s quick learning capability for simple temporal patterns and basic human activities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCharacter Trajectories\u003c/strong\u003e: This dataset, with a large training set and many classes, is an excellent test of XCM\u0026rsquo;s scalability and classification robustness in handling motion data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCricket\u003c/strong\u003e: Long sequences (1197) and a fair number of classes (12) make this dataset suited for evaluating XCM\u0026rsquo;s temporal pattern learning and generalization over longer periods.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDuck Duck Geese\u003c/strong\u003e: An Audio Spectrum dataset with a high dimensionality challenges the model to process and classify complex audio patterns, testing XCM\u0026rsquo;s ability in handling non-motion data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEigen Worms\u003c/strong\u003e: With the longest sequences in the given datasets (17,984), XCM\u0026rsquo;s performance can be interpreted as its capability in modeling highly intricate temporal behaviors.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEpilepsy\u003c/strong\u003e: Human activity recognition data like this one requires the model to be sensitive to subtle variations, a good indicator of XCM\u0026rsquo;s precision in critical classification scenarios.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEring\u003c/strong\u003e: Small datasets with higher class counts test the model\u0026rsquo;s overfitting resilience and classification dexterity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEthanol Concentration\u003c/strong\u003e: An \u0026lsquo;Other\u0026rsquo; type dataset with long sequences will challenge any classifier\u0026rsquo;s ability to handle diverse, non-standard data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFace Detection\u003c/strong\u003e: This EEG/MEG dataset has a significant number of instances for both training and testing, focusing on XCM\u0026rsquo;s performance in biometric pattern recognition scenarios.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFinger Movements\u003c/strong\u003e: Another EEG/MEG dataset, but with shorter sequences and fewer dimensions, this can highlight how well XCM captures rapid, subtle changes in electrical activity related to movements.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eHyperparameters and Metrics\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn the evaluation of XCM, a systematic approach was taken to optimize hyperparameters for each dataset. A grid search was employed, where the hyperparameters were fine-tuned to achieve the best average accuracy. This process was underpinned by a stratified 5-fold cross-validation on the training set, ensuring a robust estimation of the model\u0026rsquo;s performance.\u003c/p\u003e\n\u003cp\u003eTo benchmark against other classifiers, the primary metric used was classification accuracy. This metric is standard for evaluating MTS classifiers on the public UEA datasets. Furthermore, classifiers were ranked based on their performance, with the number of wins or ties noted to establish a comparative landscape of classifier effectiveness.\u003c/p\u003e\n\u003cp\u003eBeyond accuracy, a critical difference diagram was used to provide a visual statistical comparison of multiple classifiers across multiple datasets. This method uses the nonparametric Friedman test to highlight performance disparities. For the implementation of this statistical test, the R package scmamp was utilized, which is a recognized tool for such analyses in the machine learning community.\u003c/p\u003e\n\u003cp\u003eThese rigorous evaluation methods ensure that the performance assessment of XCM is both comprehensive and reliable, offering clear insights into its classification capabilities and its standing relative to existing MTS classifiers.\u003c/p\u003e\n\u003cp\u003eFor our research paper based on the XCM method and its performance on various datasets, here’s how we could approach Section 5, which covers the analysis and interpretation of results:\u003c/p\u003e\n\u003ch3 id=\"5-results\"\u003e5. Results\u003c/h3\u003e\n\u003cp\u003eThe performance of the XCM method was rigorously evaluated across a comprehensive set of UEA datasets with a focus on multivariate time series classification. Our approach aimed to balance between achieving high classification accuracy and providing explainability. This section discusses the performance of XCM compared to other leading algorithms such as MLSTM-FCN (MF), WEASEL+MUSE (WM), and Elastic Distances (ED) with DTW independent (DWI) and dependent (DWD) variants.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTable: Performance Comparison on UEA Datasets\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDatasets\u003c/th\u003e\n          \u003cth\u003eXC\u003c/th\u003e\n          \u003cth\u003eXC Seq\u003c/th\u003e\n          \u003cth\u003eMC\u003c/th\u003e\n          \u003cth\u003eMF\u003c/th\u003e\n          \u003cth\u003eWM\u003c/th\u003e\n          \u003cth\u003eED (n)\u003c/th\u003e\n          \u003cth\u003eDWI\u003c/th\u003e\n          \u003cth\u003eDWD\u003c/th\u003e\n          \u003cth\u003e(XC Params) Batch\u003c/th\u003e\n          \u003cth\u003eWin %\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eArticulary Word Recognition\u003c/td\u003e\n          \u003ctd\u003e98.3\u003c/td\u003e\n          \u003ctd\u003e92.7\u003c/td\u003e\n          \u003ctd\u003e92.3\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e97.0\u003c/td\u003e\n          \u003ctd\u003e98.0\u003c/td\u003e\n          \u003ctd\u003e98.7\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAtrial Fibrillation\u003c/td\u003e\n          \u003ctd\u003e46.7\u003c/td\u003e\n          \u003ctd\u003e33.3\u003c/td\u003e\n          \u003ctd\u003e33.3\u003c/td\u003e\n          \u003ctd\u003e20.0\u003c/td\u003e\n          \u003ctd\u003e26.7\u003c/td\u003e\n          \u003ctd\u003e26.7\u003c/td\u003e\n          \u003ctd\u003e26.7\u003c/td\u003e\n          \u003ctd\u003e20.0\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e60\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasic Motions\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e67.6\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e97.5\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCharacter Trajectories\u003c/td\u003e\n          \u003ctd\u003e99.5\u003c/td\u003e\n          \u003ctd\u003e98.8\u003c/td\u003e\n          \u003ctd\u003e97.4\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e99.0\u003c/td\u003e\n          \u003ctd\u003e96.4\u003c/td\u003e\n          \u003ctd\u003e96.9\u003c/td\u003e\n          \u003ctd\u003e99.0\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCricket\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e93.1\u003c/td\u003e\n          \u003ctd\u003e90.3\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e94.4\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDuck Duck Geese\u003c/td\u003e\n          \u003ctd\u003e70.0\u003c/td\u003e\n          \u003ctd\u003e52.5\u003c/td\u003e\n          \u003ctd\u003e65.0\u003c/td\u003e\n          \u003ctd\u003e67.5\u003c/td\u003e\n          \u003ctd\u003e57.5\u003c/td\u003e\n          \u003ctd\u003e27.5\u003c/td\u003e\n          \u003ctd\u003e55.0\u003c/td\u003e\n          \u003ctd\u003e60.0\u003c/td\u003e\n          \u003ctd\u003e8\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEigen Worms\u003c/td\u003e\n          \u003ctd\u003e43.5\u003c/td\u003e\n          \u003ctd\u003e45.0\u003c/td\u003e\n          \u003ctd\u003e41.9\u003c/td\u003e\n          \u003ctd\u003e80.9\u003c/td\u003e\n          \u003ctd\u003e89.0\u003c/td\u003e\n          \u003ctd\u003e55.0\u003c/td\u003e\n          \u003ctd\u003e60.3\u003c/td\u003e\n          \u003ctd\u003e61.8\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEpilepsy\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e93.5\u003c/td\u003e\n          \u003ctd\u003e94.9\u003c/td\u003e\n          \u003ctd\u003e96.4\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e66.7\u003c/td\u003e\n          \u003ctd\u003e97.8\u003c/td\u003e\n          \u003ctd\u003e96.4\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEring\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEthanol Concentration\u003c/td\u003e\n          \u003ctd\u003e34.6\u003c/td\u003e\n          \u003ctd\u003e31.6\u003c/td\u003e\n          \u003ctd\u003e30.8\u003c/td\u003e\n          \u003ctd\u003e31.6\u003c/td\u003e\n          \u003ctd\u003e29.3\u003c/td\u003e\n          \u003ctd\u003e29.3\u003c/td\u003e\n          \u003ctd\u003e30.4\u003c/td\u003e\n          \u003ctd\u003e32.3\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFace Detection\u003c/td\u003e\n          \u003ctd\u003e63.9\u003c/td\u003e\n          \u003ctd\u003e63.8\u003c/td\u003e\n          \u003ctd\u003e50.0\u003c/td\u003e\n          \u003ctd\u003e57.4\u003c/td\u003e\n          \u003ctd\u003e54.5\u003c/td\u003e\n          \u003ctd\u003e51.9\u003c/td\u003e\n          \u003ctd\u003e51.3\u003c/td\u003e\n          \u003ctd\u003e52.9\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e60\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinger Movements\u003c/td\u003e\n          \u003ctd\u003e60.0\u003c/td\u003e\n          \u003ctd\u003e60.0\u003c/td\u003e\n          \u003ctd\u003e49.0\u003c/td\u003e\n          \u003ctd\u003e61.0\u003c/td\u003e\n          \u003ctd\u003e54.0\u003c/td\u003e\n          \u003ctd\u003e55.0\u003c/td\u003e\n          \u003ctd\u003e52.0\u003c/td\u003e\n          \u003ctd\u003e53.0\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e(Note: \u0026ldquo;XC\u0026rdquo; denotes the accuracy of XCM, \u0026ldquo;XC Seq\u0026rdquo; denotes the accuracy of XCM with sequential layers, \u0026ldquo;MC\u0026rdquo; represents MTEX-CNN, \u0026ldquo;MF\u0026rdquo; denotes MLSTM-FCN, \u0026ldquo;WM\u0026rdquo; stands for WEASEL+MUSE, \u0026ldquo;ED (n)\u0026rdquo; represents Elastic Distance (normalized), \u0026ldquo;DWI\u0026rdquo; and \u0026ldquo;DWD\u0026rdquo; refer to Dynamic Time Warping independent and dependent, respectively. \u0026ldquo;Win %\u0026rdquo; indicates the percentage of times XCM achieved the highest accuracy across all folds.)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInterpretation of Results\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eArticulary Word Recognition\u003c/strong\u003e: XCM achieved a high accuracy of 98.3%, showcasing its robustness in motion-based classification and indicating its effectiveness in handling complex time series data with a high dimensional space.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAtrial Fibrillation\u003c/strong\u003e: This dataset posed a challenge with lower accuracy across all methods. XCM\u0026rsquo;s performance at 46.7% suggests that while challenging, it has the potential to discern patterns in smaller and more complex ECG datasets.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBasic Motions\u003c/strong\u003e: XCM perfected the score, highlighting its proficiency in recognizing basic human activity patterns, a crucial capability for HAR applications.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCharacter Trajectories\u003c/strong\u003e: The high score of 99.5% reflects XCM\u0026rsquo;s strength in managing datasets with numerous classes, reinforcing its scalability for extensive data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCricket\u003c/strong\u003e: A perfect score of 100.0% emphasizes XCM\u0026rsquo;s ability to capture intricate temporal patterns, suggesting its suitability for complex HAR scenarios.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDuck Duck Geese\u003c/strong\u003e: XCM\u0026rsquo;s performance at 70.0% accuracy indicates a significant capability in audio spectrum data classification, a testament to its adaptability to different data types.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEigen Worms\u003c/strong\u003e: Despite the lower score, XCM\u0026rsquo;s handling of the longest sequences among the datasets indicates its potential to model complex temporal behaviors in motion data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEpilepsy\u003c/strong\u003e: An accuracy of 99.3% portrays XCM\u0026rsquo;s precision and reliability in critical classification scenarios, essential for medical applications.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEring\u003c/strong\u003e: The universally low scores across methods reflect the dataset\u0026rsquo;s complexity, underscoring a need for specialized approaches or additional features to aid classification.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEthanol Concentration\u003c/strong\u003e: Although challenging, XCM\u0026rsquo;s relatively higher score suggests its capacity to filter meaningful information from noisy data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFace Detection\u003c/strong\u003e: XCM\u0026rsquo;s ability to handle biometric patterns is evidenced by its performance, indicating its utility in EEG/MEG data interpretation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFinger Movements\u003c/strong\u003e: The moderate score reflects the complexity of the task but also suggests XCM\u0026rsquo;s capability to capture rapid changes in EEG/MEG datasets associated with movements.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u0026ldquo;Win %\u0026rdquo; column indicates the superiority of XCM in most datasets, which combined with its explainability features, positions it as a preferred choice for MTS classification in practical applications. This comprehensive analysis not only confirms the effectiveness of the XCM approach but also guides future advancements and potential improvements.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDiscussion\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe results underscore the effectiveness of XCM in multivariate time series classification across a variety of domains, highlighting its capability to maintain high accuracy even in datasets with challenging characteristics. Moreover, the high win percentage indicates XCM\u0026rsquo;s robustness as it frequently outperforms other methods. It is crucial to note that beyond accuracy, XCM\u0026rsquo;s design enables it to offer a layer of explainability which is not captured by accuracy metrics alone but is invaluable in practical applications.\u003c/p\u003e\n\u003ch3 id=\"6-implementation\"\u003e6. Implementation\u003c/h3\u003e\n\u003cp\u003eWe decided to implement ourselves the XCM model using \u003ca href=\"https://github.com/XAIseries/XCM\"\u003ethis GitHub Repository\u003c/a\u003e on a dataset used in the original paper : BasiMotions.\u003c/p\u003e\n\u003cp\u003eThe code of the XCM model is shown in the \u003ca href=\"#appendix\"\u003eAppendix\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere are the results we obtained for a 5 fold training :\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDataset\u003c/th\u003e\n          \u003cth\u003eModel_Name\u003c/th\u003e\n          \u003cth\u003eBatch_Size\u003c/th\u003e\n          \u003cth\u003eWindow_Size\u003c/th\u003e\n          \u003cth\u003eFold\u003c/th\u003e\n          \u003cth\u003eAccuracy_Train\u003c/th\u003e\n          \u003cth\u003eAccuracy_Validation\u003c/th\u003e\n          \u003cth\u003eAccuracy_Test\u003c/th\u003e\n          \u003cth\u003eAccuracy_Test_Full_Train\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e0.90625\u003c/td\u003e\n          \u003ctd\u003e0.75\u003c/td\u003e\n          \u003ctd\u003e0.825\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e0.925\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e0.925\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e0.875\u003c/td\u003e\n          \u003ctd\u003e0.9\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n          \u003ctd\u003e0.78125\u003c/td\u003e\n          \u003ctd\u003e0.875\u003c/td\u003e\n          \u003ctd\u003e0.825\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWe then analyzed with a graph the evolution of both accuaries with regard to the epochs. The model is thus perfoming really well as explained in the paper.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/Evolution_Accuracies.png\"\n  alt=\"Evolution of accuracies during traning\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eOne of the main improvment of XCM is his explainalibily of the features which can be explicitly shown with layer activations features map. Here is the one we extracted from the model we trained on BasicMotions dataset.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/test_MTS_0_layer_2D_Activation.png\"\n  alt=\"2D_activation_layer\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch3 id=\"7-conclusion\"\u003e7. Conclusion\u003c/h3\u003e\n\u003cp\u003eThe XCM approach signifies a substantial step forward in MTS classification, achieving high accuracy while providing explainability of features which is indispensable for applications demanding transparency in AI decision-making. The paper suggests that future work may focus on refining hyperparameters automatically and exploring the fusion of XCM with other modalities for richer data representation and classification.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"appendix\"\u003eAppendix\u003c/h3\u003e\n\u003cp\u003eImplementation of the XCM model with Keras\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/code_xcm.png\"\n  alt=\"XCM\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFauvel, K.; Lin, T.; Masson, V.; Fromont, É.; Termier, A. XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification. Mathematics 2021, 9, 3137. \u003ca href=\"http://dx.doi.org/10.3390/math9233137\"\u003eDOI: 10.3390/math9233137\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLi, J.; Rong, Y.; Meng, H.; Lu, Z.; Kwok, T.; Cheng, H. TATC: Predicting Alzheimer’s Disease with Actigraphy Data. In Proceedings\nof the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, London, UK, 19–23 August 2018.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJiang, R.; Song, X.; Huang, D.; Song, X.; Xia, T.; Cai, Z.; Wang, Z.; Kim, K.; Shibasaki, R. DeepUrbanEvent: A System for Predicting\nCitywide Crowd Dynamics at Big Events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, Anchorage, AK, USA, 4–8 August 2019.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFauvel, K.; Balouek-Thomert, D.; Melgar, D.; Silva, P.; Simonet, A.; Antoniu, G.; Costan, A.; Masson, V.; Parashar, M.; Rodero, I.;\net al. A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning. In Proceedings of the 34th AAAI\nConference on Artificial Intelligence, New York, NY, USA, 7–12 February 2020.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eKarim, F.; Majumdar, S.; Darabi, H.; Harford, S. Multivariate LSTM-FCNs for Time Series Classification. Neural Netw. 2019,\n116, 237–245. [CrossRef] [PubMed]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSchäfer, P.; Leser, U. Multivariate Time Series Classification with WEASEL+MUSE. arXiv 2017, arXiv:1711.11343.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBagnall, A.; Lines, J.; Keogh, E. The UEA Multivariate Time Series Classification Archive, 2018. arXiv 2018, arXiv:1811.00075.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
      "url": "http://localhost:1313/posts/xcm/",
      "date_published": "26036-26-09T355:2626:00+01:00",
      "date_modified": "26036-26-09T355:2626:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "37b39cffb06a5944d96062052d7f779421cde6be",
      "title": "RobustAI_RegMixup",
      "summary": "",
      "content_text": " \u003c!DOCTYPE html\u003e Styled Table RegMixup : Regularizer for robust AI Improve accuracy and Out-of-Distribution Robustness Authors: Marius Ortega, Ly An CHHAY Paper : RegMixup by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania Table of Contents Abstract Introduction Prerequisites Empirical Risk Minimization Vicinal Risk Minimization Mixup RegMixup in theory RegMixup in practice Conclusion Abstract In this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\nIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\nIntroduction Most real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\nThe question is how to improve the robustness of machine learning algorithms to OOD samples ? Many researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\nThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\nRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\n1. Prerequisites In order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\n1.1. Empirical Risk Minimization (ERM) Empirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\n$$ R_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1} $$\nwhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\n1.2. Vicinal Risk Minimization (VRM) Vicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\n$$ R_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2} $$\nConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\n1.3. Mixup Mixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\nIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\nFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\n$$ \\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm} \\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j $$\nWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\n$$ P_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3} $$\nMixup is an interesting method to consider but it possesses some limitations :\nSmall $\\alpha$ issues : With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the figure below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data. Model underconfidence : When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples. Mixup vs RegMixup, underconfidence and space exploration.\n2. RegMixup in theory Now that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\nWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\n$$ P(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4} $$\nHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\nFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\n$$ \\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5} $$\nWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\nSuch a model (equation 4) exhibits properties that lacked in Mixup :\nValues of $\\alpha$ and underconfidence : As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the figure). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate. Prediction entropy : Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a regularizer in essense. As a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\n3. RegMixup in practice (implementation) Now, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\nA baseline model trained with ERM A model trained with Mixup A model trained with RegMixup To do so, we have two possibilities :\nUse the official implementation of RegMixup available on Francesco Pinto's GitHub. Use the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on GitHub. In this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\n3.1. Installation First, we need to install the torch-uncertainty library. To do so, we can use pip :\npip install torch-uncertainty Note: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from PyTorch website) :\npip unistall torch torchvision pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118 To check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\nimport torch print(torch.cuda.is_available()) 3.2. Training the models with torch-uncertainty Now that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\nfrom torch_uncertainty import cli_main, init_args from torch_uncertainty.baselines.classification import ResNet from torch_uncertainty.optimization_procedures import optim_cifar10_resnet18 from torch_uncertainty.datamodules import CIFAR10DataModule from torchvision.datasets import CIFAR10 from torchvision import transforms from torch.nn import CrossEntropyLoss import torch import os from pathlib import Path from cli_test_helpers import ArgvContext Then, we can define the 3 models we discussed earlier :\nbaseline = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18).cuda() mixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, mixup=True, mixup_alpha=0.2).cuda() regmixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, reg_mixup=True, mixup_alpha=15).cuda() Before training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\nroot = Path(os.path.abspath(\u0026#34;\u0026#34;)) # We mock the arguments for the trainer with ArgvContext( \u0026#34;file.py\u0026#34;, \u0026#34;--max_epochs\u0026#34;, \u0026#34;20\u0026#34;, \u0026#34;--enable_progress_bar\u0026#34;, \u0026#34;False\u0026#34;, \u0026#34;--num_estimators\u0026#34;, \u0026#34;8\u0026#34; ): args = init_args(network=ResNet, datamodule=CIFAR10DataModule) net_name = \u0026#34;logs/reset18-cifar10\u0026#34; # datamodule args.root = str(root / \u0026#34;data\u0026#34;) dm = CIFAR10DataModule(**vars(args)) Finally, we can train the models using the cli_main function from torch-uncertainty :\nresults_baseline = cli_main(baseline, dm, root, net_name, args=args) results_mixup = cli_main(mixup, dm, root, net_name, args=args) results_regmixup = cli_main(regmixup, dm, root, net_name, args=args) Note: If you have a gpu, you can make a slight modification to the code to use it :\nClick on cli_main and press F12 to go to the function definition. Go to line 222 and replace the trainer definition by the following one : # trainer trainer = pl.Trainer.from_argparse_args( args, accelerator=\u0026#34;gpu\u0026#34;, devices=1, callbacks=callbacks, logger=tb_logger, deterministic=(args.seed is not None), inference_mode=not (args.opt_temp_scaling or args.val_temp_scaling), ) Save the file and you are all set. 3.3. Results So as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\nWith corruption severity factor of 5, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.656294 0.7480 0.349862 0.032466 0.729336 mixup 0.640811 0.7578 0.335403 0.024429 0.703844 regmixup 0.676174 0.7564 0.340233 0.023135 0.711405 First of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\nWith corruption severity factor of 15, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.615607 0.7402 0.358522 0.048414 0.750933 mixup 0.698558 0.7558 0.338540 0.014760 0.709190 regmixup 0.702599 0.7614 0.327945 0.008439 0.687550 Here the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\n4. Conclusion As a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\n",
      "content_html": "\u003cstyle\nTYPE=\"text/css\"\u003e\n\ncode.has-jax {font:\ninherit;\nfont-size:\n100%; \nbackground: \ninherit; \nborder: \ninherit;}\n\n\u003c/style\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"UTF-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n\u003ctitle\u003eStyled Table\u003c/title\u003e\n\u003cstyle\u003e\n    table {\n        border-collapse: collapse;\n        width: 100%;\n    }\n    th, td {\n        padding: 8px;\n        text-align: center;\n        border-bottom: 1px solid #ddd;\n    }\n    th {\n        background-color: #f2f2f2;\n    }\n    tr:hover {\n        background-color: #f5f5f5;\n    }\n\u003c/style\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eRegMixup : Regularizer for robust AI\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eImprove accuracy and Out-of-Distribution Robustness\u003ch1\u003e\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: Marius Ortega, Ly An CHHAY \u003cbr /\u003e\nPaper : \u003ca href=\"https://arxiv.org/abs/2206.14502\"\u003eRegMixup\u003c/a\u003e  by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0.0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-0.1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003ePrerequisites\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1.1\"\u003eEmpirical Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.2\"\u003eVicinal Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.3\"\u003eMixup\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eRegMixup in theory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eRegMixup in practice \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0.0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eIn this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\u003c/p\u003e\n\u003cp\u003eIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\u003c/p\u003e\n\u003ch2 id=\"section-0.1\"\u003eIntroduction \u003c/h2\u003e\n\u003cp\u003eMost real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\u003c/p\u003e\n\u003cp\u003eThe question is how to improve the robustness of machine learning algorithms to OOD samples ?\nMany researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\u003c/p\u003e\n\u003cp\u003eThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\u003c/p\u003e\n\u003cp\u003eRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003e1. Prerequisites \u003c/h2\u003e\n\u003cp\u003eIn order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\u003c/p\u003e\n\u003ch3 id=\"section-1.1\"\u003e1.1. Empirical Risk Minimization (ERM)\u003c/h3\u003e\n\u003cp\u003eEmpirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\u003c/p\u003e\n\u003cp\u003e$$\nR_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\u003c/p\u003e\n\u003ch3 id=\"section-1.2\"\u003e1.2. Vicinal Risk Minimization (VRM)\u003c/h3\u003e\n\u003cp\u003eVicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\u003c/p\u003e\n\u003cp\u003e$$\nR_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2}\n$$\u003c/p\u003e\n\u003cp\u003eConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\u003c/p\u003e\n\u003ch3 id=\"section-1.3\"\u003e1.3. Mixup\u003c/h3\u003e\n\u003cp\u003eMixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\u003c/p\u003e\n\u003cp\u003eIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\u003c/p\u003e\n\u003cp\u003eFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\u003c/p\u003e\n\u003cp\u003e$$\n\\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm}\n\\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j\n$$\u003c/p\u003e\n\u003cp\u003eWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\u003c/p\u003e\n\u003cp\u003e$$\nP_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3}\n$$\u003c/p\u003e\n\u003cp\u003eMixup is an interesting method to consider but it possesses some limitations :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSmall $\\alpha$ issues :\u003c/strong\u003e With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel underconfidence :\u003c/strong\u003e When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure id=\"my-fig\" class=\"numbered\"\u003e\n    \u003cimg src=\"/images/regmixup/fig.png\" class=\"align-center\"\u003e\n    \u003cp style=\"text-align: center;\"\u003eMixup vs RegMixup, underconfidence and space exploration.\u003c/p\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"section-2\"\u003e2. RegMixup in theory\u003c/h2\u003e\n\u003cp\u003eNow that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\u003c/p\u003e\n\u003cp\u003eWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\u003c/p\u003e\n\u003cp\u003e$$\nP(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4}\n$$\u003c/p\u003e\n\u003cp\u003eHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\u003c/p\u003e\n\u003cp\u003eFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\u003c/p\u003e\n\u003cp\u003e$$\n\\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5}\n$$\u003c/p\u003e\n\u003cp\u003eWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\u003c/p\u003e\n\u003cp\u003eSuch a model (equation 4) exhibits properties that lacked in Mixup :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eValues of $\\alpha$ and underconfidence :\u003c/strong\u003e As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrediction entropy :\u003c/strong\u003e Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a \u003cstrong\u003eregularizer\u003c/strong\u003e in essense.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e3. RegMixup in practice (implementation)\u003c/h2\u003e\n\u003cp\u003eNow, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA baseline model trained with ERM\u003c/li\u003e\n\u003cli\u003eA model trained with Mixup\u003c/li\u003e\n\u003cli\u003eA model trained with RegMixup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo do so, we have two possibilities :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse the official implementation of RegMixup available on \u003ca href=\"https://github.com/FrancescoPinto/RegMixup\"\u003eFrancesco Pinto's GitHub\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eUse the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on \u003ca href=\"https://github.com/ENSTA-U2IS-AI/torch-uncertainty\"\u003eGitHub\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\u003c/p\u003e\n\u003ch3 id=\"31-installation\"\u003e3.1. Installation\u003c/h3\u003e\n\u003cp\u003eFirst, we need to install the torch-uncertainty library. To do so, we can use pip :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install torch-uncertainty\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from \u003ca href=\"https://pytorch.org/get-started/locally/\"\u003ePyTorch website\u003c/a\u003e) :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip unistall torch torchvision\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eis_available\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"32-training-the-models-with-torch-uncertainty\"\u003e3.2. Training the models with torch-uncertainty\u003c/h3\u003e\n\u003cp\u003eNow that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.baselines.classification\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.optimization_procedures\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.datamodules\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.datasets\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.nn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epathlib\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_test_helpers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen, we can define the 3 models we discussed earlier :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ereg_mixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBefore training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epath\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eabspath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# We mock the arguments for the trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003ewith\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;file.py\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--max_epochs\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;20\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--enable_progress_bar\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;False\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--num_estimators\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;8\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enetwork\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edatamodule\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;logs/reset18-cifar10\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# datamodule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;data\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003e\u003cspan style=\"color:#111\"\u003evars\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFinally, we can train the models using the \u003ccode\u003ecli_main\u003c/code\u003e function from torch-uncertainty :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_baseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_mixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_regmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you have a gpu, you can make a slight modification to the code to use it :\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClick on \u003ccode\u003ecli_main\u003c/code\u003e and press \u003ccode\u003eF12\u003c/code\u003e to go to the function definition.\u003c/li\u003e\n\u003cli\u003eGo to line 222 and replace the trainer definition by the following one :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003etrainer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epl\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTrainer\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efrom_argparse_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eaccelerator\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;gpu\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edevices\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003elogger\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etb_logger\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edeterministic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eseed\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eis\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eNone\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003einference_mode\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eopt_temp_scaling\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eval_temp_scaling\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eSave the file and you are all set.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"33-results\"\u003e3.3. Results\u003c/h3\u003e\n\u003cp\u003eSo as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 5, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth\u003eentropy\u003c/th\u003e\n          \u003cth\u003eaccuracy\u003c/th\u003e\n          \u003cth\u003ebrier\u003c/th\u003e\n          \u003cth\u003eece\u003c/th\u003e\n          \u003cth\u003enll\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ebaseline\u003c/td\u003e\n          \u003ctd\u003e0.656294\u003c/td\u003e\n          \u003ctd\u003e0.7480\u003c/td\u003e\n          \u003ctd\u003e0.349862\u003c/td\u003e\n          \u003ctd\u003e0.032466\u003c/td\u003e\n          \u003ctd\u003e0.729336\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003emixup\u003c/td\u003e\n          \u003ctd\u003e0.640811\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.7578\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e0.335403\u003c/td\u003e\n          \u003ctd\u003e0.024429\u003c/td\u003e\n          \u003ctd\u003e0.703844\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eregmixup\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.676174\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e0.7564\u003c/td\u003e\n          \u003ctd\u003e0.340233\u003c/td\u003e\n          \u003ctd\u003e0.023135\u003c/td\u003e\n          \u003ctd\u003e0.711405\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFirst of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 15, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth\u003eentropy\u003c/th\u003e\n          \u003cth\u003eaccuracy\u003c/th\u003e\n          \u003cth\u003ebrier\u003c/th\u003e\n          \u003cth\u003eece\u003c/th\u003e\n          \u003cth\u003enll\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ebaseline\u003c/td\u003e\n          \u003ctd\u003e0.615607\u003c/td\u003e\n          \u003ctd\u003e0.7402\u003c/td\u003e\n          \u003ctd\u003e0.358522\u003c/td\u003e\n          \u003ctd\u003e0.048414\u003c/td\u003e\n          \u003ctd\u003e0.750933\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003emixup\u003c/td\u003e\n          \u003ctd\u003e0.698558\u003c/td\u003e\n          \u003ctd\u003e0.7558\u003c/td\u003e\n          \u003ctd\u003e0.338540\u003c/td\u003e\n          \u003ctd\u003e0.014760\u003c/td\u003e\n          \u003ctd\u003e0.709190\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eregmixup\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.702599\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.7614\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e0.327945\u003c/td\u003e\n          \u003ctd\u003e0.008439\u003c/td\u003e\n          \u003ctd\u003e0.687550\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHere the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e4. Conclusion\u003c/h2\u003e\n\u003cp\u003eAs a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/robustai_regmixup/",
      "date_published": "24036-24-09T338:2424:00+01:00",
      "date_modified": "24036-24-09T338:2424:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "8b29896ef63d10ea5c0197faf234c9919f754125",
      "title": "Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints",
      "summary": "",
      "content_text": "Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints Authors: Godefroy LAMBERT and Louise DAVY Table of Contents Introduction Definitions AUC-based fairness constraints ROC-based fairness constraints Results Reproducibility Conclusion This is a blog post about the paper Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints, published by R. Vogel et al. in 2021 and available here.\nIntroduction With recent advances in machine learning, applications are becoming increasingly numerous and the expectations are high. Those applications will only be able to be deployed if some important issues are addressed such as bias. There are famous datasets known for containing variables that induce a lot of bias such as Compas with racial bias and gender bias in the Adult dataset. To avoid those biases, new algorithms were created to provide more fairness in the prediction by using diverse methods.\nToday, we will be reviewing the methods presented in “Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints”. This paper uses basic metrics such as AUC constraint and ROC constraint and shows some limitations. Since this is bipartite ranking, we will only focus on binary prediction, such as will this person recid for the COMPAS dataset or will this person get his loan for the Adult dataset.\nDefinitions The goal of bipartite ranking is to acquire an ordering of X where positive instances are consistently ranked above negative ones with a high probability. This is done by learning an appropriate scoring function $s$. Such scoring functions are widely used in many critical domains such as loan granting, anomaly detection, or even in court decisions. A nice way to assess their performance is through the analysis of the Receiver Operating Characteristic (ROC) curve and the Area Under the ROC Curve (AUC).\nROC stands for Receiver Operating Characteristic curve and is a graph showing the performance of a classification model at all classification thresholds for a model. This curve plots two parameters:\nTrue Positive Rate False Positive Rate The formula for the True Positive Rate (TPR) is: $$TPR = \\frac{TP}{TP + FN}$$\nAnd the formula for the False Positive Rate (FPR) is: $$FPR = \\frac{FP}{FP + TN}$$\nWith ${FP}$ = False Positive, $FN$ = False Negative, $TP$ = True Positive, $TN$ = True Negative.\nBy varying the classifier, we can obtain different ROC curves that are represented in the following image. The curve that is closer to the upper-left corner is the best one, while the curve in diagonal represents a random classifier. AUC stands for Area Under the ROC Curve and is a widely used metric in machine learning, particularly in binary classification tasks. The AUC quantifies the overall performance of the model across all possible classification thresholds.\nThat is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). The AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0, one whose predictions are 100% correct has an AUC of 1.0.\nWhile fairness seems like a desirable goal for any ranking function, there are many different definitions of what fairness really is and thus, many different metrics to assess the fairness of an algorithm. In the case of loan grants for example, one could consider that fairness is achieved between men and women if we granted the same percentage of loans for both groups. Statistical parity, which compares the proportion of positive outcomes between different demographic groups, is a good metric in this case. However, this approach might overlook underlying disparities in socioeconomic status that affect loan approval rates. Another vision of fairness might ensure that individuals are all as likely to get a wrong decision, regardless of demographic factors such as gender or ethnicity. In this case, parity of mistreatment would be a good metric, as it ensures that the proportion of errors is the same for all demographic groups. However, this considers that all errors are the same, which means that one group could have a high false positive rate and another a high false negative rate. The authors thus decided to choose parity in false positive rates and/or parity in false negative rates.\nAUC-based fairness constraints This first approach is based on the AUC, it will help us to highlight the limitations of this metric which motivated the authors to introduce another approach based on ROC constraints.\nPrecise example of AUC based constraints presented in the paper are the intra-group pairwise AUC fairness (Beutel et al., 2019), Background Negative Subgroup Positive (BNSP) AUC fairness (Borkan et al., 2019), the inter-group pairwise AUC fairness (Kallus and Zhou, 2019). The first one require the ranking performance to be equal within groups, the second one enforces that positive instances from either group have the same probability of being ranked higher than a negative example and the last one imposes that the positives of a group can be distinguished from the negatives of the other group as effectively for both groups. Those 3 AUC based constraints are only a part of the many constraints that exist.\nThe paper introduces a new framework to generalize all relevant AUC-based constraint as a linear combination of 5 relevant elementary constraints noted $C_1$ to $C_5$.\nThe value of |$ C_ {1} $(s)| (resp. |$ C_ {2} $(s)|) quantifies the resemblance of the distribution of the negatives (resp. positives) between the two sensitive attributes.\n$ C_ {1} $(s) = $ AUC_ {{H_S^{(0)}} ,{H_S^{(1)}}} $ - $\\frac{1}{2}$\n$ C_ {2} $(s) = $\\frac{1}{2}$ - $ AUC_ {{G_S^{(0)}} ,{G_S^{(1)}}} $\nThe values of $ C_ {3} $(s), $ C_ {4} $(s) and $ C_ {5} $(s) measure the difference in ability of a score to discriminate between positives and negatives for any two pairs of sensitive attributes.\n$ C_ {3} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $\n$ C_ {4} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $\n$ C_ {5} $(s) = $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(1)}}} $\nThe family of fairness constraints considered is then the set of linear combinations of the $C_l(s)$ = 0:\n\\begin{align*} % $C_l(s)$ = 0 C_Γ(s): Γ^T C(s) = \\sum_{l=1}^{5} {Γ_l}{C_l}(s) = 0 \\end{align*}\nWhere $Γ$ = $(Γ_1, \u0026hellip; Γ_5)^T$.\nThe objective function is thus defined as follows :\n\\begin{align} \\label{eq:auc_general_problem} \\textstyle\\max_{s\\in\\mathcal{S}} \\quad AUC_{H_s,G_s} - \\lambda |\\Gamma^\\top C (s)|, \\end{align} where $\\lambda\\ge 0$ is a hyperparameter balancing ranking performance and fairness.\nThe paper focuses on a special case of fairness, the intra-group pairwise AUC fairness. This was to be more concise. In this example, the objective function becomes:\n$$ L_\\lambda(s) = AUC_{H_s,G_s} - \\lambda | AUC_{H_s^{(0)}, G_s^{(0)}} - AUC_{H_s^{(1)}, G_s^{(1)} } | $$\nIssues of AUC-Based constraint:\nFairness using AUC-based constraints defined by the equality between two AUC’s only quantify a stochastic order between distributions, not the equality between these distributions, and would lead to some unfair result, for a group or for the other group.\nThe authors conducted experiments with the credit-risk dataset and found that creditworthy individuals from both groups had equal chances of being ranked higher than a \u0026ldquo;bad borrower.\u0026rdquo; However, employing high thresholds (which represent low probabilities of default on approved loans) would result in unfair outcomes for one group.\nROC-based fairness constraints A richer approach is then to use pointwised ROC-based fairness constraints. Ideally, we would want to enforce the equality of all score distributions between both groups (i.e., identical ROC curves). This would satisfy all AUC-based fairness constraints previously mentioned. However, this condition is so restrictive that it will most likely lead to a significant drop in performances. As a result, the authors propose to satisfy this constraint on only a finite number of points. They were indeed able to prove that this was sufficient to ensure maximum fairness for a fixed false positive or false negative $\\alpha$.\nAs a result, the objective function becomes :\n\\begin{align*} % L_\\Lambda(s) = AUC_{H_s,G_s} \u0026amp;- \\sum_{k=1}^{m_H} \\lambda_H^{(k)} \\big| \\Delta_{H,\\alpha_H^{ (k)}}(s) \\big| - \\sum_{k=1}^{m_G} \\lambda_G^{(k)} \\big| \\Delta_{G,\\alpha_G^{(k)}}(s) \\big|, \\end{align*}\nWhere $\\Delta_{H,\\alpha_H^{(k)}}(s)$ and $\\Delta_{G,\\alpha_G^{(k)}}(s)$ represent the deviations between the positive (resp. negative) inter-group ROCs and the identity function:\n$$ \\Delta_{G, \\alpha}(s) = ROC_{G^{(0)}_s, G^{(1)}_s}(\\alpha) - \\alpha $$\n$$ \\Delta_{H, \\alpha}(s) = ROC_{H^{(0)}_s,H^{(1)}_s}(\\alpha) - \\alpha $$\nIn practice, the objective function is slightly modified to be able to maximise it. The authors applied a classic smooth surrogate relaxations of the AUCs or ROCs based on a logistic function. They also removed the absolute values and, instead, relied on some parameters to ensure positive values.\nResults The authors tested out their results on two datasets : Compas and Adult. Both are widely used when it comes to fairness. Indeed, they are known to be biased against race (for Compas) and gender (for both). Compas is a recidivism prediction dataset, whereas Adult predicts whether income exceeds $50K/yr based on census data. The results reported in the next figure show that the ROC-based method achieves its goal of mitigating the differences between favoured and unfavoured groups with limited drop in performances (the AUC went from 0.72 to 0.70 on the Compas dataset and from 0.91 to 0.87 on the Adult dataset). Indeed, the blue ROC curve, which is the ROC curve of the unfavoured group (Afro-American people for the Compas Dataset and women for the Adult Dataset), is brought closer to the green ROC curve (the ROC curve of the favoured group).\nReproducibility We were able to run the provided code without too much trouble on WSL2. The only modification we had to make was to change the calls for python in the sh files. We replace python with python3. However, as mentionned in the cide, the experiments were very long to run (several days) and we were not able to run the generate_all_figures.sh script fully as it made our computers crash. Still, we were able to get some of the figures found in the paper (see below) by launching some scripts separately.\nHere are two figure generated for the toy 1 dataset, one for the distribution of the scores and one for the ROC curve.\nConclusion The paper \u0026ldquo;Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints\u0026rdquo; underscores the growing importance of fairness in machine learning applications. It shows the limits of AUC-based fairness constraints for their inability to ensure equality between distributions, potentially leading to unfair outcomes. In contrast, ROC-based fairness constraints offer a richer approach by enforcing equality of score distributions between groups, albeit with some performance trade-offs. The paper tests the method on typical fairness datasets, but it is also possible to apply it to reel use cases. \u0026ldquo;A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization\u0026rdquo;, for example, explores the possibility to apply ROC-based methods for similarity learning, such as face recognition.\n",
      "content_html": "\u003ch1 style=\"font-size: 24px;\"\u003eLearning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints\u003c/h1\u003e\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: Godefroy LAMBERT and Louise DAVY\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eDefinitions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eAUC-based fairness constraints\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eROC-based fairness constraints\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eResults\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReproducibility\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints, published by R. Vogel et al. in 2021 and available \u003ca href=\"http://proceedings.mlr.press/v130/vogel21a/vogel21a-supp.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"section-1\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eIntroduction\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eWith recent advances in machine learning, applications are becoming increasingly numerous and the expectations are high. Those applications will only be able to be deployed if some important issues are addressed such as bias. There are famous datasets known for containing variables that induce a lot of bias such as Compas with racial bias and gender bias in the Adult dataset. To avoid those biases, new algorithms were created to provide more fairness in the prediction by using diverse methods.\u003c/p\u003e\n\u003cp\u003eToday, we will be reviewing the methods presented in “Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints”. This paper uses basic metrics such as AUC constraint and ROC constraint and shows some limitations. Since this is bipartite ranking, we will only focus on binary prediction, such as will this person recid for the COMPAS dataset or will this person get his loan for the Adult dataset.\u003c/p\u003e\n\u003ch1 id=\"section-2\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eDefinitions\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe goal of \u003cstrong\u003ebipartite ranking\u003c/strong\u003e is to acquire an ordering of X where positive instances are consistently ranked above negative ones with a high probability. This is done by learning an appropriate scoring function $s$. Such scoring functions are widely used in many critical domains such as \u003cstrong\u003eloan granting\u003c/strong\u003e, \u003cstrong\u003eanomaly detection\u003c/strong\u003e, or even in \u003cstrong\u003ecourt decisions\u003c/strong\u003e. A nice way to assess their performance is through the analysis of the \u003cstrong\u003eReceiver Operating Characteristic\u003c/strong\u003e (ROC) curve and the \u003cstrong\u003eArea Under the ROC Curve\u003c/strong\u003e (AUC).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eROC\u003c/strong\u003e stands for \u003cstrong\u003eReceiver Operating Characteristic curve\u003c/strong\u003e and is a graph showing the performance of a classification model at all classification thresholds for a model. This curve plots two parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTrue Positive Rate\u003c/li\u003e\n\u003cli\u003eFalse Positive Rate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/roc_easy.png\"\n  alt=\"Roc_1\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe formula for the True Positive Rate (TPR) is:\n$$TPR = \\frac{TP}{TP + FN}$$\u003c/p\u003e\n\u003cp\u003eAnd the formula for the False Positive Rate (FPR) is:\n$$FPR = \\frac{FP}{FP + TN}$$\u003c/p\u003e\n\u003cp\u003eWith ${FP}$ = False Positive, $FN$ = False Negative, $TP$ = True Positive, $TN$ = True Negative.\u003c/p\u003e\n\u003cp\u003eBy varying the classifier, we can obtain different ROC curves that are represented in the following image. The curve that is closer to the upper-left corner is the best one, while the curve in diagonal represents a random classifier.\n\u003cimg\n  src=\"/images/lambert_davy/Roc_curve.svg.png\"\n  alt=\"Roc_full\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAUC\u003c/strong\u003e stands for \u003cstrong\u003eArea Under the ROC Curve\u003c/strong\u003e and is a widely used metric in machine learning, particularly in binary classification tasks. The AUC quantifies the \u003cstrong\u003eoverall performance of the model\u003c/strong\u003e across all possible classification thresholds.\u003c/p\u003e\n\u003cp\u003eThat is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). The AUC ranges in value from 0 to 1. A model whose predictions are \u003cstrong\u003e100% wrong has an AUC of 0.0\u003c/strong\u003e, one whose predictions are \u003cstrong\u003e100% correct has an AUC of 1.0\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/AUC.png\"\n  alt=\"AUC\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eWhile \u003cstrong\u003efairness\u003c/strong\u003e seems like a desirable goal for any ranking function, there are many different definitions of what fairness really is and thus, many different \u003cstrong\u003emetrics\u003c/strong\u003e to assess the fairness of an algorithm. In the case of loan grants for example, one could consider that fairness is achieved between men and women if we granted the same percentage of loans for both groups. \u003cstrong\u003eStatistical parity\u003c/strong\u003e, which  compares the proportion of positive outcomes between different demographic groups, is a good metric in this case.  However, this approach might overlook underlying disparities in socioeconomic status that affect loan approval rates. Another vision of fairness might ensure that individuals are all as likely to get a wrong decision, regardless of demographic factors such as gender or ethnicity. In this case, \u003cstrong\u003eparity of mistreatment\u003c/strong\u003e would be a good metric, as it ensures that the proportion of errors is the same for all demographic groups. However, this considers that all errors are the same, which means that one group could have a high false positive rate and another a high false negative rate. The authors thus decided to choose \u003cstrong\u003eparity in false positive rates\u003c/strong\u003e and/or \u003cstrong\u003eparity in false negative rates\u003c/strong\u003e.\u003c/p\u003e\n\u003ch1 id=\"section-3\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eAUC-based fairness constraints\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThis first approach is based on the AUC, it will help us to highlight the limitations of this metric which motivated the authors to introduce another approach based on ROC constraints.\u003c/p\u003e\n\u003cp\u003ePrecise example of AUC based constraints presented in the paper are the intra-group pairwise AUC fairness (Beutel et al., 2019), Background Negative Subgroup Positive (BNSP) AUC fairness (Borkan et al., 2019), the inter-group pairwise AUC fairness (Kallus and Zhou, 2019). The first one require the ranking performance to be equal within groups, the second one enforces that positive instances from either group have the same probability of being ranked higher than a negative example and the last one imposes that the positives of a group can be distinguished from the negatives of the other group as effectively for both groups. Those 3 AUC based constraints are only a part of the many constraints that exist.\u003c/p\u003e\n\u003cp\u003eThe paper introduces a new framework to generalize all relevant AUC-based constraint as a \u003cstrong\u003elinear combination of 5 relevant elementary constraints\u003c/strong\u003e noted $C_1$ to $C_5$.\u003c/p\u003e\n\u003cp\u003eThe value of |$ C_ {1} $(s)| (resp. |$ C_ {2} $(s)|) quantifies the \u003cstrong\u003eresemblance of the distribution\u003c/strong\u003e of the negatives (resp. positives) between the \u003cstrong\u003etwo sensitive attributes\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e$ C_ {1} $(s) = $ AUC_ {{H_S^{(0)}} ,{H_S^{(1)}}} $ - $\\frac{1}{2}$\u003c/p\u003e\n\u003cp\u003e$ C_ {2} $(s) = $\\frac{1}{2}$ - $ AUC_ {{G_S^{(0)}} ,{G_S^{(1)}}} $\u003c/p\u003e\n\u003cp\u003eThe values of $ C_ {3} $(s), $ C_ {4} $(s) and $ C_ {5} $(s) measure the \u003cstrong\u003edifference\nin ability of a score to discriminate\u003c/strong\u003e between positives and negatives for any two pairs of sensitive attributes.\u003c/p\u003e\n\u003cp\u003e$ C_ {3} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $\u003c/p\u003e\n\u003cp\u003e$ C_ {4} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $\u003c/p\u003e\n\u003cp\u003e$ C_ {5} $(s) = $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(1)}}} $\u003c/p\u003e\n\u003cp\u003eThe family of fairness constraints considered is then the set of linear combinations of the $C_l(s)$ = 0:\u003c/p\u003e\n\u003cp\u003e\\begin{align*}\n% $C_l(s)$ = 0\nC_Γ(s): Γ^T C(s) =\n\\sum_{l=1}^{5} {Γ_l}{C_l}(s) = 0\n\\end{align*}\u003c/p\u003e\n\u003cp\u003eWhere $Γ$ = $(Γ_1, \u0026hellip; Γ_5)^T$.\u003c/p\u003e\n\u003cp\u003eThe objective function is thus defined as follows :\u003c/p\u003e\n\u003cp\u003e\\begin{align}\n\\label{eq:auc_general_problem}\n\\textstyle\\max_{s\\in\\mathcal{S}} \\quad AUC_{H_s,G_s} - \\lambda\n|\\Gamma^\\top\nC\n(s)|,\n\\end{align}\nwhere $\\lambda\\ge 0$ is a hyperparameter balancing ranking performance\nand fairness.\u003c/p\u003e\n\u003cp\u003eThe paper focuses on a special case of fairness, the \u003cstrong\u003eintra-group pairwise AUC fairness\u003c/strong\u003e. This was to be more concise. In this example, the objective function becomes:\u003c/p\u003e\n\u003cp\u003e$$\nL_\\lambda(s) = AUC_{H_s,G_s} - \\lambda  | AUC_{H_s^{(0)}, G_s^{(0)}} -  AUC_{H_s^{(1)}, G_s^{(1)} } |\n$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cu\u003e Issues of AUC-Based constraint:\u003c/u\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFairness using AUC-based constraints defined by the equality between two AUC’s only quantify a stochastic order between distributions, not the equality between these distributions, and would lead to some unfair result, for a group or for the other group.\u003c/p\u003e\n\u003cp\u003eThe authors conducted experiments with the credit-risk dataset and found that creditworthy individuals from both groups had equal chances of being ranked higher than a \u0026ldquo;bad borrower.\u0026rdquo; However, employing high thresholds (which represent low probabilities of default on approved loans) would result in unfair outcomes for one group.\u003c/p\u003e\n\u003ch1 id=\"section-4\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eROC-based fairness constraints\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eA richer approach is then to use \u003cstrong\u003epointwised ROC-based fairness constraints\u003c/strong\u003e. Ideally, we would want to enforce the equality of all score distributions between both groups (i.e., identical ROC curves). This would satisfy all AUC-based fairness constraints previously mentioned. However, this condition is so restrictive that it will most likely lead to a significant drop in performances. As a result, the authors propose to satisfy this constraint on only a \u003cstrong\u003efinite number of points\u003c/strong\u003e. They were indeed able to prove that this was sufficient to ensure maximum fairness for a fixed false positive or false negative  $\\alpha$.\u003c/p\u003e\n\u003cp\u003eAs a result, the objective function becomes :\u003c/p\u003e\n\u003cp\u003e\\begin{align*}\n% L_\\Lambda(s) =\nAUC_{H_s,G_s} \u0026amp;-\n\\sum_{k=1}^{m_H} \\lambda_H^{(k)}  \\big| \\Delta_{H,\\alpha_H^{\n(k)}}(s) \\big|\n- \\sum_{k=1}^{m_G} \\lambda_G^{(k)} \\big| \\Delta_{G,\\alpha_G^{(k)}}(s) \\big|,\n\\end{align*}\u003c/p\u003e\n\u003cp\u003eWhere $\\Delta_{H,\\alpha_H^{(k)}}(s)$ and $\\Delta_{G,\\alpha_G^{(k)}}(s)$ represent the deviations between the positive (resp. negative) inter-group ROCs and the identity function:\u003c/p\u003e\n\u003cp\u003e$$\n\\Delta_{G, \\alpha}(s) = ROC_{G^{(0)}_s, G^{(1)}_s}(\\alpha) - \\alpha\n$$\u003c/p\u003e\n\u003cp\u003e$$\n\\Delta_{H, \\alpha}(s) = ROC_{H^{(0)}_s,H^{(1)}_s}(\\alpha) - \\alpha\n$$\u003c/p\u003e\n\u003cp\u003eIn practice, the objective function is slightly modified to be able to maximise it. The authors applied a classic smooth surrogate relaxations of the AUCs or ROCs based on a logistic function. They also removed the absolute values and, instead, relied on some parameters to ensure positive values.\u003c/p\u003e\n\u003ch1 id=\"section-5\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eResults\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe authors tested out their results on two datasets : \u003cstrong\u003eCompas\u003c/strong\u003e and \u003cstrong\u003eAdult\u003c/strong\u003e. Both are widely used when it comes to fairness. Indeed, they are known to be biased against race (for Compas) and gender (for both). Compas is a recidivism prediction dataset, whereas Adult predicts whether income exceeds $50K/yr based on census data. The results reported in the next figure show that the ROC-based method achieves its goal of mitigating the differences between favoured and unfavoured groups with limited drop in performances (the AUC went from 0.72 to 0.70 on the Compas dataset and from 0.91 to 0.87 on the Adult dataset). Indeed, the blue ROC curve, which is the ROC curve of the unfavoured group (Afro-American people for the Compas Dataset and women for the Adult Dataset), is brought closer to the green ROC curve (the ROC curve of the favoured group).\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/main_text_inkscape_all_rocs_no_train_new.svg\"\n  alt=\"AUC\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch1 id=\"section-6\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eReproducibility\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eWe were able to run the provided code without too much trouble on WSL2. The only modification we had to make was to change the calls for python in the sh files. We replace \u003ccode\u003epython\u003c/code\u003e with \u003ccode\u003epython3\u003c/code\u003e. However, as mentionned in the cide, the experiments were very long to run (several days) and we were not able to run the \u003ccode\u003egenerate_all_figures.sh\u003c/code\u003e script fully as it made our computers crash. Still, we were able to get some of the figures found in the paper (see below) by launching some scripts separately.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/dist.png\"\n  alt=\"dist\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/roc.png\"\n  alt=\"roc_gen\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere are two figure generated for the toy 1 dataset, one for the distribution of the scores and one for the ROC curve.\u003c/p\u003e\n\u003ch1 id=\"section-7\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eConclusion\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe paper \u0026ldquo;Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints\u0026rdquo; underscores the growing importance of fairness in machine learning applications. It shows the limits of AUC-based fairness constraints for their inability to ensure equality between distributions, potentially leading to unfair outcomes. In contrast, ROC-based fairness constraints offer a richer approach by enforcing equality of score distributions between groups, albeit with some performance trade-offs. The paper tests the method on typical fairness datasets, but it is also possible to apply it to reel use cases. \u0026ldquo;A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization\u0026rdquo;, for example, explores the possibility to apply ROC-based methods for similarity learning, such as face recognition.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e",
      "url": "http://localhost:1313/posts/lambert-davy/",
      "date_published": "23036-23-09T339:2323:00+01:00",
      "date_modified": "23036-23-09T339:2323:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "7caf312b2212cb7ecf9bc99e08261d1c412013a5",
      "title": "Label-Free Explainability",
      "summary": "",
      "content_text": "Label-Free Explainability for Unsupervised Models Authors: Valentina Hu and Selma Zarga Table of Contents Incentives Introduction Feature Importance Example Importance Experiment Conclusion This is a blog post about the paper Label-Free Explainability for Unsupervised Models, published by J. Crabbé et al. in 2022 and available here.\nWhy do we need explainability ? Machine learning models are becoming increasingly capable of making advanced predictions. While models like linear regression are relatively easy to understand and explain, more complex models, often called \u0026ldquo;black boxes\u0026rdquo; due to their complexity, present challenges in explaining how they make predictions. These models can be problematic in highstakes applications such as healthcare, finance, and justice, where it\u0026rsquo;s crucial to justify decision-making. Additionally, in case of errors, it\u0026rsquo;s important to understand the origin in order to address and correct them.\n\u0026ldquo;Explainability is the cornerstone of trust in black box models; without it, they remain inscrutable and unreliable.\u0026rdquo; - Yoshua Bengio\nTo tackle this challenge, the field of Explainable Artificial Intelligence (XAI) has emerged, offering various methods to enhance model transparency. Post-Hoc explainability methods exist, which intervene after the model has generated its results, enabling users to comprehend the reasoning behind specific decisions or predictions. These methods supplement the predictions of black box models with diverse explanations of how they arrive at their predictions.\nI. Introduction The entire post focuses on the quest for explainability of unsupervised models. In these models, no labels are assigned to the data, making understanding the model even more complicated due to the absence of explicit guidance on what the model is learning. In the supervised setting, users know the meaning of the black-box output they are trying to interpret. However, this clarity is not always available in machine learning. Therefore, elucidating concepts such as feature importance and example importance provides insights into why the model makes certain decisions or identifies specific patterns in the data.\nA recent research conducted by Crabbé and van der Schaar in 2022 explores the explainability of unsupervised models. They have developed two new methods to explain these complex models without labels. The first method highlights important features in the data, while the second identifies training examples that have the biggest impact on the model\u0026rsquo;s construction of representations. In this post, we will attempt to explain these two methods.\nII. Feature Importance Feature importance aims to explain how the model arrives at its prediction for a given input by assigning an importance scores to each feature (or attribute) of the input. This helps understand which features have the most influence on the model\u0026rsquo;s predictions. This method is developed based on a linear reasoning that is extended to label-free settings.\nGiven a model $( f : \\mathcal{X} \\rightarrow \\mathcal{Y} )$, which maps an input space $( \\mathcal{X} \\subset \\mathbb{R}^{d_X} )$ to an output space $( \\mathcal{Y} \\subset \\mathbb{R}^{d_Y} )$. Where, $( d_X )$ and $( d_Y )$ is the dimensions of the input and output spaces.\nIn the traditionnal method, the process requires selecting one component $( f_j(x) )$ of the model\u0026rsquo;s output to compute the importance score for each feature $( i )$, denoted as $( a_i(f, x) )$. The selection is based on the ground-truth label, and $( j )$ corresponds to the class predicted with the highest probability.\nTo understand how the label-free feature importance method works, let\u0026rsquo;s start by looking at the labeled case:\n1. Labelled Feature Importance\nAuthors introduces an alternative approach to calculate feature importance scores. The method proposes to combine the importance scores of different components of the model\u0026rsquo;s output by weighting them with the associated class probabilities. For each component of the model\u0026rsquo;s output, we multiply the importance score of the corresponding feature by the probability of that component.\nThese weighted importance scores are then combined to obtain the final importance score of each feature.\nLet $a_i(f_j;x)$ be the importance score of feature $x_i$ calculated with respect to the component $f_j$ of the model\u0026rsquo;s output. The method proposes to calculate the importance score $b_i(f;x)$ for feature $x_i$ as follows:\n$b_i(f;x) = \\sum_{j=1}^{d_Y} f_j(x) \\times a_i(f_j,x)$\nHere, $f_j(x)$ represents the probability of class $j$, and $a_i(f_j;x)$ is the importance score of feature $x_i$ for class $j$.\nHovewer when the class probabilities are balanced, this method accounts for the contribution of each class to the feature importance score, rather than focusing only on the class with the highest probability, which is the usual practice.\nThis method proves to be efficient predominantly when the significance scores exhibit linearity in relation to the model. To facilitate a streamlined computation of weighted importance scores, another method is to introduce an auxiliary function, denoted as $(g_x)$ :\n$\\ g_x(z) = \\sum_{j=1}^{d_Y} f_j(x) \\cdot f_j(z) $\nWith the function $(g_x)$, it becomes feasible to calculate the weighted importance score, $(b_i(f, x))$, for each feature $(i)$, by merely employing $(g_x)$. This technique significantly simplifies the computational process, obviating the need to calculate $(d_Y \\times d_X)$ importance scores. Such a calculation becomes impractically cumbersome with the escalation of the number of classes, $(d_Y)$. With this trick, we can compute the weighted importance score by only calling the auxiliary function.\nWe can see that in the labeled case, the method is quite clear. A similar reasoning is used in the label-free setting. Now, let\u0026rsquo;s move on to the label-free setting.\n2. Label-Free Feature Importance\nIn the context of the unlabelled setting, we consider a latent space $H$ of dimension $d_H$ where a black-box model $f : X \\rightarrow H$ is given. The goal is to assign an importance score to each feature of the input $x$, even if the dimensions of the latent space have no clear relations with the labels.\nA similar weighting formula for importance scores is used, where the components $f_j(x)$ do not correspond to probabilities but to neuron activations. The weighted sum is considered as a inner product in the latent space.\nThe method is developed using linear feature importance functions, and it retains the completeness property, meaning that the sum of importance scores equals the black-box prediction up to a baseline constant.\nHere is how the method operates:\nPresentation of the Latent Space: We consider a latent space $H$ of dimension $d_H$ where each input $x$ is mapped by the black-box model $f$ to obtain a representation $h = f(x)$.\nAssignment of Importance Scores: The objective is to assign an importance score $b_i(f; x)$ to each feature $x_i$ of $x$. Unlike in the previous setting, where we had probabilities associated with each component, here, we do not have a clear method to choose a particular component $f_j$ in the latent space. Therefore, we use a similar approach to the one described previously.\nCalculation of Importance Scores: We use a weighting method where the importance score is given by $b_i(f; x) = a_i(\\sum_{j=1}^{d_H} f_j(x) \\cdot f_j(x))$. The individual components $f_j(x)$ do not correspond to probabilities in this case; they generally correspond to neuron activation functions. Inactive neurons will have a corresponding component that vanishes ($f_j(x) = 0$), meaning they will not contribute to the weighted sum, while more activated neurons will contribute more.\nCompleteness: An important property shared by many feature importance methods is completeness. This means that the sum of importance scores equals the black-box prediction up to a baseline constant. This establishes a connection between importance scores and black-box predictions.\nThis method proposes an extension of linear feature importance methods to the unlabelled setting by defining an auxiliary scalar function $g_x$ that encapsulates the black-box function $f$. This extension is achieved by using a function $g_x$ that computes the inner product between the representation $f(x)$ and the representation $f(\\tilde{x})$ for all $\\tilde{x}$ in the input space.\nIII. Example Importance In this section, we explain the approach to extending example importance methods to the label-free setting. Given that example importance methods vary significantly, they are separated into two families: loss-based and representation-based methods. The extension to the label-free setting differs for these two families, so we discuss them separately in distinct subsections.\n1. Loss-Based Example Importance\nSupervised Setting\nIn supervised learning, loss-based example importance methods determine how important each training example is by assessing the impact of its removal on the model\u0026rsquo;s performance on test data. This is measured by the change in the loss function, which quantifies how well the model\u0026rsquo;s predictions match the true data.\nMathematically, let $z$ represent the data of an example required to evaluate the loss, typically corresponding to a pair $(x, y)$ in supervised settings. The loss function $L(z; \\theta)$ is optimized over a parameter space $\\Theta$ to train the model. When an example $z_n$ is removed from the training set $D_{\\text{train}}$, it results in a parameter shift $\\theta_n - \\theta\u0026rsquo;_{-n}$, impacting the loss $L(z; \\theta\u0026rsquo;)$ on a test example $z$. This loss shift provides a meaningful measure of example importance.\nTo estimate the loss shift without retraining the model, methods like the influence function and checkpoint evaluation are employed. For example, Koh \u0026amp; Liang (2017) propose using the influence function:\nInfluence Function Formula\n\\begin{equation} \\langle \\delta_{\\theta}^{n} L(z; \\theta\u0026rsquo;) \\approx \\frac{1}{N} \\langle {\\nabla L(z; \\theta_{*})}, H^{-1} {\\nabla L(z_{n}; \\theta_{*}\u0026rsquo;)} \\rangle_{\\theta} \\end{equation}\nWhere:\n$( \\nabla_{\\theta} L(z, \\theta^*) )$ is the gradient of the loss with respect to the parameters for the test example. $( H_{\\theta^*} )$ is the Hessian matrix. $( \\nabla_{\\theta} L(z^n, \\theta^*) )$ is the gradient of the loss for the removed training example. $( \\langle \\cdot, \\cdot \\rangle_{\\theta} )$ denotes the inner product in the parameter space. $( N )$ is the number of training examples. Label-Free Setting\nIn a label-free setting, the models are trained without explicit labels. Instead, they use a label-free loss function, which typically tries to capture the structure of the data itself rather than fitting to specific target labels.\nIn the context of autoencoders, determining the importance of a training example can be tricky due to the loss function used during training (uses the encoder and decoder). When we are only interested in the encoder part and it is not sufficient to only use the model\u0026rsquo;s loss function as this also include the influence of the decoder.\nTo address this, we decompose the parameter space into relevant and irrelevant components. The proposed method computes the example importance scores by considering only the relevant parameters. The model to interpret, denoted as $f_r$, is parametrized only by the relevant parameters $\\theta_r$.\nThis motivates the definition of Label-Free Loss-Based Example Importance:\n\\begin{equation} c_n(f_r; x) = \\theta_n L(x; \\theta\u0026rsquo;) \\end{equation}\nLabel-Free Loss-Based Example Importance score $( c_n(f_{\\theta_r}, x) )$ measures the impact of removing a training example $( x_n )$ from the training set on the learned latent representation $( f_{\\theta_r}(x) )$ of a test example $( x )$. It uses $( \\delta_{\\theta_r} L )$ to denote the part of the loss shift that is only due to changes in the relevant parameters $(( \\theta_r ))$.\nThis definition extends any loss-based example importance method to the label-free setting, where the unsupervised loss $L$ is used to fit the model, and the gradients with respect to the parameters of the encoder are computed.\n2. Representation-Based Example Importance\nRepresentation-based example importance methods analyze the latent representations of examples to assign importance scores.\nSupervised Setting\nThese methods quantify the affinity between a test example and the training set examples based on their latent representations. For instance, in a model $f_l \\circ f_e: X \\rightarrow Y$, where $f_e: X \\rightarrow H$ maps inputs to latent representations and $f_l: H \\rightarrow Y$ maps representations to labels, representation-based methods reconstruct the test example\u0026rsquo;s latent representation using training representations. The reconstruction involves assigning weights to training representations, typically based on nearest neighbors or learned weights. For example, using a kernel function $\\mathcal{K}$:\n\\begin{equation} w_n(x) = \\frac{1}{|KNN(x)|} \\sum_{n\u0026rsquo; \\in KNN(x)} \\mathcal{K}(\\text{fe}(x_n), \\text{fe}(x)) \\end{equation}\nLabel-Free Setting\nRrepresentation-based methods remain valid by replacing supervised representation maps with unsupervised ones. Hence, no additional modifications are needed.\nIV. Experiments: Evaluation and Results Consistency Checks Now, we are verifying the consistency of results obtained from different methods of assessing feature and example importance using the MNIST dataset.\nIn MNIST, important features are the pixels of the images, and various methods can be employed to evaluate their importance. To assess feature importance, we can measure the impact of selectively removing the most important pixels on the latent representation constructed by the encoder, as described in the previous example. By comparing the results of different methods of importance assessment, such as perturbing the most important pixels according to various importance measures, we can check if the same pixels are identified as important and if their removal consistently affects the latent representation.\nWe rerun the tests provided in the GitHub repository:\nOn the MNIST dataset, we perturb the most important pixels and observe how this perturbation affects the quality or relevance of the latent representation generated by the encoder. Here we can see the result of the experiment :\nThe results obtained from the representation shift curves as a function of the percentage of perturbed pixels demonstrate the effectiveness of Feature Importance methods on the MNIST dataset.\nWe observe that Feature Importance methods such as Gradient Shap and Integrated Gradients show a significant increase in representation shift when the most important pixels are perturbed. This indicates that these methods successfully identify the most relevant pixels for constructing the latent representation. However, after perturbing approximately 20% of the most important pixels, we notice a stabilization of the representation shift, suggesting that adding additional perturbations does not necessarily lead to a significant increase in impact on the latent representation.\nOn the other hand, the Saliency method appears to be less effective, with an almost linear representation shift curve, suggesting that it fails to selectively identify the most important pixels for the latent representation.\nOverall, this confirms the effectiveness of Feature Importance methods, particularly Integrated Gradients.\nSimilarly, to evaluate the importance of examples in MNIST, we select training examples that have a significant influence on predicting the latent representation of test examples. By comparing the results obtained with different methods of assessing example importance, we can verify if the same examples are identified as important and if their relevance is consistent with the model\u0026rsquo;s predictions.\nFor all example importance methods, we observe a decrease in similarity rates, with a consistent trend across all curves.\nThis observation highlights that the similarity rate is significantly higher among the most similar examples compared to the least similar examples, confirming the effectiveness of label-free importance scores cn(fe; x) in identifying training examples related to the test example we wish to explain.\nIn summary, these results affirm the capability of label-free importance scores in effectively selecting relevant training examples and distinguishing between similar and dissimilar examples.\nV. Conclusion In this post you learned about label-free explainability a new framework developped by Crabbé and van der Schaar in 2022, wich extend linear feature importance and example importance methods to the unsupervised setting with a focus on the MNIST dataset.\nReferences Crabbé, J. \u0026amp; van der Schaar, M.. (2022). Label-Free Explainability for Unsupervised Models. Proceedings of the 39th International Conference on Machine Learning, in Proceedings of Machine Learning Research 162:4391-4420 Available from https://proceedings.mlr.press/v162/crabbe22a.html. ",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eLabel-Free Explainability for Unsupervised Models\u003c/h1\u003e\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: \u003ca href=\"https://github.com/Valentinahxu\"\u003eValentina Hu \u003c/a\u003e and  \u003ca href=\"https://github.com/selmazrg\"\u003e Selma Zarga\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIncentives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eFeature Importance \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eExample Importance\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Label-Free Explainability for Unsupervised Models, published by J. Crabbé et al. in 2022 and available \u003ca href=\"https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eWhy do we need explainability ?\u003c/h2\u003e\n\u003cp\u003eMachine learning models are becoming increasingly capable of making advanced predictions. While models like linear regression are relatively easy to understand and explain, more complex models, often called \u003cstrong\u003e\u0026ldquo;black boxes\u0026rdquo;\u003c/strong\u003e due to their complexity, present challenges in explaining how they make predictions. These models can be problematic in highstakes applications such as healthcare, finance, and justice, where it\u0026rsquo;s crucial to justify decision-making. Additionally, in case of errors, it\u0026rsquo;s important to understand the origin in order to address and correct them.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cp\u003e\u0026ldquo;\u003cstrong\u003eExplainability is the cornerstone of trust in black box models; without it, they remain inscrutable and unreliable.\u003c/strong\u003e\u0026rdquo; - \u003cem\u003eYoshua Bengio\u003c/em\u003e\u003c/p\u003e\n\u003c/center\u003e\n\u003cp\u003eTo tackle this challenge, the field of Explainable Artificial Intelligence (XAI) has emerged, offering various methods to enhance \u003cstrong\u003emodel transparency\u003c/strong\u003e. \u003cstrong\u003ePost-Hoc explainability\u003c/strong\u003e methods exist, which intervene after the model has generated its results, enabling users to comprehend the reasoning behind specific decisions or predictions. These methods supplement the predictions of black box models with diverse explanations of how they arrive at their predictions.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/explainability/Black-Box.png\"\n  alt=\"XAI explainability\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eI. Introduction\u003c/h2\u003e\n\u003cp\u003eThe entire post focuses on the quest for explainability of unsupervised models. In these models, no labels are assigned to the data, making understanding the model even more complicated due to the absence of explicit guidance on what the model is learning. In the supervised setting, users know the meaning of the black-box output they are trying to interpret. However, this clarity is not always available in machine learning. Therefore, elucidating concepts such as feature importance and example importance provides insights into why the model makes certain decisions or identifies specific patterns in the data.\u003c/p\u003e\n\u003cp\u003eA recent research conducted by Crabbé and van der Schaar in 2022 explores the explainability of unsupervised models. They have developed two new methods to explain these complex models without labels. The first method highlights important features in the data, while the second identifies training examples that have the biggest impact on the model\u0026rsquo;s construction of representations. In this post, we will attempt to explain these two methods.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eII. Feature Importance\u003c/h2\u003e\n\u003cp\u003eFeature importance aims to explain how the model arrives at its prediction for a given input by assigning an  importance scores to each feature (or attribute) of the input. This helps understand which features have the most influence on the model\u0026rsquo;s predictions. This method is developed based on a linear reasoning that is extended to label-free settings.\u003c/p\u003e\n\u003cp\u003eGiven a model $( f : \\mathcal{X} \\rightarrow \\mathcal{Y} )$, which maps an input space $( \\mathcal{X} \\subset \\mathbb{R}^{d_X} )$ to an output space $( \\mathcal{Y} \\subset \\mathbb{R}^{d_Y} )$. Where, $( d_X )$ and $( d_Y )$ is the dimensions of the input and output spaces.\u003c/p\u003e\n\u003cp\u003eIn the traditionnal method, the process requires selecting one component $( f_j(x) )$ of the model\u0026rsquo;s output to compute the importance score for each feature $( i )$, denoted as $( a_i(f, x) )$. The selection is based on the ground-truth label, and $( j )$ corresponds to the class predicted with the highest probability.\u003c/p\u003e\n\u003cp\u003eTo understand how the label-free feature importance method works, let\u0026rsquo;s start by looking at the labeled case:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Labelled Feature Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAuthors introduces an alternative approach to calculate feature importance scores. The method proposes to combine the importance scores of different components of the model\u0026rsquo;s output by weighting them with the associated class probabilities. For each component of the model\u0026rsquo;s output, we multiply the importance score of the corresponding feature by the probability of that component.\u003c/p\u003e\n\u003cp\u003eThese weighted importance scores are then combined to obtain the final importance score of each feature.\u003c/p\u003e\n\u003cp\u003eLet $a_i(f_j;x)$ be the importance score of feature $x_i$ calculated with respect to the component $f_j$ of the model\u0026rsquo;s output. The method proposes to calculate the importance score $b_i(f;x)$ for feature $x_i$ as follows:\u003c/p\u003e\n\u003cp\u003e$b_i(f;x) = \\sum_{j=1}^{d_Y} f_j(x) \\times a_i(f_j,x)$\u003c/p\u003e\n\u003cp\u003eHere, $f_j(x)$ represents the probability of class $j$, and $a_i(f_j;x)$ is the importance score of feature $x_i$ for class $j$.\u003c/p\u003e\n\u003cp\u003eHovewer when the class probabilities are balanced, this method accounts for the contribution of each class to the feature importance score, rather than focusing only on the class with the highest probability, which is the usual practice.\u003c/p\u003e\n\u003cp\u003eThis method proves to be efficient predominantly when the significance scores exhibit linearity in relation to the model. To facilitate a streamlined computation of weighted importance scores, another method is to introduce an auxiliary function, denoted as $(g_x)$ :\u003c/p\u003e\n\u003cp\u003e$\\ g_x(z) = \\sum_{j=1}^{d_Y} f_j(x) \\cdot f_j(z) $\u003c/p\u003e\n\u003cp\u003eWith the function $(g_x)$, it becomes feasible to calculate the weighted importance score, $(b_i(f, x))$, for each feature $(i)$, by merely employing $(g_x)$. This technique significantly simplifies the computational process, obviating the need to calculate $(d_Y \\times d_X)$ importance scores. Such a calculation becomes impractically cumbersome with the escalation of the number of classes, $(d_Y)$. With this trick, we can compute the weighted importance score by only calling the auxiliary function.\u003c/p\u003e\n\u003cp\u003eWe can see that in the labeled case, the method is quite clear. A similar reasoning is used in the label-free setting. Now, let\u0026rsquo;s move on to the label-free setting.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Label-Free Feature Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn the context of the unlabelled setting, we consider a latent space $H$ of dimension $d_H$ where a black-box model $f : X \\rightarrow H$ is given. The goal is to assign an importance score to each feature of the input $x$, even if the dimensions of the latent space have no clear relations with the labels.\u003c/p\u003e\n\u003cp\u003eA similar weighting formula for importance scores is used, where the components $f_j(x)$ do not correspond to probabilities but to neuron activations. The weighted sum is considered as a inner product in the latent space.\u003c/p\u003e\n\u003cp\u003eThe method is developed using linear feature importance functions, and it retains the completeness property, meaning that the sum of importance scores equals the black-box prediction up to a baseline constant.\u003c/p\u003e\n\u003cp\u003eHere is how the method operates:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePresentation of the Latent Space:\u003c/strong\u003e We consider a latent space $H$ of dimension $d_H$ where each input $x$ is mapped by the black-box model $f$ to obtain a representation $h = f(x)$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAssignment of Importance Scores:\u003c/strong\u003e The objective is to assign an importance score $b_i(f; x)$ to each feature $x_i$ of $x$. Unlike in the previous setting, where we had probabilities associated with each component, here, we do not have a clear method to choose a particular component $f_j$ in the latent space. Therefore, we use a similar approach to the one described previously.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCalculation of Importance Scores:\u003c/strong\u003e We use a weighting method where the importance score is given by $b_i(f; x) = a_i(\\sum_{j=1}^{d_H} f_j(x) \\cdot f_j(x))$. The individual components $f_j(x)$ do not correspond to probabilities in this case; they generally correspond to neuron activation functions. Inactive neurons will have a corresponding component that vanishes ($f_j(x) = 0$), meaning they will not contribute to the weighted sum, while more activated neurons will contribute more.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCompleteness:\u003c/strong\u003e An important property shared by many feature importance methods is completeness. This means that the sum of importance scores equals the black-box prediction up to a baseline constant. This establishes a connection between importance scores and black-box predictions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis method proposes an extension of linear feature importance methods to the unlabelled setting by defining an auxiliary scalar function $g_x$ that encapsulates the black-box function $f$. This extension is achieved by using a function $g_x$ that computes the inner product between the representation $f(x)$ and the representation $f(\\tilde{x})$ for all $\\tilde{x}$ in the input space.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eIII. Example Importance\u003c/h2\u003e\n\u003cp\u003eIn this section, we explain the approach to extending example importance methods to the label-free setting. Given that example importance methods vary significantly, they are separated into two families: loss-based and representation-based methods. The extension to the label-free setting differs for these two families, so we discuss them separately in distinct subsections.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Loss-Based Example Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSupervised Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn supervised learning, loss-based example importance methods determine how important each training example is by assessing the impact of its removal on the model\u0026rsquo;s performance on test data. This is measured by the change in the loss function, which quantifies how well the model\u0026rsquo;s predictions match the true data.\u003c/p\u003e\n\u003cp\u003eMathematically, let $z$ represent the data of an example required to evaluate the loss, typically corresponding to a pair $(x, y)$ in supervised settings. The loss function $L(z; \\theta)$ is optimized over a parameter space $\\Theta$ to train the model. When an example $z_n$ is removed from the training set $D_{\\text{train}}$, it results in a parameter shift $\\theta_n - \\theta\u0026rsquo;_{-n}$, impacting the loss $L(z; \\theta\u0026rsquo;)$ on a test example $z$. This loss shift provides a meaningful measure of example importance.\u003c/p\u003e\n\u003cp\u003eTo estimate the loss shift without retraining the model, methods like the influence function and checkpoint evaluation are employed. For example, Koh \u0026amp; Liang (2017) propose using the influence function:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eInfluence Function Formula\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\\begin{equation} \\langle\n\\delta_{\\theta}^{n} L(z; \\theta\u0026rsquo;) \\approx \\frac{1}{N} \\langle {\\nabla L(z; \\theta_{*})}, H^{-1} {\\nabla L(z_{n}; \\theta_{*}\u0026rsquo;)} \\rangle_{\\theta} \u003cbr\u003e\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$( \\nabla_{\\theta} L(z, \\theta^*) )$ is the gradient of the loss with respect to the parameters for the test example.\u003c/li\u003e\n\u003cli\u003e$( H_{\\theta^*} )$ is the Hessian matrix.\u003c/li\u003e\n\u003cli\u003e$( \\nabla_{\\theta} L(z^n, \\theta^*) )$ is the gradient of the loss for the removed training example.\u003c/li\u003e\n\u003cli\u003e$( \\langle \\cdot, \\cdot \\rangle_{\\theta} )$ denotes the inner product in the parameter space.\u003c/li\u003e\n\u003cli\u003e$( N )$ is the number of training examples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLabel-Free Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn a label-free setting, the models are trained without explicit labels. Instead, they use a label-free loss function, which typically tries to capture the structure of the data itself rather than fitting to specific target labels.\u003c/p\u003e\n\u003cp\u003eIn the context of autoencoders, determining the importance of a training example can be tricky due to the loss function used during training (uses the encoder and decoder). When we are only interested in the encoder part and it is not sufficient to only use the model\u0026rsquo;s loss function as this also include the influence of the decoder.\u003c/p\u003e\n\u003cp\u003eTo address this, we decompose the parameter space into relevant and irrelevant components. The proposed method computes the example importance scores by considering only the relevant parameters. The model to interpret, denoted as $f_r$, is parametrized only by the relevant parameters $\\theta_r$.\u003c/p\u003e\n\u003cp\u003eThis motivates the definition of Label-Free Loss-Based Example Importance:\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\nc_n(f_r; x) = \\theta_n L(x; \\theta\u0026rsquo;)\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eLabel-Free Loss-Based Example Importance score $( c_n(f_{\\theta_r}, x) )$ measures the impact of removing a training example $( x_n )$ from the training set on the learned latent representation $( f_{\\theta_r}(x) )$ of a test example $( x )$. It uses $( \\delta_{\\theta_r} L )$ to denote the part of the loss shift that is only due to changes in the relevant parameters $(( \\theta_r ))$.\u003c/p\u003e\n\u003cp\u003eThis definition extends any loss-based example importance method to the label-free setting, where the unsupervised loss $L$ is used to fit the model, and the gradients with respect to the parameters of the encoder are computed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Representation-Based Example Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRepresentation-based example importance methods analyze the latent representations of examples to assign importance scores.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSupervised Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThese methods quantify the affinity between a test example and the training set examples based on their latent representations. For instance, in a model $f_l \\circ f_e: X \\rightarrow Y$, where $f_e: X \\rightarrow H$ maps inputs to latent representations and $f_l: H \\rightarrow Y$ maps representations to labels, representation-based methods reconstruct the test example\u0026rsquo;s latent representation using training representations. The reconstruction involves assigning weights to training representations, typically based on nearest neighbors or learned weights. For example, using a kernel function $\\mathcal{K}$:\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\nw_n(x) = \\frac{1}{|KNN(x)|} \\sum_{n\u0026rsquo; \\in KNN(x)} \\mathcal{K}(\\text{fe}(x_n), \\text{fe}(x))\n\\end{equation}\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLabel-Free Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRrepresentation-based methods remain valid by replacing supervised representation maps with unsupervised ones. Hence, no additional modifications are needed.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eIV. Experiments: Evaluation and Results\u003c/h2\u003e\n\u003ch3 id=\"section-111\"\u003eConsistency Checks\u003c/h3\u003e\n\u003cp\u003eNow, we are verifying the consistency of results obtained from different methods of assessing feature and example importance using the MNIST dataset.\u003c/p\u003e\n\u003cp\u003eIn MNIST, important features are the pixels of the images, and various methods can be employed to evaluate their importance. To assess feature importance, we can measure the impact of selectively removing the most important pixels on the latent representation constructed by the encoder, as described in the previous example. By comparing the results of different methods of importance assessment, such as perturbing the most important pixels according to various importance measures, we can check if the same pixels are identified as important and if their removal consistently affects the latent representation.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eWe rerun the tests provided in the \u003ca href=\"https://github.com/JonathanCrabbe/Label-Free-XAI\"\u003eGitHub repository\u003c/a\u003e:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eOn the MNIST dataset, we perturb the most important pixels and observe how this perturbation affects the quality or relevance of the latent representation generated by the encoder.\nHere we can see the result of the experiment :\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/explainability/mnist_consistency_features.png\"\n  alt=\"XAI explainability\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe results obtained from the representation shift curves as a function of the percentage of perturbed pixels demonstrate the effectiveness of Feature Importance methods on the MNIST dataset.\u003c/p\u003e\n\u003cp\u003eWe observe that Feature Importance methods such as Gradient Shap and Integrated Gradients show a significant increase in representation shift when the most important pixels are perturbed. This indicates that these methods successfully identify the most relevant pixels for constructing the latent representation. However, after perturbing approximately 20% of the most important pixels, we notice a stabilization of the representation shift, suggesting that adding additional perturbations does not necessarily lead to a significant increase in impact on the latent representation.\u003c/p\u003e\n\u003cp\u003eOn the other hand, the Saliency method appears to be less effective, with an almost linear representation shift curve, suggesting that it fails to selectively identify the most important pixels for the latent representation.\u003c/p\u003e\n\u003cp\u003eOverall, this confirms the effectiveness of Feature Importance methods, particularly Integrated Gradients.\u003c/p\u003e\n\u003cp\u003eSimilarly, to evaluate the importance of examples in MNIST, we select training examples that have a significant influence on predicting the latent representation of test examples. By comparing the results obtained with different methods of assessing example importance, we can verify if the same examples are identified as important and if their relevance is consistent with the model\u0026rsquo;s predictions.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/explainability/example.png\"\n  alt=\"XAI explainability\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eFor all example importance methods, we observe a decrease in similarity rates, with a consistent trend across all curves.\u003c/p\u003e\n\u003cp\u003eThis observation highlights that the similarity rate is significantly higher among the most similar examples compared to the least similar examples, confirming the effectiveness of label-free importance scores cn(fe; x) in identifying training examples related to the test example we wish to explain.\u003c/p\u003e\n\u003cp\u003eIn summary, these results affirm the capability of label-free importance scores in effectively selecting relevant training examples and distinguishing between similar and dissimilar examples.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eV. Conclusion\u003c/h2\u003e\n\u003cp\u003eIn this post you learned about label-free explainability a new framework developped by Crabbé and van der Schaar in 2022, wich extend linear feature importance and example importance\nmethods to the unsupervised setting with a focus on the MNIST dataset.\u003c/p\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eCrabbé, J. \u0026amp; van der Schaar, M.. (2022). Label-Free Explainability for Unsupervised Models. \u003ci\u003eProceedings of the 39th International Conference on Machine Learning\u003c/i\u003e, in \u003ci\u003eProceedings of Machine Learning Research\u003c/i\u003e 162:4391-4420 Available from \u003ca href=\"https://proceedings.mlr.press/v162/crabbe22a.html\"\u003ehttps://proceedings.mlr.press/v162/crabbe22a.html\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cstyle\u003e\n.highlight {\n  background-color: red;\n}\n.highlight-on-hover:hover {\n        background-color: yellow;\n    }\n/* Quiz form styles */\n.quiz-form {\n        max-width: 500px;\n        margin: auto;\n        padding: 20px;\n        border: 1px solid #ccc;\n        border-radius: 5px;\n        background-color: #f9f9f9;\n}\n\n.quiz-question {\n        margin-bottom: 20px;\n}\n\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n}\n\n.quiz-submit {\n        background-color: #4caf50;\n        color: white;\n        padding: 10px 20px;\n        border: none;\n        border-radius: 5px;\n        cursor: pointer;\n}\n\n.quiz-submit:hover {\n        background-color: #45a049;\n}\n\n/* Quiz results styles */\n.quiz-results {\n        margin-top: 20px;\n        font-weight: bold;\n}\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n    }\n.quiz-options label.correct {\n        color: green;\n}\n.quiz-options label.incorrect {\n        color: red;\n}\na[name]:hover {\n        background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */\n        text-decoration: none; /* Optionally remove underline on hover */\n}\n\u003c/style\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n",
      "url": "http://localhost:1313/posts/label-free-explainability/",
      "date_published": "17036-17-09T331:1717:00+01:00",
      "date_modified": "17036-17-09T331:1717:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "9bec8a0b2d12c702729b80a3b910f16d3d73250f",
      "title": "Adversarially Reweighted Learning",
      "summary": "",
      "content_text": "Fairness without Demographics through Adversarially Reweighted Learning Authors: Pierre Fihey \u0026 Guerlain Messin Table of Contents Fairness issues in ML and AI The privacy of demographic’s data The Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels Computational identifiability of protected groups The Rawlsian Max-Min Fairness principle The ARL objective The Model Architecture Results analysis Conclusion This is a blog post about the paper Fairness without Demographics through Adversarially Reweighted Learning, published by P. Lahoti et al. in 2020 and available here.\nFairness issues in ML and AI As Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\nSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments. Machine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\nThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\nThe privacy of demographic’s data Strict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\nIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\nThe Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels While access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\nThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\nThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\nComputational identifiability of protected groups Computational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\nFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\nThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\nThe Rawlsian Max-Min Fairness principle In philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\nDefinition (Rawslan Max-Min Fairness): Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility. $$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\nThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged. This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\nThe ARL objective To adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally. The aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\n$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$ $$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\nWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\nThe Model Architecture As previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\nThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\nThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\nThe learner then adjusts itself to minimize the adversarial loss: $$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\nTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\n$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\nThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\nResults analysis This section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\nReproducibility We first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\nReplicability We replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\nSignificance Evaluation We conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\nConclusion In this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\nAnnexes References [1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\n[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\n[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\n[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\n[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\n[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\n[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\n[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\n[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\n",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eFairness without Demographics through Adversarially Reweighted Learning\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Pierre Fihey \u0026 Guerlain Messin\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eFairness issues in ML and AI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eThe privacy of demographic’s data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eComputational identifiability of protected groups\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eThe ARL objective\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eThe Model Architecture\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eResults analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-9\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Fairness without Demographics through Adversarially Reweighted Learning, published by P. Lahoti et al. in 2020 and available \u003ca href=\"https://dl.acm.org/doi/abs/10.5555/3495724.3495786\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eFairness issues in ML and AI\u003c/h2\u003e\n\u003cp\u003eAs Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\u003c/p\u003e\n\u003cp\u003eSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments.\nMachine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\u003c/p\u003e\n\u003cp\u003eThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eThe privacy of demographic’s data\u003c/h2\u003e\n\u003cp\u003eStrict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\u003c/p\u003e\n\u003cp\u003eIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/h2\u003e\n\u003ch3 id=\"section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/h3\u003e\n\u003cp\u003eWhile access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\u003c/p\u003e\n\u003cp\u003eThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Fihey_Messin/Identifying_Groups.png\"\n  alt=\"Identifying Groups\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\u003c/p\u003e\n\u003ch3 id=\"section-4\"\u003eComputational identifiability of protected groups\u003c/h3\u003e\n\u003cp\u003eComputational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\u003c/p\u003e\n\u003cp\u003eFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\u003c/p\u003e\n\u003cp\u003eThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/h3\u003e\n\u003cp\u003eIn philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition (Rawslan Max-Min Fairness):\u003c/strong\u003e Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility.\n$$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\u003c/p\u003e\n\u003cp\u003eThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged.  This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\u003c/p\u003e\n\u003ch3 id=\"section-6\"\u003eThe ARL objective\u003c/h3\u003e\n\u003cp\u003eTo adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally.\nThe aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\u003c/p\u003e\n\u003cp\u003e$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$\n$$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eThe Model Architecture\u003c/h3\u003e\n\u003cp\u003eAs previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\u003c/p\u003e\n\u003cp\u003eThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\u003c/p\u003e\n\u003cp\u003eThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\u003c/p\u003e\n\u003cp\u003eThe learner then adjusts itself to minimize the adversarial loss:\n$$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\u003c/p\u003e\n\u003cp\u003e$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\u003c/p\u003e\n\u003cp\u003eThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Fihey_Messin/ARL_Computational_Graph.png\"\n  alt=\"ARL Computational Graph\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch2 id=\"section-8\"\u003eResults analysis\u003c/h2\u003e\n\u003cp\u003eThis section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\u003c/p\u003e\n\u003ch3 id=\"reproducibility\"\u003eReproducibility\u003c/h3\u003e\n\u003cp\u003eWe first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\u003c/p\u003e\n\u003ch3 id=\"replicability\"\u003eReplicability\u003c/h3\u003e\n\u003cp\u003eWe replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\u003c/p\u003e\n\u003ch3 id=\"significance-evaluation\"\u003eSignificance Evaluation\u003c/h3\u003e\n\u003cp\u003eWe conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\u003c/p\u003e\n\u003ch2 id=\"section-9\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\u003c/p\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003cp\u003e[1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\u003c/p\u003e\n\u003cp\u003e[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\u003c/p\u003e\n\u003cp\u003e[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\u003c/p\u003e\n\u003cp\u003e[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\u003c/p\u003e\n\u003cp\u003e[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\u003c/p\u003e\n\u003cp\u003e[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\u003c/p\u003e\n\u003cp\u003e[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\u003c/p\u003e\n\u003cp\u003e[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\u003c/p\u003e\n\u003cp\u003e[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n",
      "url": "http://localhost:1313/posts/adversarially_reweighted_learning/",
      "date_published": "4036-04-09T335:44:00+01:00",
      "date_modified": "4036-04-09T335:44:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "d7ac2f8435ec113c47081837ac2b76e60dcb8690",
      "title": "Packed Ensembles",
      "summary": "",
      "content_text": " This is a blog post about the paper Packed-Ensembles for Efficient Uncertainty Estimation, published by O. Laurent et al. in 2023 and available [here](https://openreview.net/pdf?id=XXTyv1zD9zD). Authors: Cynthia Obeid and Elie Nakad Introduction The document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems. Presentation of the model Packed-Ensembles\nThe base network and Packed-Ensembles\nPacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\nBenefits of Packed-Ensembles\nPacked-Ensembles offer several benefits over traditional ensemble methods, including:\nEfficiency: Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\nAccuracy: Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\nCalibration: Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\nOut-of-distribution (OOD) detection: Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\nComparison to other ensemble methods\nThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\nPacked-Ensembles: A Technique for Efficient Neural Network Ensembles Packed-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\nUnderstanding Convolutional Layers and Grouped Convolutions:\nConvolutional Layers: These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows: $z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\nwhere:\n$c$ represents the channel index\n$h^j$ denotes the input feature map\n$ω^j$ represents the weight tensor (kernel)\n$⋆$ denotes the 2D cross-correlation operator\nGrouped Convolutions: This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating independent subnetworks. The mathematical formulation for grouped convolutions is given by:\n$$ z^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right) $$\nwhere:\n$γ$ represents the number of groups $C_{j+1}$ and $C_j$ denote the number of output and input channels, respectively. The formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask $\\text{mask}_{m}^j$ $\\in \\{{ 0, 1 \\}}^{C_{j+1} \\times C_j \\times s_j^2}$ with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\nThe condition $\\text{mask}_{m}^j(k, l, :, :) = 1$ happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\nComplete Mask and Convolution: $\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$. $z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here: $z^{j+1}$: Output feature map of the layer. $h^j$: Input feature map. $ω^j$: Convolution weights for layer j. $\\otimes$: Denotes convolution operation. $◦$: Denotes Hadamard product (element-wise multiplication). In simpler terms:\nGrouped convolution divides the input channels and weights into groups. A separate mask is created for each group, ensuring elements within a group are aligned. These masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group. The final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask. Background on Deep Ensembles This section delves into Deep Ensembles (DE), a technique for image classification tasks.\nDeep Ensembles\nSetting the Scene\nWe have a dataset $D$ containing pairs of images and their corresponding labels:\n$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width). $y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes). The dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\nA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\n$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$. Traditional Approach:\nThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\nIntroducing Deep Ensembles\nDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\nThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\n$$ P(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m) $$\nThis essentially combines the outputs of multiple networks to create a more robust prediction.\nIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\nBuilding Packed-Ensembles:\nPacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\nSubnetworks: The ensemble is formed by creating $M$ smaller subnetworks within the main network architecture. These subnetworks share the same structure but have independent parameters due to the use of grouped convolutions. Hyperparameters: Packed-Ensembles are defined by three hyperparameters: $α$ (alpha): expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters). $M$: number of subnetworks in the ensemble (represents the ensemble size). $γ$ (gamma): number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity). Mathematical Implementation:\nThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\n$$ \\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j $$\nwhere:\n$\\hat{y}$ represents the ensemble\u0026rsquo;s predicted label $P(y|θ_a^m, x)$ denotes the probability of class $y$ given the input $x$ and the parameters $θ_a^m$ of the $m-th$ subnetwork $\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$ represents the parameters of the $m-th$ subnetwork, obtained by applying element-wise multiplication ($∘$) between the expanded weights ($\\omega_j^{\\alpha}$) and the group mask ($\\text{mask}_{m}$) for each layer $j$ Implementation\nEquivalent architectures for Packed-Ensembles\nThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\nDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\nExperiments The experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\nDatasets: CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels. Architectures: PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. Metrics: Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used. Implementation Details: Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift. Code: PyTorch-Lightning framework is used for implementation. Results The experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\nCIFAR-10/100: PE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet). Smaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100. ImageNet: PE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models. PE achieves better accuracy with a reasonable increase in training and inference cost. These results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\nPacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\nEthics This section emphasizes the ethical considerations of the research. Here are the key points:\nGoal: This research proposes a method to improve uncertainty estimation in deep learning models. Limitations: The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications. Concerns: The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like: Unknown situations Corner cases (uncommon but important situations) Adversarial attacks (attempts to intentionally mislead the model) Potential biases in the model Overall: The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems. Reproducibility: Packed-Ensemble on CIFAR-10 We attempted to reproduce the experiment outlined in the tutorial available at https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\nData Loading and Preprocessing: Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images. Packed-Ensemble Definition: Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture. Loss Function and Optimizer: Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training. Training: Training the Packed-Ensemble model on the CIFAR-10 training data. Testing and Evaluation: Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance. Experimental Runs and Observations:\nTest 1:\nGroundTruth: cat ship ship plane\nThe predicted labels are: cat ship ship ship\nTest 2:\nGroundTruth: dog bird horse bird\nThe predicted labels are: dog frog car dog\nTest 3:\nGroundTruth: dog truck plane car The predicted labels are: dog horse ship truck\nChallenges and Limitations:\nA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\n",
      "content_html": "\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\nThis is a blog post about the paper Packed-Ensembles for Efficient Uncertainty Estimation, published by O. Laurent et al. in 2023 and available [here](https://openreview.net/pdf?id=XXTyv1zD9zD).\n\u003ch3 id=\"authors-cynthia-obeid-and-elie-nakad\"\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e: Cynthia Obeid and Elie Nakad\u003c/h3\u003e\n\u003ch1\u003eIntroduction\u003c/h1\u003e\n\u003c/div\u003e\nThe document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems.\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003ePresentation of the model\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003ePacked-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig1.jpg\" alt=\"The base network and Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eThe base network and Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBenefits of Packed-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles offer several benefits over traditional ensemble methods, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEfficiency:\u003c/strong\u003e Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccuracy:\u003c/strong\u003e Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCalibration:\u003c/strong\u003e Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOut-of-distribution (OOD) detection:\u003c/strong\u003e Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eComparison to other ensemble methods\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003ePacked-Ensembles: A Technique for Efficient Neural Network Ensembles\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003ePacked-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUnderstanding Convolutional Layers and Grouped Convolutions:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConvolutional Layers:\u003c/strong\u003e These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$c$\u003c/strong\u003e represents the channel index\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$h^j$\u003c/strong\u003e denotes the input feature map\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$ω^j$\u003c/strong\u003e represents the weight tensor (kernel)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$⋆$\u003c/strong\u003e denotes the 2D cross-correlation operator\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGrouped Convolutions:\u003c/strong\u003e This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating \u003cstrong\u003eindependent subnetworks\u003c/strong\u003e. The mathematical formulation for grouped convolutions is given by:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nz^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right)\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$\u003c/strong\u003e represents the number of groups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$C_{j+1}$\u003c/strong\u003e and \u003cstrong\u003e$C_j$\u003c/strong\u003e denote the number of output and input channels, respectively.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask \u003cstrong\u003e$\\text{mask}_{m}^j$\u003c/strong\u003e\n\u003cstrong\u003e$\\in \\{{ 0, 1 \\}}^{C_{j+1} \\times C_j \\times s_j^2}$\u003c/strong\u003e with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\u003c/p\u003e\n\u003cp\u003eThe condition \u003cstrong\u003e$\\text{mask}_{m}^j(k, l, :, :) = 1$\u003c/strong\u003e happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComplete Mask and Convolution:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e$\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$.\u003c/li\u003e\n\u003cli\u003e$z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here:\n\u003cul\u003e\n\u003cli\u003e$z^{j+1}$: Output feature map of the layer.\u003c/li\u003e\n\u003cli\u003e$h^j$: Input feature map.\u003c/li\u003e\n\u003cli\u003e$ω^j$: Convolution weights for layer \u003ccode\u003ej\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e$\\otimes$: Denotes convolution operation.\u003c/li\u003e\n\u003cli\u003e$◦$: Denotes Hadamard product (element-wise multiplication).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eIn simpler terms:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGrouped convolution divides the input channels and weights into groups.\u003c/li\u003e\n\u003cli\u003eA separate mask is created for each group, ensuring elements within a group are aligned.\u003c/li\u003e\n\u003cli\u003eThese masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group.\u003c/li\u003e\n\u003cli\u003eThe final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eBackground on Deep Ensembles\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThis section delves into Deep Ensembles (DE), a technique for image classification tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig2.png\" alt=\"Deep Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDeep Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSetting the Scene\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe have a dataset $D$ containing pairs of images and their corresponding labels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width).\u003c/li\u003e\n\u003cli\u003e$y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\u003c/p\u003e\n\u003cp\u003eA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTraditional Approach:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroducing Deep Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\u003c/p\u003e\n\u003cp\u003eThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\u003c/p\u003e\n\u003cp\u003e$$\nP(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m)\n$$\u003c/p\u003e\n\u003cp\u003eThis essentially combines the outputs of multiple networks to create a more robust prediction.\u003c/p\u003e\n\u003cp\u003eIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBuilding Packed-Ensembles:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSubnetworks:\u003c/strong\u003e The ensemble is formed by creating \u003cstrong\u003e$M$\u003c/strong\u003e smaller subnetworks within the main network architecture. These subnetworks share the same structure but have \u003cstrong\u003eindependent parameters\u003c/strong\u003e due to the use of grouped convolutions.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHyperparameters:\u003c/strong\u003e Packed-Ensembles are defined by three hyperparameters:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$α$ (alpha):\u003c/strong\u003e expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$M$:\u003c/strong\u003e number of subnetworks in the ensemble (represents the ensemble size).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$ (gamma):\u003c/strong\u003e number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical Implementation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$\\hat{y}$\u003c/strong\u003e represents the ensemble\u0026rsquo;s predicted label\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$P(y|θ_a^m, x)$\u003c/strong\u003e denotes the probability of class \u003cstrong\u003e$y$\u003c/strong\u003e given the input \u003cstrong\u003e$x$\u003c/strong\u003e and the parameters \u003cstrong\u003e$θ_a^m$\u003c/strong\u003e of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$\u003c/strong\u003e represents the parameters of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork, obtained by applying element-wise multiplication (\u003cstrong\u003e$∘$\u003c/strong\u003e) between the expanded weights (\u003cstrong\u003e$\\omega_j^{\\alpha}$\u003c/strong\u003e) and the group mask (\u003cstrong\u003e$\\text{mask}_{m}$\u003c/strong\u003e) for each layer \u003cstrong\u003e$j$\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig4.png\" alt=\"Equivalent architectures for Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eEquivalent architectures for Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig5.png\" alt=\"subnetwork mask\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\u003c/i\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eExperiments\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThe experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDatasets:\u003c/strong\u003e CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchitectures:\u003c/strong\u003e PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetrics:\u003c/strong\u003e Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation Details:\u003c/strong\u003e Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCode:\u003c/strong\u003e PyTorch-Lightning framework is used for implementation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eResults\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThe experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCIFAR-10/100:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet).\u003c/li\u003e\n\u003cli\u003eSmaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImageNet:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models.\u003c/li\u003e\n\u003cli\u003ePE achieves better accuracy with a reasonable increase in training and inference cost.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig3.png\" alt=\"ResNet50 performance\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003ePacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\u003c/i\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eEthics\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThis section emphasizes the ethical considerations of the research. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e This research proposes a method to improve uncertainty estimation in deep learning models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimitations:\u003c/strong\u003e The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConcerns:\u003c/strong\u003e The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like:\n\u003cul\u003e\n\u003cli\u003eUnknown situations\u003c/li\u003e\n\u003cli\u003eCorner cases (uncommon but important situations)\u003c/li\u003e\n\u003cli\u003eAdversarial attacks (attempts to intentionally mislead the model)\u003c/li\u003e\n\u003cli\u003ePotential biases in the model\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOverall:\u003c/strong\u003e The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eReproducibility: Packed-Ensemble on CIFAR-10\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eWe attempted to reproduce the experiment outlined in the tutorial available at \u003ca href=\"https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html\"\u003ehttps://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html\u003c/a\u003e which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Loading and Preprocessing:\u003c/strong\u003e Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePacked-Ensemble Definition:\u003c/strong\u003e Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLoss Function and Optimizer:\u003c/strong\u003e Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining:\u003c/strong\u003e Training the Packed-Ensemble model on the CIFAR-10 training data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTesting and Evaluation:\u003c/strong\u003e Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eExperimental Runs and Observations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTest 1:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result1.png\" alt=\"First result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  cat   ship  ship  plane\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: cat   ship  ship  ship\u003c/p\u003e\n\u003cp\u003eTest 2:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result2.png\" alt=\"Second result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth: dog bird horse bird\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: dog  frog  car  dog\u003c/p\u003e\n\u003cp\u003eTest 3:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result3.png\" alt=\"Third result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  dog truck plane car \u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: dog  horse ship  truck\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChallenges and Limitations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/packed-ensembles/",
      "date_published": "27026-27-09T25:2727:00+01:00",
      "date_modified": "27026-27-09T25:2727:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "fc0f8a815b8318f3a6944187159cad98a8481ca8",
      "title": "A Framework to Learn with Interpretation",
      "summary": "",
      "content_text": " A Framework to Learn with Interpretation Authors: Maroun ABOU BOUTROS, Mohamad EL OSMAN\nArticle: A Framework to Learn with Interpretation by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc\nTable of Contents Introduction Learning a classifier and an interpreter Design of FLINT Interpretation in FLINT Learning by imposing interpretability properties Understanding encoded concepts in FLINT Reproducing the experiments Global interpretation Local interpretation Subjective evaluation Specialization of FLINT to post-hoc interpretability Conclusion 1 Introduction In this blog post, we’ll explore FLINT, a framework introduced in the paper titled “A Framework to Learn with Interpretation” by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc, available on the following link, addressing the crucial need for interpretability in machine learning as complex predictive models become more prevalent in fields like law, healthcare, and defense. Interpretability, synonymous with explainability, provides insights into a model’s decision-making process. Two main approaches, post-hoc methods and “interpretable by design” methods, tackle the challenge of interpreting models, each with its pros and cons. A new approach, Supervised Learning with Interpretation (SLI), jointly learns a predictive model and an interpreter model. FLINT, specifically designed for deep neural network classifiers, introduces a novel interpreter network architecture promoting local and global interpretability. It also proposes a criterion for concise and diverse attribute functions, enhancing interpretability. We’ll delve into the architecture of FLINT and how it works to give explainable predictions, and we will reproduce some experiments done in the experimental section of the article and evaluate their outputs to study FLINT\u0026rsquo;s performance. And finally, we will present a specialization of FLINT for post-hoc interpretability.\n2 Learning a classifier and its interpreter with FLINT The paper introduces Supervised Learning with Interpretation (SLI), a new task aimed at incorporating interpretability alongside prediction in machine learning models. In SLI, a separate model, called an interpreter, is employed to interpret the predictions made by the primary predictive model. The task involves minimizing a combined loss function consisting of prediction error and interpretability objectives. The paper focuses on addressing SLI within the context of deep neural networks for multi-class classification tasks. It proposes a framework called Framework to Learn with INTerpretation (FLINT), which utilizes a specialized architecture for the interpreter model, distinguishes between local and global interpretations, and introduces corresponding penalties in the loss function to achieve the desired interpretability.\nSo for a dataset $S$ and a given model $f \\in F$ where $F$ is a class of classifiers (here neural networks) and an interpreter model $g \\in G_f$ where $G_f$ is a family of models, the SLI problem is presented by: $$ \\arg{\\min_{f \\in F, g \\in G_f}{L_{pred}(f, S) + L_{int}(f, g, S)}} $$ Where $L_{pred}(f, S)$ denotes a loss term related to prediction error and $L_{int}(f, g, S)$ measures the ability of $g$ to provide interpretations of predictions by $f$.\n2.1 Design of FLINT In FLINT, depicted in the image above, both a prediction model ($f$) and an interpreter model ($g$) are used. The input to FLINT is a vector $x \\in X$, where $X = \\mathbb{R}^d$, and the output is a vector $y \\in Y$, where $Y$ is defined as the set of one-hot encoding vectors with binary components of size $C$ (the number of classes to predict). The prediction model $f$ is structured as a deep neural network with $l$ hidden layers, represented as $f = f_{l+1} \\circ f_l \\circ \\ldots \\circ f_1$. Each $f_k$ represents a hidden layer mapping from $R^{d_{k-1}}$ to $R^{d_k}$. To interpret the outputs of $f$, we randomly select a subset of $T$ hidden layers, indexed by $I=\\{i_1, i_2, \\ldots, i_T\\}$, and concatenate their outputs to form a new vector $f_I(x) \\in \\mathbb{R}^D$, where $D = \\sum_{t=1}^T d_{i_t}$. This vector is then fed into a neural network $\\Psi$ to produce an output vector $\\Phi(x) = \\Psi(f_I(x)) = (\\phi_1(x), \u0026hellip;, \\phi_J (x)) \\in \\mathbb{R}^J$, representing an attribute dictionary comprising functions $\\phi_j: X \\rightarrow \\mathbb{R}^+$, where $\\phi_j(x)$ captures the activation of a high-level attribute or a \u0026ldquo;concept\u0026rdquo; over $X$. Finally, $g$ computes the composition of the attribute dictionnary with an interpretable function $h: R^J \\rightarrow Y$. $$ \\forall x \\in X, g(x) = h(\\Phi(x)) $$ For now we take $h(x) = softmax(W^T \\Phi(x))$ but $h$ can be any interpretable function (like a decision tree for example).\nNote that $d$ in the image is a decoder network that takes $\\Phi(x)$ and reconstructs the input $x$. This decoder is used for training and its purpose will be detailed later on in section 2.3.\n2.2 Interpretation in FLINT With the interpreter defined, let\u0026rsquo;s clarify its role and interpretability objectives within FLINT. Interpretation serves as an additional task alongside prediction. We\u0026rsquo;re interested in two types: global interpretation, which aids in understanding which attribute functions contribute to predicting a class, and local interpretation, which pinpoints the attribute functions involved in predicting a specific sample.\nTo interpret a local prediction $f(x)$, it\u0026rsquo;s crucial that the interpreter\u0026rsquo;s output $g(x)$ aligns with $f(x)$. Any discrepancy prompts analysis of conflicting data, potentially raising concerns about the prediction\u0026rsquo;s confidence.\nTo establish local and global interpretation, we rely on attribute relevance. Given an interpreter with parameters $\\Theta_g = (\\theta_\\Psi, \\theta_h)$ and an input $x$, an attribute $\\phi_j$\u0026rsquo;s relevance is defined concerning the prediction $g(x) = f(x) = \\hat{y}$. The attribute\u0026rsquo;s contribution to the unnormalized score of class $\\hat{y}$ is $\\alpha_{j, \\hat{y}, x} = \\phi_j(x) \\cdot w_{j, \\hat{y}}$, where $w_{j, \\hat{y}}$ is the coefficient associated with this class. Relevance score $r_{j, x}$ is computed by normalizing $\\alpha$ as $r_{j, x} = \\frac{\\alpha_{j, \\hat{y}, x}}{\\max_i |\\alpha_{i, \\hat{y}, x}|}$. An attribute $\\phi_j$ is considered relevant for a local prediction if it\u0026rsquo;s both activated and effectively used in the linear model.\nAttribute relevance extends to its overall importance in predicting any class $c$. This is achieved by averaging relevance scores from local interpretations over a random subset or the entirety of the training set $S$ where the predicted class is $c$. Thus, $r_{j, c} = \\frac{1}{|S_c|} \\sum_{x \\in S_c} r_{j, x}$, where $S_c = \\{x \\in S \\mid \\hat{y} = c\\}$.\nNow, let\u0026rsquo;s introduce the local and global interpretations the interpreter will provide:\nGlobal interpretation ($G(g, f)$) identifies class-attribute pairs $(c, \\phi_j)$ where the global relevance $r_{j, c}$ exceeds a threshold $\\frac{1}{\\tau}$.\nLocal interpretation ($L(x, g, f)$) for a sample $x$ includes attribute functions $\\phi_j$ with local relevance $r_{j, x}$ surpassing $\\frac{1}{\\tau}$. These definitions don\u0026rsquo;t assess interpretation quality directly.\n2.3 Learning by imposing interpretability properties For learning, the paper defines certain penalties to minimize, where each one aims to enforce a certain desirable property:\nFidelity to output: The output of $g(x)=h(\\Psi(f_I(x)))$ should be close to $f(x)$ for any x. This can be imposed through a cross-entropy loss: $$ L_{of}(f, g, S) = - \\sum_{x \\in S} h(\\Psi(f_I(x)))^T \\log(f(x)) $$\nConciseness and Diversity of Interpretations: We aim for concise local interpretations, containing only essential attributes per sample, promoting clearer understanding and capturing high-level concepts. Simultaneously, we seek diverse interpretations across samples to prevent attribute functions from being class-exclusive. To achieve this, the paper proposes that we leverage entropy (defined for a vector as $\\mathcal{E}(v) = - \\sum_i p_i \\log(p_i)$), which quantifies uncertainty in real vectors. Conciseness is fostered by minimizing the entropy of the interpreter\u0026rsquo;s output, $\\Phi(x) = \\Psi(f_I(x))$, while diversity is encouraged by maximizing the entropy of the average $\\Psi(f_I(x))$ over a mini-batch. This approach promotes sparse and varied coding of $f_I(x)$, enhancing interpretability. However, as entropy-based losses lack attribute activation constraints, leading to suboptimal optimization, we also minimize the $l_1$ norm of $\\Psi(f_I(x))$ with hyperparameter $\\eta$. Although $l_1$-regularization commonly encourages sparsity, the experiments done in the paper show that entropy-based methods are more effective. $$ L_{cd}(f, g, S) = -\\mathcal{E}(\\frac{1}{\\lvert S \\lvert} \\sum_{x \\in S} \\Psi(f_I(x))) + \\sum_{x \\in S} \\mathcal{E}(\\Psi(f_I(x))) + \\sum_{x \\in S} \\eta \\lVert \\Psi(f_I(x)) \\lVert_1 $$\nFidelity to input: In order to promote the representation of intricate patterns associated with the input within $\\Phi(x)$, a decoder network $d : \\mathbb{R}^J \\rightarrow X$ is employed. This network is designed to take the attribute dictionary $\\Phi(x)=\\Psi(f_I(x))$ as input and reconstruct the original input $x$. $$ L_{if}(f, g, d, S) = \\sum_{x \\in S} (d(\\Psi(f_I(x))) - x)^2 $$\nGiven the proposed loss terms, the loss for the interpretability model writes as follows: $$ L_{int}(f, g, d, S) = \\beta L_{of}(f, g, S) + \\gamma L_{if}(f, g, d, S) + \\delta L_{cd}(f, g, S) $$ Where $\\beta, \\gamma, \\delta$ are non-negative hyperparameters. the total loss to be minimized $L = L_{pred} + L_{int}$, where the prediction loss, $L_{pred}$, is the well-know cross entropy loss (since this a classification problem).\n3 Understanding encoded concepts in FLINT Once the predictor and interpreter networks are jointly learned, interpretation can be conducted at both global and local levels . A critical aspect highlighted by the authors is understanding the concepts encoded by each individual attribute function ​$\\phi_j$ . Focusing on image classification, the authors propose representing an encoded concept as a collection of visual patterns in the input space that strongly activate $\\phi_j$ . They present a pipeline for generating visualizations for both global and local interpretation, adapting various existing tools .\nFor global interpretation visualization, the authors propose starting by selecting a small subset of training samples from a given class c that maximally activate ​$\\phi_j$ . This subset, referred to as Maximum Activating Samples (MAS), is denoted as $MAS(c , ​\\phi_j , l)$ where $l$ is the subset size (set as 3 in their experiments). However, while MAS provides some insight into the encoded concept, further analysis is required to understand the specific aspects of these samples that cause ​$\\phi_j$ activation. To achieve this, the authors propose utilizing a modified version of activation maximization called Activation Maximization with Partial Initialization (AM+PI). This technique aims to synthesize input that maximally activates ​$\\phi_j$ by optimizing a common activation maximization objective, initialized with a low-intensity version of the sample from MAS.\nFor local analysis, given any test sample $x_{0}$ , its local interpretation $L(x_{0},f,g)$ can be determined, representing the relevant attribute functions . To visualize a relevant attribute ​$\\phi_j$, the authors suggest repeating the AM+PI procedure with initialization using a low-intensity version of $x_{0}$ to enhance the concept detected by ​$\\phi_j$ in $x_{0}$ .\n4 Reproducing the experiments In the experimental section of the article, several experiments were conducted to do a quantitative evaluation of FLINT\u0026rsquo;s performance compared to other state-of-the-art models designed for interpretability, such as SENN and PrototypeDNN. Additionally, FLINT was compared to LIME and VIBI to evaluate the fidelity of its interpretations, measuring the proportion of samples where the predictions of a model and its interpreter agree. Across these tests, FLINT consistently outperformed the other models, demonstrating its reliability and effectiveness.\nHowever, in this blog post we will specifically focus on reproducing the experiments in the article related to FLINT\u0026rsquo;s explainability, that aim to do a qualitative analysis of it. To achieve a thorough understanding of the model and its operational dynamics across prevalent datasets, we replicated the study by cloning the project from the GitHub repository referenced in the article (repo link). Our experimentation involved the CIFAR10 and QuickDRAW datasets, employing a ResNet18-based network for both. For the QuickDRAW dataset, we utilized J=24 attributes, while for the CIFAR10 dataset, we used J=36 attributes.\nThe instructions provided in the GitHub repository for executing the model are clear, and the model runs flawlessly. We have the option to either train the model ourselves or download the pre-trained models. Furthermore, there is a well-detailed Python notebook named \u0026ldquo;FLINT demo.ipynb\u0026rdquo;, which contains code for visualizing data, such as attribute relevance scores for each class and local interpretations for data samples. We will execute FLINT on test images and take a look at how interpretability is done with FLINT in this section.\n4.1 Global interpretation In the article, the authors explore global interpretation using a figure similar to the one provided below which was reproduced from the notebook, and which illustrates the generated global relevances $r_{j,c}$ for all class-attribute pairs in the QuickDraw dataset.\nAdditionally, by running the model on the CIFAR10 and QuickDRAW dataset we got visual outputs representative of class-attribute pair analyses for both datasets. These outputs served as pivotal tools in elucidating interrelations and facilitating comparative assessments between attributes and classes. We present below two figures derived from the resultant class-attribute pair analyses for each of the 2 datasets. The class-attribute pairs shown are different from the examples shown in the paper.\nCaption: Class-attribute pair analysis on dataset CIFAR10\nCaption: Class-attribute pair analysis on dataset QuickDraw\nWe focus on class-attribute pairs with high relevance, showcasing examples in the provided figure above . For each pair, we examine Maximum Activating Samples (MAS) alongside their corresponding Activation Maximization with Partial Initialization (AM+PI) outputs.\nMAS analysis alone provides valuable insights into the encoded concept. For instance, on QuickDRAW dataset, attribute $\\phi_{16}$ relevant for class \u0026lsquo;Banana\u0026rsquo; activates the curve shape of the banana. However, AM+PI outputs offer deeper insights by elucidating which parts of the input activate an attribute function more clearly. And on CIFAR10 dataset , attribute $\\phi_{12}$ activates for \u0026lsquo;Deer\u0026rsquo; class , but the specific focus of the attribute remains ambiguous. The outputs of the AM+PI method indicate that attribute $\\phi_{12}$ predominantly highlights the area encompassing the legs and the horns of the deer, characterized as the most prominently enhanced regions.\n4.2 Local interpretation Similarly to the article, we explored local interpretation through the figure provided below which was generated in the notebook, which showcases visualizations for 4 test samples of the QuickDRAW dataset. Both predictor $f$ and interpreter $g$ accurately predict the true class in all cases, for the first 2 it\u0026rsquo;s \u0026ldquo;Cat\u0026rdquo; and the last 2 it\u0026rsquo;s \u0026ldquo;Banana\u0026rdquo;. For each case, they highlighted the top 3 relevant attributes to the prediction along with their relevances and corresponding AM+PI outputs.\nAnalysis of the AM+PI outputs reveals that attribute functions generally activate for patterns corresponding to the same concept inferred during global analysis. This consistency is evident for attribute functions present in the previous figures. Additionaly, by looking at the figure showing the relevance of class-attribute pairs in section 4.1 for the QuickDRAW dataset we observe that the 3 most important features for each class in the local interpretations are also those having the highest relevence for these classes. For example for the \u0026ldquo;Banana\u0026rdquo; class, $\\phi_{16}$, which activates the curve shape, is by far the most important feature for identifying this class by looking at both the local interpretations and the class-attribute relevences. While for the \u0026ldquo;Cat\u0026rdquo; class, it seems that the most important features are in order $\\phi_{23}$, $\\phi_1$ and $\\phi_{19}$ when looking at both the local interpretations and the class-attribute relevences.\n5 Subjective evaluation In the article, a subjective evaluation survey with 20 respondents using the QuickDraw dataset to assess FLINT\u0026rsquo;s interpretability is conducted. The authors selected 10 attributes covering 17 class-attribute pairs and presented visualizations (3 MAS and AM+PI outputs) along with textual descriptions for each attribute to the respondents. They were asked to indicate their level of agreement with the association between the descriptions and the patterns in the visualizations using predefined choices.\nDescriptions were manually generated, including 40% incorrect ones to ensure informed responses. Results showed that for correct descriptions, 77.5% of respondents agreed, 10.0% were unsure, and 12.5% disagreed. For incorrect descriptions, 83.7% disagreed, 7.5% were unsure, and 8.8% agreed. These results affirm that the concepts encoded in FLINT\u0026rsquo;s learned attributes are understandable to humans.\n6 Specialization of FLINT to post-hoc interpretability FLINT primarily aims for interpretability by design, but the authors of the article propose that it can also be adapted to provide post-hoc interpretations when a classifier $\\hat{f}$ is already available. Post-hoc interpretation learning, a special case of SLI, involves building an interpreter for $\\hat{f}$ by minimizing a certain objective function. Specifically, Given a classifier $\\hat{f} \\in F$ and a training set $S$, the goal is to build an interpreter of $\\hat{f}$ by solving: $$ \\text{arg} \\min_{g \\in G_{f}} L_{int}(\\hat{f}, g, S) $$ Where $g(x)=h(\\Phi(\\hat{f_I} (x)))$ for a given set of $I$ hidden layers and an attribute dictionnary of size $J$. The learning is performed the same as before but we only keep the parameters $\\theta_\\Psi$, $\\theta_h$ and $\\theta_d$. We fix $\\theta_\\hat{f}$ and remove $L_{pred}$ from the training loss $L$.\nThere are experimental results in the article and in the supplements that are not mentionned here that demonstrate the effectiveness of post-hoc interpretation within FLINT, showing that even without fine-tuning the internal layers of the classifier, meaningful interpretations can be generated with high fidelity.\n7 Conclusion In conclusion, FLINT offers a robust framework for enhancing the interpretability of machine learning models, particularly deep neural networks, in critical domains like healthcare, law, and defense. By jointly learning predictor and interpreter models, FLINT addresses the challenge of providing both global and local interpretations of model predictions. Through carefully designed loss functions, FLINT ensures fidelity to input and output, promotes concise and diverse interpretations, and facilitates the representation of intricate patterns associated with input data. Reproducing experiments on datasets such as CIFAR10 and QuickDRAW showcases FLINT\u0026rsquo;s effectiveness in providing interpretable insights into model predictions. Subjective evaluations affirm the understandability of FLINT\u0026rsquo;s learned attributes, reinforcing its potential for real-world applications. Moreover, FLINT\u0026rsquo;s adaptability for post-hoc interpretability underscores its versatility, enabling meaningful interpretations without extensive modification of the underlying classifier. Overall, FLINT emerges as a valuable tool for fostering transparency and trust in complex machine learning models, contributing to the development of interpretable AI systems across various domains.\n",
      "content_html": "\u003chr\u003e\u003c/hr\u003e\n\u003cstyle\nTYPE=\"text/css\"\u003e\n\u003cp\u003ecode.has-jax {font:\ninherit;\nfont-size:\n100%;\nbackground:\ninherit;\nborder:\ninherit;}\u003c/p\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eA Framework to Learn with Interpretation\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors: Maroun ABOU BOUTROS, Mohamad EL OSMAN\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArticle: \u003ca href=\"https://arxiv.org/abs/2010.09345\"\u003eA Framework to Learn with Interpretation\u003c/a\u003e by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eLearning a classifier and an interpreter\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-2.1\"\u003eDesign of FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.2\"\u003eInterpretation in FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.3\"\u003eLearning by imposing interpretability properties\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eUnderstanding encoded concepts in FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eReproducing the experiments\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-4.1\"\u003eGlobal interpretation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4.2\"\u003eLocal interpretation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eSubjective evaluation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eSpecialization of FLINT to post-hoc interpretability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"section-1\"\u003e1 Introduction\u003c/h1\u003e\n\u003cp\u003eIn this blog post, we’ll explore FLINT, a framework introduced in the paper titled “A Framework to Learn with Interpretation” by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc, available on the following \u003ca href=\"https://arxiv.org/abs/2010.09345\"\u003elink\u003c/a\u003e, addressing the crucial need for interpretability in machine learning as complex predictive models become more prevalent in fields like law, healthcare, and defense. Interpretability, synonymous with explainability, provides insights into a model’s decision-making process. Two main approaches, post-hoc methods and “interpretable by design” methods, tackle the challenge of interpreting models, each with its pros and cons. A new approach, Supervised Learning with Interpretation (SLI), jointly learns a predictive model and an interpreter model. FLINT, specifically designed for deep neural network classifiers, introduces a novel interpreter network architecture promoting local and global interpretability. It also proposes a criterion for concise and diverse attribute functions, enhancing interpretability. We’ll delve into the architecture of FLINT and how it works to give explainable predictions, and we will reproduce some experiments done in the experimental section of the article and evaluate their outputs to study FLINT\u0026rsquo;s performance. And finally, we will present a specialization of FLINT for post-hoc interpretability.\u003c/p\u003e\n\u003ch1 id=\"section-2\"\u003e2 Learning a classifier and its interpreter with FLINT\u003c/h1\u003e\n\u003cp\u003eThe paper introduces Supervised Learning with Interpretation (SLI), a new task aimed at incorporating interpretability alongside prediction in machine learning models. In SLI, a separate model, called an interpreter, is employed to interpret the predictions made by the primary predictive model. The task involves minimizing a combined loss function consisting of prediction error and interpretability objectives. The paper focuses on addressing SLI within the context of deep neural networks for multi-class classification tasks. It proposes a framework called Framework to Learn with INTerpretation (FLINT), which utilizes a specialized architecture for the interpreter model, distinguishes between local and global interpretations, and introduces corresponding penalties in the loss function to achieve the desired interpretability.\u003cbr\u003e\nSo for a dataset $S$ and a given model $f \\in F$ where $F$ is a class of classifiers (here neural networks) and an interpreter model $g \\in G_f$ where $G_f$ is a family of models, the SLI problem is presented by:\n$$\n\\arg{\\min_{f \\in F, g \\in G_f}{L_{pred}(f, S) + L_{int}(f, g, S)}}\n$$\nWhere $L_{pred}(f, S)$ denotes a loss term related to prediction error and $L_{int}(f, g, S)$ measures the ability of $g$ to provide interpretations of predictions by $f$.\u003c/p\u003e\n\u003ch2 id=\"section-2.1\"\u003e2.1 Design of FLINT\u003c/h2\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/FLINT_design.png\"\n  alt=\"design of FLINT\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn FLINT, depicted in the image above, both a prediction model ($f$) and an interpreter model ($g$) are used. The input to FLINT is a vector $x \\in X$, where $X = \\mathbb{R}^d$, and the output is a vector $y \\in Y$, where $Y$ is defined as the set of one-hot encoding vectors with binary components of size $C$ (the number of classes to predict). The prediction model $f$ is structured as a deep neural network with $l$ hidden layers, represented as $f = f_{l+1} \\circ f_l \\circ \\ldots \\circ f_1$. Each $f_k$ represents a hidden layer mapping from $R^{d_{k-1}}$ to $R^{d_k}$. To interpret the outputs of $f$, we randomly select a subset of $T$ hidden layers, indexed by $I=\\{i_1, i_2, \\ldots, i_T\\}$, and concatenate their outputs to form a new vector $f_I(x) \\in \\mathbb{R}^D$, where $D = \\sum_{t=1}^T d_{i_t}$. This vector is then fed into a neural network $\\Psi$ to produce an output vector $\\Phi(x) = \\Psi(f_I(x)) = (\\phi_1(x), \u0026hellip;, \\phi_J (x)) \\in \\mathbb{R}^J$, representing an attribute dictionary comprising functions $\\phi_j: X \\rightarrow \\mathbb{R}^+$, where $\\phi_j(x)$ captures the activation of a high-level attribute or a \u0026ldquo;concept\u0026rdquo; over $X$. Finally, $g$ computes the composition of the attribute dictionnary with an interpretable function $h: R^J \\rightarrow Y$.\n$$\n\\forall x \\in X, g(x) = h(\\Phi(x))\n$$\nFor now we take $h(x) = softmax(W^T \\Phi(x))$ but $h$ can be any interpretable function (like a decision tree for example).\u003c/p\u003e\n\u003cp\u003eNote that $d$ in the image is a decoder network that takes $\\Phi(x)$ and reconstructs the input $x$. This decoder is used for training and its purpose will be detailed later on in section 2.3.\u003c/p\u003e\n\u003ch2 id=\"section-2.2\"\u003e2.2 Interpretation in FLINT\u003c/h2\u003e\n\u003cp\u003eWith the interpreter defined, let\u0026rsquo;s clarify its role and interpretability objectives within FLINT. Interpretation serves as an additional task alongside prediction. We\u0026rsquo;re interested in two types: global interpretation, which aids in understanding which attribute functions contribute to predicting a class, and local interpretation, which pinpoints the attribute functions involved in predicting a specific sample.\u003c/p\u003e\n\u003cp\u003eTo interpret a local prediction $f(x)$, it\u0026rsquo;s crucial that the interpreter\u0026rsquo;s output $g(x)$ aligns with $f(x)$. Any discrepancy prompts analysis of conflicting data, potentially raising concerns about the prediction\u0026rsquo;s confidence.\u003c/p\u003e\n\u003cp\u003eTo establish local and global interpretation, we rely on attribute relevance. Given an interpreter with parameters $\\Theta_g = (\\theta_\\Psi, \\theta_h)$ and an input $x$, an attribute $\\phi_j$\u0026rsquo;s relevance is defined concerning the prediction $g(x) = f(x) = \\hat{y}$. The attribute\u0026rsquo;s contribution to the unnormalized score of class $\\hat{y}$ is $\\alpha_{j, \\hat{y}, x} = \\phi_j(x) \\cdot w_{j, \\hat{y}}$, where $w_{j, \\hat{y}}$ is the coefficient associated with this class. Relevance score $r_{j, x}$ is computed by normalizing $\\alpha$ as $r_{j, x} = \\frac{\\alpha_{j, \\hat{y}, x}}{\\max_i |\\alpha_{i, \\hat{y}, x}|}$. An attribute $\\phi_j$ is considered relevant for a local prediction if it\u0026rsquo;s both activated and effectively used in the linear model.\u003c/p\u003e\n\u003cp\u003eAttribute relevance extends to its overall importance in predicting any class $c$. This is achieved by averaging relevance scores from local interpretations over a random subset or the entirety of the training set $S$ where the predicted class is $c$. Thus, $r_{j, c} = \\frac{1}{|S_c|} \\sum_{x \\in S_c} r_{j, x}$, where $S_c = \\{x \\in S \\mid \\hat{y} = c\\}$.\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s introduce the local and global interpretations the interpreter will provide:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGlobal interpretation ($G(g, f)$) identifies class-attribute pairs $(c, \\phi_j)$ where the global relevance $r_{j, c}$ exceeds a threshold $\\frac{1}{\\tau}$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLocal interpretation ($L(x, g, f)$) for a sample $x$ includes attribute functions $\\phi_j$ with local relevance $r_{j, x}$ surpassing $\\frac{1}{\\tau}$. These definitions don\u0026rsquo;t assess interpretation quality directly.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2.3\"\u003e2.3 Learning by imposing interpretability properties\u003c/h2\u003e\n\u003cp\u003eFor learning, the paper defines certain penalties to minimize, where each one aims to enforce a certain desirable property:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eFidelity to output:\u003c/strong\u003e\u003c/em\u003e The output of $g(x)=h(\\Psi(f_I(x)))$ should be close to $f(x)$ for any x. This can be imposed through a cross-entropy loss:\n$$\nL_{of}(f, g, S) = - \\sum_{x \\in S} h(\\Psi(f_I(x)))^T \\log(f(x))\n$$\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eConciseness and Diversity of Interpretations:\u003c/strong\u003e\u003c/em\u003e We aim for concise local interpretations, containing only essential attributes per sample, promoting clearer understanding and capturing high-level concepts. Simultaneously, we seek diverse interpretations across samples to prevent attribute functions from being class-exclusive. To achieve this, the paper proposes that we leverage entropy (defined for a vector as $\\mathcal{E}(v) = - \\sum_i p_i \\log(p_i)$), which quantifies uncertainty in real vectors. Conciseness is fostered by minimizing the entropy of the interpreter\u0026rsquo;s output, $\\Phi(x) = \\Psi(f_I(x))$, while diversity is encouraged by maximizing the entropy of the average $\\Psi(f_I(x))$ over a mini-batch. This approach promotes sparse and varied coding of $f_I(x)$, enhancing interpretability. However, as entropy-based losses lack attribute activation constraints, leading to suboptimal optimization, we also minimize the $l_1$ norm of $\\Psi(f_I(x))$ with hyperparameter $\\eta$. Although $l_1$-regularization commonly encourages sparsity, the experiments done in the paper show that entropy-based methods are more effective.\n$$\nL_{cd}(f, g, S) = -\\mathcal{E}(\\frac{1}{\\lvert S \\lvert} \\sum_{x \\in S} \\Psi(f_I(x))) + \\sum_{x \\in S} \\mathcal{E}(\\Psi(f_I(x))) + \\sum_{x \\in S} \\eta \\lVert \\Psi(f_I(x)) \\lVert_1\n$$\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eFidelity to input:\u003c/strong\u003e\u003c/em\u003e In order to promote the representation of intricate patterns associated with the input within $\\Phi(x)$, a decoder network $d : \\mathbb{R}^J \\rightarrow X$ is employed. This network is designed to take the attribute dictionary $\\Phi(x)=\\Psi(f_I(x))$ as input and reconstruct the original input $x$.\n$$\nL_{if}(f, g, d, S) = \\sum_{x \\in S} (d(\\Psi(f_I(x))) - x)^2\n$$\u003c/p\u003e\n\u003cp\u003eGiven the proposed loss terms, the loss for the interpretability model writes as follows:\n$$\nL_{int}(f, g, d, S) = \\beta L_{of}(f, g, S) + \\gamma L_{if}(f, g, d, S) + \\delta L_{cd}(f, g, S)\n$$\nWhere $\\beta, \\gamma, \\delta$ are non-negative hyperparameters. the total loss to be minimized $L = L_{pred} + L_{int}$, where the prediction loss, $L_{pred}$, is the well-know cross entropy loss (since this a classification problem).\u003c/p\u003e\n\u003ch1 id=\"section-3\"\u003e3 Understanding encoded concepts in FLINT\u003c/h1\u003e\n\u003cp\u003eOnce the predictor and interpreter networks are jointly learned, interpretation can be conducted at both global and local levels . A critical aspect highlighted by the authors is understanding the concepts encoded by each individual attribute function ​$\\phi_j$ . Focusing on image classification, the authors propose representing an encoded concept as a collection of visual patterns in the input space that strongly activate $\\phi_j$ . They present a pipeline for generating visualizations for both global and local interpretation, adapting various existing tools .\u003c/p\u003e\n\u003cp\u003eFor global interpretation visualization, the authors propose starting by selecting a small subset of training samples from a given class c that maximally activate ​$\\phi_j$ . This subset, referred to as Maximum Activating Samples (MAS), is denoted as $MAS(c , ​\\phi_j , l)$ where $l$ is the subset size (set as 3 in their experiments). However, while MAS provides some insight into the encoded concept, further analysis is required to understand the specific aspects of these samples that cause ​$\\phi_j$ activation. To achieve this, the authors propose utilizing a modified version of activation maximization called Activation Maximization with Partial Initialization (AM+PI). This technique aims to synthesize input that maximally activates ​$\\phi_j$ by optimizing a common activation maximization objective, initialized with a low-intensity version of the sample from MAS.\u003c/p\u003e\n\u003cp\u003eFor local analysis, given any test sample $x_{0}$ , its local interpretation $L(x_{0},f,g)$ can be determined, representing the relevant attribute functions . To visualize a relevant attribute ​$\\phi_j$, the authors suggest repeating the AM+PI procedure with initialization using a low-intensity version of $x_{0}$ to enhance the concept detected by ​$\\phi_j$ in $x_{0}$ .\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e4 Reproducing the experiments\u003c/h2\u003e\n\u003cp\u003eIn the experimental section of the article, several experiments were conducted to do a quantitative evaluation of FLINT\u0026rsquo;s performance compared to other state-of-the-art models designed for interpretability, such as SENN and PrototypeDNN. Additionally, FLINT was compared to LIME and VIBI to evaluate the fidelity of its interpretations, measuring the proportion of samples where the predictions of a model and its interpreter agree. Across these tests, FLINT consistently outperformed the other models, demonstrating its reliability and effectiveness.\u003c/p\u003e\n\u003cp\u003eHowever, in this blog post we will specifically focus on reproducing the experiments in the article related to FLINT\u0026rsquo;s explainability, that aim to do a qualitative analysis of it. To achieve a thorough understanding of the model and its operational dynamics across prevalent datasets, we replicated the study by cloning the project from the GitHub repository referenced in the article (\u003ca href=\"https://github.com/jayneelparekh/FLINT\"\u003erepo link\u003c/a\u003e). Our experimentation involved the CIFAR10 and QuickDRAW datasets, employing a ResNet18-based network for both. For the QuickDRAW dataset, we utilized J=24 attributes, while for the CIFAR10 dataset, we used J=36 attributes.\u003c/p\u003e\n\u003cp\u003eThe instructions provided in the GitHub repository for executing the model are clear, and the model runs flawlessly. We have the option to either train the model ourselves or download the pre-trained models. Furthermore, there is a well-detailed Python notebook named \u0026ldquo;FLINT demo.ipynb\u0026rdquo;, which contains code for visualizing data, such as attribute relevance scores for each class and local interpretations for data samples. We will execute FLINT on test images and take a look at how interpretability is done with FLINT in this section.\u003c/p\u003e\n\u003ch3 id=\"section-4.1\"\u003e4.1 Global interpretation\u003c/h3\u003e\n\u003cp\u003eIn the article, the authors explore global interpretation using a figure similar to the one provided below which was reproduced from the notebook, and which illustrates the generated global relevances $r_{j,c}$ for all class-attribute pairs in the QuickDraw dataset.\u003c/p\u003e\n\u003c!-- ![Global class-attribute relevances](/images/FLINT/Global_class_attribute_QuickDRAW.png) --\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/FLINT/Global_class_attribute_QuickDRAW.png\" alt=\"Image\" width=\"300\" height=\"200\"\u003e\n\u003c/div\u003e\n\u003cp\u003eAdditionally, by running the model on the CIFAR10 and QuickDRAW dataset we got visual outputs representative of class-attribute pair analyses for both datasets. These outputs served as pivotal tools in elucidating interrelations and facilitating comparative assessments between attributes and classes. We present below two figures derived from the resultant class-attribute pair analyses for each of the 2 datasets. The class-attribute pairs shown are different from the examples shown in the paper.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Class_attribute_pair_CIFAR10.png\"\n  alt=\"Class-attribute pair analysis on dataset CIFAR10\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Class-attribute pair analysis on dataset CIFAR10\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Class_attribute_pair_QuickDraw.png\"\n  alt=\"Class-attribute pair analysis on dataset QuickDraw\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Class-attribute pair analysis on dataset QuickDraw\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe focus on class-attribute pairs with high relevance, showcasing examples in the provided figure above . For each pair, we examine Maximum Activating Samples (MAS) alongside their corresponding Activation Maximization with Partial Initialization (AM+PI) outputs.\u003c/p\u003e\n\u003cp\u003eMAS analysis alone provides valuable insights into the encoded concept. For instance, on QuickDRAW dataset, attribute $\\phi_{16}$  relevant for class \u0026lsquo;Banana\u0026rsquo; activates the curve shape of the banana. However, AM+PI outputs offer deeper insights by elucidating which parts of the input activate an attribute function more clearly. And on CIFAR10 dataset , attribute $\\phi_{12}$ activates for \u0026lsquo;Deer\u0026rsquo; class , but the specific focus of the attribute remains ambiguous. The outputs of the AM+PI method indicate that attribute $\\phi_{12}$ predominantly highlights the area encompassing the legs and the horns of the deer, characterized as the most prominently enhanced regions.\u003c/p\u003e\n\u003ch3 id=\"section-4.2\"\u003e4.2 Local interpretation\u003c/h3\u003e\n\u003cp\u003eSimilarly to the article, we explored local interpretation through the figure provided below which was generated in the notebook, which showcases visualizations for 4 test samples of the QuickDRAW dataset. Both predictor $f$ and interpreter $g$ accurately predict the true class in all cases, for the first 2 it\u0026rsquo;s \u0026ldquo;Cat\u0026rdquo; and the last 2 it\u0026rsquo;s \u0026ldquo;Banana\u0026rdquo;. For each case, they highlighted the top 3 relevant attributes to the prediction along with their relevances and corresponding AM+PI outputs.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Local_interpretations.jpg\"\n  alt=\"Local interpretations for test samples\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAnalysis of the AM+PI outputs reveals that attribute functions generally activate for patterns corresponding to the same concept inferred during global analysis. This consistency is evident for attribute functions present in the previous figures. Additionaly, by looking at the figure showing the relevance of class-attribute pairs in section 4.1 for the QuickDRAW dataset we observe that the 3 most important features for each class in the local interpretations are also those having the highest relevence for these classes. For example for the \u0026ldquo;Banana\u0026rdquo; class, $\\phi_{16}$, which activates the curve shape, is by far the most important feature for identifying this class by looking at both the local interpretations and the class-attribute relevences. While for the \u0026ldquo;Cat\u0026rdquo; class, it seems that the most important features are in order $\\phi_{23}$, $\\phi_1$ and $\\phi_{19}$ when looking at both the local interpretations and the class-attribute relevences.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003e5 Subjective evaluation\u003c/h2\u003e\n\u003cp\u003eIn the article,  a subjective evaluation survey with 20 respondents using the QuickDraw dataset to assess FLINT\u0026rsquo;s interpretability is conducted. The authors selected 10 attributes covering 17 class-attribute pairs and presented visualizations (3 MAS and AM+PI outputs) along with textual descriptions for each attribute to the respondents. They were asked to indicate their level of agreement with the association between the descriptions and the patterns in the visualizations using predefined choices.\u003c/p\u003e\n\u003cp\u003eDescriptions were manually generated, including 40% incorrect ones to ensure informed responses. Results showed that for correct descriptions, 77.5% of respondents agreed, 10.0% were unsure, and 12.5% disagreed. For incorrect descriptions, 83.7% disagreed, 7.5% were unsure, and 8.8% agreed. These results affirm that the concepts encoded in FLINT\u0026rsquo;s learned attributes are understandable to humans.\u003c/p\u003e\n\u003ch1 id=\"section-6\"\u003e6 Specialization of FLINT to post-hoc interpretability\u003c/h1\u003e\n\u003cp\u003eFLINT primarily aims for interpretability by design, but the authors of the article propose that it can also be adapted to provide post-hoc interpretations when a classifier $\\hat{f}$ is already available. Post-hoc interpretation learning, a special case of SLI, involves building an interpreter for $\\hat{f}$ by minimizing a certain objective function. Specifically, Given a classifier\n$\\hat{f} \\in F$ and a training set $S$, the goal is to build an interpreter of $\\hat{f}$ by solving:\n$$\n\\text{arg} \\min_{g \\in G_{f}} L_{int}(\\hat{f}, g, S)\n$$\nWhere $g(x)=h(\\Phi(\\hat{f_I} (x)))$ for a given set of $I$ hidden layers and an attribute dictionnary of size $J$. The learning is performed the same as before but we only keep the parameters $\\theta_\\Psi$, $\\theta_h$ and $\\theta_d$. We fix $\\theta_\\hat{f}$ and remove $L_{pred}$ from the training loss $L$.\u003c/p\u003e\n\u003cp\u003eThere are experimental results in the article and in the supplements that are not mentionned here that demonstrate the effectiveness of post-hoc interpretation within FLINT, showing that even without fine-tuning the internal layers of the classifier, meaningful interpretations can be generated with high fidelity.\u003c/p\u003e\n\u003ch1 id=\"section-7\"\u003e7 Conclusion\u003c/h1\u003e\n\u003cp\u003eIn conclusion, FLINT offers a robust framework for enhancing the interpretability of machine learning models, particularly deep neural networks, in critical domains like healthcare, law, and defense. By jointly learning predictor and interpreter models, FLINT addresses the challenge of providing both global and local interpretations of model predictions. Through carefully designed loss functions, FLINT ensures fidelity to input and output, promotes concise and diverse interpretations, and facilitates the representation of intricate patterns associated with input data. Reproducing experiments on datasets such as CIFAR10 and QuickDRAW showcases FLINT\u0026rsquo;s effectiveness in providing interpretable insights into model predictions. Subjective evaluations affirm the understandability of FLINT\u0026rsquo;s learned attributes, reinforcing its potential for real-world applications. Moreover, FLINT\u0026rsquo;s adaptability for post-hoc interpretability underscores its versatility, enabling meaningful interpretations without extensive modification of the underlying classifier. Overall, FLINT emerges as a valuable tool for fostering transparency and trust in complex machine learning models, contributing to the development of interpretable AI systems across various domains.\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/a-framework-to-learn-with-interpretation/",
      "date_published": "13026-13-09T256:1313:00+01:00",
      "date_modified": "13026-13-09T256:1313:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "2b742e35078ad5eed5470d116570f0fc58f294aa",
      "title": "NTK-SAP: IMPROVING NEURAL NETWORK PRUNING BY ALIGNING TRAINING DYNAMICS",
      "summary": "",
      "content_text": "This is a blog post about the paper NTK-SAP: Improving neural network pruning by aligning training dynamics, published by Y. Wang et al. in 2023 and available here.\nIntroduction:\nIn a world increasingly driven by demand for data and computational resources, the narrative of artificial intelligence has been one of abundance: more data, more power, more precision. Yet, nestled within this grand tale, lies a quieter narrative - one that champions the concept of achieving more with less—Frugal AI.\nImagine a craftsman from a bygone era, working in a workshop filled with natural light. Instead of an overwhelming array of tools, he possesses only a few, each worn and refined by years of careful use. With these simple instruments, he creates works of unexpected beauty, demonstrating that the value lies not in the abundance of resources, but in the skill and wisdom with which they are used. Frugal AI embodies this craftsman’s spirit in the digital age. It does not revel in the excesses of computational power or data. Instead, it thrives in constraint, finding clever pathways through the limitations, optimizing algorithms not just for performance, but for efficiency and accessibility.\nIn the quest for efficiency, neural network pruning has emerged as a foundation of Frugal AI principles. Just as craftsmen meticulously select and refine their tools, neural network pruning systematically removes redundant, non-critical components from a network, optimizing its performance without compromising its functionality.\nNeural network pruning\nNeural network pruning stems from the recognition that many models, especially deep learning networks, are often over-parameterized. This means they contain more parameters than are necessary for effective learning or inference. In the context of Frugal AI, this over-parameterization is analogous to an artist\u0026rsquo;s studio cluttered with unused tools and materials, which, rather than aiding, only serve to overwhelm and complicate. The act of pruning, therefore, can be seen as an effort to streamline and refine. It\u0026rsquo;s about identifying and removing the \u0026rsquo;excess\u0026rsquo; in the network—those weights and connections that contribute little to the output. This not only reduces the computational load, making the network faster and more energy-efficient, but also often improves its generalization ability, making the model less prone to overfitting and more adaptable to different tasks or datasets.\nPruning Methods:\nPruning methodologies come in various forms, each tailored to specific needs and objectives. These methodologies can be categorized into three main types: post-hoc pruning, pruning during training, and foresight pruning.\nPost-hoc Pruning: This technique trims neural networks after training, typically requiring multiple train-prune-retrain cycles. It utilizes various metrics, like magnitude and Hessian values, to determine which weights to eliminate, primarily aiming to reduce inference time.\nPruning During Training: This approach involves gradually removing connections within a neural network as it trains, employing regularization or trainable masks. It aims to save training time but doesn\u0026rsquo;t necessarily reduce memory costs.\nForesight Pruning: This strategy prunes networks before training begins to prevent unnecessary computational waste. It seeks to address issues like layer collapse collapse at high sparsity levels. Recent advancements aim to overcome the limitations of early pruning methods by incorporating more informed strategies, such as meta-gradients.\nForesight pruning methods - saliency score:\nForesight pruning methods optimize neural network structures by identifying and removing less important connections, reducing computational complexity while maintaining performance. At the heart of these methods lies the loss function, which serves as the guiding metric for evaluating the network\u0026rsquo;s performance on a given dataset and determining which connections to prune. Given the complexity of directly solving the loss function, an indirect method is employed. Each potential connection within the network is assigned a \u0026ldquo;saliency score,\u0026rdquo; reflecting its influence on the loss function. This score is computed by assessing how changes in the connection impact the loss function, scaled by the initial weight value. Essentially, connections with higher saliency scores, indicating greater impact on the loss function, are retained, while those with lower scores are pruned. This systematic approach ensures that the network remains efficient while preserving its effectiveness in solving tasks.\nKey pruning methods such as SNIP, Iterative SNIP, GraSP, and Synflow, introduce specific saliency measures to assess the importance of connections:\n1. SNIP calculates saliency as $S_{\\text{SNIP}}(m\u0026rsquo;) = \\left|\\frac{\\partial L}{\\partial \\theta}\\odot \\theta\\right|$, focusing on the impact of each connection on the loss. SNIP\u0026rsquo;s saliency score is the difference in the loss function before and after pruning a connection.\n2. Iterative SNIP repeats the process of SNIP multiple times for a refined pruning.\n3. GraSP employs the Hessian-gradient product to identify connections important for preserving gradient flow, with saliency defined as $S_{\\text{GraSP}}(m\u0026rsquo;) = -\\left[H(\\theta \\odot m\u0026rsquo;; D)\\frac{\\partial L}{\\partial \\theta}\\right] \\odot \\theta$.\n4. Synflow uses $S_{\\text{Synflow}}(m\u0026rsquo;) = \\left|\\theta\\right| \\odot \\left|\\frac{\\partial L}{\\partial \\theta}\\right|$ as a data-agnostic measure, emphasizing connections\u0026rsquo; overall contribution to the network\u0026rsquo;s output irrespective of the dataset.\nEach method\u0026rsquo;s saliency score guides the pruning process by ranking the connections based on their calculated importance to only keep the top-ranked connections - the most salient ones. Therefore, the overall idea is to start with a complex network, score each connection by importance, and keep only the most important connections. This results in a simpler network that is cheaper to train and run but still capable of learning effectively from the data.\nNeural Tangent Kernel (NTK):\nIn recent studies, there has been significant exploration into optimizing neural networks on a global scale. One notable area of focus involves leveraging the neural tangent kernel (NTK) to gain deeper insights into how gradient descent functions within extensive deep neural networks. The NTK spectrum provides valuable information about convergence patterns. Remarkably, researchers have observed that the NTK remains consistent throughout training in sufficiently large DNNs. This suggests that the NTK spectrum could serve as a comprehensive measure for understanding training dynamics.\nNeural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP):\nConsequently, a novel pruning approach has emerged: selectively removing connections that exert minimal influence on the NTK spectrum.\nIn order to implement this conceptual pruning methods, there are a few considerations:\n1. Metric Selection: Due to the complexity and time required to calculate the full range of eigenvalues (the eigenspectrum) of the Neural Tangent Kernel, the nuclear norm—essentially the sum of these eigenvalues—is used instead as a scalar to summarize the characteristics of the eigenspectrum.\n2. Choosing the Right NTK Matrix:\nWe can distinguish between wo types of NTK matrices:\nFixed-Weight NTK: Related to the network\u0026rsquo;s initial setup. Analytic NTK: A theoretical model assuming a network of infinite size However, since calculating the Analytic NTK is highly resource-intensive, the researchers use a practical workaround. They approximate the Analytic NTK by averaging multiple Fixed-Weight NTKs from various initial setups, balancing computational efficiency with accuracy.\n3. Computational Efficiency: To manage computation costs, there is a technique known as the \u0026ldquo;new-input-new-weight\u0026rdquo; (NINW) method. This approach involves changing the network\u0026rsquo;s weights for each new set of input data. By doing this, they can efficiently evaluate the properties of the Neural Tangent Kernel (NTK) across different scenarios without significantly adding to the computational load.\nBased on these considerations, Wang and colleagues have developed an innovative approach called Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP).\nNTK-SAP leverages the NTK spectrum for efficient foresight pruning by using multi-sampling to predict pruning outcomes and ensure accuracy. It also incorporates the Novel Iterative Network Weighting (NINW) technique to reduce computation costs. This method streamlines neural networks by preemptively removing less impactful parts, optimizing both the pruning process and the network\u0026rsquo;s performance with minimal resource expenditure.\nNTK-SAP follows the following implementation:\nCalculation of NTK-SAP Saliency Score:\n1. Finite Approximation Approach\nThe NTK-SAP method introduces a finite approximation expression to calculate a saliency score S-NTK-SA, which leverages the pruning dataset to approximate the entire training set. This foresight pruning approach identifies and prunes weights with the lowest saliency scores.\nSaliency score based on a fixed-weight Neural Tangent Kernel:\n$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\mathbb{E}_{\\Delta\\theta \\sim \\mathcal{N}(0, \\epsilon I)} \\left[ \\left| f(\\mathbf{X}_D; \\theta_0 \\odot m) - f(\\mathbf{X}_D; (\\theta_0 + \\Delta\\theta) \\odot m) \\right|_2^2 \\right] \\right|$$\n2. Multi-Sampling Approach:\nWhile a single fixed-weight-NTK provides an approximation of the analytic NTK, averaging over multiple fixed-weight-NTKs offers a closer approximation to the expected behavior of the analytic NTK. This method entails sampling several independent weight configurations and averaging their fixed-weight-NTKs to better understand the parameter space and the anticipated performance of pruned networks.\nA stabilized version of the saliency score, S-NTK-SAP(mj) is introduced and incorporates the average of fixed-weight-NTKs computed across multiple random weight configurations, to assess the impact of pruning. Unlike most existing foresight pruning scores, which are dependent on specific weight configurations, this proposed saliency score is weight-agnostic; it primarily reflects the structure of the mask applied for pruning rather than the weights themselves. This distinction highlights the score\u0026rsquo;s focus on the inherent characteristics of the pruning method over the variability of weight initializations.\n3. New-input-new-weight (NINW) trick:\nTo reconcile the theoretical aspirations with practical viability, NTK-SAP leverage the \u0026rsquo;new-input-new-weight\u0026rsquo; (NINW) trick. This technique estimates the expected behavior of pruned networks by utilizing a new set of weights for each mini-batch of input data. This approach ensures that the pruning algorithm remains computationally feasible, allowing for the real-world application without prohibitive resource demands.\n4. Random Input Trick:\nNTK-SAP relies on another trick that consists in replacing the pruning set with random inputs. This allows to approximate the network\u0026rsquo;s behavior without depending on real data, thus highlighting NTK-SAP\u0026rsquo;s ability to adapt to any dataset without requiring specific adjustments or optimization.\n$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\left[ \\left| f\\left(Z_i; \\theta_{0,i} \\odot m\\right) - f\\left(Z_i; \\left(\\theta_{0,i} + \\Delta\\theta_i\\right) \\odot m\\right) \\right|_2^2 \\right] \\right|$$\nExperimental validation:\nExperiments were performed on CIFAR-10, CIFAR-100, and Tiny-ImageNet data sets to validate NTK-SAP\u0026rsquo;s superiority across various sparsity levels. Particularly noteworthy is its robust performance at extreme sparsity ratios, where traditional methods falter. These results underscore the efficacy of our multi-sampling strategy and the practical utility of the NINW trick.\nExtending the analysis to the more challenging ImageNet dataset, NTK-SAP consistently outperforms baseline pruning methods, including SNIP and GraSP, especially at high sparsity levels. This success highlights NTK-SAP\u0026rsquo;s scalability and its potential to facilitate efficient neural network training on large-scale datasets.\nReproductive experiments:\nTo ensure reproducibility, begin by installing the required packages:\npip install -r requirements.txt Next, to run NTK-SAP with the default dataset and parameters using the following command:\npython main.py The default parameters are as follows:\n--dataset: Mnist --model-class: default --model: fc --pruner: rand --prune-batch-size: 256 --compression: 0.0 --prune-train-mode: False --prune-epochs: 1 --ntksap_R: 1 --ntk_epsilon: 0.01 For experimenting with different parameters, proceed with the desired adjustments.\n1. Experiment NTK-SAP with Cifar100 dataset, a 0.01 perturbation hyper-parameter\npython main.py --dataset cifar100 --ntksap_epsilon 0.01 Train results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.607083 1.00 4.96 Pre-Prune 0 NaN 4.607083 1.00 4.96 Post-Prune 0 NaN 4.607083 1.00 4.96 Final 10 3.337817 3.421804 17.91 45.41 2. Experiment NTK-SAP with Cifar100 dataset and a 0.02 perturbation hyper-parameter\npython main.py --dataset cifar100 --ntksap_epsilon 0.02 Train results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.607163 1.02 4.72 Pre-Prune 0 NaN 4.607163 1.02 4.72 Post-Prune 0 NaN 4.607163 1.02 4.72 Final 10 3.341863 3.460254 17.74 43.78 3. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 3\npython main.py --dataset cifar100 --prune-epochs 3 Train results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.606948 0.96 5.02 Pre-Prune 0 NaN 4.606948 0.96 5.02 Post-Prune 0 NaN 4.606948 0.96 5.02 Final 10 3.337061 3.448972 18.09 43.97 4. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 7\nTrain results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.606786 1.01 4.95 Pre-Prune 0 NaN 4.606786 1.01 4.95 Post-Prune 0 NaN 4.606786 1.01 4.95 Final 10 3.335409 3.397401 18.93 44.89 Analysis from experiments:\n1. Dataset Adaptability:\nThe study demonstrated NTK-SAP as being data-free. This quality allows pruned networks developed via these methods to be seamlessly adapted to various datasets without requiring additional data, highlighting their versatility and efficiency.\n2. Robustness across hyper-parameter variations:\nThe robustness of NTK-SAPP is evident through its consistent performance across varying perturbation hyper-parameters (ϵ) in experiments conducted on the Cifar100 dataset. When the perturbation hyper-parameter is set to 0.01, the model exhibits stable behavior throughout training and pruning phases, yielding a final top-1 accuracy of 17.91% and a top-5 accuracy of 45.41%. Similarly, when the perturbation hyper-parameter is increased to 0.02, the model maintains its stability, with minimal fluctuations observed in performance metrics compared to the unperturbed model. Both pre-prune and post-prune stages demonstrate resilience to perturbations, showcasing nearly identical results to the unperturbed model. This consistency across different perturbation levels underscores the robustness of NTK-SAPP, making it a reliable choice for tasks where stability under varying conditions is crucial.\n3. Fewer iterations for small datasets:\nAn exploration into how the number of iterations (T) affects performance across datasets reveals that for smaller datasets, reducing T slightly impacts outcomes, suggesting that computational efficiency can be achieved without significantly compromising results.\nConclusion:\nIn conclusion, NTK-SAP stands as a pivotal advancement in the realm of neural network pruning, showcasing its efficacy across diverse datasets and network architectures. By pruning at initialization, it eliminates the necessity for post-training methods and mask training. Moreover, by leveraging NTK theory, it addresses the oversight of training dynamics post-pruning, enabling iterative pruning without data dependency. NTK-SAP effectively bridges the theoretical underpinnings of optimization with practical neural network training, thus pushing the boundaries of frugal neural networks.\nWhile NTK-SAP represents a significant leap forward, it also unveils several avenues for future exploration. Subsequent research could delve into alternative spectral measures or extend the methodology to other forms of network optimization.\nIn essence, NTK-SAP not only signifies a crucial stride towards more efficient and theoretically grounded neural network pruning but also sets the stage for future innovations in enhancing network frugality. By Elia Lejzerowicz and Adrien Oleksiak.\n",
      "content_html": "\u003cp\u003eThis is a blog post about the paper NTK-SAP: Improving neural network pruning by aligning training dynamics, published by Y. Wang et al. in 2023 and available \u003ca href=\"https://openreview.net/pdf?id=-5EWhW_4qWP\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn a world increasingly driven by demand for data and computational resources, the narrative of artificial intelligence has been one of abundance: more data, more power, more precision. Yet, nestled within this grand tale, lies a quieter narrative - one that champions the concept of achieving more with less—Frugal AI.\u003c/p\u003e\n\u003cp\u003eImagine a craftsman from a bygone era, working in a workshop filled with natural light. Instead of an overwhelming array of tools, he possesses only a few, each worn and refined by years of careful use. With these simple instruments, he creates works of unexpected beauty, demonstrating that the value lies not in the abundance of resources, but in the skill and wisdom with which they are used.\nFrugal AI embodies this craftsman’s spirit in the digital age. It does not revel in the excesses of computational power or data. Instead, it thrives in constraint, finding clever pathways through the limitations, optimizing algorithms not just for performance, but for efficiency and accessibility.\u003c/p\u003e\n\u003cp\u003eIn the quest for efficiency, neural network pruning has emerged as a foundation of Frugal AI principles. Just as craftsmen meticulously select and refine their tools, neural network pruning systematically removes redundant, non-critical components from a network, optimizing its performance without compromising its functionality.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNeural network pruning\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNeural network pruning stems from the recognition that many models, especially deep learning networks, are often over-parameterized. This means they contain more parameters than are necessary for effective learning or inference. In the context of Frugal AI, this over-parameterization is analogous to an artist\u0026rsquo;s studio cluttered with unused tools and materials, which, rather than aiding, only serve to overwhelm and complicate. The act of pruning, therefore, can be seen as an effort to streamline and refine. It\u0026rsquo;s about identifying and removing the \u0026rsquo;excess\u0026rsquo; in the network—those weights and connections that contribute little to the output. This not only reduces the computational load, making the network faster and more energy-efficient, but also often improves its generalization ability, making the model less prone to overfitting and more adaptable to different tasks or datasets.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePruning Methods:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePruning methodologies come in various forms, each tailored to specific needs and objectives. These methodologies can be categorized into three main types: \u003cstrong\u003epost-hoc pruning\u003c/strong\u003e, \u003cstrong\u003epruning during training\u003c/strong\u003e, and \u003cstrong\u003eforesight pruning\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePost-hoc Pruning:\u003c/strong\u003e This technique trims neural networks after training, typically requiring multiple train-prune-retrain cycles. It utilizes various metrics, like magnitude and Hessian values, to determine which weights to eliminate, primarily aiming to reduce inference time.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePruning During Training:\u003c/strong\u003e This approach involves gradually removing connections within a neural network as it trains, employing regularization or trainable masks. It aims to save training time but doesn\u0026rsquo;t necessarily reduce memory costs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eForesight Pruning:\u003c/strong\u003e This strategy prunes networks before training begins to prevent unnecessary computational waste. It seeks to address issues like layer collapse collapse at high sparsity levels. Recent advancements aim to overcome the limitations of early pruning methods by incorporating more informed strategies, such as meta-gradients.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eForesight pruning methods - saliency score:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eForesight pruning methods optimize neural network structures by identifying and removing less important connections, reducing computational complexity while maintaining performance. At the heart of these methods lies the loss function, which serves as the guiding metric for evaluating the network\u0026rsquo;s performance on a given dataset and determining which connections to prune. Given the complexity of directly solving the loss function, an indirect method is employed. Each potential connection within the network is assigned a \u0026ldquo;saliency score,\u0026rdquo; reflecting its influence on the loss function. This score is computed by assessing how changes in the connection impact the loss function, scaled by the initial weight value. Essentially, connections with higher saliency scores, indicating greater impact on the loss function, are retained, while those with lower scores are pruned. This systematic approach ensures that the network remains efficient while preserving its effectiveness in solving tasks.\u003c/p\u003e\n\u003cp\u003eKey pruning methods such as \u003cstrong\u003eSNIP\u003c/strong\u003e, \u003cstrong\u003eIterative SNIP\u003c/strong\u003e, \u003cstrong\u003eGraSP\u003c/strong\u003e, and \u003cstrong\u003eSynflow\u003c/strong\u003e, introduce specific saliency measures to assess the importance of connections:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. SNIP\u003c/strong\u003e calculates saliency as $S_{\\text{SNIP}}(m\u0026rsquo;) = \\left|\\frac{\\partial L}{\\partial \\theta}\\odot \\theta\\right|$, focusing on the impact of each connection on the loss.  SNIP\u0026rsquo;s saliency score is the difference in the loss function before and after pruning a connection.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Iterative SNIP\u003c/strong\u003e  repeats the process of SNIP multiple times for a refined pruning.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. GraSP\u003c/strong\u003e employs the Hessian-gradient product to identify connections important for preserving gradient flow, with saliency defined as $S_{\\text{GraSP}}(m\u0026rsquo;) = -\\left[H(\\theta \\odot m\u0026rsquo;; D)\\frac{\\partial L}{\\partial \\theta}\\right] \\odot \\theta$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Synflow\u003c/strong\u003e  uses $S_{\\text{Synflow}}(m\u0026rsquo;) = \\left|\\theta\\right| \\odot \\left|\\frac{\\partial L}{\\partial \\theta}\\right|$ as a data-agnostic measure, emphasizing connections\u0026rsquo; overall contribution to the network\u0026rsquo;s output irrespective of the dataset.\u003c/p\u003e\n\u003cp\u003eEach method\u0026rsquo;s saliency score guides the pruning process by ranking the connections based on their calculated importance to only keep the top-ranked connections - the most salient ones. Therefore, the overall idea is to start with a complex network, score each connection by importance, and keep only the most important connections. This results in a simpler network that is cheaper to train and run but still capable of learning effectively from the data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNeural Tangent Kernel (NTK):\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn recent studies, there has been significant exploration into optimizing neural networks on a global scale. One notable area of focus involves leveraging the neural tangent kernel (NTK) to gain deeper insights into how gradient descent functions within extensive deep neural networks. The NTK spectrum provides valuable information about convergence patterns. Remarkably, researchers have observed that the NTK remains consistent throughout training in sufficiently large DNNs. This suggests that the NTK spectrum could serve as a comprehensive measure for understanding training dynamics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNeural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP):\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eConsequently, a novel pruning approach has emerged: selectively removing connections that exert minimal influence on the NTK spectrum.\u003c/p\u003e\n\u003cp\u003eIn order to implement this conceptual pruning methods, there are a few considerations:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Metric Selection:\u003c/strong\u003e  Due to the complexity and time required to calculate the full range of eigenvalues (the eigenspectrum) of the Neural Tangent Kernel, the nuclear norm—essentially the sum of these eigenvalues—is used instead as a scalar to summarize the characteristics of the eigenspectrum.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Choosing the Right NTK Matrix:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe can distinguish between wo types of NTK matrices:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFixed-Weight NTK: Related to the network\u0026rsquo;s initial setup.\u003c/li\u003e\n\u003cli\u003eAnalytic NTK: A theoretical model assuming a network of infinite size\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, since calculating the Analytic NTK is highly resource-intensive, the researchers use a practical workaround. They approximate the Analytic NTK by averaging multiple Fixed-Weight NTKs from various initial setups, balancing computational efficiency with accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Computational Efficiency:\u003c/strong\u003e To manage computation costs, there is a technique known as the \u0026ldquo;new-input-new-weight\u0026rdquo; (NINW) method. This approach involves changing the network\u0026rsquo;s weights for each new set of input data. By doing this, they can efficiently evaluate the properties of the Neural Tangent Kernel (NTK) across different scenarios without significantly adding to the computational load.\u003c/p\u003e\n\u003cp\u003eBased on these considerations, Wang and colleagues have developed an innovative approach called \u003cstrong\u003eNeural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eNTK-SAP leverages the NTK spectrum for efficient foresight pruning by using multi-sampling to predict pruning outcomes and ensure accuracy. It also incorporates the Novel Iterative Network Weighting (NINW) technique to reduce computation costs. This method streamlines neural networks by preemptively removing less impactful parts, optimizing both the pruning process and the network\u0026rsquo;s performance with minimal resource expenditure.\u003c/p\u003e\n\u003cp\u003eNTK-SAP follows the following implementation:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Adrien_Elia/algo.png\"\n  alt=\"algorithm\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCalculation of NTK-SAP Saliency Score:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Finite Approximation Approach\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe NTK-SAP method introduces a finite approximation expression to calculate a saliency score S-NTK-SA, which leverages the pruning dataset to approximate the entire training set. This foresight pruning approach identifies and prunes weights with the lowest saliency scores.\u003c/p\u003e\n\u003cp\u003eSaliency score based on a fixed-weight Neural Tangent Kernel:\u003c/p\u003e\n\u003cp\u003e$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\mathbb{E}_{\\Delta\\theta \\sim \\mathcal{N}(0, \\epsilon I)} \\left[ \\left| f(\\mathbf{X}_D; \\theta_0 \\odot m) - f(\\mathbf{X}_D; (\\theta_0 + \\Delta\\theta) \\odot m) \\right|_2^2 \\right] \\right|$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Multi-Sampling Approach:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhile a single fixed-weight-NTK provides an approximation of the analytic NTK, averaging over multiple fixed-weight-NTKs offers a closer approximation to the expected behavior of the analytic NTK. This method entails sampling several independent weight configurations and averaging their fixed-weight-NTKs to better understand the parameter space and the anticipated performance of pruned networks.\u003c/p\u003e\n\u003cp\u003eA stabilized version of the saliency score, S-NTK-SAP(mj) is introduced and incorporates the average of fixed-weight-NTKs computed across multiple random weight configurations, to assess the impact of pruning. Unlike most existing foresight pruning scores, which are dependent on specific weight configurations, this proposed saliency score is weight-agnostic; it primarily reflects the structure of the mask applied for pruning rather than the weights themselves. This distinction highlights the score\u0026rsquo;s focus on the inherent characteristics of the pruning method over the variability of weight initializations.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. New-input-new-weight (NINW) trick:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo reconcile the theoretical aspirations with practical viability, NTK-SAP leverage the \u0026rsquo;new-input-new-weight\u0026rsquo; (NINW) trick. This technique estimates the expected behavior of pruned networks by utilizing a new set of weights for each mini-batch of input data. This approach ensures that the pruning algorithm remains computationally feasible, allowing for the real-world application without prohibitive resource demands.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Random Input Trick:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNTK-SAP relies on another trick that consists in replacing the pruning set with random inputs. This allows to approximate the network\u0026rsquo;s behavior without depending on real data, thus highlighting NTK-SAP\u0026rsquo;s ability to adapt to any dataset without requiring specific adjustments or optimization.\u003c/p\u003e\n\u003cp\u003e$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\left[ \\left| f\\left(Z_i; \\theta_{0,i} \\odot m\\right) - f\\left(Z_i; \\left(\\theta_{0,i} + \\Delta\\theta_i\\right) \\odot m\\right) \\right|_2^2 \\right] \\right|$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExperimental validation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eExperiments were performed on CIFAR-10, CIFAR-100, and Tiny-ImageNet data sets to validate NTK-SAP\u0026rsquo;s superiority across various sparsity levels. Particularly noteworthy is its robust performance at extreme sparsity ratios, where traditional methods falter. These results underscore the efficacy of our multi-sampling strategy and the practical utility of the NINW trick.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Adrien_Elia/performance_curves.png\"\n  alt=\"performance_curves\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eExtending the analysis to the more challenging ImageNet dataset, NTK-SAP consistently outperforms baseline pruning methods, including SNIP and GraSP, especially at high sparsity levels. This success highlights NTK-SAP\u0026rsquo;s scalability and its potential to facilitate efficient neural network training on large-scale datasets.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Adrien_Elia/performance_table.png\"\n  alt=\"performance_table\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReproductive experiments:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo ensure reproducibility, begin by installing the required packages:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install -r requirements.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNext, to run NTK-SAP with the default dataset and parameters using the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe default parameters are as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e--dataset\u003c/code\u003e: Mnist\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--model-class\u003c/code\u003e: default\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--model\u003c/code\u003e:  fc\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--pruner\u003c/code\u003e: rand\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--prune-batch-size\u003c/code\u003e: 256\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--compression\u003c/code\u003e: 0.0\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--prune-train-mode\u003c/code\u003e: False\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--prune-epochs\u003c/code\u003e: 1\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--ntksap_R\u003c/code\u003e:  1\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--ntk_epsilon\u003c/code\u003e: 0.01\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor experimenting with different parameters, proceed with the desired adjustments.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Experiment NTK-SAP with Cifar100 dataset, a 0.01 perturbation hyper-parameter\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py --dataset cifar100 --ntksap_epsilon 0.01\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607083\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.00\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.96\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607083\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.00\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.96\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607083\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.00\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.96\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.337817\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.421804\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e17.91\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e45.41\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e2. Experiment NTK-SAP with Cifar100 dataset and a 0.02 perturbation hyper-parameter\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py --dataset cifar100 --ntksap_epsilon  0.02\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607163\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.02\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.72\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607163\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.02\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.72\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607163\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.02\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.72\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.341863\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.460254\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e17.74\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e43.78\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e3. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 3\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py --dataset cifar100 --prune-epochs \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606948\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0.96\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e5.02\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606948\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0.96\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e5.02\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606948\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0.96\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e5.02\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.337061\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.448972\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e18.09\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e43.97\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e4. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 7\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606786\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.01\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.95\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606786\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.01\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.95\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606786\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.01\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.95\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.335409\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.397401\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e18.93\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e44.89\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eAnalysis from experiments:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Dataset Adaptability:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe study demonstrated NTK-SAP as being data-free. This quality allows pruned networks developed via these methods to be seamlessly adapted to various datasets without requiring additional data, highlighting their versatility and efficiency.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Robustness across hyper-parameter variations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe robustness of NTK-SAPP is evident through its consistent performance across varying perturbation hyper-parameters (ϵ) in experiments conducted on the Cifar100 dataset. When the perturbation hyper-parameter is set to 0.01, the model exhibits stable behavior throughout training and pruning phases, yielding a final top-1 accuracy of 17.91% and a top-5 accuracy of 45.41%. Similarly, when the perturbation hyper-parameter is increased to 0.02, the model maintains its stability, with minimal fluctuations observed in performance metrics compared to the unperturbed model. Both pre-prune and post-prune stages demonstrate resilience to perturbations, showcasing nearly identical results to the unperturbed model. This consistency across different perturbation levels underscores the robustness of NTK-SAPP, making it a reliable choice for tasks where stability under varying conditions is crucial.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Fewer iterations for small datasets:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAn exploration into how the number of iterations (T) affects performance across datasets reveals that for smaller datasets, reducing T slightly impacts outcomes, suggesting that computational efficiency can be achieved without significantly compromising results.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConclusion:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn conclusion, NTK-SAP stands as a pivotal advancement in the realm of neural network pruning, showcasing its efficacy across diverse datasets and network architectures. By pruning at initialization, it eliminates the necessity for post-training methods and mask training. Moreover, by leveraging NTK theory, it addresses the oversight of training dynamics post-pruning, enabling iterative pruning without data dependency. NTK-SAP effectively bridges the theoretical underpinnings of optimization with practical neural network training, thus pushing the boundaries of frugal neural networks.\u003c/p\u003e\n\u003cp\u003eWhile NTK-SAP represents a significant leap forward, it also unveils several avenues for future exploration. Subsequent research could delve into alternative spectral measures or extend the methodology to other forms of network optimization.\u003c/p\u003e\n\u003cp\u003eIn essence, NTK-SAP not only signifies a crucial stride towards more efficient and theoretically grounded neural network pruning but also sets the stage for future innovations in enhancing network frugality.\n\u003cbr\u003e\u003cbr\u003e\u003cbr\u003e\nBy Elia Lejzerowicz and Adrien Oleksiak.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n",
      "url": "http://localhost:1313/posts/ntk-sap/",
      "date_published": "7026-07-09T27:77:00+01:00",
      "date_modified": "7026-07-09T27:77:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "de45490fdb0386b037083151783da0d850a9940c",
      "title": "Do Perceptually Aligned Gradients imply Robustness?",
      "summary": "",
      "content_text": " Robustness and Perceptually Aligned Gradients : does the converse stand ? Author: Yohann Zerbib Table of Contents Introduction Adversarial Attacks Perceptually Aligned Gradients Experiment To go further Conclusion References This is a blog post about the paper Do Perceptually Aligned Gradients Imply Robustness?, published by R. Ganz et al. in 2023 and available here.\nIntroduction In the context of image recognition in Machine Learning, one could quickly realize that building robust models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against adversarials attacks, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily interpretable by humans, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\nAdversarial attacks But before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\nAdversarial attacks refer to a class of techniques in machine learning where intentionally crafted input data is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be imperceptible to humans. They are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\nConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a stop sign as speed limit sign.\n(Eykholt et al. [1])\nNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen. Several points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the loss is steep. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a small change of the input can cause abrupt shifts in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\nThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\na model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$ the input to pertub : $x \\in \\mathcal{X}$ a potential target label : $t \\in \\mathcal{Y}$ a small perturbation : $\\eta$ Then, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\nNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\nFast Gradient Sign Method (FGSM) : This method can be targeted or untargeted. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]: One compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$. The perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\nBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\n$L_{2 }$ norm : This norm captures the global quantity of changes. It is the euclidean distance.\n$L_{\\infty }$ : This norm captures the maximum change in the vector.\nSo, we have several ways to have a level of control over the changed features.\nNow that the first intuition for attack is understood, one should take a rapid look at PGD (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\nThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the opposite direction, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target. After taking a step, the perturbation is projected back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range. This process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\nHowever, our role here is not to learn how to create the best attacks, but more to learn how to defend them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks. Then, it all comes down to this optimization problem :\n$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\nA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\nThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the best trade-off on a model.\nPerceptually Aligned gradients Finally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have perceptually aligned gradients. Here, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are perceptually relevant. In other words, the gradients make sense from a human perspective.\nHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a ghost information.\nNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\nThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\nThen, it is shown that models with aligneds gradients can be considered as robust.\nFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\n1. Algorithm of the Model\nTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\nthe classical cross-entropy loss from the usual categorization problem framework,\nan auxiliary loss on the input-gradients, differentiable.\nThen, our global loss function would look like this :\n$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\nIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\nThis does not use robust model of any sort, on the hypothesis that we have ground-true PAG in the input. This is a strong hypothesis, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\nAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\n2. Creation of Perceptually Aligned Gradients\nAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are approximated. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\nWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\nTo implement this heuristic, three setups are provided.\n$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\n$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\n$\\textbf{Nearest Neighbor (NN):}$ For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define $r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l} \\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x \\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\nwhere $ D_{y_{t}}$ is the set of sample images with class $y_t$.\nNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used Denoising Diffusion Probabilistic Models (DDPMs), to generate approximations of PAG.\nLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\n$(p_t({x_{t})})_{t=1}^{T}$.\nAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\n\\begin{equation} \\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t), \\end{equation}\nwhich results in\n\\begin{equation} \\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t). \\end{equation}\nThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\nTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\nAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\nExperiment Now, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains three mods (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\nThe code is available at this link.\nTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\nAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\nLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600. How can this be explained ? One should observe the decision boundaries.\nThis is what is obtained for the regular neural network with cross-entropy Loss.\nHere is the result obtained for the particular neural network with the new loss.\nWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really stick to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\nHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a much greater margin of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\nAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\nTo go further What\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\nPAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\nBut, the question is not yet answered : Do Perceptually Aligned Gradients imply Robustness?\nAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\nAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the ghost features of the target class are visible (even if it not always comprehensible), the more the model is robust.\nSo, it seems that yes, models with PAG would be more robust.\nConclusion To draw a conclusion, this paper has empirically shown that PAG lead to more robustness in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it ouperforms Adversarially Training and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\nReferences EYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\nGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\nGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\n",
      "content_html": "\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eRobustness and Perceptually Aligned Gradients : does the converse stand ?\u003c/h1\u003e\n\u003ch3 style=\"font-size: 24px;\"\u003eAuthor: Yohann Zerbib\u003c/h3\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eAdversarial Attacks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003ePerceptually Aligned Gradients\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eTo go further\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Do Perceptually Aligned Gradients Imply Robustness?, published by R. Ganz et al. in 2023 and available \u003ca href=\"https://openreview.net/pdf?id=W6topEXC2-v\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the context of image recognition in Machine Learning, one could quickly realize that building \u003cem\u003erobust\u003c/em\u003e models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against \u003cem\u003e\u003cstrong\u003eadversarials attacks\u003c/strong\u003e\u003c/em\u003e, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily \u003cstrong\u003einterpretable by humans\u003c/strong\u003e, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eAdversarial attacks\u003c/h2\u003e\n\u003cp\u003eBut before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\u003c/p\u003e\n\u003cp\u003eAdversarial attacks refer to a class of techniques in machine learning where \u003cstrong\u003eintentionally crafted input data\u003c/strong\u003e is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be \u003cstrong\u003eimperceptible to humans\u003c/strong\u003e.\nThey are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\u003c/p\u003e\n\u003cp\u003eConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a \u003cem\u003e\u003cstrong\u003estop sign as speed limit sign\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/stop.png\"\n  alt=\"stop\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e(Eykholt et al. [1])\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen.\nSeveral points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the \u003cstrong\u003eloss is steep\u003c/strong\u003e. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a \u003cstrong\u003esmall change\u003c/strong\u003e of the input can cause \u003cstrong\u003eabrupt shifts\u003c/strong\u003e in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\u003c/p\u003e\n\u003cp\u003eThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ethe input to pertub : $x \\in \\mathcal{X}$\u003c/li\u003e\n\u003cli\u003ea potential target label : $t \\in  \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ea small perturbation : $\\eta$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\u003c/p\u003e\n\u003cp\u003eNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\u003c/p\u003e\n\u003ch3 id=\"fast-gradient-sign-method-fgsm-\"\u003eFast Gradient Sign Method (FGSM) :\u003c/h3\u003e\n\u003cp\u003eThis method can be \u003cem\u003e\u003cstrong\u003etargeted\u003c/strong\u003e\u003c/em\u003e or \u003cem\u003e\u003cstrong\u003euntargeted\u003c/strong\u003e\u003c/em\u003e. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]:\nOne compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$.\nThe perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\u003c/p\u003e\n\u003cp\u003eBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{2 }$ norm : This norm captures the \u003cstrong\u003eglobal quantity of changes\u003c/strong\u003e. It is the euclidean distance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{\\infty }$ : This norm captures the \u003cstrong\u003emaximum change\u003c/strong\u003e in the vector.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, we have several ways to have a level of control over the changed features.\u003c/p\u003e\n\u003cp\u003eNow that the first intuition for attack is understood, one should take a rapid look at \u003cstrong\u003ePGD\u003c/strong\u003e (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\u003c/p\u003e\n\u003cp\u003eThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the \u003cstrong\u003eopposite direction\u003c/strong\u003e, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target.\nAfter taking a step, the perturbation is \u003cem\u003eprojected\u003c/em\u003e back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range.\nThis process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\u003c/p\u003e\n\u003cp\u003eHowever, our role here is not to learn how to create the best attacks, but more to learn how to \u003cem\u003e\u003cstrong\u003edefend\u003c/strong\u003e\u003c/em\u003e them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks.\nThen, it all comes down to this optimization problem :\u003c/p\u003e\n\u003cp\u003e$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\u003c/p\u003e\n\u003cp\u003eA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\u003c/p\u003e\n\u003cp\u003eThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the \u003cstrong\u003ebest trade-off\u003c/strong\u003e on a model.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003ePerceptually Aligned gradients\u003c/h2\u003e\n\u003cp\u003eFinally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have \u003cem\u003eperceptually aligned gradients\u003c/em\u003e.\nHere, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are \u003cstrong\u003eperceptually relevant\u003c/strong\u003e. In other words, the gradients \u003cem\u003emake sense\u003c/em\u003e from a human perspective.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/pagdemo.png\"\n  alt=\"demopag\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a \u003cem\u003eghost\u003c/em\u003e information.\u003c/p\u003e\n\u003cp\u003eNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\u003c/p\u003e\n\u003cp\u003eThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\u003c/p\u003e\n\u003cp\u003eThen, it is shown that models with aligneds gradients can be considered as robust.\u003c/p\u003e\n\u003cp\u003eFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Algorithm of the Model\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ethe classical cross-entropy loss from the usual categorization problem framework,\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ean auxiliary loss on the input-gradients, differentiable.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, our global loss function would look like this :\u003c/p\u003e\n\u003cp\u003e$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\u003c/p\u003e\n\u003cp\u003eIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\u003c/p\u003e\n\u003cp\u003eThis does not use robust model of any sort, on the hypothesis that we have \u003cstrong\u003eground-true PAG\u003c/strong\u003e in the input. This is a \u003cstrong\u003estrong hypothesis\u003c/strong\u003e, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\u003c/p\u003e\n\u003cp\u003eAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Creation of Perceptually Aligned Gradients\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are \u003cstrong\u003eapproximated\u003c/strong\u003e. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\u003c/p\u003e\n\u003cp\u003eWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/target.png\"\n  alt=\"target\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eTo implement this heuristic, three setups are provided.\u003c/p\u003e\n\u003cp\u003e$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Nearest Neighbor (NN):}$  For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define\n$r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l}\n\\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x\n\\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\u003c/p\u003e\n\u003cp\u003ewhere $ D_{y_{t}}$\nis the set of sample images with class $y_t$.\u003c/p\u003e\n\u003cp\u003eNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used \u003cstrong\u003eDenoising Diffusion Probabilistic Models\u003c/strong\u003e (DDPMs), to generate approximations of PAG.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\u003c/p\u003e\n\u003cp\u003e$(p_t({x_{t})})_{t=1}^{T}$.\u003c/p\u003e\n\u003cp\u003eAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t),\n\\end{equation}\u003c/p\u003e\n\u003cp\u003ewhich results in\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t).\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\u003c/p\u003e\n\u003cp\u003eTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\u003c/p\u003e\n\u003cp\u003eAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eExperiment\u003c/h2\u003e\n\u003cp\u003eNow, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains \u003cstrong\u003ethree mods\u003c/strong\u003e (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\u003c/p\u003e\n\u003cp\u003eThe code is available at this \u003ca href=\"https://github.com/YohannZe/responsible-ai-datascience-ipParis.github.io.git\"\u003elink\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\u003c/p\u003e\n\u003cp\u003eAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600.\nHow can this be explained ? One should observe the decision boundaries.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/vanilla_l2_toy.png\"\n  alt=\"vanillal2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis is what is obtained for the regular neural network with cross-entropy Loss.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/nn_l2_toy.png\"\n  alt=\"nnl2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere is the result obtained for the particular neural network with the new loss.\u003c/p\u003e\n\u003cp\u003eWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really \u003cem\u003e\u003cstrong\u003estick\u003c/strong\u003e\u003c/em\u003e to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\u003c/p\u003e\n\u003cp\u003eHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a \u003cstrong\u003emuch greater margin\u003c/strong\u003e of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\u003c/p\u003e\n\u003cp\u003eAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003eTo go further\u003c/h2\u003e\n\u003cp\u003eWhat\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\u003c/p\u003e\n\u003cp\u003ePAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\u003c/p\u003e\n\u003cp\u003eBut, the question is not yet answered : \u003cem\u003e\u003cstrong\u003eDo Perceptually Aligned Gradients imply Robustness?\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/regu.png\"\n  alt=\"regu\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the \u003cem\u003eghost\u003c/em\u003e features of the target class are visible (even if it not always comprehensible), the more the model is robust.\u003c/p\u003e\n\u003cp\u003eSo, it seems that yes, models with \u003cstrong\u003ePAG would be more robust\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eTo draw a conclusion, this paper has empirically shown that \u003cstrong\u003ePAG lead to more robustness\u003c/strong\u003e in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it \u003cstrong\u003eouperforms Adversarially Training\u003c/strong\u003e and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eReferences\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eEYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
      "url": "http://localhost:1313/posts/robustness-and-pag-the-converse/",
      "date_published": "7026-07-09T26:77:00+01:00",
      "date_modified": "7026-07-09T26:77:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "546ff9c63aab59b8a7ddb24d9f5939b60b89ee70",
      "title": "To update or not to update? Neurons at equilibrium in deep models",
      "summary": "",
      "content_text": "To update or not to update? Neurons at equilibrium in deep models Author: Alexis WINTER Augustin CREUSILLET Table of content Introduction NEq Experiments Results Reproducibility Conclusion References This is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).\nIntroduction Background Recent advances in deep learning have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for computational power has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both affordability and environmental sustainability, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.\nThis paper tries to focus on the overall behavior of neurons, leveraging the notion of neuronal equilibrium (NEq). When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.\nRelated works Pruning strategies Pruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to improve efficiency and reduce computational complexity. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.\nDespite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically do not alleviate the computational complexity associated with training neural networks. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.\nLottery ticket hypothesis The lottery ticket hypothesis is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or \u0026ldquo;winning tickets,\u0026rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even surpass the performance of the original dense network.\nThe hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.\nThe significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these winning tickets and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.\nNEq Neuronal equilibrium The concept of neuronal equilibrium aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.\nTo assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\\Xi_{val}$ as follows:\nThe neuron $i$-th reaches the equilibrium when $(\\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium we need to detect when :\n$$\\lim_{t\\rightarrow \\infty} \\phi_{i}^t = k,$$\nSince it is not trivial to assess this statment we prefer to work with variations of $(\\phi_{i})_t$ that can be defined as :\n\\begin{equation} v_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1}, \\end{equation}\nWith $\\mu_{eq}$ the momentum coefficient.\nThis only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\\Delta \\phi_i^t \\rightarrow 0$$\nSince we want to track the evolution of $\\Delta \\phi_i^t$ over time we introduce the velocity of the variations: $$ v_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1}, $$\nWith $\\mu_{eq}$ the momentum coefficient.\nRewrited:\nWe need to have $$\\mu_{eq} \\in [0; 0.5]$$ to prevent the velocity from exploding.\nFinally we can set the condition for the neuron to be at the equilibrium as: \\begin{equation} \\left| v_{\\Delta \\phi}^t \\right | \u0026lt; \\varepsilon,~~~~~\\varepsilon \\geq 0. \\end{equation}\nIt is important to know that this relation might not hold for all $t$ since there could be an instant $t\u0026rsquo; \u0026lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.\nTraining scheme The training scheme can be presented according to this scheme:\nAt the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:\nAn epoch of training is made for all trainable neurons on the training set. The training either stops due to the end of training criterion being met or continues to the next step. The velocity of the similarities is evaluated for every neuron. The set of trainable neurons is determined for the next step according to the equilibrium criterion. Comparing with regular training, we can see two more hyper-parameters:\n$\\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities. $\\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities. Experiments SGD vs Adam The authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.\nIn the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.\nAs training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that Adam brings the network towards this equilibrium faster than SGD, but also note that in this specific task, SGD achieves a slightly higher final accuracy than Adam. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD\u0026rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.\nThe experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.\nDistribution of $\\phi$ \u0026amp; choice of $µ_{eq}$ The paper also discusses the distribution of $\\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.\nThe parameter $\\phi$ measures the cosine similarity between the outputs of a particular neuron at two consecutive training epochs, over the validation set. It is used to determine if a neuron\u0026rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\\phi$ equals 1, it indicates that the neuron\u0026rsquo;s output is stable across the epochs, signifying it has reached equilibrium.\nThe paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\\phi$ values across epochs, and $v_{∆\\phi}$, which measures the velocity of this change considering a momentum coefficient $µ_{eq}$. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.\nBy examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper\u0026rsquo;s Figure 5, which shows the distribution of $\\phi$, $∆\\phi$, and $v_{∆\\phi}$ for a ResNet-32 model trained on CIFAR-10.\nIn summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally \u0026ldquo;unfreeze\u0026rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.\nImpact of the validation set size and ε The authors found that the size of the validation set does not significantly impact the performance of the model. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.\nWhen examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, striking a balance between model performance and computational efficiency.\nResults Reproducibility Using the author\u0026rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from limited computational resources. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.\nExperiment This experiment aims to replicate the section 4.1.1 \u0026ldquo;SGD vs Adam\u0026rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:\npython3 train_classification.py --amp=1 --arch=resnet32-cifar --batch-size=100 --dataset=cifar10 --device=cuda --epochs=250 --eps=0.001 --lr=0.1 --momentum=0.9 --optim=sgd --val-size=0.01 --velocity-mu=0.5 --weight-decay=0.0005 The code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named Weights \u0026amp; Biases (wandb) to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.\nAs expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.\nConclusion From the initial problem of computational resources saving, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, NEq produces a new knowledge that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and might lead to new training strategies.\nOne possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.\nReferences Bragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022. Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017. J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019. ",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eTo update or not to update? Neurons at equilibrium in deep models\n\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Alexis WINTER Augustin CREUSILLET\u003c/h1\u003e\n\u003ch1 id=\"table-of-content\"\u003eTable of content\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eNEq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eExperiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eResults\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eReproducibility\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e\n\u003cp\u003eRecent advances in \u003cstrong\u003edeep learning\u003c/strong\u003e have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for \u003cstrong\u003ecomputational power\u003c/strong\u003e has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both \u003cstrong\u003eaffordability and environmental sustainability\u003c/strong\u003e, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.\u003c/p\u003e\n\u003cp\u003eThis paper tries to focus on the overall behavior of neurons, leveraging the notion of \u003cstrong\u003eneuronal equilibrium (NEq)\u003c/strong\u003e. When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.\u003c/p\u003e\n\u003ch3 id=\"related-works\"\u003eRelated works\u003c/h3\u003e\n\u003ch4 id=\"pruning-strategies\"\u003ePruning strategies\u003c/h4\u003e\n\u003cp\u003ePruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to \u003cstrong\u003eimprove efficiency and reduce computational complexity\u003c/strong\u003e. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.\u003c/p\u003e\n\u003cp\u003eDespite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically \u003cstrong\u003edo not alleviate the computational complexity associated with training neural networks\u003c/strong\u003e. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.\u003c/p\u003e\n\u003ch4 id=\"lottery-ticket-hypothesis\"\u003eLottery ticket hypothesis\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003elottery ticket hypothesis\u003c/strong\u003e is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or \u0026ldquo;winning tickets,\u0026rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even \u003cstrong\u003esurpass the performance of the original dense network\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.\u003c/p\u003e\n\u003cp\u003eThe significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these \u003cstrong\u003ewinning tickets\u003c/strong\u003e and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eNEq\u003c/h2\u003e\n\u003ch3 id=\"neuronal-equilibrium\"\u003eNeuronal equilibrium\u003c/h3\u003e\n\u003cp\u003eThe concept of \u003cstrong\u003eneuronal equilibrium\u003c/strong\u003e aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.\u003c/p\u003e\n\u003cp\u003eTo assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\\Xi_{val}$ as follows:\u003c/p\u003e\n\u003cimg src=\"/images/images_Winter_Creusillet/neq_formula.png\" width=\"300\"/\u003e\n\u003cp\u003eThe neuron $i$-th reaches the equilibrium when $(\\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium  we need to detect when :\u003c/p\u003e\n\u003cp\u003e$$\\lim_{t\\rightarrow \\infty} \\phi_{i}^t = k,$$\u003c/p\u003e\n\u003cp\u003eSince it is not trivial to assess this statment we prefer to work with variations of $(\\phi_{i})_t$ that can be defined as :\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\nv_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1},\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eWith $\\mu_{eq}$ the momentum coefficient.\u003c/p\u003e\n\u003cp\u003eThis only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\\Delta \\phi_i^t \\rightarrow 0$$\u003c/p\u003e\n\u003cp\u003eSince we want to track the evolution of $\\Delta \\phi_i^t$ over time we introduce the \u003cstrong\u003evelocity of the variations\u003c/strong\u003e:\n$$\nv_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1},\n$$\u003c/p\u003e\n\u003cp\u003eWith $\\mu_{eq}$ the momentum coefficient.\u003c/p\u003e\n\u003cp\u003eRewrited:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/momentum_coef.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eWe need to have $$\\mu_{eq} \\in [0; 0.5]$$ to prevent the velocity from exploding.\u003c/p\u003e\n\u003cp\u003eFinally we can set the condition for the neuron to be at the equilibrium as:\n\\begin{equation}\n\\left| v_{\\Delta \\phi}^t \\right | \u0026lt; \\varepsilon,~~~~~\\varepsilon \\geq 0.\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eIt is important to know that this relation might not hold for all $t$ since there could be an instant $t\u0026rsquo; \u0026lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.\u003c/p\u003e\n\u003ch3 id=\"training-scheme\"\u003eTraining scheme\u003c/h3\u003e\n\u003cp\u003eThe training scheme can be presented according to this scheme:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/prunedbackprop-scheme_full-1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAt the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn epoch of training is made for all trainable neurons on the training set.\u003c/li\u003e\n\u003cli\u003eThe training either stops due to the end of training criterion being met or continues to the next step.\u003c/li\u003e\n\u003cli\u003eThe velocity of the similarities is evaluated for every neuron.\u003c/li\u003e\n\u003cli\u003eThe set of trainable neurons is determined for the next step according to the equilibrium criterion.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eComparing with regular training, we can see two more hyper-parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities.\u003c/li\u003e\n\u003cli\u003e$\\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2\"\u003eExperiments\u003c/h2\u003e\n\u003ch3 id=\"sgd-vs-adam\"\u003eSGD vs Adam\u003c/h3\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/sgd_vs_adam.png\"\n  alt=\"adam/sgd\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.\u003c/p\u003e\n\u003cp\u003eIn the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.\u003c/p\u003e\n\u003cp\u003eAs training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that \u003cstrong\u003eAdam brings the network towards this equilibrium faster than SGD\u003c/strong\u003e, but also note that in this specific task, \u003cstrong\u003eSGD achieves a slightly higher final accuracy than Adam\u003c/strong\u003e. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD\u0026rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.\u003c/p\u003e\n\u003cp\u003eThe experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.\u003c/p\u003e\n\u003ch3 id=\"distribution-of-phi--choice-of-µ_eq\"\u003eDistribution of $\\phi$ \u0026amp; choice of $µ_{eq}$\u003c/h3\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/mu-line-1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe paper also discusses the distribution of $\\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.\u003c/p\u003e\n\u003cp\u003eThe parameter $\\phi$ measures the \u003cstrong\u003ecosine similarity between the outputs of a particular neuron at two consecutive training epochs\u003c/strong\u003e, over the validation set. It is used to determine if a neuron\u0026rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\\phi$ equals 1, it indicates that the neuron\u0026rsquo;s output is stable across the epochs, signifying it has reached equilibrium.\u003c/p\u003e\n\u003cp\u003eThe paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\\phi$ values across epochs, and $v_{∆\\phi}$, which measures the velocity of this change considering a \u003cstrong\u003emomentum coefficient $µ_{eq}$\u003c/strong\u003e. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.\u003c/p\u003e\n\u003cp\u003eBy examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper\u0026rsquo;s Figure 5, which shows the distribution of $\\phi$, $∆\\phi$, and $v_{∆\\phi}$ for a ResNet-32 model trained on CIFAR-10.\u003c/p\u003e\n\u003cp\u003eIn summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally \u0026ldquo;unfreeze\u0026rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.\u003c/p\u003e\n\u003ch3 id=\"impact-of-the-validation-set-size-and-ε\"\u003eImpact of the validation set size and ε\u003c/h3\u003e\n\u003cp\u003eThe authors found that the size of the validation set \u003cstrong\u003edoes not significantly impact the performance of the model\u003c/strong\u003e. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.\u003c/p\u003e\n\u003cp\u003eWhen examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, \u003cstrong\u003estriking a balance between model performance and computational efficiency\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eResults\u003c/h2\u003e\n\u003ch2 id=\"section-4\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003eUsing the author\u0026rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from \u003cstrong\u003elimited computational resources\u003c/strong\u003e. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.\u003c/p\u003e\n\u003ch3 id=\"experiment\"\u003eExperiment\u003c/h3\u003e\n\u003cp\u003eThis experiment aims to replicate the section 4.1.1 \u0026ldquo;SGD vs Adam\u0026rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython3 train_classification.py --amp\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e --arch\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eresnet32-cifar --batch-size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e --dataset\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ecifar10 --device\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ecuda --epochs\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e250\u003c/span\u003e --eps\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.001 --lr\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.1 --momentum\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.9 --optim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003esgd --val-size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.01 --velocity-mu\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.5 --weight-decay\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.0005\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named \u003cstrong\u003eWeights \u0026amp; Biases (wandb)\u003c/strong\u003e to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/frozen_sgd_vs_adam1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cimg\n  src=\"/images/images_Winter_Creusillet/accuracy_sgd_vs_adam1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eFrom the initial problem of \u003cstrong\u003ecomputational resources saving\u003c/strong\u003e, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, \u003cstrong\u003eNEq produces a new knowledge\u003c/strong\u003e that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and \u003cstrong\u003emight lead to new training strategies\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eOne possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eReferences\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eBragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022.\u003c/li\u003e\n\u003cli\u003eDmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017.\u003c/li\u003e\n\u003cli\u003eJ. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n",
      "url": "http://localhost:1313/posts/neq/",
      "date_published": "7026-07-09T255:77:00+01:00",
      "date_modified": "7026-07-09T255:77:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "0af752892a7ab6b8e738b7ff6f9468359b3cb382",
      "title": "Optimal Transport Based Adversarial Patch Attacks",
      "summary": "",
      "content_text": " Authors: Mohammed Jawhar Aymane Rahmoune Paper : Optimal Transport Based Adversarial Based Patch To Leverage Large Scale Attack Transferability Table of contents : Introduction Understanding Adversarial Patch Attacks Decision boundary based Feature point based Distribution based Transferability Optimal Transport Experiments Experimental Setup Results and Findings Digital Experiments Hybrid Experiments Physical Experiments Reproducibility Conclusion Introduction Imagine you\u0026rsquo;re showing a picture to a friend, asking them to guess who\u0026rsquo;s in it, then sticking a tiny, almost invisible sticker on that photo. For some reason, this sticker makes your friend completely unable to recognize who\u0026rsquo;s in the picture. This might sound like magic, but something similar can happen with Computer Vision models designed to capture an image content, either through a classification, a segmentation or even a generation task. These AI programs can be vulnerable to such tricks, that we call technically, Adversarial Patch Attacks.\nAs AI becomes increasingly integrated into various aspects of our lives, including critical applications like passport security systems, autonomous vehicles, traffic sign detection, and surgical assistance; the reliability, trustworthiness, and performance of these systems under all conditions became of prime importance. This has led to a growing interest in the area of Robust AI, which focuses on enhancing the safety and security of AI technologies by improving their resilience to adverse conditions and digital threats. Within this domain, the study of Attacks and Defense ways plays a pivotal role.\nWhile these attacks might not seem like a big deal, nor dangerous in this context, the consequences can be severe in critical scenarios - take for example an autonomous vehicle failing to recognize a stop sign, hurting potentially a pedestrian. In this blog we will explore a new approach used for developping such adversarial patch attacks, based on Optimal Transport, as outlined in the paper Optimal Transport Based Adversarial Patch To Leverage Large Scale Attack Transferability. We will try to follow the same structure as in the paper to make the reading easier for you, but with much more simplicity.\nUnderstanding Adversarial Attacks First thing first, let us redefine some previously mentionned concepts, while making them into context.\nAs deep neural networks keep getting better, developers are working hard to make sure they are trustworthy and reliable. This means constantly testing them to see how well they can handle different challenges, quantifying their robustness, and developping some robustification methods. In the context of image classification for instance, one way to do this is by designing adversarial attacks, which consists of a perturbation or noise, sometimes invisible patterns added to the input images in order to confuse the model and make it misclassify them, causing a huge drop in the accuracy.\nAdversarial Patch Attacks are a specific type that consists of altering only a small part(patch) of the input, either physically or digitally by inserting a crafted \u0026ldquo;sticker\u0026rdquo;. These attacks happen to be more threatful as they can be easily applied in real life, they do not require modification of the entire image, and they can fool multiple, vastly different models with the same crafted patch. This last property is called transferability and aims to test these engineered adversarial patches on various target models, beyond the original one used for learning, even if the two models(source and target) have been trained on different data or use different architectures, to evaluate the attack\u0026rsquo;s efficacy, and measure the models robustness.\nDespite the fact that crafting adversarial patch attacks is mainly based around maximizing the classification error through a gradient ascent, we can differenciate between three distinct approaches:\nDecision boundaries based : Which is the most applied approach in previous works and litterature. It focuses on pushing the image\u0026rsquo;s representation in the neural network\u0026rsquo;s decision space, across the decision boundary, making the network perceive it as belonging to a different, probability maximized class.\nTo simplify this approach, imagine a group of fans attempting to sneak into a VIP section at a concert by dressing in a fancy way, like known VIP guests(targeted class). The idea is to blend in so well that they are indistinguishable from actual VIPs to the security guards (the ML model). Despite the simplicity and goodness of this strategy, it has some drawbacks :\nIt is highly dependant on the model on which the attack is based, which makes it not really transferable: The success of this method hinges on the security\u0026rsquo;s lack of detail. If they are controlled by another security gard who is very familiar with the actual VIPs, the disguises will fail.\nThe patch may push the corrupted image representations into unknown regions of the representation space: In their attempt to mimic the VIPs, there\u0026rsquo;s a risk that their disguises might be so overdone that they don\u0026rsquo;t resemble any actual VIPs, pushing them to have a weird unique look. Hence, they end up in a no-man\u0026rsquo;s-land, not fitting in with either the regular attendees or the VIPs.\nFeature point based : Instead of crossing a decision boundary, this strategy aims to modify the input so its representation in the feature space matches the one of a target point belonging to a different class. This is like fine-tuning the attack to match a specific \u0026ldquo;signature\u0026rdquo; that the model associates to a specific point.\nRevisiting our concert analogy, consider the fans now opting to mimic a specific celebrity known to be attending the concert, assuming that matching this one high-profile individual\u0026rsquo;s appearance will guarantee them entry. Although it seems more precise and effective than the first approach, this strategy has a significan drawback :\nIt depends heavily on the targeted point selection, this later may be not representative of all instances in the target class : For instance, if the celebrity is known for a distinctive but uncommon style or if it\u0026rsquo;s unusual for such celebrities to attend such events, their attempt to copy him might not match what the security team usually expects from VIP guests. Distribution based : This new approach implemented in the paper we are analyzing , is based on Optimal Transport theory, and aims to alter the overall feature distribution ofa set of input images belonging to a specific class, to resemble another class\u0026rsquo;s distribution, reducing the gap between them in the feature space. It is more sophisticated than the previous ones as it exploits the fundamental way neural networks process and classify images based on learned distributions.\nThis time, the group studies a wide variety of guests behaviors and appearances to craft a new, ambiguous look that doesn\u0026rsquo;t specifically mimic any single guest type, nor disguise blindly in a \u0026ldquo;VIP\u0026rdquo; style, but instead blends into the overall crowd, avoiding easy detection.\nThe main advantage of this approach is that it allows a better transferability between models, enhancing the performance in the blackbox configuration, as it is independant of the classifier\u0026rsquo;s decision boundary , and the choice of a specific target point. Furthermore it captures the useful characteristics (features) from an input in a more universal way. Why do we need transferability ? You surely noticed that we mentionned the transferability term many times in the last section, showing that is an essential property for designing such attacks, but why do we focus so much to make our patch transferable through many models? Well, it is like discovering a master key for many locks : It enables bad actors to compromise and confuse an AI system using a crafted patch they made without knowing anything about that system(architecture, training,\u0026hellip;).\nThis ability to create a \u0026lsquo;one-size-fits-all\u0026rsquo; adversarial patch allows to challenge many models, making it more difficult to develop defense mechanisms, and fostering the development of more robust AI systems. Unfortunately, this important property, which confronts the real-world variability of target systems, whose specific architectures or training details are often unknown, was not achieved strongly by previously developped Adversarial attacks; it was studied only by some specialized Adversarial Patch Attacks models(GAP, LaVan, PS-GAN) and gave very modest rsults, being evaluated on dated, non state of the art models Other models (TTP, M3D, Inkawhich et al.) conducted some experiments to measure the transferability of ivisible adversarial attacks and gave promizing results, but they didn\u0026rsquo;t focus i their work on patch attacks transferability.\nDiving into Optimal Transport theory The method introduced in this paper represents a remarkable success, as it bridges the gap between transferability studies of invisible adversarial examples and adversarial patch attacks, and provides a trade-off between an efficient non complex patch designing approach, and an exceptional transferability among many advanced state-of-the-art models. The key reason for this success lies in the inherent capabilities of optimal transport to measure the distance between two distributions. Particularly, the loss optimized in this method is relevant, as it can be used when the distributions do not overlap, and the theory behind it is intuitive. It is based mainly on the Wasserstain distance defined as :\n$$W_{p}^p(\\mu,\\nu) = \\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} ||x - y||^p d\\pi(x, y)$$\n\u003c!DOCTYPE html\u003e MathJax Visualization Example or its more computationnaly efficient Sliced version, which compares the two distributions by computing the expected Wasserstein distance between their one-dimensional linear projections : $$SW_{p}^p(\\mu,\\nu) = \\int_{S^{d-1}} W_{p}^p(\\theta_{\\#}^{*}\\mu, \\theta_{\\#}^{*}\\nu) d\\sigma(\\theta)$$\nWhere $\\mu$ and $\\nu$ are two propbability distributions on $\\mathbb{R}^d$, $||.||$ the euclidean norm, $\\pi$ is a transport plan between $\\mu$ and $\\nu$, and $ \\theta_{\\#}^{*} \\mu $ and $ \\theta_{\\#}^{*} \\nu $ the push-forward by $\\theta^{*}(x)=\u003c\\theta, x\u003e$ of $\\mu$ and $\\nu$ respectively. This image is taken and adapted from the Sliced-Wasserstein distance for large-scale machine learning: theory, methodology and extensions paper. To delve more into the mathematical details, let us explore how Optimal Transport, specifically the Wasserstein distance, is employed to craft effective adversarial examples: In the context of image classification, we consider the standard notation where a set of image-label pairs $(x_i, y_i)$ is drawn from a joint distribution of random variables $X$ and $Y$. The images $X$ are typically multi-dimensional arrays representing the height, width, and color channels of an image (e.g., a colored $256 \\times 256$ pixel image would have $h = 256$, $w = 256$, and $c = 3$). Meanwhile, $Y$ is a set of discrete labels that classify these images (e.g., \u0026lsquo;cat\u0026rsquo;, \u0026lsquo;dog\u0026rsquo;, etc.). Within a given encoder-decoder neural network $F$, designed to predict these labels, the encoder function $f$ compresses the raw image data $X$ throughout each pooling layer into a feature space $S^{(l)}$, capturing essential patterns.\nThe Wasserstein distance $W_p$, calculated between the distributions of these feature spaces, reflects how much \u0026ldquo;effort\u0026rdquo; it would take to transform the distribution of features from one class into another. In the case of the proposed method, crafting the patch consits of minimizing the transformation cost (distance)of the features distribution from a corrupted \u0026ldquo;true\u0026rdquo; class into a \u0026ldquo;target\u0026rdquo; adversarial class across multiple layers. This can be formulated as follows:\n\u003c!DOCTYPE html\u003e MathJax Visualization Example $$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_X \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{X_{\\delta}}^{(l)}, \\nu_y^{(l)}) \\right]$$\nWhere $OT$ is the optimal transport distance (Wasserstein or Sliced Wasserstein), $\\mu_{X_{\\delta}}^{(l)}$ is the feature distribution of images with the patch and $\\nu_y^{(l)}$ is the target feature distribution for the incorrect class.\nThis can be further enhanced by adding a regularization term to ensure that the patches are effective under various conditions, and can be physically realisable. The problem becomes as follows :\n$$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_{X, t\\sim \\tau, e\\sim E} \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{A(\\delta, X, e, t)}^{(l)}, \\nu_y^{(l)}) + TV(\\delta)\\right]$$ where TV is the total variation loss discouraging high-frequency patterns.\nExperiments Experimental setup To confirm the theoretical results and assumptions, several experiments were conducted under different conditions and settings. For the sake of simplicity, we will not delve into the exhaustive details of the experimental setup, procedures, and results. In summary:\nThe experiments aimed to evaluate the impact and transferability of the proposed adversarial patch - referred to as $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ - across a range of models.\n$(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ performance was benchmarked against other adversarial patch attack (APA) methods such as GAP, LaVAN, TNT, TTP, M3D, and others.\nThe source and target models chosen for this analysis were regrouped into six categories based on their architecture: CNNs-V1, CNNs-V2, ENet, CNext(ConvNext), DeiT, and Swin.\nTested patches were randomly placed to the side of images, in order to avoid occluding the object of interest and replicate more closely the real world conditions.\nTargeted success rate (tSuc) metric was used for evaluating transferability. It consists of the percentage of instances where the network, when presented with an image containing the adversarial patch, incorrectly classifies the image as the attacker\u0026rsquo;s intended target class, out of the total number of attempts.\nResults and Findings : The experiments are structured into three main categories:\nDigital experiments : Simple configuration : In this configuration, the patches efficacy was tested in a purely digital environment, using images from the ImageNet-1K dataset, which was used also for training. Patches were first designed to attack one of the source models, then tested on other target models to measure the attacking transferability. The table below summarizes for each APA method, the best transferring attack performance achieved :\nAs expected through the novelty of $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, it shows the highest transferability capacity(mean, min and max) and outperforms all the other methods. Additionaly, we can make the following observations:\nNetworks trained with older training recipes (CNNs-v1) seem more vulnerable to attacks, unlike tansformers and models trained with new training recipes (scheduler, augmenting training data like RandAug and Mixup, \u0026hellip;) which appear to be more robust.\nFor all APA methods, patches learned using Swin or CNext are more universal as they can transfer uniformly to multiple models.\nIn general, baseline methods tend to overfit and fail to generate patches that effectively transfer to complex architectures like CNext and Swin models, even if these patches are developed using the same category of models.\nMethods based on feature space optimization, including L2 and the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, demonstrate improved transferability and are less likely to overfit.\nRobustified configuration : In the second configuration of the digital experiments, the same procedures were reapplied. However this time, the methods learn on Swin, and transfer to a robustified version, by Local Gradients Smoothing (LGS) - a defense mechanism smoothing salient regions in images before passing them to the network - , of the six model categories.\nSimilarly, $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ outperforms significantly all other methods as we can see in the following table :\nHybrid experiments: In order to simulate real-world applications more closely, the hybrid experiments conducted within this section involved printing adversarial patches trained with Swin, placing them in physical environments, capturing the images, and then digitally analyzing the results, for simple, and robustified models.\nThe table below shows the criticality of the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ giving very large tSuc in comparison with the other methods, for all settings:\nPhysical experiments: In this last experiments category, we get closer to the real world situations, by recording a video of some ImageNet-1K objects (banana, cup , keyboard) while moving a designed patch in the set. This aims to quantify the severity of each attack, for realistic scenarios (as the example provided above about the autonomous vehicule not detecting the stop sign while driving due to an adversarial patch designed without knowing the AI system at all).\nAll APA methods failed to transfer properly on all architectures except for L2 with a modest tSuc(9.3%) and $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ that gave much better results (23.4% and 29.3%)\nReproducibility In this section, we wanted to reproduce some of the experiments conducted in the paper to validate the results and the findings. However, by exploring the code provided with the paper, and analyzing the python files, we found that it is not well documented, and the structure is not very clear, which makes it difficult to understand and reproduce the complex experiments involving transferability evaluation. Furthermore, given that the paper is based on the ImageNet dataset, which is very large and requires a lot of computational resources, we were not able to run the experiments on our local machines, as we do not have access to a powerful GPU cluster. Consequently, we opted for the CIFAR-10 dataset, which is smaller and more manageable. Despite this adjustment, we still faced some issues Specifically, the model is built from scratch without an available pre-trained, and there are missing components, notably the function required to extract feature vectors from each layer of the target models. To address these challenges and make the reproduction process easier, we decided to develop the missing feature extraction function as an enhancement, and save the obtained results into files(in the same way it was done in the code), to be able to apply the optimal transport method and craft the adversarial patches later as perspectives\nHere are the code that we developed :\nimport torch import torchvision.models as models import torchvision.transforms as transforms from torch.utils.data import DataLoader import torchvision.datasets as datasets def get_loader(dataset: str, split: str, batch_size: int) -\u0026gt; torch.utils.data.DataLoader: \u0026#34;\u0026#34;\u0026#34;Return a DataLoader object for a given dataset and split.\u0026#34;\u0026#34;\u0026#34; return torch.utils.data.DataLoader(get_dataset(dataset, split), batch_size=batch_size, shuffle=True) def extract_features(data_loader, list_models): \u0026#34;\u0026#34;\u0026#34; Extracts features from each layer of the pre-trained models provided in the list_models by applying average pooling, and saves the extracted features into files. \u0026#34;\u0026#34;\u0026#34; for model_name in list_models: model = models.__dict__[model_name](pretrained=True) model.eval() for layer_name, layer in model.named_children(): if isinstance(layer, torch.nn.Sequential): layer_features = [] for i, (input, target) in enumerate(data_loader): with torch.no_grad(): output = layer(input) output = torch.nn.functional.adaptive_avg_pool2d(output, (1, 1)) output = output.view(output.size(0), -1) layer_features.append(output) layer_features = torch.cat(layer_features) torch.save(layer_features, f\u0026#34;./data/CIFAR/all_images_feature/{layer_name}/{model_name}.pt\u0026#34;) # Apply the function extract_features to some targeted models list_models = [\u0026#34;resnet18\u0026#34;, \u0026#34;vgg19\u0026#34;, \u0026#34;convnext_tiny\u0026#34;, \u0026#34;swin_t\u0026#34;] data_loader = get_loader(\u0026#34;CIFAR10\u0026#34;, \u0026#34;train\u0026#34;, batch_size=64) extract_features(data_loader, list_models) Conclusion In conclusion, our exploration of the paper OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY, revealed an innovative and promizing technique that uses Optimal Transport to make adversarial patches more effectively fool different models. This method, focusing on altering image feature distributions to match a target distribution from another class, has proven to be both theoretically sound and practically successful. It significantly outperforms current state of the art methods in creating patches that can be highly transferable between models and potentially very harmful, showing great promise for both advancements in the field and potential challenges in security applications.\nReferences: OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY\nSliced-Wasserstein distance for large-scale machine learning : theory, methodology and extensions\nComputational Optimal Transport\nFeature Space Perturbations Yield More Transferable Adversarial Examples\n",
      "content_html": "\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 id=\"authors\"\u003eAuthors:\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eMohammed Jawhar\u003c/li\u003e\n\u003cli\u003eAymane Rahmoune\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"paper--optimal-transport-based-adversarial-based-patch-to-leverage-large-scale-attack-transferabilityhttpsopenreviewnetforumidnzp10evtkv\"\u003ePaper : \u003ca href=\"https://openreview.net/forum?id=nZP10evtkV\"\u003eOptimal Transport Based Adversarial Based Patch To Leverage Large Scale Attack Transferability\u003c/a\u003e\u003c/h3\u003e\n\u003ch1 id=\"table-of-contents-\"\u003eTable of contents :\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eUnderstanding Adversarial Patch Attacks\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subsection-11\"\u003eDecision boundary based\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-12\"\u003eFeature point based\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-13\"\u003eDistribution based\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eTransferability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eOptimal Transport\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eExperiments\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subsection-41\"\u003eExperimental Setup\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-42\"\u003eResults and Findings\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subsection-421\"\u003eDigital Experiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-422\"\u003eHybrid Experiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-423\"\u003ePhysical Experiments\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eReproducibility\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eImagine you\u0026rsquo;re showing a picture to a friend, asking them to guess who\u0026rsquo;s in it, then sticking a tiny, almost invisible sticker on that photo. For some reason, this sticker makes your friend completely unable to recognize who\u0026rsquo;s in the picture. This might sound like magic, but something similar can happen with Computer Vision models designed to capture an image content, either through a classification, a segmentation or even a generation task. These AI programs can be vulnerable to such tricks, that we call technically, Adversarial Patch Attacks.\u003c/p\u003e\n\u003cp\u003eAs AI becomes increasingly integrated into various aspects of our lives, including critical applications like passport security systems, autonomous vehicles, traffic sign detection, and surgical assistance; the reliability, trustworthiness, and performance of these systems under all conditions became of prime importance. This has led to a growing interest in the area of Robust AI, which focuses on enhancing the safety and security of AI technologies by improving their resilience to adverse conditions and digital threats. Within this domain, the study of Attacks and Defense ways plays a pivotal role.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/road_scene.png\" alt=\"Road_scene\"\u003e\n\u003c/p\u003e\n\u003cp\u003eWhile these attacks might not seem like a big deal, nor dangerous in this context, the consequences can be severe in critical scenarios - take for example an autonomous vehicle failing to recognize a stop sign, hurting potentially a pedestrian. In this blog we will explore a new approach used for developping such adversarial patch attacks, based on Optimal Transport, as outlined in the paper \u003cem\u003e\u003cstrong\u003eOptimal Transport Based Adversarial Patch To Leverage Large Scale Attack Transferability\u003c/strong\u003e\u003c/em\u003e. We will try to follow the same structure as in the paper to make the reading easier for you, but with much more simplicity.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eUnderstanding Adversarial Attacks\u003c/h2\u003e\n\u003cp\u003eFirst thing first, let us redefine some previously mentionned concepts, while making them into context.\u003c/p\u003e\n\u003cp\u003eAs deep neural networks keep getting better, developers are working hard to make sure they are trustworthy and reliable. This means constantly testing them to see how well they can handle different challenges, quantifying their robustness, and developping some robustification methods. In the context of image classification for instance, one way to do this is by designing adversarial attacks, which consists of a perturbation or noise, sometimes invisible patterns added to the input images in order to confuse the model and make it misclassify them, causing a huge drop in the accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAdversarial Patch Attacks\u003c/strong\u003e are a specific type that consists of altering only a small part(patch) of the input, either physically or digitally by inserting a crafted \u0026ldquo;sticker\u0026rdquo;. These attacks happen to be more threatful as they can be easily applied in real life, they do not require modification of the entire image, and they can fool multiple, vastly different models with the same crafted patch. This last property is called \u003cstrong\u003etransferability\u003c/strong\u003e and aims to test these engineered adversarial patches on various target models, beyond the original one used for learning, even if the two models(source and target) have been trained on different data or use different architectures, to evaluate the attack\u0026rsquo;s efficacy, and measure the models robustness.\u003c/p\u003e\n\u003cp\u003eDespite the fact that crafting adversarial patch attacks is mainly based around maximizing the classification error through a gradient ascent, we can differenciate between three distinct approaches:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/APA_strategies.png\" alt=\"APA_strategies\"\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDecision boundaries based :\u003c/strong\u003e \u003ca name=\"subsection-11\"\u003e\u003c/a\u003e Which is the most applied approach in previous works and litterature. It focuses on pushing the image\u0026rsquo;s representation in the neural network\u0026rsquo;s \u003cstrong\u003edecision\u003c/strong\u003e space, across the decision boundary, making the network perceive it as belonging to a different, probability maximized class.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTo simplify this approach, imagine a group of fans attempting to sneak into a VIP section at a concert by dressing in a fancy way, like known VIP guests(targeted class). The idea is to blend in so well that they are indistinguishable from actual VIPs to the security guards (the ML model). Despite the simplicity and goodness of this strategy, it has some drawbacks :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIt is highly dependant on the model on which the attack is based, which makes it not really transferable: The success of this method hinges on the security\u0026rsquo;s lack of detail. If they are controlled by another security gard who is very familiar with the actual VIPs, the disguises will fail.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe patch may push the corrupted image representations into unknown regions of the representation space: In their attempt to mimic the VIPs, there\u0026rsquo;s a risk that their disguises might be so overdone that they don\u0026rsquo;t resemble any actual VIPs, pushing them to have a weird unique look. Hence, they end up in a no-man\u0026rsquo;s-land, not fitting in with either the regular attendees or the VIPs.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFeature point based :\u003c/strong\u003e \u003ca name=\"subsection-12\"\u003e\u003c/a\u003eInstead of crossing a decision boundary, this strategy aims to modify the input so its representation in the \u003cstrong\u003efeature space\u003c/strong\u003e matches the one of a target point belonging to a different class. This is like fine-tuning the attack to match a specific \u0026ldquo;signature\u0026rdquo; that the model associates to a specific point.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRevisiting our concert analogy, consider the fans now opting to mimic a specific celebrity known to be attending the concert, assuming that matching this one high-profile individual\u0026rsquo;s appearance will guarantee them entry. Although it seems more precise and effective than the first approach, this strategy has a significan drawback :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt depends heavily on the targeted point selection, this later may be not representative of all instances in the target class :  For instance, if the celebrity is known for a distinctive but uncommon style or if it\u0026rsquo;s unusual for such celebrities to attend such events, their attempt to copy him might not match what the security team usually expects from VIP guests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDistribution based :\u003c/strong\u003e \u003ca name=\"subsection-13\"\u003e\u003c/a\u003eThis new approach implemented in the paper we are analyzing , is based on Optimal Transport theory, and aims to alter the overall feature distribution ofa set of input images belonging to a specific class, to resemble another class\u0026rsquo;s distribution, reducing the gap between them in the \u003cstrong\u003efeature space\u003c/strong\u003e. It is more sophisticated than the previous ones as it exploits the fundamental way neural networks process and classify images based on learned distributions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThis time, the group studies a wide variety of guests behaviors and appearances to craft a new, ambiguous look that doesn\u0026rsquo;t specifically mimic any single guest type, nor disguise blindly in a \u0026ldquo;VIP\u0026rdquo; style, but instead blends into the overall crowd, avoiding easy detection.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe main advantage of this approach is that it allows a better transferability between models, enhancing the performance in the blackbox configuration, as it is independant of the classifier\u0026rsquo;s decision boundary , and the choice of a specific target point. Furthermore it captures the useful characteristics (features) from an input in a more universal way.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2\"\u003eWhy do we need transferability ?\u003c/h2\u003e\n\u003cp\u003eYou surely noticed that we mentionned the transferability term many times in the last section, showing that is an essential property for designing such attacks, but why do we focus so much to make our patch transferable through many models? Well, it is like discovering a master key for many locks : It enables bad actors to compromise and confuse an AI system using a crafted patch they made without knowing anything about that system(architecture, training,\u0026hellip;).\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/transferability_diagram.png\" alt=\"transferability_diagram\"\u003e\n\u003c/p\u003e\n\u003cp\u003eThis ability to create a \u0026lsquo;one-size-fits-all\u0026rsquo; adversarial patch allows to challenge many models, making it more difficult to develop defense mechanisms, and fostering the development of more robust AI systems. Unfortunately, this important property, which confronts the real-world variability of target systems, whose specific architectures or training details are often unknown, was not achieved strongly by previously developped Adversarial attacks; it was studied only by some specialized Adversarial Patch Attacks models(GAP, LaVan, PS-GAN) and gave very modest rsults, being evaluated on dated, non state of the art models Other models (TTP, M3D, Inkawhich et al.) conducted some experiments to measure the transferability of ivisible adversarial attacks and gave promizing results, but they didn\u0026rsquo;t focus i their work on patch attacks transferability.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eDiving into Optimal Transport theory\u003c/h2\u003e\n\u003cp\u003eThe method introduced in this paper represents a remarkable success, as it bridges the gap between transferability studies of invisible adversarial examples and adversarial patch attacks, and provides a trade-off between an efficient non complex patch designing approach, and an exceptional transferability among many advanced state-of-the-art models. The key reason for this success lies in the inherent capabilities of \u003cstrong\u003eoptimal transport\u003c/strong\u003e to measure the distance between two distributions. Particularly, the loss optimized in this method is relevant, as it can be used when the distributions do not overlap, and the theory behind it is intuitive. It is based mainly on the \u003cstrong\u003eWasserstain distance\u003c/strong\u003e defined as :\u003c/p\u003e\n\u003cp\u003e$$W_{p}^p(\\mu,\\nu) = \\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} ||x - y||^p d\\pi(x, y)$$\u003c/p\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003ctitle\u003eMathJax Visualization Example\u003c/title\u003e\n    \u003cscript type=\"text/x-mathjax-config\"\u003e\n    MathJax.Hub.Config({\n        tex2jax: {\n            inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n            displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']\n        }\n    });\n    \u003c/script\u003e\n    \u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eor its more computationnaly efficient Sliced version, which compares the two distributions by computing the expected Wasserstein distance between their one-dimensional linear projections :\n$$SW_{p}^p(\\mu,\\nu) = \\int_{S^{d-1}} W_{p}^p(\\theta_{\\#}^{*}\\mu, \\theta_{\\#}^{*}\\nu) d\\sigma(\\theta)$$\u003c/p\u003e\n\u003cp\u003e\nWhere $\\mu$ and $\\nu$ are two propbability distributions on $\\mathbb{R}^d$, $||.||$ the euclidean norm, $\\pi$ is a transport plan between $\\mu$ and $\\nu$, and $ \\theta_{\\#}^{*} \\mu $ and $ \\theta_{\\#}^{*} \\nu $ the push-forward by $\\theta^{*}(x)=\u003c\\theta, x\u003e$ of $\\mu$ and $\\nu$ respectively.\n\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Sliced_wasserstain.png\" alt=\"Sliced Wasserstain\"\u003e\n  \u003cbr\u003e\n  \u003cem\u003eThis image is taken and adapted from the \u003ca href=\"https://theses.hal.science/tel-03533097/document\"\u003eSliced-Wasserstein distance for large-scale machine learning: theory, methodology and extensions\u003c/a\u003e paper.\u003c/em\u003e\n\u003c/p\u003e\n\u003c!-- $$SW_{p}(\\mu,\\nu) = \\int_{S^{d-1}} W_{p}(\\theta_{\\#}\\mu, \\theta_{\\#}\\nu) d\\sigma(\\theta)$$--\u003e\n\u003cp\u003eTo delve more into the mathematical details, let us explore how Optimal Transport, specifically the Wasserstein distance, is employed to craft effective adversarial examples:\nIn the context of image classification, we consider the standard notation where a set of image-label pairs $(x_i, y_i)$ is drawn from a joint distribution of random variables $X$ and $Y$. The images $X$ are typically multi-dimensional arrays representing the height, width, and color channels of an image (e.g., a colored $256 \\times 256$ pixel image would have $h = 256$, $w = 256$, and $c = 3$). Meanwhile, $Y$ is a set of discrete labels that classify these images (e.g., \u0026lsquo;cat\u0026rsquo;, \u0026lsquo;dog\u0026rsquo;, etc.). Within a given encoder-decoder neural network $F$, designed to predict these labels, the encoder function $f$ compresses the raw image data $X$ throughout each pooling layer into a feature space $S^{(l)}$, capturing essential patterns.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Optimal transport.png\" alt=\"Optimal Transport\"\u003e\n\u003c/p\u003e\n\u003cp\u003eThe Wasserstein distance $W_p$, calculated between the distributions of these feature spaces, reflects how much \u0026ldquo;effort\u0026rdquo; it would take to transform the distribution of features from one class into another. In the case of the proposed method, crafting the patch consits of minimizing the transformation cost (distance)of the features distribution from a corrupted \u0026ldquo;true\u0026rdquo; class into a \u0026ldquo;target\u0026rdquo; adversarial class across multiple layers. This can be formulated as follows:\u003c/p\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003ctitle\u003eMathJax Visualization Example\u003c/title\u003e\n    \u003cscript type=\"text/x-mathjax-config\"\u003e\n    MathJax.Hub.Config({\n        tex2jax: {\n            inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n            displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']\n        }\n    });\n    \u003c/script\u003e\n    \u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\n$$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_X \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{X_{\\delta}}^{(l)}, \\nu_y^{(l)}) \\right]$$\u003c/p\u003e\n\u003cp\u003e\nWhere $OT$ is the optimal transport distance (Wasserstein or Sliced Wasserstein), $\\mu_{X_{\\delta}}^{(l)}$ is the feature distribution of images with the patch and $\\nu_y^{(l)}$ is the target feature distribution for the incorrect class.\u003c/p\u003e\n\u003cp\u003eThis can be further enhanced by adding a regularization term to ensure that the patches are effective under various conditions, and can be physically realisable. The problem becomes as follows :\u003c/p\u003e\n\u003cp\u003e\n$$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_{X, t\\sim \\tau, e\\sim E} \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{A(\\delta, X, e, t)}^{(l)}, \\nu_y^{(l)}) + TV(\\delta)\\right]$$\nwhere TV is the total variation loss discouraging high-frequency patterns.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\u003ch2 id=\"section-4\"\u003eExperiments\u003c/h2\u003e\n\u003ch3 id=\"experimental-setup-a-namesubsection-41a\"\u003eExperimental setup \u003ca name=\"subsection-41\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eTo confirm the theoretical results and assumptions, several experiments were conducted under different conditions and settings. For the sake of simplicity, we will not delve into the exhaustive details of the experimental setup, procedures, and results. In summary:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe experiments aimed to evaluate the impact and transferability of the proposed adversarial patch - referred to as $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ - across a range of models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ performance was benchmarked against other adversarial patch attack (APA) methods such as GAP, LaVAN, TNT, TTP, M3D, and others.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe source and target models chosen for this analysis were regrouped into six categories based on their architecture: CNNs-V1, CNNs-V2, ENet, CNext(ConvNext), DeiT, and Swin.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTested patches were randomly placed to the side of images, in order to avoid occluding the object of interest and replicate more closely the real world conditions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTargeted success rate (tSuc)\u003c/strong\u003e metric was used for evaluating transferability. It consists of the percentage of instances where the network, when presented with an image containing the adversarial patch, incorrectly classifies the image as the attacker\u0026rsquo;s intended target class, out of the total number of attempts.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"results-and-findings--a-namesubsection-42a\"\u003eResults and Findings : \u003ca name=\"subsection-42\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe experiments are structured into three main categories:\u003c/p\u003e\n\u003ch4 id=\"digital-experiments--a-namesubsection-421a\"\u003eDigital experiments : \u003ca name=\"subsection-421\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003ch5 id=\"simple-configuration-\"\u003eSimple configuration :\u003c/h5\u003e\n\u003cp\u003eIn this configuration, the patches efficacy was tested in a purely digital environment, using images from the ImageNet-1K dataset, which was used also for training. Patches were first designed to attack one of the source models, then tested on other target models to measure the attacking transferability. The table below summarizes for each APA method, the best transferring attack performance achieved :\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Digital_transferability.png\" alt=\"Digital Transferability\"\u003e\n\u003c/p\u003e\n\u003cp\u003eAs expected through the novelty of $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, it shows the highest transferability capacity(mean, min and max) and outperforms all the other methods. Additionaly, we can make the following observations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNetworks trained with older training recipes (CNNs-v1) seem more vulnerable to attacks, unlike tansformers and models trained with new training recipes (scheduler, augmenting training data like RandAug and Mixup, \u0026hellip;) which appear to be more robust.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor all APA methods, patches learned using Swin or CNext are more universal as they can transfer uniformly to multiple models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn general, baseline methods tend to overfit and fail to generate patches that effectively transfer to complex architectures like CNext and Swin models, even if these patches are developed using the same category of models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMethods based on feature space optimization, including L2 and the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, demonstrate improved transferability and are less likely to overfit.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"robustified-configuration-\"\u003eRobustified configuration :\u003c/h5\u003e\n\u003cp\u003eIn the second configuration of the digital experiments, the same procedures were reapplied. However this time, the methods learn on Swin, and transfer to a robustified version, by Local Gradients Smoothing (LGS) - a defense mechanism smoothing salient regions in images before passing them to the network - , of the six model categories.\u003c/p\u003e\n\u003cp\u003eSimilarly, $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ outperforms significantly all other methods as we can see in the following table :\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Digital_robustified.png\" alt=\"Digital robustified\"\u003e\n\u003c/p\u003e\n\u003ch4 id=\"hybrid-experiments-a-namesubsection-422a\"\u003eHybrid experiments: \u003ca name=\"subsection-422\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eIn order to simulate real-world applications more closely, the hybrid experiments conducted within this section involved printing adversarial patches trained with Swin, placing them in physical environments, capturing the images, and then digitally analyzing the results, for simple, and robustified models.\u003c/p\u003e\n\u003cp\u003eThe table below shows the criticality of the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ giving very large tSuc in comparison with the other methods, for all settings:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Digital_robustified.png\" alt=\"Digital robustified\"\u003e\n\u003c/p\u003e\n\u003ch4 id=\"physical-experiments-a-namesubsection-423a\"\u003ePhysical experiments: \u003ca name=\"subsection-423\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eIn this last experiments category, we get closer to the real world situations, by recording a video of some ImageNet-1K objects (banana, cup , keyboard) while moving a designed patch in the set. This aims to quantify the severity of each attack, for realistic scenarios (as the example provided above about the autonomous vehicule not detecting the stop sign while driving due to an adversarial patch designed without knowing the AI system at all).\u003c/p\u003e\n\u003cp\u003eAll APA methods failed to transfer properly on all architectures except for L2 with a modest tSuc(9.3%) and $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ that gave much better results (23.4% and 29.3%)\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003eIn this section, we wanted to reproduce some of the experiments conducted in the paper to validate the results and the findings. However, by exploring the code provided with the paper, and analyzing the python files, we found that it is not well documented, and the structure is not very clear, which makes it difficult to understand and reproduce the complex experiments involving transferability evaluation. Furthermore, given that the paper is based on the ImageNet dataset, which is very large and requires a lot of computational resources, we were not able to run the experiments on our local machines, as we do not have access to a powerful GPU cluster. Consequently, we opted for the CIFAR-10 dataset, which is smaller and more manageable. Despite this adjustment, we still faced some issues Specifically, the model is built from scratch without an available pre-trained, and there are missing components, notably the function required to extract feature vectors from each layer of the target models. To address these challenges and make the reproduction process easier, we decided to develop the missing feature extraction function as an enhancement, and save the obtained results into files(in the same way it was done in the code), to be able to apply the optimal transport method and craft the adversarial patches later as perspectives\u003c/p\u003e\n\u003cp\u003eHere are the code that we developed :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.models\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodels\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.transforms\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.utils.data\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.datasets\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edatasets\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eget_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esplit\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eutils\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u0026#34;Return a DataLoader object for a given dataset and split.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eutils\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eget_dataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esplit\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eshuffle\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eextract_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    Extracts features from each layer of the pre-trained models provided in the list_models\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    by applying average pooling, and saves the extracted features into files.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodel_name\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e__dict__\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emodel_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e](\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epretrained\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eeval\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enamed_children\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eisinstance\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eSequential\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ei\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003einput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eenumerate\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#00a8c8\"\u003ewith\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eno_grad\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003einput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efunctional\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eadaptive_avg_pool2d\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eview\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esize\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecat\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esave\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;./data/CIFAR/all_images_feature/\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer_name\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emodel_name\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.pt\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Apply the function extract_features to some targeted models\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;resnet18\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;vgg19\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;convnext_tiny\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;swin_t\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eget_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;CIFAR10\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;train\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e64\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eextract_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"section-6\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn conclusion, our exploration of the paper \u003cem\u003e\u003cstrong\u003eOPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY\u003c/strong\u003e\u003c/em\u003e, revealed an innovative and promizing technique that uses Optimal Transport to make adversarial patches more effectively fool different models. This method, focusing on altering image feature distributions to match a target distribution from another class, has proven to be both theoretically sound and practically successful. It significantly outperforms current state of the art methods in creating patches that can be highly transferable between models and potentially very harmful, showing great promise for both advancements in the field and potential challenges in security applications.\u003c/p\u003e\n\u003ch2 id=\"references\"\u003eReferences:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://openreview.net/forum?id=nZP10evtkV\"\u003eOPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://theses.hal.science/tel-03533097/document\"\u003eSliced-Wasserstein distance for large-scale machine learning : theory, methodology and extensions\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1803.00567.pdf\"\u003eComputational Optimal Transport\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf\"\u003eFeature Space Perturbations Yield More Transferable Adversarial Examples\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
      "url": "http://localhost:1313/posts/optimal_transport_based_adversarial_patch/",
      "date_published": "3026-03-09T222:33:00+01:00",
      "date_modified": "3026-03-09T222:33:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "00988a4ecebb51af6cf28a7b318568929bf7a01d",
      "title": "Statistical Minimax Rates Under Privacy",
      "summary": "",
      "content_text": "Estimating Privacy in Data Science: A Comprehensive Guide Author: Antoine Klein Github Link Table of Contents Incentives Introduction Definition Theory The case of multinomial estimation The case of density estimation Experiment Conclusion Quizz Why do we care about privacy ? Imagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you struggle. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an ethical dilemma: transparency towards the state versus protection of personal data.\n$$\\text{In short, transparency goes against your privacy. }$$\nThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you underestimate your answers. On a wider scale, this leads to a suffrage bias and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\n\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\nThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is not to have protected yourself against a malicious agent.\nAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better certify usage by means of cyber protection labels and leads to such a norm to achieve trust: In this blog, we propose to tackle this problem from a completely different angle: how to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data. We\u0026rsquo;ll also use minimax bounds to answer the question: for a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation? (fundamental trade-offs between privacy and convergence rate)\nScientific introduction Our blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that response randomization achieves optimal convergence in the case of multinomial estimation, and then that this process can be generalized to any nonparametric distribution estimation. To this end, we will introduce the notion of local differential privacy as well as the minimax theory for obtaining optimal limits. All this will shed light on the trade-off between privacy and estimation rates. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\nSome key definitions Let assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a markov kernel that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\nThe privacy mechanism is to be said non interactive if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is memory less. If not, the mechnism is said to be interactive.\nIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\n$Z_i$ is said to be α-local-differentially private for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\nAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more difficult it is to distinguish the distribution of Z conditional on two different X data.\nTheoretical results The case of multinomial estimation In this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$. This problem is a special case of multinomial estimation, where θ is now a multidimensional parameter that is amenable to simplex probability. $∆d := (θ ∈ ℝ+ |∑θ_j = 1)$.\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and $$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\nRecall from standard statistics: For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that: $$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\nIn others term, providing α-local-differentially privacy causes a reduction in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the asymptotically rate of convergences remains unchanged which is a really good news !\nPractical strategies The paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\nRandomized responses Laplace Noise (beyond paper) Randomized responses The intuition of this section is the following : to not allow the statistician to retrieve your personnal data in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, he can\u0026rsquo;t distinguish if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\nFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\n$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$ $$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\nSuch a mechanism achieves α-local-differentially privacy because one can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\nWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\n$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\nThis leads to the natural moment-estimator :\n$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\nOne can also show that it verifies :\n$$E[ ||θ_{hat}- θ||_2] ≤ \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\nLaplace Noise (beyond paper) Instead of saying the truth with some probability, one may think of adding noise to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is not covered in the paper.\nDefinition: A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\nA visualisation for differents parameters is given below. We can see that Laplace distribution is a shaper verson of the gaussian distribution : The trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\nOne can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\nThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, it verifies α-local-differentially privacy. The proposed estimator is the following :\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\nOne can show that it is an unbiaised estimator that achieves the optimal rates:\n$$E[\\hat{Z}] = E[X]$$\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$ $$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\nThis is exactly the optimal rates, quite outstanding !\nThe case of density estimation One accurate question that can raise is : what about others distribution ? Is privacy more costly in general cases ? What is the trade-off ?\nTo answer this question, let\u0026rsquo;s precise the problem.\nWe want to estimate in a non-parametric way a 1D-density function f belonging to one of theses classes :\n-Hölder Class (β, L): $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\n-Sobolev Class: $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\nIn a intuitition way, those two classes express that f is smooth enough to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\nTheorem Without privacy One can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\nIn the case of d-multidimensionnal density f, the optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\nThis illustrates once again the curse of dimensionnality.\nWith privacy Let assume that f bellongs to one of the two classes with β as smoothness parameter.\nThen, the optimal α-local-differentially private optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\nOne may observe two pessimistic news:\n-The rate is affected by a factor of $\\alpha^2$ as for the multinomial estimation\n-More damageable: the rate is slower in term of n unlike the previous problem which make privacy in this case more costly.\nPractical strategies Eventhough this rate is pessimistic and proves that privacy comes at a cost, it remains to illustrates how can we achieves this best but not great rate. For this end, once again, two strategies are possible.\nRandomized responses Laplace Noise (beyond paper) Randomized responses This is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of α. As it is not the most comprehensive and straightforward method, we prefer to dive in depth into the second one; uncovered in the paper.\nLaplace Noise (beyond paper) Let assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\nWe consider the histogramm estimator: $$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\nWe now construct the private mechanism as follow:\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\nIn an intuitive way, we add a Laplace noise realisation for each bin.\nThis guarantees α-local-differentially privacy as : $$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\nThis leads to the α-local-differentially private estimator :\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\nThe biais is the same as the unprivate case as :\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\nOne may prove that if f bellongs to the β-Hölder Class:\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\nMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total :\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$ Minimizing over K (hyperparameters) leads to : $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\nExperiment: Illustration of the Minimax privacy rate Overview The aim of this section is to provide illustrations of the theoretical results set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\nFor the sake of reproducibility and transparency, the source code can be found in the notebook at this: Github link.\nMethodology Data Preparation: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects. More precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\nPrivacy Metric Calculation: We will look at the use case of estimating the mean of a distribution.\nEvaluation: The results will be compared in terms of Mean Square Error (MSE).\nResults In terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\nAs expected, the greater the desired privacy (low $\\alpha$), the more spread out the distribution of observed data.\nWhen it comes to estimating the true average from private data, we obtain the following figure:\nThis figure illustrates two major points:\n-The first is that whatever the level of privacy, we have an unbiased estimator of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\n-The second is that, unfortunately, the greater the privacy (low alpha), the greater the variance of this estimator.\nWe recall our main theorem demonstrated above Previous theorem :\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\nWe now want to compare the theoretical optimal rate with empirical results. To do this, we distinguish two situations:\n-The first is with fixed alpha, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\n-The second has a fixed n and determines the MSE as a function of alpha. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\nConclusion From a problem rooted in an ethical dilemma (privacy versus completeness and transparency), we have looked at the cost of guaranteeing one at the expense of the other, to better sketch out desirable situations.\nThis has enabled us to develop theoretical results in terms of minimax rates. There is indeed a trade-off between these criteria, which is even more costly in the case of non-parametric density estimation.\nFinally, we have compared these theoretical limits with empirical results, which confirm the conformity of the statements.\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following quiz to ensure his or her understanding.\nQuizz To test yourself abour privacy:\nWhat is privacy?\nAvoid asking questions that can raise private information A mechanism that prevents other agent to retrieve personnal information in your answer An ethical-washing trend Which situation is α-local-differentially privacy?\nsup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α) You tell the truth half the time, you lie otherwise. Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1) What is the privacy cost in term of optimal rate ?\nMultinomial estimation: A factor α^2/d Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2)) We loose nothing, that's the surprising finding of the paper Submit Annexes References Warner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830. John C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013) Dwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407. Narayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE. ",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eEstimating Privacy in Data Science: A Comprehensive Guide\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Antoine Klein \u003ca href=\"https://github.com/AntoineTSP\"\u003eGithub Link\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIncentives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eDefinition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eTheory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eThe case of multinomial estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe case of density estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eQuizz\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eWhy do we care about privacy ?\u003c/h2\u003e\n\u003cp\u003eImagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you \u003cem\u003estruggle\u003c/em\u003e. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an \u003cstrong\u003eethical dilemma\u003c/strong\u003e: transparency towards the state versus protection of personal data.\u003cbr\u003e\n$$\\text{In short, transparency goes against your privacy. }$$\u003c/p\u003e\n\u003cp\u003eThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you \u003cstrong\u003eunderestimate\u003c/strong\u003e your answers. On a wider scale, this leads to a \u003cstrong\u003esuffrage bias\u003c/strong\u003e and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\u003cbr\u003e\n\u003cstrong\u003e\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is \u003cstrong\u003enot to have protected yourself against a malicious agent\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better \u003cstrong\u003ecertify usage\u003c/strong\u003e by means of cyber protection labels and leads to such a norm to achieve trust:\n\u003cimg\n  src=\"/images/Antoine_Klein/Umbrella.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn this blog, we propose to tackle this problem from a completely different angle: \u003cstrong\u003ehow to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data\u003c/strong\u003e. We\u0026rsquo;ll also use minimax bounds to answer the question: \u003cstrong\u003efor a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation?\u003c/strong\u003e (fundamental trade-offs between privacy and convergence rate)\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eScientific introduction\u003c/h2\u003e\n\u003cp\u003eOur blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that \u003cstrong\u003eresponse randomization achieves optimal convergence\u003c/strong\u003e in the case of multinomial estimation, and then that this process can be generalized to any \u003cem\u003enonparametric distribution estimation\u003c/em\u003e. To this end, we will introduce the notion of \u003cstrong\u003elocal differential privacy\u003c/strong\u003e as well as the \u003cstrong\u003eminimax theory\u003c/strong\u003e for obtaining optimal limits. All this will shed light on the \u003cstrong\u003etrade-off between privacy and estimation rates\u003c/strong\u003e. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eSome key definitions\u003c/h2\u003e\n\u003cp\u003eLet assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a \u003cstrong\u003emarkov kernel\u003c/strong\u003e that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\u003c/p\u003e\n\u003cp\u003eThe privacy mechanism is to be said \u003cstrong\u003enon interactive\u003c/strong\u003e if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is \u003cstrong\u003ememory less\u003c/strong\u003e. If not, the mechnism is said to be interactive.\u003c/p\u003e\n\u003cp\u003eIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\u003c/p\u003e\n\u003cp\u003e$Z_i$ is said to be \u003cstrong\u003eα-local-differentially private\u003c/strong\u003e for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\u003c/p\u003e\n\u003cp\u003eAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more \u003cstrong\u003edifficult it is to distinguish\u003c/strong\u003e the distribution of Z conditional on two different X data.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eTheoretical results\u003c/h2\u003e\n\u003ch3 id=\"section-4\"\u003eThe case of multinomial estimation\u003c/h3\u003e\n\u003cp\u003eIn this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$.\nThis problem is a special case of multinomial estimation, where \u003ccode\u003eθ\u003c/code\u003e is now a multidimensional parameter that is amenable to simplex probability. $∆\u003cem\u003ed := (θ ∈ ℝ\u003c/em\u003e+ |∑θ_j = 1)$.\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"Recall\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem :\u003c/strong\u003e Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and\n$$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRecall from standard statistics:\u003c/strong\u003e For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that:\n$$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\u003c/p\u003e\n\u003cp\u003eIn others term, providing α-local-differentially privacy \u003cstrong\u003ecauses a reduction\u003c/strong\u003e in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the \u003cstrong\u003easymptotically rate of convergences remains unchanged\u003c/strong\u003e which is a really good news !\u003c/p\u003e\n\u003ch4 id=\"practical-strategies\"\u003ePractical strategies\u003c/h4\u003e\n\u003cp\u003eThe paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-10\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-11\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-10\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThe \u003cem\u003eintuition\u003c/em\u003e of this section is the following : \u003cstrong\u003eto not allow the statistician to retrieve your personnal data\u003c/strong\u003e in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, \u003cstrong\u003ehe can\u0026rsquo;t distinguish\u003c/strong\u003e if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\u003c/p\u003e\n\u003cp\u003eFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\u003c/p\u003e\n\u003cp\u003e$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$\n$$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\u003c/p\u003e\n\u003cp\u003eSuch a mechanism achieves \u003cem\u003eα-local-differentially privacy\u003c/em\u003e because one can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\u003c/p\u003e\n\u003cp\u003eWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\u003c/p\u003e\n\u003cp\u003e$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\u003c/p\u003e\n\u003cp\u003eThis leads to the natural moment-estimator :\u003c/p\u003e\n\u003cp\u003e$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\u003c/p\u003e\n\u003cp\u003eOne can also show that it verifies :\u003c/p\u003e\n\u003cp\u003e$$E[ ||θ_{hat}- θ||_2] ≤  \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\u003c/p\u003e\n\u003ch5 id=\"section-11\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eInstead of saying the truth with some probability, one may think of \u003cstrong\u003eadding noise\u003c/strong\u003e to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is \u003cstrong\u003enot covered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition:\u003c/strong\u003e A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\u003cbr\u003e\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\u003c/p\u003e\n\u003cp\u003eA visualisation for differents parameters is given below. We can see that Laplace distribution is a \u003cstrong\u003eshaper verson of the gaussian distribution\u003c/strong\u003e :\n\u003cimg\n  src=\"/images/Antoine_Klein/Laplace.png\"\n  alt=\"Laplace\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\u003c/p\u003e\n\u003cp\u003eOne can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\u003c/p\u003e\n\u003cp\u003eThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, \u003cstrong\u003eit verifies α-local-differentially privacy\u003c/strong\u003e. The proposed estimator is the following :\u003cbr\u003e\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\u003c/p\u003e\n\u003cp\u003eOne can show that it is an unbiaised estimator that achieves the optimal rates:\u003cbr\u003e\n$$E[\\hat{Z}] = E[X]$$\u003cbr\u003e\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$\n$$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\u003c/p\u003e\n\u003cp\u003eThis is \u003cstrong\u003eexactly the optimal rates\u003c/strong\u003e, quite outstanding !\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe case of density estimation\u003c/h3\u003e\n\u003cp\u003eOne accurate question that can raise is : \u003cstrong\u003ewhat about others distribution ?\u003c/strong\u003e Is privacy more costly in general cases ? What is the trade-off ?\u003c/p\u003e\n\u003cp\u003eTo answer this question, let\u0026rsquo;s precise the problem.\u003c/p\u003e\n\u003cp\u003eWe want to estimate in a non-parametric way a 1D-density function \u003ccode\u003ef\u003c/code\u003e belonging to one of theses classes :\u003cbr\u003e\n-\u003cstrong\u003eHölder Class (β, L):\u003c/strong\u003e $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\u003cbr\u003e\n-\u003cstrong\u003eSobolev Class:\u003c/strong\u003e $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\u003c/p\u003e\n\u003cp\u003eIn a intuitition way, those two classes express that \u003ccode\u003ef\u003c/code\u003e is \u003cstrong\u003esmooth enough\u003c/strong\u003e to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\u003c/p\u003e\n\u003ch4 id=\"theorem\"\u003eTheorem\u003c/h4\u003e\n\u003ch5 id=\"without-privacy\"\u003eWithout privacy\u003c/h5\u003e\n\u003cp\u003eOne can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\u003c/p\u003e\n\u003cp\u003eIn the case of d-multidimensionnal density \u003ccode\u003ef\u003c/code\u003e, the optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\u003c/p\u003e\n\u003cp\u003eThis illustrates once again the \u003cstrong\u003ecurse of dimensionnality\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"with-privacy\"\u003eWith privacy\u003c/h5\u003e\n\u003cp\u003eLet assume that \u003ccode\u003ef\u003c/code\u003e bellongs to one of the two classes with  \u003ccode\u003eβ\u003c/code\u003e as smoothness parameter.\u003cbr\u003e\nThen, the optimal α-local-differentially private optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\u003c/p\u003e\n\u003cp\u003eOne may observe \u003cstrong\u003etwo pessimistic news\u003c/strong\u003e:\u003cbr\u003e\n-The rate is \u003cstrong\u003eaffected by a factor\u003c/strong\u003e of $\\alpha^2$ as for the multinomial estimation\u003cbr\u003e\n-More damageable: the \u003cstrong\u003erate is slower\u003c/strong\u003e in term of \u003ccode\u003en\u003c/code\u003e unlike the previous problem which make privacy in this case \u003cstrong\u003emore costly\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"practical-strategies-1\"\u003ePractical strategies\u003c/h5\u003e\n\u003cp\u003eEventhough this rate is pessimistic and proves that \u003cstrong\u003eprivacy comes at a cost\u003c/strong\u003e, it remains to illustrates how can we achieves this best but not great rate.\nFor this end, once again, two strategies are possible.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-12\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-13\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-12\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThis is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of \u003ccode\u003eα\u003c/code\u003e.\nAs it is not the most comprehensive and straightforward method, \u003cstrong\u003ewe prefer to dive in depth into the second one; uncovered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"section-13\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eLet assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\u003c/p\u003e\n\u003cp\u003eWe consider the histogramm estimator:\n$$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\u003c/p\u003e\n\u003cp\u003eWe now construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\u003c/p\u003e\n\u003cp\u003eIn an intuitive way, we add a Laplace noise realisation for each bin.\u003c/p\u003e\n\u003cp\u003eThis guarantees α-local-differentially privacy as :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\u003c/p\u003e\n\u003cp\u003eThis leads to the α-local-differentially private estimator :\u003cbr\u003e\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\u003c/p\u003e\n\u003cp\u003eThe biais is the same as the unprivate case as :\u003cbr\u003e\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\u003c/p\u003e\n\u003cp\u003eOne may prove that if f bellongs to the β-Hölder Class:\u003cbr\u003e\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\u003c/p\u003e\n\u003cp\u003eMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total  :\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$\nMinimizing over K (hyperparameters) leads to :  $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"section-6\"\u003eExperiment: Illustration of the Minimax privacy rate\u003c/h2\u003e\n\u003ch3 id=\"section-111\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe aim of this section is to \u003cstrong\u003eprovide illustrations of the theoretical results\u003c/strong\u003e set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\u003c/p\u003e\n\u003cp\u003eFor the sake of \u003cstrong\u003ereproducibility and transparency\u003c/strong\u003e, the source code can be found in the notebook at this: \u003ca href=\"https://github.com/AntoineTSP/responsible-ai-datascience-ipParis.github.io.git\"\u003eGithub link\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"methodology\"\u003eMethodology\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Preparation\u003c/strong\u003e: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMore precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\u003cbr\u003e\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrivacy Metric Calculation\u003c/strong\u003e: We will look at the use case of estimating the mean of a distribution.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: The results will be compared in terms of Mean Square Error (MSE).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003cp\u003eIn terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Private_distribution.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs expected, the greater the desired privacy (low $\\alpha$), \u003cstrong\u003ethe more spread out\u003c/strong\u003e the distribution of observed data.\u003c/p\u003e\n\u003cp\u003eWhen it comes to estimating the true average from private data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Estimated_mean.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis figure illustrates two major points:\u003cbr\u003e\n-The first is that whatever the level of privacy, we have an \u003cstrong\u003eunbiased estimator\u003c/strong\u003e of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\u003cbr\u003e\n-The second is that, unfortunately, the greater the privacy (low alpha), \u003cstrong\u003ethe greater the variance\u003c/strong\u003e of this estimator.\u003c/p\u003e\n\u003cp\u003eWe recall our main theorem demonstrated above \u003ca href=\"#Recall\" style=\"background-color: yellow; padding: 2px 5px; border-radius: 3px;\"\u003ePrevious theorem\u003c/a\u003e :\u003cbr\u003e\n\u003cstrong\u003eTheorem\u003c/strong\u003e : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\u003c/p\u003e\n\u003cp\u003eWe now want to \u003cstrong\u003ecompare the theoretical optimal rate with empirical results\u003c/strong\u003e. To do this, we distinguish two situations:\u003cbr\u003e\n-The first is with \u003cstrong\u003efixed alpha\u003c/strong\u003e, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Minimax_rate_n.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\u003c/p\u003e\n\u003cp\u003e-The second has a \u003cstrong\u003efixed n\u003c/strong\u003e and determines the MSE as a function of alpha. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Minimax_rate_alpha.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eFrom a problem rooted in an \u003cstrong\u003eethical dilemma\u003c/strong\u003e (privacy versus completeness and transparency), we have looked at the \u003cstrong\u003ecost of guaranteeing\u003c/strong\u003e one at the expense of the other, to better sketch out desirable situations.\u003cbr\u003e\nThis has enabled us to develop theoretical results in terms of \u003cstrong\u003eminimax rates\u003c/strong\u003e. There is indeed a \u003cstrong\u003etrade-off\u003c/strong\u003e between these criteria, which is even more costly in the case of non-parametric density estimation.\u003cbr\u003e\nFinally, we have compared these theoretical limits with empirical results, which \u003cstrong\u003econfirm the conformity of the statements\u003c/strong\u003e.\u003cbr\u003e\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following \u003cstrong\u003equiz\u003c/strong\u003e to ensure his or her understanding.\u003c/p\u003e\n\u003ch1 id=\"section-8\"\u003eQuizz\u003c/h1\u003e\n\u003cp\u003eTo test yourself abour privacy:\u003c/p\u003e\n\u003cform id=\"quiz-form\" class=\"quiz-form\"\u003e\n    \u003cdiv class=\"quiz-question\"\u003e\n        \u003cp\u003eWhat is privacy?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"1\"\u003e\n                Avoid asking questions that can raise private information\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"2\"\u003e\n                A mechanism that prevents other agent to retrieve personnal information in your answer\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"3\"\u003e\n                An ethical-washing trend\n            \u003c/label\u003e\n        \u003c/div\u003e\n        \u003cp\u003eWhich situation is α-local-differentially privacy?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"1\"\u003e\n                sup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α)\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"2\"\u003e\n                You tell the truth half the time, you lie otherwise.\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"3\"\u003e\n                Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1)\n            \u003c/label\u003e\n        \u003c/div\u003e\n        \u003cp\u003eWhat is the privacy cost in term of optimal rate ?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"1\"\u003e\n                Multinomial estimation: A factor α^2/d\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"2\"\u003e\n                Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2))\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"3\"\u003e\n                We loose nothing, that's the surprising finding of the paper\n            \u003c/label\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c!-- Add more quiz questions as needed --\u003e\n    \u003cbutton type=\"submit\" class=\"quiz-submit\"\u003eSubmit\u003c/button\u003e\n\u003c/form\u003e\n\u003cdiv id=\"quiz-results\" class=\"quiz-results\"\u003e\u003c/div\u003e\n\u003cscript\u003e\n    // Define quiz questions and correct answers\n    const quizQuestions = [\n        {\n            question: \"What is privacy?\",\n            answer: \"2\"\n        },\n        //Add more quiz questions as needed\n        {\n            question: \"Which situation is α-local-differentially privacy?\",\n            answer: \"3\"\n        },\n        //Add more quiz questions as needed\n        {\n            question: \"What is the privacy cost in term of optimal rate ?\",\n            answer: \"1\"\n        }\n    ];\n\n    // Handle form submission\n    document.getElementById('quiz-form').addEventListener('submit', function(event) {\n        event.preventDefault();\n\n        // Calculate quiz score\n        let score = 0;\n        quizQuestions.forEach(question =\u003e {\n            const selectedAnswer = document.querySelector(`input[name=\"question${quizQuestions.indexOf(question) + 1}\"]:checked`);\n            if (selectedAnswer) {\n                if (selectedAnswer.value.toLowerCase() === question.answer) {\n                    score++;\n                    selectedAnswer.parentElement.classList.add('correct');\n                } else {\n                    selectedAnswer.parentElement.classList.add('incorrect');\n                }\n            }\n        });\n\n        // Display quiz results\n        const quizResults = document.getElementById('quiz-results');\n        quizResults.innerHTML = `\u003cp\u003eYou scored ${score} out of ${quizQuestions.length}.\u003c/p\u003e`;\n    });\n\u003c/script\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eWarner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830.\u003c/li\u003e\n\u003cli\u003eJohn C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013)\u003c/li\u003e\n\u003cli\u003eDwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407.\u003c/li\u003e\n\u003cli\u003eNarayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cscript\u003e\nfunction highlight(text) {\n  var inputText = document.getElementById(\"markdown-content\");\n  var innerHTML = inputText.innerHTML;\n  var index = innerHTML.indexOf(text);\n  if (index \u003e= 0) {\n    innerHTML = innerHTML.substring(0,index) + \"\u003cspan class='highlight'\u003e\" + innerHTML.substring(index,index+text.length) + \"\u003c/span\u003e\" + innerHTML.substring(index + text.length);\n    inputText.innerHTML = innerHTML;\n  }\n}\nhighlight(\"Estimating Privacy in Data Science\");\n\n\u003c/script\u003e\n\u003chr\u003e\n\u003cscript\u003e\n    function displayInput() {\n        var inputValue = document.getElementById(\"inputField\").value;\n        document.getElementById(\"output\").innerText = \"You typed: \" + inputValue;\n    }\n\u003c/script\u003e\n\u003cstyle\u003e\n.highlight {\n  background-color: red;\n}\n.highlight-on-hover:hover {\n        background-color: yellow;\n    }\n/* Quiz form styles */\n.quiz-form {\n        max-width: 500px;\n        margin: auto;\n        padding: 20px;\n        border: 1px solid #ccc;\n        border-radius: 5px;\n        background-color: #f9f9f9;\n}\n\n.quiz-question {\n        margin-bottom: 20px;\n}\n\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n}\n\n.quiz-submit {\n        background-color: #4caf50;\n        color: white;\n        padding: 10px 20px;\n        border: none;\n        border-radius: 5px;\n        cursor: pointer;\n}\n\n.quiz-submit:hover {\n        background-color: #45a049;\n}\n\n/* Quiz results styles */\n.quiz-results {\n        margin-top: 20px;\n        font-weight: bold;\n}\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n    }\n.quiz-options label.correct {\n        color: green;\n}\n.quiz-options label.incorrect {\n        color: red;\n}\na[name]:hover {\n        background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */\n        text-decoration: none; /* Optionally remove underline on hover */\n}\n\u003c/style\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n",
      "url": "http://localhost:1313/posts/statistical_minimax_rates_under_privacy/",
      "date_published": "31016-31-09T122:3131:00+01:00",
      "date_modified": "31016-31-09T122:3131:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "64ca00e483d62f1f64025e3844d00befbed90629",
      "title": "Measuring the Transferability of Pre-trained Models: a link with Neural Collapse Distances on Target Datasets",
      "summary": "",
      "content_text": " Authors : Marion Chadal and Julie Massé\nThis blog post discusses the paper \u0026ldquo;How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability\u0026rdquo; [1]. It provides an explanation of it so that you can understand the usefulness of measuring transferability, and a reproduction of the authors\u0026rsquo; experiment so that you can better visualize their methodology.\nPre-trained models and fine-tuning Pre-trained models are currently one of the most active fields in Machine Learning. They can be found in a wide range of applications, from image recognition and natural language processing to autonomous driving and medical diagnosis. These models are \u0026ldquo;pre-trained\u0026rdquo; on massive datasets, most of the time encompassing millions of examples across diverse domains. The training process leverages Deep Learning algorithms and can take weeks or even months, utilizing powerful computing resources to iteratively adjust the model\u0026rsquo;s parameters until it achieves high accuracy on the training data.\nThe first purpose of pre-training is to enable the model to learn a broad understanding of the world, capturing intricate patterns, relationships, and features that are not easily discernible. This extensive learning phase allows the model to develop a deep amount of knowledge, which it can then apply to more specific tasks through a process known as fine-tuning.\nWhat is fine-tuning? It consists in adapting a general-purpose model to perform well on a specific task. This adaptation allows the model to fine-tune its learned features to better align with the nuances of the new task, enhancing its accuracy and performance. Whether it\u0026rsquo;s identifying specific types of objects in images, understanding the subtleties of natural language in a particular context, or diagnosing medical conditions from scans, fine-tuning enables pre-trained models to become specialized tools capable of tackling a wide range of applications.\nFine-tuning begins with a pre-trained model—a model that has already learned a vast array of features and patterns from a comprehensive dataset, often spanning millions of examples. This model, equipped with a deep understanding of various data representations, serves as a robust starting point. The fine-tuning process then adapts this model to a specific task by continuing the training process on a smaller, task-specific dataset. This additional training phase is typically shorter and requires significantly fewer data and computational resources than training a model from scratch, as the model already possesses a foundational knowledge base.\nOne of the key aspects of fine-tuning is its efficiency in data utilization. Since the model has already learned general features and patterns, the fine-tuning process can achieve high performance with relatively small datasets. This characteristic is particularly valuable in domains where collecting large amounts of labeled data is challenging or expensive.\nTraining from scratch is the complete opposite of fine-tuned pre-trained models, as it involves starting with randomly initialized parameters and requires a substantial dataset specific to the task at hand, along with considerable computational resources and time to achieve comparable performance to a fine-tuned pre-trained model. While training from scratch can be beneficial in certain scenarios where highly specialized knowledge is required or when a suitable pre-trained model is not available, the efficiency and effectiveness of leveraging pre-trained models are nowadays undeniable.\nTransferability Transferability caracterizes the ability of pre-trained models to run on downstream tasks without performing fine-tuning, but achieving comparable results. Models that exhibit high transferability are those that have learned generalizable features during pre-training—features that are not overly specific to the training data but that capture universal patterns or structures present across different datasets and domains.\nBeside, transferability arises as an attempt of improvement in scalable AI, as it enables researchers and practitioners to build upon existing knowledge without reinventing the wheel for every new task. This characteristic is especially crucial in our current case where data is abundant, but labeled data is scarce or expensive to obtain. Transferable models can leverage unlabeled data from similar domains, or even entirely different domains, to achieve impressive results with minimal effort.\nMoreover, the pursuit of enhancing transferability has led to innovations in model architecture, training strategies, and domain adaptation techniques. Few-shot learning for instance, where models learn from a very small amount of labeled data, and zero-shot learning, where models apply their knowledge to tasks they have not explicitly been trained on.\nThe concept of transferability also intersects with ethical AI development, as it encourages the use of more generalizable models that can perform equitably across diverse datasets and demographics, reducing the risk of biased or unfair outcomes.\nWhy measuring transferability? Fine-tuning pre-trained models works as follows. First, you pick a downstream task, for which you have at your disposal several pre-trained models candidates. You want to compare their performances to pick the best one on test set, with the optimal fine-tuning configuration. Then, you have to fine-tune each of them. Even if the dataset to train on is smaller, thanks to fine-tuning, you have to repeat it for all your models candidates, and one does not want that, as it can quickly become computationnally expensive.\nTransferability estimation arises as a solution to anticipate and avoid unnecessary fine-tuning, by ranking the performances of pre-trained models on a downstream task without any fine-tuning. Having a benchmark on the pre-trained models\u0026rsquo; transferability would allow you to pick the relevant ones for your own downstream task.\nThis measure is also in line with frugality in AI, which means using limited resources at every step of the Machine Learning lifecycle, while maintaining an acceptable accuracy. This frugality is especially relevant for small and medium-sized enterprises (SMEs) or startups, which may not have the vast computational resources that larger corporations possess. Transferable models democratize access to advanced AI capabilities, enabling these smaller entities to innovate and compete effectively. Frugality in AI also speaks to the broader goal of creating models that are not only powerful but also lean and efficient. Models with high transferability can achieve excellent performance across multiple tasks using significantly less data and fewer computational resources. This efficiency reduces the carbon footprint of training models and makes AI more accessible to a wider range of users and applications.\nNeural Collapse Neural Collapse happens when training beyond 0 training error, i.e training error is at 0 while pushing training loss approaching 0 even further down. Imagine training a deep neural network on a dataset for a classification task. As the training process nears its end—particularly when the model is trained to a point of perfect or near-perfect classification accuracy on the training data. Intuitively, one would expect a highly overfitted and noisy model. Instead, a remarkable simplification occurs in the way the model represents the data, as it was shown in [2]. This training approach offers better generalization performance, better robustness, and better interpretability.\nNeural Collapse is characterized by three distinct proxies:\nWithin-Class Variability Collapse: for any given class, the feature vectors of all samples converge to a singular point or a tightly compact cluster in the high-dimensional feature space. This collapsing effect reduces the within-class variance to near zero, meaning that all samples of a class are represented almost identically from the model\u0026rsquo;s perspective ; Simplex Encoded Label Interpolation (SELI) geometry: measures the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. The higher the rank, the smaller the difference, the closer to Neural Collapse ; Nearest Center Classifier: ensures that the means of the collapsed points for different classes are maximally separated in the feature space. Let\u0026rsquo;s look at this visual example of neural collapse :\nWhere :\nThe Green Balls represent the coordinates of a simplex equiangular tight frame (ETF). The Red Lines represent the Final Layer Classifier. The direction of the sticks indicates the orientation of its decision boundaries, while the ball-end represents the centroid in the feature space used for classification. The Blue Lines represent the class means of the activations in the last hidden layer. The sticks show the variance around these means. The Small Blue Balls represent the last hidden layer activations. It shows how data points from each class are distributed around the class means, forming tight clusters. Initially these elements are all scattered, but as training progresses and neuronal collapse occurs, at each epoch, they move and converged gradually as shown in the GIF.\nWhy choosing Neural Collapse proxies? Let\u0026rsquo;s go back to imagining you have to perform a downstream task, and to do so you have to measure transferability between pre-trained models candidates. The three Neural Collapse proxies were previously defined, but we did not mention yet the three model\u0026rsquo;s aspects that are crucial to evaluate when choosing one:\nGeneralization: through Within-Class Variability Collapse, we gain insight into a model\u0026rsquo;s ability to generalize ; Interpretability: the convergence toward SELI geometry not only enhances the model\u0026rsquo;s interpretability but also its alignment with optimal data representation structures. This alignment signifies a model\u0026rsquo;s capacity to distill and encode information in a way that mirrors the inherent structure of the data itself ; Robustness: the Nearest Center Classifier proxy underscores a model\u0026rsquo;s robustness. By ensuring that class means are well-separated, the model demonstrates resilience against noise and variability in data. Authors in [3] demonstrate both theoretically and empirically that Neural Collapse not only generalizes to new samples from the same classes seen during training but also, and more crucially, to entirely new classes. Also, a more recent research [4] proposes a fine-tuning method based on Neural Collapse that achieves even better performance while reducing fine-tuning parameters by at least 70% !\nThe NCTI Given these promising results, the authors developed a transferability estimation metric : the Neural Collapse Transferability Index (NCTI). This metric measures the proximity between the current state of a pre-trained model and its final fine-tuning stage on target, using the three neural collapse proxies defined above : Within-Class Variability Collapse, SELI geometry and Nearest Center Classifier. For each of them, a score is established : $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$. These three scores are then grouped together using normalization to prevent one score from dominating due to different scales. The final transferability estimation metric is obtained by adding the normalized scores:\n$$ S^m_{total} = S^m_{vc}(H^m) + S^m_{seli}(H^m) + S^{m}_{ncc}(H^m) $$\nWhere $H_m$ is the feature extracted by the $m$-th pre-trained model (after ranking a set of $M$ pre-trained models).\nThe higher the score $S^m_{total}$, the better the transferability of the model for target dataset.\nLet\u0026rsquo;s detail the scores $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$:\nWithin-Class Variability Collapse The authors noticed that larger singular values indicate higher within-class variability because the features within the class exhibit significant variation from the mean, which is desirable for effective feature representation. But since singular value decomposition (SVD) is computationally expensive for large matrices, the nuclear norm which calculates the sum of singular values in a less expensive way was used. Additionally, as feature spaces are high dimensionnal, noise may appear and affect the calculation of variability. Therefore, instead of using the feature matrix $H^m_c$ directly, the classwise logits $Z^m_c$ are substituted to calculate the feature variability.\nThus, the score $S_{vc}$ is calculated as follow :\n$$ S^m_{vc}(H^m) = - \\sum_{c=1}^{C} ||Z^m_c||_* $$\nWhere $Z^m_c$ denotes the logits of the $c$-th class extracted by the $m$-th model.\nThe higher the score $S_{vc}$, the higher the within-class variability, which means that the pre-trained model is closer to the final fine-tuning stage.\nSELI geometry SELI geometry is a concept proposed in [6] as a generalized geometric structure version of the simplex equiangular tight frame (ETF). ETF is defined in the context of the phenomenon of neuronal collapse, but it is limited to balanced datasets. In contrast, SELI extends this concept to both balanced and unbalanced datasets. Difference between the two geometries is shown in the figure below :\nEmbeddings $H$ (in blue) and classifiers $W$ (in red) follow the SELI geometry if :\n$$ W^T W \\alpha V \\Lambda V^T, H^T H \\alpha U \\Lambda U^T \\text{and} W^T H \\alpha \\hat{Z} $$\nWhere $\\hat{Z} = V \\Lambda U^T$ is the SEL matrix [6]. $U$ and $V$ denote the left and right singular vector matrix of $\\hat{Z}$. $\\Lambda$ represents the diagonal singular value matrix.\nA method to assess the SELI geometry structure involves computing the difference between the logits $Z^m$ extracted from the pre-trained model and the optimal logits $\\hat{Z}$. However, obtaining $Z^m$ directly without fine-tuning on the target dataset is time-consuming. Therefore, features $H^m$ of the model are extracted and their difference is measured to form the SELI structure. The complexity of achieving the optimal logits $\\hat{Z}$ through features $H_m$ is approximated via the nuclear norm.\nThus, the score $S^m_{seli}$ is calculated as :\n$$S^m_{seli}(H^m) = ||H^m||_*$$\nThe higher the score $S^m_{seli}$ the higher the rank of the feature matrix $H_m$, making $Z$ closer to a full rank matrix.\nNearest Center Classifier First, the posterior probability $P(y = c|h)$ for each class $c$ is calculated using Bayes\u0026rsquo; Rule:\n$$ \\log P(y = c|h) = \\frac{1}{2}(h_i - \\mu_c)^T \\Sigma (h_j - \\mu_c) + \\log P(y = c) $$\nWhere:\n$\\mu_c$ is the mean vector for class $c$. $\\Sigma$ is the covariance matrix. $P(y = c)$ is the prior probability of class $c$. $h$ is the feature vector extracted by the pre-trained model. Next, the softmax function is applied to obtain the normalized posterior probability $z^m_{i,c}$ for each class $c$ of the $i$-th sample:\n$$ z^m_{i,c} = \\frac{\\exp(\\log P(y = c|h^m_i))}{\\Sigma ^C_{k=1} \\exp(\\log P(y = k|h^m_i))} $$\nWhere:\n$C$ is the number of classes. $h^m_i$ is the feature vector of the $i$-th sample extracted by the m-th pre-trained model. Finally, the score $S^m_{ncc}$ is computed as the average of the dot product of the normalized posterior probabilities $z^m_i$ and the ground truth labels $y_i$ for all samples:\n$$ S^m_{ncc}(H^m) = \\frac{1}{N} \\Sigma ^N_{i=1} z^m_i \\cdot y_i $$\nWhere:\n$N$ is the number of samples. $y_i$ is the ground truth label of the $i$-th sample (in one-hot encoding). The higher the score $S^{m}_{ncc}(H^m)$, the smaller the deviation to the nearest optimal centroid classifier and therefore the greater the transferability to the target dataset.\nNumerical Experiment To reproduce their experiment, the authors\u0026rsquo; code available on a Github repository was used. A first encountered issue was the required torch and torchvision versions, which are quite old, and thus not always available to install, which was the case here. Fortunately, the most recent versions were compatible with the code. A requirements.txt file would have been welcome.\nA second issue is that there are remaining personal paths in some scripts, which should be replaced by downloading paths to PyTorch source models. As a consequence, the loading method from torch should also be replaced.\nOther issues considering the datasets loading remained unsolved.\nAfter these modifications, it is possible to run the authors\u0026rsquo; experiments on the CIFAR10 dataset for the group of supervised pre-trained models. Consisting of 60 000 32x32 colour images in 10 classes, this dataset is broadly used in benchmarks for image classification. 12 pre-trained models were ran on CIFAR10 to establish a ranking based on their performances in terms of NCTI available below.\nModel NCTI Score ResNet152 2.0 ResNet101 1.799 DenseNet201 1.434 DenseNet169 1.146 ResNet34 0.757 ResNet50 0.709 DenseNet121 0.655 MnasNet1_0 0.031 GoogleNet -0.251 MobileNetV2 -0.444 InceptionV3 -0.732 Results show that the deepest architectures offer the best NCTI scores. The depth of a network is closely related to its ability to learn and represent complex features and patterns from the training data, which contributes to a model\u0026rsquo;s superior transferability. The different performances between ResNet and DenseNet could be attributed to the way DenseNet connects each layer to every other layer in a feed-forward fashion, which, while efficient in parameter use and reducing overfitting, may not capture as complex a feature hierarchy as ResNet. Models like MnasNet, MobileNetV2, and InceptionV3, designed for efficiency and speed with a compromise on depth, understandably score lower in transferability, as reflected by their NCTI scores.\nThen, we evaluated the transferability of the supervised pre-trained models, in terms of weighted Kendall\u0026rsquo; τ, and obtained the exact same result as the one presented in the paper: 0.843.\nIt was not possible for us to run the experiment on the group of self-supervised pre-trained models as the authors\u0026rsquo; code included personal paths, and we were not able to find them online.\nA Github repository with all the necessary modifications from the original code is at your disposal here.\nWhat about source features? Through extensive testing, authors have identified that two specific attributes related to neural collapse, observed in the source features, consistently predicted the model\u0026rsquo;s performance on new tasks. These attributes were the diversity within data categories and the compactness of category representations. Remarkably, models showing higher within-category diversity and more compact category representations in their source features tended to adapt better to new tasks. On the other hand, SELI did not consistently correlate with transferability.\nChallenges Authors did experiments on the effectiveness of each individual component in NCTI. They used the three terms individually and removed them one at a time from the full system, and it turned out that for supervised learning, the NCTI without NCC achieved the best weighted Kendall\u0026rsquo; τ. Instead of having normalized the three NCTI components equally, it could have been interesting to tune hyperparameters. Moreover, the current implementation and validation of NCTI are confined to image classification tasks, suggesting its applicability may be limited to similar types of problems. Future work could extend the method\u0026rsquo;s applicability to a broader range of tasks beyond classification, such as detection or segmentation​​. Pre-trained language models could also be considered to measure their transferability based on Neural Collapse. For example, the Fair Collapse (FaCe) method [7] considers both Computer Vision and Natural Language Processing tasks, using different proxies of Neural Collapse than NCTI, and producing a slightly less good τ on the CIFAR-10 dataset (0.81).\nTakeaways Key points to remember are :\nCalculating model transferability and choosing the optimal pre-trained model is important for reasons of computational cost, environmental impact, and overall performance.\nThe authors have developed a new metric, the Neural Collapse informed Transferability Index (NCTI), which is based on the concept of neural collapse and measures the gap between the current feature geometry and the geometry at the terminal stage after hypothetical fine-tuning on the downstream task.\nThe NCTI metric integrates three aspects equally: SELI geometry, within-class variability, and nearest center classifier.\nThis method is light to compute, enabling rapid evaluation of model transferability.\nEmpirical results demonstrate that the ranking of model transferability has a very strong correlation with the ground truth ranking and compares with state-of-the-art methods, highlighting its effectiveness in selecting pre-trained models for specific tasks.\nIn summary, the development of metrics such as NCTI is crucial for optimizing the use of pre-trained models, considering both performance and associated costs in real-world applications.\nReferences 1. Z. Wang Y.Luo, L.Zheng, Z.Huang, M.Baktashmotlagh (2023), How far pre-trained models are from neural collapse on the target dataset informs their transferabilityWang, ICCV.\n2. V. Papyan,1 , X. Y. Hanb,1 , and D.L. Donoho (2020), Prevalence of neural collapse during the terminal phase of deep learning training, National Academy of Sciences.\n3. Galanti, T., György, A., \u0026amp; Hutter, M. (2021). On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121.\n4. Li, X., Liu, S., Zhou, J., Lu, X., Fernandez-Granda, C., Zhu, Z., \u0026amp; Qu, Q. (2022). Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206.\n5. Vignesh Kothapalli, (2023). Neural Collapse: A Review on Modelling Principles and Generalization. arXiv preprint arXiv:2206.04041.\n6. Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. (2022). Imbalance trouble: Revisiting neural-collapse geometry. arXiv preprint arXiv:2208.05512.\n7. Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang. (2023). Unleashing the power of neural collapse for transferability estimation. arXiv preprint arXiv:2310.05754v1.\nStart writing here !\n",
      "content_html": "\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : Marion Chadal and Julie Massé\u003c/p\u003e\n\u003cp\u003eThis blog post discusses the paper \u0026ldquo;How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability\u0026rdquo; \u003ca href=\"#ref1\"\u003e[1]\u003c/a\u003e. It provides an explanation of it so that you can understand the usefulness of measuring transferability, and a reproduction of the authors\u0026rsquo; experiment so that you can better visualize their methodology.\u003c/p\u003e\n\u003ch1 id=\"pre-trained-models-and-fine-tuning\"\u003ePre-trained models and fine-tuning\u003c/h1\u003e\n\u003cp\u003ePre-trained models are currently one of the most active fields in Machine Learning. They can be found in a wide range of applications, from image recognition and natural language processing to autonomous driving and medical diagnosis. These models are \u0026ldquo;pre-trained\u0026rdquo; on massive datasets, most of the time encompassing millions of examples across diverse domains. The training process leverages Deep Learning algorithms and can take weeks or even months, utilizing powerful computing resources to iteratively adjust the model\u0026rsquo;s parameters until it achieves high accuracy on the training data.\u003c/p\u003e\n\u003cp\u003eThe first purpose of pre-training is to enable the model to learn a broad understanding of the world, capturing intricate patterns, relationships, and features that are not easily discernible. This extensive learning phase allows the model to develop a deep amount of knowledge, which it can then apply to more specific tasks through a process known as fine-tuning.\u003c/p\u003e\n\u003cp\u003eWhat is \u003cstrong\u003efine-tuning\u003c/strong\u003e? It consists in adapting a general-purpose model to perform well on a specific task. This adaptation allows the model to fine-tune its learned features to better align with the nuances of the new task, enhancing its accuracy and performance. Whether it\u0026rsquo;s identifying specific types of objects in images, understanding the subtleties of natural language in a particular context, or diagnosing medical conditions from scans, fine-tuning enables pre-trained models to become specialized tools capable of tackling a wide range of applications.\u003c/p\u003e\n\u003cp\u003eFine-tuning begins with a pre-trained model—a model that has already learned a vast array of features and patterns from a comprehensive dataset, often spanning millions of examples. This model, equipped with a deep understanding of various data representations, serves as a robust starting point. The fine-tuning process then adapts this model to a specific task by continuing the training process on a smaller, task-specific dataset. This additional training phase is typically shorter and requires significantly fewer data and computational resources than training a model from scratch, as the model already possesses a foundational knowledge base.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"/images/ChadalMasse/schema.png\" width=\"600\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eOne of the key aspects of fine-tuning is its efficiency in data utilization. Since the model has already learned general features and patterns, the fine-tuning process can achieve high performance with relatively small datasets. This characteristic is particularly valuable in domains where collecting large amounts of labeled data is challenging or expensive.\u003c/p\u003e\n\u003cp\u003eTraining from scratch is the complete opposite of fine-tuned pre-trained models, as it involves starting with randomly initialized parameters and requires a substantial dataset specific to the task at hand, along with considerable computational resources and time to achieve comparable performance to a fine-tuned pre-trained model. While training from scratch can be beneficial in certain scenarios where highly specialized knowledge is required or when a suitable pre-trained model is not available, the efficiency and effectiveness of leveraging pre-trained models are nowadays undeniable.\u003c/p\u003e\n\u003ch1 id=\"transferability\"\u003eTransferability\u003c/h1\u003e\n\u003cp\u003eTransferability caracterizes the \u003cem\u003eability of pre-trained models to run on downstream tasks without performing fine-tuning, but achieving comparable results\u003c/em\u003e. Models that exhibit \u003cstrong\u003ehigh transferability\u003c/strong\u003e are those that have learned \u003cstrong\u003egeneralizable features\u003c/strong\u003e during pre-training—features that are not overly specific to the training data but that capture universal patterns or structures present across different datasets and domains.\u003c/p\u003e\n\u003cp\u003eBeside, transferability arises as an attempt of improvement in \u003cstrong\u003escalable AI\u003c/strong\u003e, as it enables researchers and practitioners to build upon existing knowledge without reinventing the wheel for every new task. This characteristic is especially crucial in our current case where data is abundant, but labeled data is scarce or expensive to obtain. Transferable models can leverage unlabeled data from similar domains, or even entirely different domains, to achieve impressive results with minimal effort.\u003c/p\u003e\n\u003cp\u003eMoreover, the pursuit of enhancing transferability has led to innovations in model architecture, training strategies, and domain adaptation techniques. \u003cstrong\u003eFew-shot learning\u003c/strong\u003e for instance, where models learn from a very small amount of labeled data, and zero-shot learning, where models apply their knowledge to tasks they have not explicitly been trained on.\u003c/p\u003e\n\u003cp\u003eThe concept of transferability also intersects with \u003cstrong\u003eethical AI\u003c/strong\u003e development, as it encourages the use of more generalizable models that can perform equitably across diverse datasets and demographics, reducing the risk of biased or unfair outcomes.\u003c/p\u003e\n\u003ch1 id=\"why-measuring-transferability\"\u003eWhy measuring transferability?\u003c/h1\u003e\n\u003cp\u003eFine-tuning pre-trained models works as follows. First, you \u003cstrong\u003epick a downstream task\u003c/strong\u003e, for which you have at your disposal several pre-trained models candidates. You want to compare their performances to pick the best one on test set, with the \u003cstrong\u003eoptimal fine-tuning configuration\u003c/strong\u003e. Then, you have to fine-tune each of them. Even if the dataset to train on is smaller, thanks to fine-tuning, you have to repeat it for all your models candidates, and one does not want that, as it can quickly become \u003cstrong\u003ecomputationnally expensive\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eTransferability estimation arises as a solution to anticipate and avoid unnecessary fine-tuning, by \u003cstrong\u003eranking the performances of pre-trained models\u003c/strong\u003e on a downstream task without any fine-tuning. Having a \u003cstrong\u003ebenchmark on the pre-trained models\u0026rsquo; transferability\u003c/strong\u003e would allow you to pick the relevant ones for your own downstream task.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"/images/ChadalMasse/machine-learning-life-cycle.png\" width=\"250\" height=\"250\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eThis measure is also in line with \u003cstrong\u003efrugality in AI\u003c/strong\u003e, which means using limited resources at every step of the Machine Learning lifecycle, while maintaining an acceptable accuracy. This frugality is especially relevant for small and medium-sized enterprises (SMEs) or startups, which may not have the vast computational resources that larger corporations possess. Transferable models democratize access to advanced AI capabilities, enabling these smaller entities to innovate and compete effectively. Frugality in AI also speaks to the broader goal of creating models that are not only powerful but also lean and efficient. Models with high transferability can achieve excellent performance across multiple tasks using significantly less data and fewer computational resources. This efficiency reduces the carbon footprint of training models and makes AI more accessible to a wider range of users and applications.\u003c/p\u003e\n\u003ch1 id=\"neural-collapse\"\u003eNeural Collapse\u003c/h1\u003e\n\u003cp\u003eNeural Collapse happens when training beyond 0 training error, i.e training error is at 0 while pushing training loss approaching 0 even further down. Imagine training a deep neural network on a dataset for a classification task. As the training process nears its end—particularly when the model is trained to a point of perfect or near-perfect classification accuracy on the training data. Intuitively, one would expect a highly overfitted and noisy model. Instead, a remarkable simplification occurs in the way the model represents the data, as it was shown in \u003ca href=\"#ref2\"\u003e[2]\u003c/a\u003e. This training approach offers better \u003cstrong\u003egeneralization\u003c/strong\u003e performance, better \u003cstrong\u003erobustness\u003c/strong\u003e, and better \u003cstrong\u003einterpretability\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eNeural Collapse is characterized by three distinct proxies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWithin-Class Variability Collapse:\u003c/strong\u003e for any given class, the feature vectors of all samples converge to a singular point or a tightly compact cluster in the high-dimensional feature space. This collapsing effect reduces the within-class variance to near zero,  meaning that all samples of a class are represented almost identically from the model\u0026rsquo;s perspective ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSimplex Encoded Label Interpolation (SELI) geometry:\u003c/strong\u003e measures the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. The higher the rank, the smaller the difference, the closer to Neural Collapse ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNearest Center Classifier:\u003c/strong\u003e ensures that the means of the collapsed points for different classes are maximally separated in the feature space.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet\u0026rsquo;s look at this visual example of neural collapse :\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"/images/ChadalMasse/neural_collapse.gif\" width=\"250\" height=\"250\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eWhere :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003eGreen Balls\u003c/strong\u003e  represent the coordinates of a simplex equiangular tight frame (ETF).\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eRed Lines\u003c/strong\u003e represent the Final Layer Classifier. The direction of the sticks indicates the orientation of its decision boundaries, while the ball-end represents the centroid in the feature space used for classification.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eBlue Lines\u003c/strong\u003e represent the class means of the activations in the last hidden layer. The sticks show the variance around these means.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eSmall Blue Balls\u003c/strong\u003e represent the last hidden layer activations. It shows how data points from each class are distributed around the class means, forming tight clusters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInitially these elements are all scattered, but as training progresses and neuronal collapse occurs, at each epoch, they move and converged gradually as shown in the GIF.\u003c/p\u003e\n\u003ch1 id=\"why-choosing-neural-collapse-proxies\"\u003eWhy choosing Neural Collapse proxies?\u003c/h1\u003e\n\u003cp\u003eLet\u0026rsquo;s go back to imagining you have to perform a downstream task, and to do so you have to measure transferability between pre-trained models candidates. The three Neural Collapse proxies were previously defined, but we did not mention yet the three model\u0026rsquo;s aspects that are crucial to evaluate when choosing one:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization:\u003c/strong\u003e through Within-Class Variability Collapse, we gain insight into a model\u0026rsquo;s ability to generalize ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInterpretability:\u003c/strong\u003e the convergence toward SELI geometry not only enhances the model\u0026rsquo;s interpretability but also its alignment with optimal data representation structures. This alignment signifies a model\u0026rsquo;s capacity to distill and encode information in a way that mirrors the inherent structure of the data itself ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobustness:\u003c/strong\u003e the Nearest Center Classifier proxy underscores a model\u0026rsquo;s robustness. By ensuring that class means are well-separated, the model demonstrates resilience against noise and variability in data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAuthors in \u003ca href=\"#ref3\"\u003e[3]\u003c/a\u003e demonstrate \u003cstrong\u003eboth theoretically and empirically\u003c/strong\u003e that Neural Collapse not only generalizes to new samples from the same classes seen during training but also, and more crucially, to entirely new classes. Also, a more recent research \u003ca href=\"#ref4\"\u003e[4]\u003c/a\u003e proposes a fine-tuning method based on Neural Collapse that achieves even better performance while reducing fine-tuning parameters by at least \u003cstrong\u003e70%\u003c/strong\u003e !\u003c/p\u003e\n\u003ch1 id=\"the-ncti\"\u003eThe NCTI\u003c/h1\u003e\n\u003cp\u003eGiven these promising results, the authors developed a transferability estimation metric : the Neural Collapse Transferability Index (NCTI). This metric measures the proximity between the current state of a pre-trained model and its final fine-tuning stage on target, using the three neural collapse proxies defined above : Within-Class Variability Collapse, SELI geometry and Nearest Center Classifier. For each of them, a score is established :  $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$. These three scores are then grouped together using normalization to prevent one score from dominating due to different scales. The final transferability estimation metric is obtained by adding the normalized scores:\u003c/p\u003e\n\u003cp\u003e$$ S^m_{total} = S^m_{vc}(H^m) + S^m_{seli}(H^m) + S^{m}_{ncc}(H^m) $$\u003c/p\u003e\n\u003cp\u003eWhere $H_m$ is the feature extracted by the $m$-th pre-trained model (after ranking a set of $M$ pre-trained models).\u003c/p\u003e\n\u003cp\u003eThe higher the score $S^m_{total}$, the better the transferability of the model for target dataset.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s detail the scores $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$:\u003c/p\u003e\n\u003ch3 id=\"within-class-variability-collapse\"\u003eWithin-Class Variability Collapse\u003c/h3\u003e\n\u003cp\u003eThe authors noticed that larger singular values indicate higher within-class variability because the features within the class exhibit significant variation from the mean, which is desirable for effective feature representation. But since singular value decomposition (SVD) is computationally expensive for large matrices, the nuclear norm which calculates the sum of singular values in a less expensive way was used. Additionally, as feature spaces are high dimensionnal, noise may appear and affect the calculation of variability. Therefore, instead of using the feature matrix $H^m_c$ directly, the classwise logits $Z^m_c$ are substituted to calculate the feature variability.\u003c/p\u003e\n\u003cp\u003eThus, the score $S_{vc}$ is calculated as follow :\u003c/p\u003e\n\u003cp\u003e$$ S^m_{vc}(H^m) = - \\sum_{c=1}^{C} ||Z^m_c||_* $$\u003c/p\u003e\n\u003cp\u003eWhere $Z^m_c$ denotes the logits of the $c$-th class extracted by the $m$-th model.\u003c/p\u003e\n\u003cp\u003eThe higher the score $S_{vc}$, the higher the within-class variability, which means that the pre-trained model is closer to the final fine-tuning stage.\u003c/p\u003e\n\u003ch3 id=\"seli-geometry\"\u003eSELI geometry\u003c/h3\u003e\n\u003cp\u003eSELI geometry is a concept proposed in \u003ca href=\"#ref6\"\u003e[6]\u003c/a\u003e as a generalized geometric structure version of the simplex equiangular tight frame (ETF). ETF is defined in the context of the phenomenon of neuronal collapse, but it is limited to balanced datasets. In contrast, SELI extends this concept to both balanced and unbalanced datasets. Difference between the two geometries is shown in the figure below :\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center; align-items: center;\"\u003e\n    \u003cimg src=\"/images/ChadalMasse/geometry.png\" alt=\"Image 1\" style=\"width: 49%; max-width: 100%; height: auto;\"\u003e\n    \u003cimg src=\"/images/ChadalMasse/neural_network.png\" alt=\"Image 2\" style=\"width: 49%; max-width: 100%; height: auto;\"\u003e\n\u003c/div\u003e\n\u003cp\u003eEmbeddings $H$ (in blue) and classifiers $W$ (in red) follow the SELI geometry if :\u003c/p\u003e\n\u003cp\u003e$$ W^T W \\alpha V \\Lambda V^T,  H^T H \\alpha U \\Lambda U^T \\text{and} W^T H \\alpha \\hat{Z} $$\u003c/p\u003e\n\u003cp\u003eWhere $\\hat{Z} = V \\Lambda U^T$ is the SEL matrix \u003ca href=\"#ref6\"\u003e[6]\u003c/a\u003e. $U$ and $V$ denote the left and right singular vector matrix of $\\hat{Z}$. $\\Lambda$ represents the diagonal singular value matrix.\u003c/p\u003e\n\u003cp\u003eA method to assess the SELI geometry structure involves computing the difference between the logits $Z^m$ extracted from the pre-trained model and the optimal logits $\\hat{Z}$. However, obtaining $Z^m$ directly without fine-tuning on the target dataset is time-consuming. Therefore, features $H^m$ of the model are extracted and their difference is measured to form the SELI structure. The complexity of achieving the optimal logits $\\hat{Z}$ through features $H_m$ is approximated via the nuclear norm.\u003c/p\u003e\n\u003cp\u003eThus, the score $S^m_{seli}$ is calculated as :\u003c/p\u003e\n\u003cp\u003e$$S^m_{seli}(H^m) = ||H^m||_*$$\u003c/p\u003e\n\u003cp\u003eThe higher the score $S^m_{seli}$ the higher the rank of the feature matrix $H_m$, making $Z$ closer to a full rank matrix.\u003c/p\u003e\n\u003ch3 id=\"nearest-center-classifier\"\u003eNearest Center Classifier\u003c/h3\u003e\n\u003cp\u003eFirst, the posterior probability $P(y = c|h)$ for each class $c$ is calculated using Bayes\u0026rsquo; Rule:\u003c/p\u003e\n\u003cp\u003e$$ \\log P(y = c|h) = \\frac{1}{2}(h_i - \\mu_c)^T \\Sigma (h_j - \\mu_c) + \\log P(y = c) $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\mu_c$ is the mean vector for class $c$.\u003c/li\u003e\n\u003cli\u003e$\\Sigma$ is the covariance matrix.\u003c/li\u003e\n\u003cli\u003e$P(y = c)$ is the prior probability of class $c$.\u003c/li\u003e\n\u003cli\u003e$h$ is the feature vector extracted by the pre-trained model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNext, the softmax function is applied to obtain the normalized posterior probability $z^m_{i,c}$ for each class $c$ of the $i$-th sample:\u003c/p\u003e\n\u003cp\u003e$$ z^m_{i,c} = \\frac{\\exp(\\log P(y = c|h^m_i))}{\\Sigma ^C_{k=1} \\exp(\\log P(y = k|h^m_i))} $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$C$ is the number of classes.\u003c/li\u003e\n\u003cli\u003e$h^m_i$ is the feature vector of the $i$-th sample extracted by the m-th pre-trained model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinally, the score $S^m_{ncc}$ is computed as the average of the dot product of the normalized posterior probabilities $z^m_i$ and the ground truth labels $y_i$ for all samples:\u003c/p\u003e\n\u003cp\u003e$$ S^m_{ncc}(H^m) = \\frac{1}{N} \\Sigma ^N_{i=1} z^m_i \\cdot y_i $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$N$ is the number of samples.\u003c/li\u003e\n\u003cli\u003e$y_i$ is the ground truth label of the $i$-th sample (in one-hot encoding).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe higher the score $S^{m}_{ncc}(H^m)$, the smaller the deviation to the nearest optimal centroid classifier and therefore the greater the transferability to the target dataset.\u003c/p\u003e\n\u003ch1 id=\"numerical-experiment\"\u003eNumerical Experiment\u003c/h1\u003e\n\u003cp\u003eTo reproduce their experiment, the authors\u0026rsquo; code available on a \u003ca href=\"https://github.com/BUserName/NCTI/tree/main\"\u003eGithub\u003c/a\u003e repository was used. A first encountered issue was the required \u003ccode\u003etorch\u003c/code\u003e and \u003ccode\u003etorchvision\u003c/code\u003e versions, which are quite old, and thus not always available to install, which was the case here. Fortunately, the  most recent versions were compatible with the code. A \u003ccode\u003erequirements.txt\u003c/code\u003e file would have been welcome.\u003c/p\u003e\n\u003cp\u003eA second issue is that there are remaining personal paths in some scripts, which should be replaced by downloading paths to PyTorch source models. As a consequence, the loading method from \u003ccode\u003etorch\u003c/code\u003e should also be replaced.\u003c/p\u003e\n\u003cp\u003eOther issues considering the datasets loading remained unsolved.\u003c/p\u003e\n\u003cp\u003eAfter these modifications, it is possible to run the authors\u0026rsquo; experiments on the CIFAR10 dataset for the group of supervised pre-trained models. Consisting of 60 000 32x32 colour images in 10 classes, this dataset is broadly used in benchmarks for image classification. 12 pre-trained models were ran on CIFAR10 to establish a ranking based on their performances in terms of NCTI available below.\u003c/p\u003e\n\u003ctable style=\"width:100%; border-collapse: collapse;\" border=\"1\"\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align:left; padding: 8px;\"\u003eModel\u003c/th\u003e\n      \u003cth style=\"text-align:left; padding: 8px;\"\u003eNCTI Score\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet152\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e2.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet101\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.799\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet201\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.434\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet169\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.146\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet34\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.757\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet50\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.709\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet121\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.655\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eMnasNet1_0\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.031\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eGoogleNet\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.251\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eMobileNetV2\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.444\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eInceptionV3\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.732\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eResults show that the deepest architectures offer the best NCTI scores.  The depth of a network is closely related to its ability to learn and represent complex features and patterns from the training data, which contributes to a model\u0026rsquo;s superior transferability. The different performances between ResNet and DenseNet could be attributed to the way DenseNet connects each layer to every other layer in a feed-forward fashion, which, while efficient in parameter use and reducing overfitting, may not capture as complex a feature hierarchy as ResNet. Models like MnasNet, MobileNetV2, and InceptionV3, designed for efficiency and speed with a compromise on depth, understandably score lower in transferability, as reflected by their NCTI scores.\u003c/p\u003e\n\u003cp\u003eThen, we evaluated the transferability of the supervised pre-trained models, in terms of weighted Kendall\u0026rsquo; τ, and obtained the exact same result as the one presented in the paper: \u003cstrong\u003e0.843\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIt was not possible for us to run the experiment on the group of self-supervised pre-trained models as the authors\u0026rsquo; code included personal paths, and we were not able to find them online.\u003c/p\u003e\n\u003cp\u003eA Github repository with all the necessary modifications from the original code is at your disposal \u003ca href=\"https://github.com/marionchadal/NCTI\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"what-about-source-features\"\u003eWhat about source features?\u003c/h1\u003e\n\u003cp\u003eThrough extensive testing, authors have identified that two specific attributes related to neural collapse, observed in the source features, consistently predicted the model\u0026rsquo;s performance on new tasks. These attributes were the diversity within data categories and the compactness of category representations. Remarkably, models showing higher within-category diversity and more compact category representations in their source features tended to adapt better to new tasks. On the other hand, SELI did not consistently correlate with transferability.\u003c/p\u003e\n\u003ch1 id=\"challenges\"\u003eChallenges\u003c/h1\u003e\n\u003cp\u003eAuthors did experiments on the effectiveness of each individual component in NCTI. They used the three terms individually and removed them one at a time from the full system, and it turned out that for supervised learning, the NCTI without NCC achieved the best weighted Kendall\u0026rsquo; τ. Instead of having normalized the three NCTI components equally, it could have been interesting to tune hyperparameters.\nMoreover, the current implementation and validation of NCTI are confined to image classification tasks, suggesting its applicability may be limited to similar types of problems. Future work could extend the method\u0026rsquo;s applicability to a broader range of tasks beyond classification, such as detection or segmentation​​. Pre-trained language models could also be considered to measure their transferability based on Neural Collapse. For example, the Fair Collapse (FaCe) method \u003ca href=\"#ref7\"\u003e[7]\u003c/a\u003e considers both Computer Vision and Natural Language Processing tasks, using different proxies of Neural Collapse than NCTI, and producing a slightly less good τ on the CIFAR-10 dataset (0.81).\u003c/p\u003e\n\u003ch1 id=\"takeaways\"\u003eTakeaways\u003c/h1\u003e\n\u003cp\u003eKey points to remember are :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCalculating model transferability and choosing the optimal pre-trained model is important for reasons of computational cost, environmental impact, and overall performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe authors have developed a new metric, the \u003cstrong\u003eNeural Collapse informed Transferability Index (NCTI)\u003c/strong\u003e, which is based on the concept of \u003cstrong\u003eneural collapse\u003c/strong\u003e and measures \u003cem\u003ethe gap between the current feature geometry and the geometry at the terminal stage after hypothetical fine-tuning on the downstream task.\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe NCTI metric integrates three aspects equally: \u003cstrong\u003eSELI geometry\u003c/strong\u003e, \u003cstrong\u003ewithin-class variability\u003c/strong\u003e, and \u003cstrong\u003enearest center classifier\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis method is \u003cem\u003elight to compute\u003c/em\u003e, enabling rapid evaluation of model transferability.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEmpirical results demonstrate that \u003cem\u003ethe ranking of model transferability has a very strong correlation with the ground truth ranking\u003c/em\u003e and \u003cstrong\u003ecompares with state-of-the-art methods\u003c/strong\u003e, highlighting its effectiveness in selecting pre-trained models for specific tasks.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn summary, the development of metrics such as NCTI is crucial for optimizing the use of pre-trained models, considering both performance and associated costs in real-world applications.\u003c/p\u003e\n\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003cp\u003e\u003ca id=\"ref1\"\u003e\u003c/a\u003e1. Z. Wang Y.Luo, L.Zheng, Z.Huang, M.Baktashmotlagh (2023), How far pre-trained models are from neural collapse on the target dataset informs their transferabilityWang, ICCV.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref2\"\u003e\u003c/a\u003e2. V. Papyan,1 , X. Y. Hanb,1 , and D.L. Donoho (2020), Prevalence of neural collapse during the terminal phase of deep learning training, National Academy of Sciences.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref3\"\u003e\u003c/a\u003e3. Galanti, T., György, A., \u0026amp; Hutter, M. (2021). On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref4\"\u003e\u003c/a\u003e4. Li, X., Liu, S., Zhou, J., Lu, X., Fernandez-Granda, C., Zhu, Z., \u0026amp; Qu, Q. (2022). Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref5\"\u003e\u003c/a\u003e5. Vignesh Kothapalli, (2023). Neural Collapse: A Review on Modelling Principles and Generalization. arXiv preprint arXiv:2206.04041.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref6\"\u003e\u003c/a\u003e6. Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. (2022). Imbalance trouble: Revisiting neural-collapse\ngeometry. arXiv preprint arXiv:2208.05512.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref7\"\u003e\u003c/a\u003e7. Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang. (2023). Unleashing the power of neural collapse for transferability estimation. arXiv preprint arXiv:2310.05754v1.\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\n\u003cp\u003eStart writing here !\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/transferability/",
      "date_published": "8016-08-09T126:88:00+01:00",
      "date_modified": "8016-08-09T126:88:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    }
    
  ]
}