<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Liste - http://localhost:1313/">
    <title>MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais | Bloggin on Responsible AI</title>
    <meta name="description" content="Bloggin on Responsible AI">
    <meta property="og:url" content="http://localhost:1313/posts/mano-mania/">
  <meta property="og:site_name" content="Bloggin on Responsible AI">
  <meta property="og:title" content="MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais">
  <meta property="og:description" content="MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais Authors: Alice Devilder, Sibylle Degos | Affiliations: IP Paris, Responsible AI | Published: 2025-02-10">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-10T18:25:03+01:00">
    <meta property="article:modified_time" content="2025-02-10T18:25:03+01:00">

    
  <meta itemprop="name" content="MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais">
  <meta itemprop="description" content="MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais Authors: Alice Devilder, Sibylle Degos | Affiliations: IP Paris, Responsible AI | Published: 2025-02-10">
  <meta itemprop="datePublished" content="2025-02-10T18:25:03+01:00">
  <meta itemprop="dateModified" content="2025-02-10T18:25:03+01:00">
  <meta itemprop="wordCount" content="2285">
    
    <link rel="canonical" href="http://localhost:1313/posts/mano-mania/">
    <link rel="icon" href="http://localhost:1313//assets/favicon.ico">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
    <link rel="alternate" type="application/atom+xml" title="Bloggin on Responsible AI" href="http://localhost:1313//atom.xml" />
    <link rel="alternate" type="application/json" title="Bloggin on Responsible AI" href="http://localhost:1313//feed.json" />
    <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
    
    
    <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%} </style>
  
    
  
  
  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "BlogPosting",
      "articleSection": "posts",
      "name": "MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais",
      "headline": "MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais",
      "alternativeHeadline": "",
      "description": "\u003c!-- Custom CSS for MathJax and Tables --\u003e\r\n\u003cstyle type=\u0022text\/css\u0022\u003e\r\ncode.has-jax { \r\n    font: inherit;\r\n    font-size: 100%; \r\n    background: inherit; \r\n    border: inherit;\r\n}\r\n\r\ntable {\r\n    border-collapse: collapse;\r\n    width: 100%;\r\n}\r\nth, td {\r\n    padding: 8px;\r\n    text-align: center;\r\n    border-bottom: 1px solid #ddd;\r\n}\r\nth {\r\n    background-color: #f2f2f2;\r\n}\r\ntr:hover {\r\n    background-color: #f5f5f5;\r\n}\r\n\u003c\/style\u003e\r\n\u003c!-- MathJax Configuration --\u003e\r\n\u003cscript type=\u0022text\/x-mathjax-config\u0022\u003e\r\nMathJax.Hub.Config({\r\n    tex2jax: {\r\n        inlineMath: [[\u0027$\u0027,\u0027$\u0027], [\u0027\\\\(\u0027,\u0027\\\\)\u0027]],\r\n        displayMath: [[\u0027$$\u0027,\u0027$$\u0027], [\u0027\\\\[\u0027,\u0027\\\\]\u0027]],\r\n        skipTags: [\u0027script\u0027, \u0027noscript\u0027, \u0027style\u0027, \u0027textarea\u0027, \u0027pre\u0027] \/\/ Removed \u0027code\u0027 entry\r\n    }\r\n});\r\nMathJax.Hub.Queue(function() {\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n    for(i = 0; i \u003c all.length; i \u002b= 1) {\r\n        all[i].SourceElement().parentNode.className \u002b= \u0027 has-jax\u0027;\r\n    }\r\n});\r\n\u003c\/script\u003e\r\n\u003c!-- Load MathJax --\u003e\r\n\u003cscript type=\u0022text\/javascript\u0022 \r\n    src=\u0022https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.7\/MathJax.js?config=TeX-AMS_HTML-full\u0022\u003e\r\n\u003c\/script\u003e\r\n\u003ch1 style=\u0022font-size: 28px; text-align: center;\u0022\u003e MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais \u003c\/h1\u003e\r\n\u003cstyle\u003e\r\n.hr-line {\r\n    border: none;\r\n    height: 2px;\r\n    background-color: black;\r\n    margin: 10px 0;\r\n}\r\n\u003c\/style\u003e\r\n\u003chr class=\u0022hr-line\u0022\u003e\r\n\u003cp\u003e\u003cstrong\u003eAuthors:\u003c\/strong\u003e Alice Devilder, Sibylle Degos | \u003cstrong\u003eAffiliations:\u003c\/strong\u003e IP Paris, Responsible AI | \u003cstrong\u003ePublished:\u003c\/strong\u003e 2025-02-10\u003c\/p\u003e",
      "inLanguage": "en-us",
      "isFamilyFriendly": "true",
      "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "http:\/\/localhost:1313\/posts\/mano-mania\/"
      },
      "author" : {
          "@type": "Person",
          "name": ""
      },
      "creator" : {
          "@type": "Person",
          "name": ""
      },
      "accountablePerson" : {
          "@type": "Person",
          "name": ""
      },
      "copyrightHolder" : "Bloggin on Responsible AI",
      "copyrightYear" : "2025",
      "dateCreated": "2025-02-10T18:25:03.00Z",
      "datePublished": "2025-02-10T18:25:03.00Z",
      "dateModified": "2025-02-10T18:25:03.00Z",
      "publisher":{
          "@type":"Organization",
          "name": "Bloggin on Responsible AI",
          "url": "http://localhost:1313/",
          "logo": {
              "@type": "ImageObject",
              "url": "http:\/\/localhost:1313\/assets\/favicon.ico",
              "width":"32",
              "height":"32"
          }
      },
      "image": "http://localhost:1313/assets/favicon.ico",
      "url" : "http:\/\/localhost:1313\/posts\/mano-mania\/",
      "wordCount" : "2285",
      "genre" : [ ],
      "keywords" : [ ]
  }
  </script>
  
  
  </head>

<body>
  <a class="skip-link" href="#main">Skip to main</a>
  <main id="main">
  <div class="content">
    <header>
<p style="padding: 0;margin: 0;">
  <a href="../../">
    <b>Bloggin on Responsible AI</b>
    <span class="text-stone-500 animate-blink">‚ñÆ</span>
  </a>
</p>
<ul style="padding: 0;margin: 0;">
  
  
  <li class="">
    <a href="../../posts/"><span>Post</span></a>
    
  <li class="">
    <a href="../../tutorial/"><span>Tutorial</span></a>
    
  <li class="">
    <a href="../../about/"><span>About</span></a>
    
  <li class="">
    <a href="../../articles/"><span>Articles</span></a>
    
  </li>
</ul>
</header>
<hr class="hr-list" style="padding: 0;margin: 0;">
    <section>
      <h2 class="post">MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais</h2>
      <!-- Custom CSS for MathJax and Tables -->
<style type="text/css">
code.has-jax { 
    font: inherit;
    font-size: 100%; 
    background: inherit; 
    border: inherit;
}

table {
    border-collapse: collapse;
    width: 100%;
}
th, td {
    padding: 8px;
    text-align: center;
    border-bottom: 1px solid #ddd;
}
th {
    background-color: #f2f2f2;
}
tr:hover {
    background-color: #f5f5f5;
}
</style>
<!-- MathJax Configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // Removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<!-- Load MathJax -->
<script type="text/javascript" 
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full">
</script>
<h1 style="font-size: 28px; text-align: center;"> MaNo: A Smarter Way to Estimate Model Accuracy to Face Distribution Shifts Biais </h1>
<style>
.hr-line {
    border: none;
    height: 2px;
    background-color: black;
    margin: 10px 0;
}
</style>
<hr class="hr-line">
<p><strong>Authors:</strong> Alice Devilder, Sibylle Degos | <strong>Affiliations:</strong> IP Paris, Responsible AI | <strong>Published:</strong> 2025-02-10</p>
<hr class="hr-line">
<h2 id="table-of-contents"><strong>Table of Contents</strong></h2>
<ul>
<li><a href="#section-0.0">Introduction</a>
<ul>
<li><a href="#section-0.1">Why Do Logits Matter For Generalization Performance?</a></li>
<li><a href="#section-0.2">Why Does softmax normalisation fail to alleviate the overconfidence issues of logits-based methods?</a></li>
</ul>
</li>
<li><a href="#section-1">Introducing MANO: A Two-Step Approach</a>
<ul>
<li><a href="#section-1.1">Normalization with Softrun</a></li>
<li><a href="#section-1.2">Aggregation Using Matrix Norms</a></li>
</ul>
</li>
<li><a href="#section-2">Empirical Success: MANO vs. Baselines</a></li>
<li><a href="#section-3">Applications and Future Directions</a></li>
<li><a href="#section-4">Conclusion</a></li>
</ul>
<p>This is a blog post about the paper <em><strong>MaNo: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts</strong></em>, published by <em>Renchunzi Xie</em>, <em>Ambroise Odonnat</em>, <em>Vasilii Feofanov</em>, <em>Weijian Deng</em>, <em>Jianfeng Zhang</em> and <em>Bo An</em> in November 2024 and avalaible on <a href="https://arxiv.org/abs/2405.18979">arXiv</a>.</p>
<hr>
<p>Usually, in machine learning, data is divided into train/test sets. But you already know that! The problem is that there is often a shift in the way data is distributed or collected between train and test set. This shift distribution can disrupt predictive models, and can be a risk for AI safety.
To illustrate, imagine a pedestrian image recognition model used for autonomous cars. Trained on images of pedestrians during the day, fails to detect pedestrians at night due to a shift in data distribution, leading to accidents.
Traditional approaches to evaluate models rely on costly, computationally expensive ground-truth labels, making the evaluation difficult. Thanks to the solution in the paper ‚Äú<em><strong>MANO: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts</strong></em>‚Äù, we can estimate model accuracy without labeled test data. MANO (Matrix Norm-based Accuracy Estimation) is presented as a novel solution, leveraging logits (the raw outputs of a model) to infer confidence and predict accuracy in an unsupervised manner. It is splitted in two steps:</p>
<ul>
<li><strong>Softrun</strong> normalization to calibrate logits</li>
<li><strong>Lp norm</strong> to quantify decision boundary distances.</li>
</ul>
<p>Let‚Äôs deep into MANO ! You will understand the theoretical foundations and empirical success that make this method very interesting to estimate the accuracy in an unsupervised environment.</p>
<hr>
<h2 id="section-0.0"><strong>Introduction</strong></h2>
<p>A common method for estimating accuracy without labels is analyzing a model‚Äôs <strong>logits</strong>, the raw outputs before softmax. However, existing methods suffer from overconfidence issues and biased predictions under distribution shifts.</p>
<h3 id="section-0.1"><strong>Let‚Äôs understand why Logits Matter For Generalization Performance !</strong></h3>
<p><strong>Logits</strong> represent the raw values generated by a model before they are passed through a normalization function, such as softmax. In simple terms, in classification tasks, logits are the <strong>raw scores</strong> associated with each class that help evaluate the model&rsquo;s performance. These scores are particularly important for measuring a model&rsquo;s generalization ability, i.e., its capacity to make accurate predictions on new, unseen data.</p>
<p>Mathematically, for a given input $x$, the model computes logits as:</p>
<p>$$ q = f(x) = (\omega_k^T z)_k \in \mathbb{R}^K $$</p>
<p>where $z$ is the learned feature representation, $\omega_k$ is the classifier‚Äôs weight vector, and $K$ is the number of classes. The magnitude of logits correlates with the distance to decision boundaries, making them valuable for accuracy estimation.</p>
<p>The concept of logits is based on the <strong>low-density separation</strong> assumption. According to this assumption, data points located near the decision boundaries of the model (i.e., where the model is uncertain) are more likely to be misclassified. This means that a model may struggle to make reliable predictions for these ambiguous examples. By analyzing logits, we can gain insights into the model&rsquo;s confidence levels and its ability to generalize to unseen data.</p>
<p>Now you understand that logits are very important for generalisation performance, but one question remains&hellip;</p>
<h3 id="section-0.2"><strong>Why Does softmax normalisation fail to alleviate the overconfidence issues of logits-based methods?</strong></h3>
<p><strong>Softmax normalization</strong> is commonly used to transform these logits into <strong>class probabilities</strong>, which makes the predictions interpretable. By applying softmax, logits are converted into values between 0 and 1, representing the probability that each class is correct.</p>
<p style="display: inline-block; vertical-align: middle;">
    Mathematically, softmax is defined as:
</p>
<figure id="my-fig" class="numbered" style="display: inline-block; vertical-align: middle; margin-left: 10px;">
    <img src="../../images/Mano/softmax_img.png" class="align-center" style="width: 250px; height: auto;">
</figure>
<p>However, this approach has a major issue: it is <strong>sensitive to prediction bias</strong> and can lead to <strong>overconfidence</strong>. In other words, if a model generates very high logits for a class (indicating strong confidence in its prediction), but that prediction is incorrect, it can skew the results. This phenomenon is largely due to the <strong>exponential function</strong> in the softmax formula, which amplifies the differences between logits. This can lead to significant errors, especially when the model is overly confident without being accurate.</p>
<p>This overconfidence bias is a critical issue when evaluating a model‚Äôs performance.
To address this challenge, the paper introduces <strong>MANO</strong>, a novel method that leverages logits to estimate model accuracy without labeled data.</p>
<h2 id="section-1"><strong>Introducing MANO: A Two-Step Approach</strong></h2>
<p>MANO addresses these challenges through a two-step process: <strong>Normalization with Softrun</strong> and <strong>Aggregation using Matrix Norms</strong>. Here is a scheme so you can visualize the process :</p>
<p><img
  src="../../images/Mano/Mano_schema.png"
  alt="Mano schema"
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<h3 id="section-1.1"><strong>1. Normalization with Softrun</strong></h3>
<p>As explained before, Softmax is a very common activation function to transform logits into probabilities. But its exponential nature exaggerates differences between logits, making the model appear more confident than it actually is.</p>
<p>First, let‚Äôs calculate $\Phi(\mathcal{D}_{test})$, a function that measures the amount of complete information on the logits.</p>
<p>$\Phi(\mathcal{D}_{test}) =$</p>
<p>$$ -\frac{1}{NK} \sum_{j=1}^{N} \sum_{k=1}^{K} \log \left(\frac{\exp(q_{j,k})}{\sum_{j=1}^{K} \exp(\mathbf{q}_{i,j})} \right) $$</p>
<p>The general formula has the same structure of the softmax but with a dynamic $v(\mathbf{q}_i)$:</p>
<p>$$ \sigma(q_i) = \frac{v(q_i)}{\sum_{k=1}^{K} v(q_i)_k} \in \Delta_K$$</p>
<p>Thanks to $\Phi(\mathcal{D}_{test})$, it will determine whether to apply a Taylor or softmax normalization term. The function $v(q)$ is defined as:</p>
<figure id="my-fig_eq_v" class="numbered" >
    <img src="../../images/Mano/equation_v.png" class="align-center">
    <p style="text-align: center;"></p>
</figure>
<p>When the model‚Äôs predictions are unreliable, Softrun applies a Taylor approximation rather than the softmax. The Taylor approximation smooths out the effect of large logits, preventing the model from being overly confident in any particular prediction. By contrast, when the dataset is well-calibrated, the function behaves like softmax, preserving probability distributions where confidence is warranted.</p>
<figure id="my-fig" class="numbered" style="float: left; margin-left: 10px; width: 50%;">
    <img src="../../images/Mano/Lp_norm_schema.png" class="align-center" style="width: 100%; height: auto;">
    <p style="text-align: center;"></p>
</figure>
<p>To recapitulate, there are 3 studied cases :</p>
<p><strong>Case 1 - High Confidence, Low Bias</strong> When the model is both confident and has a low bias, its logits are highly reliable. This is an ideal case where we can safely apply softmax normalization without worrying about introducing additional bias. The softmax probabilities will be well-calibrated, and no extra correction is needed.</p>
<p><strong>Case 2 - Low Confidence, High Bias</strong> If the model is not confident in its predictions and shows high bias, it means that its predictions are skewed but also uncertain. In this situation, we use the Taylor normalization. The smooth properties of Taylor normalization help mitigate bias while maintaining better uncertainty estimation.</p>
<p><strong>Case 3 - Grey zone</strong> Sometimes, the model&rsquo;s behavior doesn‚Äôt fit neatly into one category. In this scenario, different examples fall into different cases, making it difficult to determine the best normalization method. In these cases, it is safer to use Taylor normalization because it avoids exacerbating bias in the same way softmax does.</p>
<p>Besides, the output of this first step is scaled logits: $Q_i = \sigma(q_i) \in \Delta_K$.</p>
<h3 id="section-1.2"><strong>2. Aggregation Using Matrix Norms</strong></h3>
<p>After normalization, MANO <strong>aggregates</strong> the logits using the <strong>Lp norm</strong> of the matrix $Q$, defined as:</p>
<figure id="my-fig" class="numbered" >
    <img src="../../images/Mano/equation_s.png" class="align-center">
    <p style="text-align: center;"></p>
</figure>
<p>where:</p>
<ul>
<li>$Q$ represents the matrix of softmax probabilities,</li>
<li>$N$ is the number of test samples,</li>
<li>$K$ is the number of classes,</li>
<li>$œÉ(q_i)_k$ is the result of the step of normalisation seen before,</li>
<li>$p$ is a hyperparameter controlling the sensitivity of the aggregation.</li>
</ul>
<p>The normalization factor $\frac{1}{^p\sqrt{NK}}$‚Äã ensures that $S(f,D_{test})$ is independent of dataset size and number of classes, providing a standardized metric across different test distributions.</p>
<p><strong>Advantages of Using the Lp‚Äã Norm</strong>
One of the main advantages of the Lp‚Äã norm over the Nuclear Norm is its <strong>computational efficiency</strong>. The Lp‚Äã norm is straightforward to compute, requiring only element-wise operations and summations. In contrast, nuclear norm-based methods involve Singular Value Decomposition (SVD), which is computationally expensive and less scalable for large datasets.</p>
<p><strong>Effect of p on Aggregation Sensitivity</strong></p>
<figure id="my-fig" class="numbered" style="float: right; margin-right: 10px; width: 45%;">
    <img src="../../images/Mano/impact_Lp_norm.png" class="align-center" style="width: 100%; height: auto;">
    <p style="text-align: center;"></p>
</figure>
<p>In the formula, the parameter p controls the sensitivity of the metric to high-confidence predictions:</p>
<ul>
<li>For <strong>small p</strong>, all predictions contribute relatively equally to the final score.</li>
<li>As <strong>p increases</strong>, the aggregation puts more emphasis on confident predictions (i.e., softrun values close to 1).</li>
<li>In the case <strong>p‚Üí‚àû</strong>, the norm becomes equivalent to the maximum prediction confidence.</li>
</ul>
<p>In practice, the authors of the paper conducted a sensitivity analysis on 5 datasets using ResNet-18 and found that $p=4$ provides the best balance between capturing model confidence and maintaining robustness. This is illustrated in the experimental results on the right.</p>
<p>Now, let‚Äôs dive into the empirical results of MaNo across various datasets.</p>
<h2 id="section-2"><strong>Empirical Success: MANO vs. Baselines</strong></h2>
<p>MANO has been evaluated against <strong>11 baseline methods</strong>, including Rotation Prediction (Rotation) <a href="#rotation">[2]</a>, Averaged Confidence (ConfS core) <a href="#confscore">[3]</a> and Entropy <a href="#entropy">[4]</a> amongst others. To show the versatility of Mano across different architectures, it has been evaluated across 3 different neural network architectures: ResNet18, ResNet50 <a href="#resnet50">[5]</a>, and WRN-50-2. The experiments were conducted on a range of classification tasks, including image recognition benchmarks such as CIFAR-10, CIFAR-100, TinyImageNet, and ImageNet, as well as domain adaptation datasets like PACS and Office-Home.</p>
<p>In this comprehensive evaluation, the authors have considered 3 types of distribution shifts: <strong>synthetic shifts</strong>, where models were tested against artificially corrupted images; <strong>natural shifts</strong>, which involved datasets collected from different distributions than the training data; and <strong>subpopulation shifts</strong>, where certain classes or groups were underrepresented in the training data. To evaluate Mano under synthetic shifts, the authors have used CIFAR-10C, CIFAR-100C, ImageNet-C, and TinyImageNet-C, covering various corruption types and severity levels. For natural shifts, they tested on OOD datasets from PACS, Office-Home, DomainNet, and RR1 WILDS. To assess subpopulation shifts, they used the BREEDS benchmark, including Living-17, Nonliving-26, Entity-13, and Entity-30 from ImageNet-C.</p>
<figure id="my-fig" class="numbered" style="float: left; margin-left: 10px; width: 45%;">
    <img src="../../images/Mano/R2_scores.png" class="align-center" style="width: 100%; height: auto;">
    <!-- <p style="text-align: center;">$R^2$ distribution ResNet18 on all distribution shifts </p> -->
</figure>
<p>On the left, we can see a box plot of $R^2$ distribution showing the estimation robustness across different shifts on all datasets except ImageNet, using ResNet18. We observe that MANO consistently outperformed existing methods in all three scenarios (achieving the highest median estimation performance), demonstrating its robustness to varying degrees of domain shifts. For more details, you can find numerical results for the different shifts in the paper. For instance, MANO achieves $R^2 &gt; 0.960$ and $œÅ &gt; 0.990$ under subpopulation shift, where as the performance of other baselines does not reach such consistently high levels.</p>
<p>Additionally, in the figure below, we can see a scatter plot illustrating the outperforming results of Mano on natural shift compared to Dispersion Score and ProjNorm on Entity-18 using ResNet-18.</p>
<figure id="my-fig" class="numbered">
    <img src="../../images/Mano/results_plot.png" class="align-center" style="width: 100%; height: auto;">
    <p style="text-align: center;"></p>
</figure>
<p>We can observe that MANO scores demonstrate a robust linear relationship with ground-truth OOD errors, whereas other state-of-the-art baselines tend to produce biased estimations, particularly for high test errors. Therefore, MANO significantly boosts performance under the natural shift.</p>
<p>Unlike traditional approaches that either rely on softmax probabilities or require retraining on new distributions, MANO provides a label-free and computation-efficient accuracy estimation method that scales well across different domains. By using <strong>Softrun normalization and matrix norm aggregation</strong>, MANO achieves a stronger correlation with actual accuracy, ensuring that model performance estimates remain reliable even when faced with extreme distribution shifts.</p>
<h2 id="section-3"><strong>Applications and Future Directions</strong></h2>
<p>Let&rsquo;s discuss now how MANO can be applied in practice, the benefit of combining Softrun with other estimation baselines, and the limitations of this approach.</p>
<p>One crucial application in the real world is <strong>deployment risk estimation</strong>, where real-time insights into model reliability can be obtained without costly manual labeling. This is particularly useful for models deployed in dynamic environments, such as healthcare and autonomous systems, where distribution shifts are frequent and unpredictable.</p>
<p>Now, what is the impact of Softrun on other estimation baselines? The authors have conducted an ablation study to assess how Softrun enhances the performance of Nuclear <a href="#nuclear">[6]</a>, ConfScore <a href="#confscore">[3]</a>, and MANO. As shown in the paper, Softrun significantly improves Nuclear‚Äôs $R^2$ score, particularly in datasets like Office-Home, where its performance increases from $0.692$ to $0.826$. These findings suggest that integrating Softrun into existing methods can improve their estimation reliability, making them more robust to poorly calibrated datasets.</p>
<p>Despite its strong theoretical foundation and empirical performance, MANO has certain limitations. One challenge is its reliance on the selection criterion parameter $Œ∑$ in Equation <a href="#my-fig_eq_v">v</a>, which requires careful tuning. To overcome this dependency, future research will focus on developing an automated approach to selecting the optimal normalization function without manual hyperparameter adjustments. Additionally, if multiple validation sets are available, as suggested in previous works (<a href="#nuclear">[5]</a>; <a href="#other">[6]</a>), the selection of $Œ∑$ could be refined based on these datasets, further improving MANO‚Äôs adaptability and robustness across different tasks.</p>
<h2 id="section-4"><strong>Conclusion</strong></h2>
<p>MANO represents a significant breakthrough in unsupervised accuracy estimation. By addressing logit overconfidence and introducing Softrun normalization, MANO provides a scalable, robust, and theoretically grounded approach for evaluating model accuracy under distribution shifts.</p>
<p>üîó <strong>Code available at:</strong> <a href="https://github.com/Renchunzi-Xie/MaNo">MANO GitHub Repository</a></p>
<p>MANO isn‚Äôt just a step forward‚Äîit‚Äôs a leap toward trustworthy AI deployment in the wild!</p>
<hr>
<h2 id="references"><strong>References</strong></h2>
<p><span id="mano">1.</span> Renchunzi Xie and Ambroise Odonnat and Vasilii Feofanov and Weijian Deng and Jianfeng Zhang and Bo An,
MANO: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts,
arXiv:2405.18979, 2024.</p>
<p><span id="rotation">2.</span>  Deng, W., Gould, S., and Zheng, L. (2021). What does rotation prediction tell us about classifier accuracy under varying testing environments? In International Conference on Machine Learning (ICML), pages 2579‚Äì2589.</p>
<p><span id="confscore">3.</span>  Hendrycks, D. and Gimpel, K. (2016). A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136.</p>
<p><span id="entropy">4.</span>  Guillory, D., Shankar, V., Ebrahimi, S., Darrell, T., and Schmidt, L. (2021). Predicting with confidence on unseen distributions. In Proceedings of the IEEE/CVF international Conference on Computer Vision (ICCV), pages 1134‚Äì1144.</p>
<p><span id="resnet50">5.</span>   He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770‚Äì778.</p>
<p><span id="nuclear">5.</span>  Deng, W., Suh, Y., Gould, S., and Zheng, L. (2023). Confidence and dispersity speak: Characterising prediction matrix for unsupervised accuracy estimation. arXiv preprint arXiv:2302.01094.</p>
<p><span id="other">6.</span>  Deng, W. and Zheng, L. (2021). Are labels always necessary for classifier accuracy evaluation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15069‚Äì15078.</p>

      
      <div class="post-date">
        <span class="g time">February 10, 2025 </span> &#8729;
         
      </div>
      
    </section>
    
    <div id="comments">
      <script src="https://utteranc.es/client.js"
    repo=ZgotmplZ
    issue-term="pathname"
    theme=ZgotmplZ
    crossorigin="anonymous"
    async>
</script>

    </div>
    
  </div>
</main>
</body>
</html>
