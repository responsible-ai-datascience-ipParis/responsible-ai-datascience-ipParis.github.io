<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fine-Tuning on Bloggin on Responsible AI</title>
    <link>https://responsible-ai-datascience-ipParis.github.io/tags/fine-tuning/</link>
    <description>Recent content in Fine-Tuning on Bloggin on Responsible AI</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2025 15:20:48 +0100</lastBuildDate>
    <atom:link href="https://responsible-ai-datascience-ipParis.github.io/tags/fine-tuning/atom.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BitFit: BIas-Term FIne-Tuning</title>
      <link>https://responsible-ai-datascience-ipParis.github.io/posts/bitfit/</link>
      <pubDate>Wed, 19 Feb 2025 15:20:48 +0100</pubDate>
      <guid>https://responsible-ai-datascience-ipParis.github.io/posts/bitfit/</guid>
      <description>&lt;style&#xD;&#xA;TYPE=&#34;text/css&#34;&gt;&#xD;&#xA;&#xD;&#xA;code.has-jax {font:&#xD;&#xA;inherit;&#xD;&#xA;font-size:&#xD;&#xA;100%; &#xD;&#xA;background: &#xD;&#xA;inherit; &#xD;&#xA;border: &#xD;&#xA;inherit;}&#xD;&#xA;&#xD;&#xA;&lt;/style&gt;&#xD;&#xA;&lt;script&#xD;&#xA;type=&#34;text/x-mathjax-config&#34;&gt;&#xD;&#xA;&#xD;&#xA;MathJax.Hub.Config({&#xD;&#xA;&#xD;&#xA;    tex2jax: {&#xD;&#xA;&#xD;&#xA;        inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],&#xD;&#xA;&#xD;&#xA;        skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;] // removed &#39;code&#39; entry&#xD;&#xA;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;});&#xD;&#xA;&#xD;&#xA;MathJax.Hub.Queue(function() {&#xD;&#xA;&#xD;&#xA;    var all = MathJax.Hub.getAllJax(), i;&#xD;&#xA;&#xD;&#xA;    for(i = 0; i &lt; all.length; i += 1) {&#xD;&#xA;&#xD;&#xA;        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;&#xD;&#xA;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;});&#xD;&#xA;&#xD;&#xA;&lt;/script&gt;&#xD;&#xA;&lt;script&#xD;&#xA;type=&#34;text/javascript&#34;&#xD;&#xA;src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full&#34;&gt;&lt;/script&gt;&#xD;&#xA;&lt;h1 style=&#34;font-size: 24px;&#34;&gt;BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers&lt;/h1&gt;&#xD;&#xA;&lt;h3 id=&#34;authors--abdoul-r-zeba-nour-yahya-nourelhouda-klich&#34;&gt;Authors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich&lt;/h3&gt;&#xA;&lt;h2 style=&#34;font-size: 20px;&#34;&gt; 1. Introduction &lt;/h2&gt;&#xD;&#xA;&lt;p&gt;Fine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
