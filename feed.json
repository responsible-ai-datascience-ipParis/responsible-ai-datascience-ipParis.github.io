{"version":"https://jsonfeed.org/version/1","title":"Bloggin on Responsible AI","home_page_url":"https://responsible-ai-datascience-ipParis.github.io/","feed_url":"https://responsible-ai-datascience-ipParis.github.io/feed.json","description":"Bloggin on Responsible AI","favicon":"https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico","expired":false,"author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"},"items":[{"id":"ffdbfd19f6febeddb9bf7cdcc0b0712a15cb49ae","title":"Robust Classifiers Energy Based Models","summary":"","content_text":"Unlocking the Secrets of Robust AI: How Energy-Based Models Are Revolutionizing Adversarial Training Authors: Lucas ALJANCIC, Solal DANAN, Maxime APPERT Introduction: The Hidden World of Adversarial Attacks ===================================================== Imagine you\u0026rsquo;ve built a state-of-the-art AI model that can classify images with near-perfect accuracy. But then, someone adds a tiny, almost invisible perturbation to an image, and suddenly your model confidently misclassifies it. This is the world of adversarial attacks, where small, carefully crafted changes can fool some advanced AI systems.\nIn this post, we\u0026rsquo;ll dive into a groundbreaking research paper that rethinks how we train AI models to resist these attacks. By leveraging Energy-Based Models (EBMs), the authors propose a novel approach called Weighted Energy Adversarial Training (WEAT) that not only makes models more robust but also unlocks surprising generative capabilities. Let\u0026rsquo;s break it down step by step, from the basics to the big picture.\nUnderstanding Adversarial Attacks and Robust AI =============================================== What Are Adversarial Attacks? Adversarial attacks can be thought of as carefully crafted optical illusions for AI. By making tiny changes to an input, attackers can manipulate the model\u0026rsquo;s output, causing incorrect predictions. These perturbations are often imperceptible to the human eye but can significantly impact AI decision-making.\nTechnical Explanation: Mathematically, an adversarial example $x\u0026rsquo;$ is generated by adding a small perturbation $\\delta$ to an original input $x$, such that: $$x\u0026rsquo; = x + \\delta,$$ where $\\delta$ is chosen to maximize the model\u0026rsquo;s prediction error, often by solving: $$\\arg\\max_{\\delta} ; L\\bigl(f(x+\\delta), y\\bigr) \\quad \\text{subject to} \\quad |\\delta| \\le \\epsilon,$$ ensuring that the perturbation remains small. Here, $f(x)$ represents the model\u0026rsquo;s prediction, $y$ is the true label, and $L$ is the loss function.\nSchematic Explanation: A simple visualization consists of two decision boundaries: one for clean samples and another distorted by adversarial perturbations. The adversarial example, though close to the clean sample in the input space, crosses the decision boundary, leading to a misclassification. This is illustrated in the following image, which showcases the adversarial perturbation technique. For example, a small amount of noise\u0026mdash;imperceptible to humans\u0026mdash;can be added to an image of a panda, changing its classification to a different label (e.g., from \u0026quot;panda\u0026quot; to \u0026quot;gibbon\u0026quot;) with high confidence, even though the image still looks exactly like a panda to the human eye. This occurs because the noise is specifically designed to fool the model by maximizing the prediction error.\nFigure 1: A small perturbation (middle) is added to a correctly classified image (left), causing a deep neural network to misclassify it with high confidence (right).\nThese adversarial attacks can also be used maliciously in what is known as a backdoor attack, where mislabeling during training leads to misclassification at test time. For instance, a stop sign with a slight alteration may be classified as a speed limit sign due to adversarial manipulation of the training data.\nFigure 2: Poisoning the training data by mislabeling specific inputs (top right) forces the model to associate a manipulated stop sign with a speed limit sign (bottom), leading to targeted misclassification at test time.\nWe can easily imagine scenarios in which an adversarial attack can be critical. For example, a self-driving car misinterpreting a slightly modified stop sign as a yield sign due to adversarial perturbations could lead to catastrophic consequences.\nTo counteract these risks, engineers strive to build robust models where adversarial attacks are ineffective. Techniques such as feature squeezing or input transformations can sometimes remove adversarial perturbations, reducing their effectiveness.\nThe Quest for Robust AI: Adversarial Training To combat adversarial attacks, researchers have developed Adversarial Training. The idea is simple: train the model on both normal data and adversarial examples (data modified to fool the model). This way, the model learns to recognize and resist these attacks.\nDuring training, adversarial examples are generated using algorithms like Projected Gradient Descent (PGD) or the Fast Gradient Sign Method (FGSM). The model is then trained to correctly classify both clean and adversarial examples, thereby improving its resilience.\nMathematically, adversarial training involves solving the following optimization problem: $$\\underbrace{\\min_{\\theta} E_{(x,y) \\sim \\mathcal{D}} \\left[ \\overbrace{\\max_{\\delta: |\\delta| \\le \\epsilon} L\\bigl(f(x+\\delta;\\theta), y\\bigr)}^{\\text{Adversarial Attack}} \\right]}_{\\text{Adversarial Training}}.$$ Here, the inner maximization aims to generate the worst-case perturbation $\\delta$, while the outer minimization updates the model parameters $\\theta$ to minimize the loss on these adversarial examples. This iterative process enhances the model\u0026rsquo;s robustness against adversarial attacks.\nA schematic representation helps visualize this concept. The image below shows how a labeled or unlabeled image is used to generate an adversarial example, which is then processed by a model. The predictions of both the original and adversarial images are compared using KL divergence, with a weighted sum incorporating cross-entropy loss (for labeled images) to compute the final loss.\nFigure 3: Representation of Adversarial Training, a model processes both clean and adversarial examples, minimizing cross-entropy loss for labeled images and KL divergence between predictions.\nA useful analogy is training a security system against burglary attempts. Imagine a house protected against intruders. A basic security system might work well against naive burglars but fail against sophisticated thieves who exploit specific vulnerabilities. To strengthen the system, the homeowner continuously updates it by testing against simulated break-in attempts\u0026mdash;much like adversarial training tests the model against simulated attacks. However, if the security system memorizes these specific attack patterns, it may remain vulnerable to novel intrusion techniques. This weakness highlights what is known as robust overfitting: the model learns to defend against the training attacks but generalizes poorly to new, unseen attacks.\nTo truly address robust overfitting, we must understand why it occurs. One way to do this is to analyze adversarial training through an energy-based perspective. In the next section, we explore how energy-based models offer critical insights that can help address this problem.\nEnergy-Based Models (EBMs) -- A New Perspective =============================================== Inspired by previous works linking adversarial training (AT) and energy-based models\u0026mdash;which reveal a shared contrastive approach\u0026mdash;the authors reinterpret robust discriminative classifiers as EBMs. This new perspective provides fresh insights into the dynamics of AT.\nWhat Are Energy-Based Models? In traditional classification, a neural network is trained to output the most likely label for an input. However, when recast as an EBM, the classifier assigns an energy to each input-label pair, $E_\\theta(x,y)$, which reflects how \u0026ldquo;plausible\u0026rdquo; that combination is. In this framework, lower energy indicates that the model is more confident the input belongs to that class. EBMs rely on the assumption that any probability density function $p(x)$ can be represented via a Boltzmann distribution: $$p_\\theta(x) = \\frac{\\exp\\bigl(-E_\\theta(x)\\bigr)}{Z(\\theta)}$$,\nwhere $E_\\theta(x)$ is the energy function mapping each input $x$ to a scalar value, and $Z(\\theta) = \\int \\exp\\bigl(-E_\\theta(x)\\bigr) , dx,$ is the normalizing constant ensuring that $p_\\theta(x)$ is a valid probability distribution.\nSimilarly, the joint probability of an input and a label can be defined as: $$p_\\theta(x,y) = \\frac{\\exp\\bigl(-E_\\theta(x,y)\\bigr)}{Z\u0026rsquo;(\\theta)}$$,\nwhich leads to the formulation of a discriminative classifier: $$p_\\theta(y \\mid x) = \\frac{\\exp\\bigl(-E_\\theta(x,y)\\bigr)}{\\sum_{k=1}^{K} \\exp\\bigl(-E_\\theta(x,k)\\bigr)}.$$\nIn this context, energy serves as a measure of confidence for both the input-label pair and, when marginalized over labels, the input itself:\nAdversarial Attacks Through the Energy Lens An intriguing insight from the paper is that adversarial attacks can be interpreted by examining how they alter the energy landscape. Recall that the cross-entropy loss can be written in terms of energy as: $$L_{CE}(x,y;\\theta) = -\\log p_\\theta(y\\mid x) ;=; E_\\theta(x,y) - E_\\theta(x),$$ implying that an attack impact the loss\u0026mdash;by modifying both $E_\\theta(x,y)$ and $E_\\theta(x)$.\nUntargeted Attacks. These attacks aim to make the classifier output any incorrect label. They raise the joint energy $E_\\theta(x,y)$ (making the model to \u0026quot;dislike\u0026quot; the correct label) while lowering the marginal energy $E_\\theta(x)$. Consequently, the adversarial example looks very \u0026ldquo;natural\u0026rdquo; (low $E_\\theta(x)$) yet becomes unrecognizable for the true label (high energy).\nFigure 4: PGD Untargeted attacks: (Above) Distributions of $(E_\\theta(x))$; (Below) Distributions of $(E_\\theta(x, y))$. Blue indicates natural data, red indicates adversarial data.\nTargeted Attacks. These attacks force the classifier to predict a specific (incorrect) label $t$. The adversary perturbs $x$ to minimize the energy for the target label $E_\\theta(x,t)$, thereby steering the classifier\u0026rsquo;s decision toward $t$ but raise $E_\\theta(x)$ overall, making the input look out-of-distribution despite being confidently misclassified as $t$.\nFigure 5: APGD Targeted attacks: (Above) Distributions of $(E_\\theta(x))$; (Below) Distributions of $(E_\\theta(x, y))$. Blue indicates natural data, red indicates adversarial data.\nBy revealing how each attack reshapes the energies of clean vs. adversarial examples, the authors highlight distinct patterns for untargeted and targeted strategies\u0026mdash;informing more effective defenses.\nEnergy-Based Insights on Robust Overfitting Overfitting means a model excels on training data but falters on unseen examples. In the robust setting, this manifests when a network memorizes the specific adversarial perturbations seen during training, yet struggles with new adversarial attacks. The authors reveal that robust overfitting is closely linked to a widening energy gap between clean and adversarial samples. As adversarial training progresses, the energy $E_\\theta(x)$ of natural (clean) examples and the energy $E_\\theta(x^\\ast)$ associated with their adversarially perturbed versions diverge significantly. Surprisingly, even if a sample is \u0026ldquo;easy\u0026rdquo; (i.e., it has a low loss and the model is highly confident), only minimal perturbations are needed to flip its label due to the model\u0026rsquo;s over-confidence. These weaker perturbations, paradoxically, cause a larger distortion in the energy landscape, so that a slight adversarial change on a low-loss sample can result in a substantial discrepancy between the natural and adversarial energies.\nTRADES: Aligning Clean and Adversarial Energies TRADES (TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization) is a refined adversarial training approach that adds a term to the standard loss, ensuring that a model\u0026rsquo;s predictions on a clean input $x$ remain close to those on its adversarially perturbed version $x+\\delta$. When expressed in energy terms, this extra term aligns the energies of natural and adversarial samples, thereby reducing robust overfitting. In essence, TRADES enforces similar energy values for $x$ and $x+\\delta$, creating a smoother energy landscape and helping the model generalize more effectively against unseen attacks.\nFigure 6: While SAT (line curve) experiences a steep divergence between clean and adversarial energies ($\\Delta E_\\theta(x)$ = $E_\\theta(x)$ - $E_\\theta(x^\\ast)$ ) in the 3rd phase (leading to robust overfitting), TRADES (dashed curve) maintains a relatively constant energy gap. This smoother alignment mitigates overfitting and improves robustness.\nOne of the key empirical observations is that top-performing robust classifiers tend to exhibit a smooth energy landscape around natural data points. Concretely, the energies $E_\\theta(x)$ and $E_\\theta(x+\\delta)$ remain closer in these models and this alignment strongly correlates with improved adversarial defense.\nTo sum up, these insights clarify that robust overfitting is not merely about memorizing specific adversarial examples, but rather about how the model\u0026rsquo;s internal energy representation becomes distorted. When the gap between $E_\\theta(x)$ and $E_\\theta(x+\\delta)$ grows, the model\u0026rsquo;s ability to generalize its robustness to new attacks is compromised. These observations deepen our understanding of adversarial dynamics and informed the strategy for effective robust training methods WEAT, as described in the following section.\nWeighted Energy Adversarial Training (WEAT) =============================================== Main Principle of WEAT The authors propose WEAT, a novel method that weights training samples based on their energy. WEAT relies on Energy-Based Models (EBMs) to measure the model\u0026rsquo;s \u0026quot;confidence\u0026quot;:\nMarginal Energy $E_\\theta(x) = -\\log \\sum_k \\exp(\\theta(x)[k])$: The lower it is, the more \u0026quot;probable\u0026quot; the input $x$ is according to the model.\nJoint Energy $E_\\theta(x, y) = -\\log \\exp(\\theta(x)[y])$: Measures the confidence for a specific class $y$.\nThe article classifies samples according to their energy into three categories:\nHigh-energy samples ($E_\\theta (x) \u0026gt; -3.87$): These are difficult examples, close to decision boundaries. WEAT gives them more weight because they help the model learn better.\nLow-energy samples ($E_\\theta (x) \\leq -11.47$): These are easy examples. WEAT gives them less weight to prevent overfitting.\nIntermediate samples (between these two thresholds)\nKey Idea: Samples with high energy (hard to classify) are crucial for robustness, while those with low energy (easy) risk causing overfitting. Therefore, WEAT weights them differently:\n$$\\text{weight}(x) = \\frac{1}{\\log(1 + \\exp(|E_\\theta(x)|))}$$\nFigure 7: Weighting Visualization.\nWEAT in details The core formula of WEAT is an improvement of TRADES by introducing a dynamic weighting weight$(x)$ based on the energy $E_\\theta(x)$. WEAT combines:\nCross-entropy loss (CE): Standard classification performance measure. It contains $E_{\\boldsymbol{\\theta}}(\\mathbf{x}, y) - E_{\\boldsymbol{\\theta}}(\\mathbf{x})$. When this term is minimized, it allows to \u0026quot;dig\u0026quot; the valleys representing the good predictions (low energy = high confidence). For the correct class y, $E_{\\boldsymbol{\\theta}}(\\mathbf{x}, y)$ becomes \u0026quot;lower\u0026quot; than $E_{\\boldsymbol{\\theta}}(\\mathbf{x})$.\nKL divergence: Controls the gap between predictions on natural data $p(y|x)$ and adversarial data $p(y|x^\\ast)$ with the marginal term $E_{\\boldsymbol{\\theta}}(\\mathbf{x}) - E_{\\boldsymbol{\\theta}}(\\mathbf{x}^\\ast)$. By minimizing this gap, the model smooths the energy landscape and flattens out disturbed areas.\nWEAT formula:\n$$L_{\\text{WEAT}} = \\underset{\\text{Weighting}}{\\boxed{\\text{weight}(x)}} \\cdot \\left[ \\underset{\\text{Standard Loss}}{\\boxed{L_{\\text{CE}}(x, y)}} + \\beta \\cdot \\underset{\\text{Robust Regularization}}{\\boxed{\\text{KL}(p(y|x) || p(y|x^\\ast))}} \\right] $$\nFigure 8: Diagram of WEAT dynamics\nThe figure above highlights the trade-off with the $\\beta$ coefficient for regularization to maintain a smooth energy landscape despite adversarial attacks.\nWhile not the main focus of this article (to avoid technical overload), WEAT also demonstrates remarkable performance in image generation through its integration with Stochastic Gradient Langevin Dynamics (SGLD). On standard benchmarks like CIFAR-10, WEAT matches the performance of hybrid models such as JEM while offering superior robustness (see Table 2c in the paper).\nConclusion: A New Era for Robust AI =============================================== Why This Matters This research isn\u0026rsquo;t just about making AI models more robust. It\u0026rsquo;s about fundamentally understanding how these models work and how we can improve them. By rethinking adversarial training through the lens of EBMs, the authors have opened up new possibilities for both robustness and generative modeling. As AI continues to evolve, approaches like WEAT will be crucial for building models that are both accurate and secure. Surely, this type of model will play a key role in improving trust in critical technologies such as autonomous vehicles.\nPotential Societal Impact Although robust models are often considered safe from adversarial attacks, their susceptibility to inversion poses a privacy risk. Because robust classifiers can be interpreted as energy-based models, they capture substantial information about their training data\u0026mdash;including its distribution and structure. This makes it possible, using inversion techniques, to reconstruct or approximate the original training data. If sensitive information (e.g., personal data, proprietary content, or other confidential details) is exposed, it could lead to significant privacy breaches with broader societal implications.\nReferences Sik-Ho Tsang. Review: Virtual Adversarial Training (VAT). Apr 21, 2022. Available at: https://sh-tsang.medium.com/review-virtual-adversarial-training-vat-4b3d8b7b2e92.\nGaudenz Boesch. Attack Methods: What Is Adversarial Machine Learning?. December 2, 2023. Available at: https://viso.ai/deep-learning/adversarial-machine-learning/.\nMujtaba Hussain Mirza1, Maria Rosaria Briglia, Senad Beadini, and Iacopo Masi. Shedding More Light on Robust Classifiers under the lens of Energy-based Models. 2025. Available at: https://arxiv.org/abs/2407.06315.\n","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eUnlocking the Secrets of Robust AI: How Energy-Based Models Are\n  Revolutionizing Adversarial Training\u003c/h1\u003e\n\u003ch1 style=\"font-size: 20px;\"\u003eAuthors: Lucas ALJANCIC, Solal DANAN, Maxime APPERT\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eIntroduction: The Hidden World of Adversarial Attacks \u003c/h1\u003e\n=====================================================\n\u003cp\u003eImagine you\u0026rsquo;ve built a state-of-the-art AI model that can classify\nimages with near-perfect accuracy. But then, someone adds a tiny, almost\ninvisible perturbation to an image, and suddenly your model confidently\nmisclassifies it. This is the world of \u003cstrong\u003eadversarial attacks\u003c/strong\u003e, where\nsmall, carefully crafted changes can fool some advanced AI systems.\u003c/p\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;ll dive into a groundbreaking research paper that\nrethinks how we train AI models to resist these attacks. By leveraging\n\u003cstrong\u003eEnergy-Based Models (EBMs)\u003c/strong\u003e, the authors propose a novel approach\ncalled \u003cstrong\u003eWeighted Energy Adversarial Training (WEAT)\u003c/strong\u003e that not only\nmakes models more robust but also unlocks surprising generative\ncapabilities. Let\u0026rsquo;s break it down step by step, from the basics to the\nbig picture.\u003c/p\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eUnderstanding Adversarial Attacks and Robust AI \u003c/h1\u003e\n===============================================\n\u003ch2 id=\"what-are-adversarial-attacks\"\u003eWhat Are Adversarial Attacks?\u003c/h2\u003e\n\u003cp\u003eAdversarial attacks can be thought of as carefully crafted optical\nillusions for AI. By making tiny changes to an input, attackers can\nmanipulate the model\u0026rsquo;s output, causing incorrect predictions. These\nperturbations are often imperceptible to the human eye but can\nsignificantly impact AI decision-making.\u003c/p\u003e\n\u003ch4 id=\"technical-explanation\"\u003eTechnical Explanation:\u003c/h4\u003e\n\u003cp\u003eMathematically, an adversarial example $x\u0026rsquo;$ is generated by adding a\nsmall perturbation $\\delta$ to an original input $x$, such that:\n$$x\u0026rsquo; = x + \\delta,$$ where $\\delta$ is chosen to maximize the model\u0026rsquo;s\nprediction error, often by solving:\n$$\\arg\\max_{\\delta} ; L\\bigl(f(x+\\delta), y\\bigr) \\quad \\text{subject to} \\quad |\\delta| \\le \\epsilon,$$\nensuring that the perturbation remains small. Here, $f(x)$ represents\nthe model\u0026rsquo;s prediction, $y$ is the true label, and $L$ is the loss\nfunction.\u003c/p\u003e\n\u003ch4 id=\"schematic-explanation\"\u003eSchematic Explanation:\u003c/h4\u003e\n\u003cp\u003eA simple visualization consists of two decision boundaries: one for\nclean samples and another distorted by adversarial perturbations. The\nadversarial example, though close to the clean sample in the input\nspace, crosses the decision boundary, leading to a misclassification.\nThis is illustrated in the following image, which showcases the\nadversarial perturbation technique. For example, a small amount of\nnoise\u0026mdash;imperceptible to humans\u0026mdash;can be added to an image of a panda,\nchanging its classification to a different label (e.g., from \u0026quot;panda\u0026quot;\nto \u0026quot;gibbon\u0026quot;) with high confidence, even though the image still looks\nexactly like a panda to the human eye. This occurs because the noise is\nspecifically designed to fool the model by maximizing the prediction\nerror.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/AA.png\"\n  alt=\"Adversarial Example: A small perturbation (middle) is added to a\ncorrectly classified image (left), causing a deep neural network to\nmisclassify it with high confidence (right).\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 1:\u003c/strong\u003e A small perturbation (middle) is added to a\ncorrectly classified image (left), causing a deep neural network to\nmisclassify it with high confidence (right).\u003c/p\u003e\n\u003cp\u003eThese adversarial attacks can also be used maliciously in what is known\nas a backdoor attack, where mislabeling during training leads to\nmisclassification at test time. For instance, a stop sign with a slight\nalteration may be classified as a speed limit sign due to adversarial\nmanipulation of the training data.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/stop.png\"\n  alt=\"Backdoor Attack: Poisoning the training data by mislabeling\nspecific inputs (top right) forces the model to associate a manipulated\nstop sign with a speed limit sign (bottom), leading to targeted\nmisclassification at test time.\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 2:\u003c/strong\u003e Poisoning the training data by mislabeling\nspecific inputs (top right) forces the model to associate a manipulated\nstop sign with a speed limit sign (bottom), leading to targeted\nmisclassification at test time.\u003c/p\u003e\n\u003cp\u003eWe can easily imagine scenarios in which an adversarial attack can be\ncritical. For example, a self-driving car misinterpreting a slightly\nmodified stop sign as a yield sign due to adversarial perturbations\ncould lead to catastrophic consequences.\u003c/p\u003e\n\u003cp\u003eTo counteract these risks, engineers strive to build robust models where\nadversarial attacks are ineffective. Techniques such as feature\nsqueezing or input transformations can sometimes remove adversarial\nperturbations, reducing their effectiveness.\u003c/p\u003e\n\u003ch2 id=\"the-quest-for-robust-ai-adversarial-training\"\u003eThe Quest for Robust AI: Adversarial Training\u003c/h2\u003e\n\u003cp\u003eTo combat adversarial attacks, researchers have developed Adversarial\nTraining. The idea is simple: train the model on both normal data and\nadversarial examples (data modified to fool the model). This way, the\nmodel learns to recognize and resist these attacks.\u003c/p\u003e\n\u003cp\u003eDuring training, adversarial examples are generated using algorithms\nlike Projected Gradient Descent (PGD) or the Fast Gradient Sign Method\n(FGSM). The model is then trained to correctly classify both clean and\nadversarial examples, thereby improving its resilience.\u003c/p\u003e\n\u003cp\u003eMathematically, adversarial training involves solving the following\noptimization problem:\n$$\\underbrace{\\min_{\\theta} E_{(x,y) \\sim \\mathcal{D}} \\left[ \\overbrace{\\max_{\\delta: |\\delta| \\le \\epsilon} L\\bigl(f(x+\\delta;\\theta), y\\bigr)}^{\\text{Adversarial Attack}} \\right]}_{\\text{Adversarial Training}}.$$\nHere, the inner maximization aims to generate the worst-case\nperturbation $\\delta$, while the outer minimization updates the model\nparameters $\\theta$ to minimize the loss on these adversarial examples.\nThis iterative process enhances the model\u0026rsquo;s robustness against\nadversarial attacks.\u003c/p\u003e\n\u003cp\u003eA schematic representation helps visualize this concept. The image below\nshows how a labeled or unlabeled image is used to generate an\nadversarial example, which is then processed by a model. The predictions\nof both the original and adversarial images are compared using KL\ndivergence, with a weighted sum incorporating cross-entropy loss (for\nlabeled images) to compute the final loss.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/schema.png\"\n  alt=\"Adversarial Training with Consistency Loss: A model processes both\nclean and adversarial examples, minimizing cross-entropy loss for\nlabeled images and KL divergence between predictions to improve\nrobustness. The final loss is a weighted sum of both\nterms.\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 3:\u003c/strong\u003e Representation of Adversarial Training, a model processes both\nclean and adversarial examples, minimizing cross-entropy loss for\nlabeled images and KL divergence between predictions.\u003c/p\u003e\n\u003cp\u003eA useful analogy is training a security system against burglary\nattempts. Imagine a house protected against intruders. A basic security\nsystem might work well against naive burglars but fail against\nsophisticated thieves who exploit specific vulnerabilities. To\nstrengthen the system, the homeowner continuously updates it by testing\nagainst simulated break-in attempts\u0026mdash;much like adversarial training\ntests the model against simulated attacks. However, if the security\nsystem memorizes these specific attack patterns, it may remain\nvulnerable to novel intrusion techniques. This weakness highlights what\nis known as robust overfitting: the model learns to defend against the\ntraining attacks but generalizes poorly to new, unseen attacks.\u003c/p\u003e\n\u003cp\u003eTo truly address robust overfitting, we must understand why it occurs.\nOne way to do this is to analyze adversarial training through an\nenergy-based perspective. In the next section, we explore how\nenergy-based models offer critical insights that can help address this\nproblem.\u003c/p\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eEnergy-Based Models (EBMs) -- A New Perspective \u003c/h1\u003e\n===============================================\n\u003cp\u003eInspired by previous works linking adversarial training (AT) and\nenergy-based models\u0026mdash;which reveal a shared contrastive approach\u0026mdash;the\nauthors reinterpret robust discriminative classifiers as EBMs. This new\nperspective provides fresh insights into the dynamics of AT.\u003c/p\u003e\n\u003ch2 id=\"what-are-energy-based-models\"\u003eWhat Are Energy-Based Models?\u003c/h2\u003e\n\u003cp\u003eIn traditional classification, a neural network is trained to output the\nmost likely label for an input. However, when recast as an EBM, the\nclassifier assigns an \u003cem\u003eenergy\u003c/em\u003e to each input-label pair,\n$E_\\theta(x,y)$, which reflects how \u0026ldquo;plausible\u0026rdquo; that combination is. In\nthis framework, lower energy indicates that the model is more confident\nthe input belongs to that class. EBMs rely on the assumption that any\nprobability density function $p(x)$ can be represented via a Boltzmann\ndistribution:\n$$p_\\theta(x) = \\frac{\\exp\\bigl(-E_\\theta(x)\\bigr)}{Z(\\theta)}$$,\u003c/p\u003e\n\u003cp\u003ewhere $E_\\theta(x)$ is the energy function mapping each input $x$ to a\nscalar value, and $Z(\\theta) = \\int \\exp\\bigl(-E_\\theta(x)\\bigr) , dx,$\nis the normalizing constant ensuring that $p_\\theta(x)$ is a valid\nprobability distribution.\u003c/p\u003e\n\u003cp\u003eSimilarly, the joint probability of an input and a label can be defined\nas:\n$$p_\\theta(x,y) = \\frac{\\exp\\bigl(-E_\\theta(x,y)\\bigr)}{Z\u0026rsquo;(\\theta)}$$,\u003c/p\u003e\n\u003cp\u003ewhich leads to the formulation of a discriminative classifier:\n$$p_\\theta(y \\mid x) = \\frac{\\exp\\bigl(-E_\\theta(x,y)\\bigr)}{\\sum_{k=1}^{K} \\exp\\bigl(-E_\\theta(x,k)\\bigr)}.$$\u003c/p\u003e\n\u003cp\u003eIn this context, \u003cstrong\u003eenergy serves as a measure of confidence\u003c/strong\u003e for both\nthe input-label pair and, when marginalized over labels, the input\nitself:\u003c/p\u003e\n\u003ch2 id=\"adversarial-attacks-through-the-energy-lens\"\u003eAdversarial Attacks Through the Energy Lens\u003c/h2\u003e\n\u003cp\u003eAn intriguing insight from the paper is that adversarial attacks can be\ninterpreted by examining how they alter the energy landscape. Recall\nthat the cross-entropy loss can be written in terms of energy as:\n$$L_{CE}(x,y;\\theta) = -\\log p_\\theta(y\\mid x)\n;=; E_\\theta(x,y) - E_\\theta(x),$$ implying that an attack impact the\nloss\u0026mdash;by modifying both $E_\\theta(x,y)$ and $E_\\theta(x)$.\u003c/p\u003e\n\u003ch4 id=\"untargeted-attacks\"\u003eUntargeted Attacks.\u003c/h4\u003e\n\u003cp\u003eThese attacks aim to make the classifier output \u003cem\u003eany\u003c/em\u003e incorrect label.\nThey raise the joint energy $E_\\theta(x,y)$ (making the model to\n\u0026quot;dislike\u0026quot; the correct label) while lowering the marginal energy\n$E_\\theta(x)$. Consequently, the adversarial example looks very\n\u0026ldquo;natural\u0026rdquo; (low $E_\\theta(x)$) yet becomes unrecognizable for the true\nlabel (high energy).\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/pgd1.png\"\n  alt=\"PGD Untargeted attacks: (Left) Distributions of (E_\\theta(x)); (Right) Distributions of (E_\\theta(x, y)). Blue indicates natural data, red indicates adversarial data.\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/pgd2.png\"\n  alt=\"PGD Untargeted attacks: (Left) Distributions of (E_\\theta(x)); (Right) Distributions of (E_\\theta(x, y)). Blue indicates natural data, red indicates adversarial data.\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 4:\u003c/strong\u003e PGD Untargeted attacks: (Above) Distributions of $(E_\\theta(x))$; (Below) Distributions of $(E_\\theta(x, y))$. Blue indicates natural data, red indicates adversarial data.\u003c/p\u003e\n\u003ch4 id=\"targeted-attacks\"\u003eTargeted Attacks.\u003c/h4\u003e\n\u003cp\u003eThese attacks force the classifier to predict a specific (incorrect)\nlabel $t$. The adversary perturbs $x$ to \u003cem\u003eminimize\u003c/em\u003e the energy for the\ntarget label $E_\\theta(x,t)$, thereby steering the classifier\u0026rsquo;s decision\ntoward $t$ but raise $E_\\theta(x)$ overall, making the input look\nout-of-distribution despite being confidently misclassified as $t$.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/apgd1.png\"\n  alt=\"APGD Targeted attacks: (Left) Distributions of $E_\\theta(x)$;\n(Right) Distributions of $E_\\theta(x,y)$. Blue indicates natural data,\nred indicates adversarial data.\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/apgd2.png\"\n  alt=\"APGD Targeted attacks: (Left) Distributions of $E_\\theta(x)$; (Right)\nDistributions of $E_\\theta(x,y)$. Blue indicates natural data, red\nindicates adversarial data.\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 5:\u003c/strong\u003e APGD Targeted attacks: (Above) Distributions of $(E_\\theta(x))$; (Below) Distributions of $(E_\\theta(x, y))$. Blue indicates natural data, red indicates adversarial data.\u003c/p\u003e\n\u003cp\u003eBy revealing how each attack reshapes the energies of clean vs.\nadversarial examples, the authors highlight distinct patterns for\nuntargeted and targeted strategies\u0026mdash;informing more effective defenses.\u003c/p\u003e\n\u003ch2 id=\"energy-based-insights-on-robust-overfitting\"\u003eEnergy-Based Insights on Robust Overfitting\u003c/h2\u003e\n\u003cp\u003eOverfitting means a model excels on training data but falters on unseen\nexamples. In the robust setting, this manifests when a network memorizes\nthe specific adversarial perturbations seen during training, yet\nstruggles with new adversarial attacks. The authors reveal that robust\noverfitting is closely linked to a widening \u003cem\u003eenergy gap\u003c/em\u003e between clean\nand adversarial samples. As adversarial training progresses, the energy\n$E_\\theta(x)$ of natural (clean) examples and the energy $E_\\theta(x^\\ast)$\nassociated with their adversarially perturbed versions diverge\nsignificantly. Surprisingly, even if a sample is \u0026ldquo;easy\u0026rdquo; (i.e., it has a\nlow loss and the model is highly confident), only minimal perturbations\nare needed to flip its label due to the model\u0026rsquo;s over-confidence. These\nweaker perturbations, paradoxically, cause a larger distortion in the\nenergy landscape, so that a slight adversarial change on a low-loss\nsample can result in a substantial discrepancy between the natural and\nadversarial energies.\u003c/p\u003e\n\u003ch2 id=\"trades-aligning-clean-and-adversarial-energies\"\u003eTRADES: Aligning Clean and Adversarial Energies\u003c/h2\u003e\n\u003cp\u003eTRADES (\u003cem\u003eTRadeoff-inspired Adversarial DEfense via Surrogate-loss\nminimization\u003c/em\u003e) is a refined adversarial training approach that adds a\nterm to the standard loss, ensuring that a model\u0026rsquo;s predictions on a\nclean input $x$ remain close to those on its adversarially perturbed\nversion $x+\\delta$. When expressed in energy terms, this extra term\naligns the energies of natural and adversarial samples, thereby reducing\nrobust overfitting. In essence, TRADES enforces similar energy values\nfor $x$ and $x+\\delta$, creating a smoother energy landscape and helping\nthe model generalize more effectively against unseen attacks.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/trades.png\"\n  alt=\"Energy alignment in TRADES vs. SAT (another AT approach). While\nSAT (line curve) experiences a steep divergence between clean and\nadversarial energies ($\\Delta E_\\theta(x)$ = $E_\\theta(x)$ -\n$E_\\theta(x^\\ast)$ ) in the 3rd phase (leading to robust overfitting),\nTRADES (dashed curve) maintains a relatively constant energy gap. This\nsmoother alignment mitigates overfitting and improves\nrobustness.\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 6:\u003c/strong\u003e While\nSAT (line curve) experiences a steep divergence between clean and\nadversarial energies ($\\Delta E_\\theta(x)$ = $E_\\theta(x)$ -\n$E_\\theta(x^\\ast)$ ) in the 3rd phase (leading to robust overfitting),\nTRADES (dashed curve) maintains a relatively constant energy gap. This\nsmoother alignment mitigates overfitting and improves\nrobustness.\u003c/p\u003e\n\u003cp\u003eOne of the key empirical observations is that top-performing robust\nclassifiers tend to exhibit a smooth energy landscape around natural\ndata points. Concretely, the energies $E_\\theta(x)$ and\n$E_\\theta(x+\\delta)$ remain closer in these models and this alignment\nstrongly correlates with improved adversarial defense.\u003cbr\u003e\n\u003cbr\u003e\nTo sum up, these insights clarify that robust overfitting is not merely\nabout memorizing specific adversarial examples, but rather about how the\nmodel\u0026rsquo;s internal energy representation becomes distorted. When the gap\nbetween $E_\\theta(x)$ and $E_\\theta(x+\\delta)$ grows, the model\u0026rsquo;s\nability to generalize its robustness to new attacks is compromised.\nThese observations deepen our understanding of adversarial dynamics and\ninformed the strategy for effective robust training methods WEAT, as\ndescribed in the following section.\u003c/p\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eWeighted Energy Adversarial Training (WEAT) \u003c/h1\u003e\n===============================================\n\u003ch2 id=\"main-principle-of-weat\"\u003eMain Principle of WEAT\u003c/h2\u003e\n\u003cp\u003eThe authors propose \u003cstrong\u003eWEAT\u003c/strong\u003e, a novel method that weights training\nsamples based on their energy. WEAT relies on \u003cstrong\u003eEnergy-Based Models\n(EBMs)\u003c/strong\u003e to measure the model\u0026rsquo;s \u0026quot;confidence\u0026quot;:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMarginal Energy\u003c/strong\u003e $E_\\theta(x) = -\\log \\sum_k \\exp(\\theta(x)[k])$:\nThe lower it is, the more \u0026quot;probable\u0026quot; the input $x$ is according to\nthe model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJoint Energy\u003c/strong\u003e $E_\\theta(x, y) = -\\log \\exp(\\theta(x)[y])$:\nMeasures the confidence for a specific class $y$.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe article classifies samples according to their energy into three\ncategories:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHigh-energy samples\u003c/strong\u003e ($E_\\theta (x) \u0026gt; -3.87$): These are\ndifficult examples, close to decision boundaries. WEAT gives them\nmore weight because they help the model learn better.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLow-energy samples\u003c/strong\u003e ($E_\\theta (x) \\leq -11.47$): These are easy\nexamples. WEAT gives them less weight to prevent overfitting.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eIntermediate samples\u003c/strong\u003e (between these two thresholds)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eKey Idea:\u003c/strong\u003e Samples with \u003cstrong\u003ehigh energy\u003c/strong\u003e (hard to classify) are\ncrucial for robustness, while those with \u003cstrong\u003elow energy\u003c/strong\u003e (easy) risk\ncausing overfitting. Therefore, WEAT weights them differently:\u003c/p\u003e\n\u003cp\u003e$$\\text{weight}(x) = \\frac{1}{\\log(1 + \\exp(|E_\\theta(x)|))}$$\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/weighting.png\"\n  alt=\"Weighting Visualization\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 7:\u003c/strong\u003e Weighting Visualization.\u003c/p\u003e\n\u003ch2 id=\"weat-in-details\"\u003eWEAT in details\u003c/h2\u003e\n\u003cp\u003eThe core formula of WEAT is an improvement of TRADES by introducing a\n\u003cstrong\u003edynamic weighting weight$(x)$\u003c/strong\u003e based on the energy $E_\\theta(x)$.\nWEAT combines:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCross-entropy loss (CE)\u003c/strong\u003e: Standard classification performance\nmeasure. It contains\n$E_{\\boldsymbol{\\theta}}(\\mathbf{x}, y) - E_{\\boldsymbol{\\theta}}(\\mathbf{x})$.\nWhen this term is minimized, it allows to \u0026quot;dig\u0026quot; the valleys\nrepresenting the good predictions (low energy = high confidence).\nFor the correct class y, $E_{\\boldsymbol{\\theta}}(\\mathbf{x}, y)$\nbecomes \u0026quot;lower\u0026quot; than $E_{\\boldsymbol{\\theta}}(\\mathbf{x})$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eKL divergence\u003c/strong\u003e: Controls the gap between predictions on natural\ndata $p(y|x)$ and adversarial data $p(y|x^\\ast)$ with the marginal term\n$E_{\\boldsymbol{\\theta}}(\\mathbf{x}) - E_{\\boldsymbol{\\theta}}(\\mathbf{x}^\\ast)$.\nBy minimizing this gap, the model smooths the energy landscape and\nflattens out disturbed areas.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWEAT formula\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003e$$L_{\\text{WEAT}} = \\underset{\\text{Weighting}}{\\boxed{\\text{weight}(x)}} \\cdot \\left[\n\\underset{\\text{Standard Loss}}{\\boxed{L_{\\text{CE}}(x, y)}} +\n\\beta \\cdot \\underset{\\text{Robust Regularization}}{\\boxed{\\text{KL}(p(y|x) || p(y|x^\\ast))}}\n\\right]\n$$\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/valley1.png\"\n  alt=\"Diagram of WEAT dynamics\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/valley2.png\"\n  alt=\"Diagram of WEAT dynamics\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Robust_Classifiers_EBM/valley3.png\"\n  alt=\"Diagram of WEAT dynamics\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFigure 8\u003c/strong\u003e: Diagram of WEAT dynamics\u003c/p\u003e\n\u003cp\u003eThe figure above highlights the trade-off with the $\\beta$ coefficient\nfor regularization to maintain a smooth energy landscape despite\nadversarial attacks.\u003c/p\u003e\n\u003cp\u003eWhile not the main focus of this article (to avoid technical overload),\nWEAT also demonstrates remarkable performance in image generation\nthrough its integration with Stochastic Gradient Langevin Dynamics\n(SGLD). On standard benchmarks like CIFAR-10, WEAT matches the\nperformance of hybrid models such as JEM while offering superior\nrobustness (see Table 2c in the paper).\u003c/p\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eConclusion: A New Era for Robust AI \u003c/h1\u003e\n===============================================\n\u003ch2 id=\"why-this-matters\"\u003eWhy This Matters\u003c/h2\u003e\n\u003cp\u003eThis research isn\u0026rsquo;t just about making AI models more robust. It\u0026rsquo;s about\nfundamentally understanding how these models work and how we can improve\nthem. By rethinking adversarial training through the lens of EBMs, the\nauthors have opened up new possibilities for both robustness and\ngenerative modeling. As AI continues to evolve, approaches like WEAT\nwill be crucial for building models that are both accurate and secure.\nSurely, this type of model will play a key role in improving trust in\ncritical technologies such as autonomous vehicles.\u003c/p\u003e\n\u003ch2 id=\"potential-societal-impact\"\u003ePotential Societal Impact\u003c/h2\u003e\n\u003cp\u003eAlthough robust models are often considered safe from adversarial\nattacks, their susceptibility to inversion poses a privacy risk. Because\nrobust classifiers can be interpreted as energy-based models, they\ncapture substantial information about their training data\u0026mdash;including\nits distribution and structure. This makes it possible, using inversion\ntechniques, to reconstruct or approximate the original training data. If\nsensitive information (e.g., personal data, proprietary content, or\nother confidential details) is exposed, it could lead to significant\nprivacy breaches with broader societal implications.\u003c/p\u003e\n\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003cp\u003eSik-Ho Tsang. \u003cem\u003eReview: Virtual Adversarial Training (VAT)\u003c/em\u003e. Apr 21,\n2022. Available at:\n\u003ca href=\"https://sh-tsang.medium.com/review-virtual-adversarial-training-vat-4b3d8b7b2e92\"\u003ehttps://sh-tsang.medium.com/review-virtual-adversarial-training-vat-4b3d8b7b2e92\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eGaudenz Boesch. \u003cem\u003eAttack Methods: What Is Adversarial Machine Learning?\u003c/em\u003e.\nDecember 2, 2023. Available at:\n\u003ca href=\"https://viso.ai/deep-learning/adversarial-machine-learning/\"\u003ehttps://viso.ai/deep-learning/adversarial-machine-learning/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMujtaba Hussain Mirza1, Maria Rosaria Briglia, Senad Beadini, and Iacopo\nMasi. \u003cem\u003eShedding More Light on Robust Classifiers under the lens of\nEnergy-based Models\u003c/em\u003e. 2025. Available at:\n\u003ca href=\"https://arxiv.org/abs/2407.06315\"\u003ehttps://arxiv.org/abs/2407.06315\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e","url":"https://responsible-ai-datascience-ipParis.github.io/posts/robust-classifiers-energy-based-models/","date_published":"27036-27-09T353:2727:00+01:00","date_modified":"27036-27-09T353:2727:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"04090848145d713807306da87921fd6816a12bf5","title":"Fairness in Social Influence Maximization via Optimal Transport","summary":"","content_text":"Authors: Guillaume MARIN-BERTIN \u0026amp; Jaishan BURTON ELMO Table of Contents\n1. Introduction 2. Mutual Fairness: A New Metric 2.1 Why Make a New Metric? 2.2 Proposed Fairness Metric 2.3 Short Example 3. Metric in Practice 3.1 Mutual Fairness in Practice 3.2 Impact of β 4. Improving Fairness with S3D 4.1 Stochastic Seed Selection Descent (S3D) 4.2 Experimentation 4.3 Impact of S3D 5. Conclusion References This is a blog post about the article “Fairness in Social Influence Maximization via Optimal Transport” published by Shubham Chowdhary et al. in 2024 and available here.\n1. Introduction In today’s digital society, social networks play a major role in how information spreads. Whether it is a public health campaign, a political message, or viral marketing, the ability to maximize influence is crucial. Companies, governments, and organizations leverage Influence Maximization (IM) algorithms to strategically select key individuals—called seeds—who will initiate a diffusion process, ensuring that information reaches the largest possible audience.\nImagine a scenario where a job-matching platform uses an Influence Maximization (IM) strategy to promote career opportunities to young professionals. The goal is to spread job postings efficiently across different professional communities. However, due to the structure of the social network, the algorithm selects key individuals (seeds) in such a way that:\nIn 50% of cases, all the job opportunities are shared within a network of university graduates, while vocational school graduates receive none. In the other 50% of cases, the opposite happens. At first glance, this might seem fair: both groups have an equal expected chance of seeing the job offers. However, in practice, one group is always entirely excluded in each scenario, creating a systemic disadvantage for those who miss out on critical career opportunities.\nTo fix this, researchers have tried adding fairness constraints to IM algorithms. The main ideas include:\nEquity-based fairness (Stoica et al., 2020): Ensures that each group has the same expected proportion of influenced users. Max-min fairness (Fish et al., 2019; Zhu et al., 2019): Maximizes the minimum probability that any group receives information. Diversity-aware methods (Tsang et al., 2019): Ensure that no single group dominates the influence process. Despite these efforts, a fundamental issue remains: existing metrics treat groups independently and do not capture the joint probability of outreach. This means they can create the illusion of fairness while still allowing systematic exclusion of certain communities.\nTo address this, we propose a new fairness-aware framework that better distributes influence across communities:\nMutual Fairness: A better way to measure fairness, inspired by Optimal Transport. Instead of just looking at how much information each group gets in total, it ensures they receive the message at the same time. S3D Algorithm: A smarter way to choose influencers, balancing fairness and efficiency. It adjusts seed selection dynamically to improve fairness while still reaching as many people as possible. 2. Mutual Fairness: A New Metric Why make a new metric? To understand why a new metric is necessary, let’s examine two situations where information propagation appears fair,we take the example of the introduction.\nConsider two groups, C1 and C2, each with an outreach probability distribution:\nFigure 1 : Outreach Probability Distribution for Groups C1 and C2 δ0 represents the case where the information is not transmitted. δ1 represents the case where the information is successfully received by the group. On average, both groups have a 50% chance of receiving the information, which might suggest a fair situation. However, this average hides significant differences in the actual distribution of information, as illustrated in the example below:\nFigure 2 : Comparison of Two Joint Probability Distributions: γa and γb First case (γa) :\nδ(0,0): Neither group receives the information (50%). δ(1,1): Both groups receive the information simultaneously (50%). This configuration is fair because if one group does not receive the information, the other does not either.\nSecond case (γb) :\nδ(0,0): Neither group receives the information (25%). δ(1,1): Both groups receive the information simultaneously (25%). δ(0,1): Group 1 does not receive the information, but Group 2 does (25%). δ(1,0): Group 1 receives the information, but Group 2 does not (25%). In this case, there is a 50% chance that only one group will receive the information, creating an inequality. However, the marginal averages remain the same (µ1=µ2=50%).\nThe distributions γa and γb are very different, yet the marginal averages do not capture these nuances. This highlights the need for a new metric to evaluate fairness more precisely, beyond simple averages.\nProposed fairness metric The idea is to start from a joint probability measure γ rather than the marginal averages μi. From γ, we aim to measure the distance between it and an ideal distribution γ*, where each group receives the same amount of information in all cases.\nTo illustrate this on a figure:\nThe x-axis represents the amount of information received by group 1. The y-axis represents the amount of information received by group 2. The diagonal signifies that both groups receive exactly the same amount of information, which corresponds to the ideal sought. A movement perpendicular to the diagonal should be penalized, as it indicates an imbalance in the distribution of information between the groups. Similarly, we must penalize movement along the diagonal, as it affects efficiency. These costs are measured by Euclidean distance. In the Figure 3 below, we observe the transition from a distribution γa to a distribution γb, broken down into two components. One is dashed, representing the efficiency, and the other is solid, representing the fairness.\nFigure 3 : Representation of the transport cost between two points (x1,x2) and(y1,y2) with an intermediate point z(x1,x2,y1,y2) To define this cost, we use the following formula:\nFigure 4 : Optimal Transport Cost Wcβ(γa,γb) Between Distributions γa and γb We can draw an analogy with a move, where the goal is to transport items from House A to House B at the lowest possible cost, while ensuring that each item reaches its final destination according to Wcβ(γa, γb).\nγa: represents the list of items present in House A, specifying where they are located and in what quantity. γb: represents the list of items expected in House B, indicating where they should arrive and in what quantity. c: This is the function that specifies the cost of moving an item from one location to another. In figure 5, we evaluate the transport cost between the point (x1, x2) and (y1, y2). We take z(x1, x2, y1, y2) as an intermediate point in the movement of an item to simplify the calculations. The transport cost is then defined as follows: Figure 5 : Cost Function cβ((x1,x2),(y1,y2)) Combining Fairness and Efficiency β: A coefficient used to weight the importance of fairness relative to efficiency. π: This is the \u0026ldquo;moving plan.\u0026rdquo; It indicates how many items are moved from each location in House A to each location in House B. The goal is to find the plan π that minimizes the total moving cost while ensuring an equitable distribution of items between the destinations.\nTo arrive at the final formula, we start with the Wcβ distance, which measures the transport cost between the current distribution γ and the ideal distribution γ*. Using the Euclidean norm to measure the distance between the points (x1, x2) and (y1, y2), we factor and generalize the terms to obtain the following equality:\nFigure 6 : β-Fairness Metric: Combining Fairness and Efficiency in Distribution γ This formula combines both fairness (measured by |x1 - x2|) and efficiency (measured by |x1 + x2 - 2|), weighted by the parameter β. The term max{1, 2 - 2β} ensures that the metric remains normalized between 0 and 1.\nShort example To better understand the impact of this metric, let\u0026rsquo;s examine several concrete cases of information distribution. We will compare different distributions and see how the mutual fairness metric allows us to evaluate them, considering both fairness and efficiency.\nConsider the cases where γ = δ(0,0) and γ* = δ(1,1). We also add an intermediate example γex = δ(0.8, 0.2), where 80% of group 1 has access to the information, compared to only 20% for group 2.\nBy taking β = 0.6, which slightly favors fairness, we obtain the following results:\nfairness(γ*) \u0026gt; fairness(γ), confirming that the information is better distributed when everyone receives it. fairness(γex) ≈ fairness(γ), because although γex is twice as efficient (|0.8 + 0.2 - 2| = 1 versus |0 + 0 - 2| = 2), its score is heavily penalized by the lack of fairness. Indeed, one group is significantly favored over the other, making it as unbalanced as γ. Figure 7 : Fairness Scores for Different Distributions: γ, γex, and γ∗ 3. Metric in practice Mutual fairness in practice We apply this metric to different datasets, which include social networks and communities partitioned into two groups. To do this, we load the dataset as a graph (V, E) and select a seedset S of size varying between 2 and 90. To diffuse the information, we use a probability p in the interval [0,1]. Since the process is stochastic, we repeat the operation 1000 times.\nThe results in Figure 8 illustrate how information propagation varies depending on the probability p. In Figure 8(a), with p = 0.5, the information is disseminated in a way that is both efficient and equitable, with the groups being highly connected. The behavior is deterministic, with similar results in each iteration. In Figure 8(b), when p is reduced to 0.1, the propagation remains equitable but becomes less efficient, reaching on average only 20% of each group instead of nearly 100% as in (a).\nIn Figure 8(c), the outcomes are highly random: depending on the iteration, either one group receives all the information or the other, highlighting an inequity that is overlooked by traditional metrics. Finally, Figure 8(d) reveals a slight bias, where the variance of the distribution extends but does not remain centered on the diagonal. Although some metrics might consider this situation fair, mutual fairness provides more insight by evaluating fairness in each realization rather than just averaging it.\nFigure 8 : Outreach Probability Distributions for Different Propagation Scenarios Impact of β The results of the metric as a function of β are illustrated in Figure 9. Yellow indicates a low transport cost, while blue signals a deviation from the ideal.\nWhen β = 0 Figure 9(a), only efficiency is considered: the information reaches the maximum number of people, but without regard for fairness (low transport cost in (1,1)). In contrast, with β = 1 (Figure 9(d)), only fairness is optimized, resulting in a perfectly equitable distribution but with reduced efficiency because the point (1,1) is no longer the only ideal one.\nAn optimal balance is achieved for β = 0.66 (Figure 9(c)), where the transport cost is minimized at the top-right of the plane. The further one moves away from this point, the higher the cost (blue areas), indicating a reduced level of optimization.\nIn summary, as β increases, fairness is prioritized, and getting closer to the diagonal reduces the transport cost, ensuring an equitable and efficient diffusion of information. Thus, the formula dynamically adjusts the trade-off between fairness and efficiency, with β acting as the weighting parameter that influences the distribution of the cost.\nFigure 9 : Impact of β on Outreach Probability Distributions: Balancing Fairness and Efficiency Now that we have our metric to determine if a distribution is fair and efficient, we want to focus on an algorithm to select the right seeds.\n4. Improving Fairness with S3D Stochastic Seed Selection Descent (S3D) While Mutual Fairness provides a way to measure fairness, Influence Maximization (IM) algorithms still need a method to optimize it without sacrificing outreach. S3D addresses this by dynamically adjusting seed selection to ensure a balanced information spread across all communities.\nInstead of selecting influential nodes solely based on popularity, S3D explores alternative seed sets, optimizing both fairness and outreach through an iterative process:\nThe algorithm follows these key steps:\nInitial Seed Selection:\nInfluential nodes are chosen using traditional heuristics (e.g., degree centrality, community detection).\nExploration of Neighboring States:\nThe algorithm tests alternative seed sets by adding, swapping, or removing nodes.\nFairness Evaluation:\nEach set is scored using the β-Fairness metric, which balances fairness and efficiency.\nAcceptance Criteria (Metropolis-Hastings Selection Rule):\nThe new seed set (S\u0026rsquo;) is accepted with a probability defined as:\nensuring fairness-improving modifications are favored while maintaining some randomness.\nConvergence:\nThe process runs until the fairness score stabilizes, achieving an optimal trade-off between fairness and outreach. Experiment To evaluate the effectiveness of S3D, we compare it against traditional influence maximization algorithms on real-world social networks.\nDataset : Here we can take for example job-matching networks where nodes represent individuals looking for job opportunities and edges represent connections between people (e.g., same school, same company, LinkedIn network).\nTraditional IM methods (bas_d, bas_g) → Select highly connected influencers, ignoring fairness. Fairness-aware heuristics (hrt_d, hrt_g) → Older fairness-based methods with static rules. S3D (our approach) → Dynamically selects seeds to balance fairness and efficiency. Fairness (Mutual Fairness Score) – Measures how equally information is distributed across different groups. Efficiency (Total Outreach) – Measures how many people receive job opportunities in total. Does S3D Improve Fairness? Imagine a job-matching platform that aims to spread job opportunities equally across university graduates and vocational school graduates.\nTraditional IM methods pick influencers mostly from highly connected elite universities, leaving out vocational school graduates. S3D Solution: By dynamically adjusting seed selection, S3D ensures that both communities receive job postings more equitably. Figure 10 : S3D Improvement Across Datasets and Parameters S3D (red points) significantly improves fairness compared to label-blind methods (blue). The outreach distribution shifts towards the diagonal, meaning both groups receive information more equally. Without S3D, certain communities miss job postings entirely in some scenarios. How Does S3D Balance Fairness and Efficiency? S3D achieves the highest fairness scores (y-axis). Minimal efficiency loss (x-axis) → Proves that fairness gains do not come at a high cost. Insight: S3D is most useful in moderately connected networks (e.g., workplaces, schools). Impact of S3D S3D improves information dissemination by ensuring a more equitable distribution between different groups, without sacrificing efficiency. In our job-matching example, it allows graduates of professional schools to access the same opportunities as those from prestigious universities, thus leading to inequalities linked to the social network. Thanks to its adaptive approach, S3D stands out as an effective solution to correct the biases of traditional algorithms while maintaining a broad reach.\n5. Conclusion As a conclusion, we can say that fairness in Social Influence Maximization is crucial to prevent systemic exclusion in information dissemination. Traditional methods often favor well-connected individuals, reinforcing existing inequalities. By introducing Mutual Fairness and the S3D algorithm, we provide a framework that balances fairness and efficiency, ensuring a more equitable outreach. Through our job-matching case study for example, we demonstrated that S3D significantly reduces bias while maintaining high influence spread. These results confirm that fairness-aware approaches can be both practical and impactful, making them essential for real-world applications such as hiring, education, or public awareness campaigns.\nReferences Chowdhary, S., et al. (2024). Fairness in Social Influence Maximization via Optimal Transport. NeurIPS 2024. Available here https://neurips.cc/virtual/2024/poster/94521. ","content_html":"\u003ch3 id=\"authors-guillaume-marin-bertin--jaishan-burton-elmo\"\u003eAuthors: Guillaume MARIN-BERTIN \u0026amp; Jaishan BURTON ELMO\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTable of Contents\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003e1. Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#mutual-fairness\"\u003e2. Mutual Fairness: A New Metric\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#why-make-a-new-metric\"\u003e2.1 Why Make a New Metric?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#proposed-fairness-metric\"\u003e2.2 Proposed Fairness Metric\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#short-example\"\u003e2.3 Short Example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#fairness-evaluation\"\u003e3. Metric in Practice\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#mutual-fairness-practice\"\u003e3.1 Mutual Fairness in Practice\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#impact-of-beta\"\u003e3.2 Impact of β\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#s3d-algorithm\"\u003e4. Improving Fairness with S3D\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#stochastic-seed-selection-descent\"\u003e4.1 Stochastic Seed Selection Descent (S3D)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#experimentation\"\u003e4.2 Experimentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#impact-of-s3d\"\u003e4.3 Impact of S3D\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003e5. Conclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the article “Fairness in Social Influence Maximization via Optimal Transport” published by Shubham Chowdhary et al. in 2024 and available \u003ca href=\"https://neurips.cc/virtual/2024/poster/94521\"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"1-introduction\"\u003e\u003ca href=\"#introduction\"\u003e1. Introduction\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIn today’s digital society, social networks play a major role in how information spreads. Whether it is a public health campaign, a political message, or viral marketing, the ability to maximize influence is crucial. Companies, governments, and organizations leverage Influence Maximization (IM) algorithms to strategically select key individuals—called seeds—who will initiate a diffusion process, ensuring that information reaches the largest possible audience.\u003c/p\u003e\n\u003cp\u003eImagine a scenario where a job-matching platform uses an Influence Maximization (IM) strategy to promote career opportunities to young professionals. The goal is to spread job postings efficiently across different professional communities. However, due to the structure of the social network, the algorithm selects key individuals (seeds) in such a way that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn 50% of cases, all the job opportunities are shared within a network of university graduates, while vocational school graduates receive none.\u003c/li\u003e\n\u003cli\u003eIn the other 50% of cases, the opposite happens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAt first glance, this might seem fair: both groups have an equal expected chance of seeing the job offers. However, in practice, one group is always entirely excluded in each scenario, creating a systemic disadvantage for those who miss out on critical career opportunities.\u003c/p\u003e\n\u003cp\u003eTo fix this, researchers have tried adding fairness constraints to IM algorithms. The main ideas include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEquity-based fairness\u003c/strong\u003e (Stoica et al., 2020): Ensures that each group has the same expected proportion of influenced users.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMax-min fairness\u003c/strong\u003e (Fish et al., 2019; Zhu et al., 2019): Maximizes the minimum probability that any group receives information.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDiversity-aware methods\u003c/strong\u003e (Tsang et al., 2019): Ensure that no single group dominates the influence process.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDespite these efforts, a fundamental issue remains: existing metrics treat groups independently and do not capture the joint probability of outreach. This means they can create the illusion of fairness while still allowing systematic exclusion of certain communities.\u003c/p\u003e\n\u003cp\u003eTo address this, we propose a new fairness-aware framework that better distributes influence across communities:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eMutual Fairness\u003c/strong\u003e: A better way to measure fairness, inspired by Optimal Transport. Instead of just looking at how much information each group gets in total, it ensures they receive the message at the same time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eS3D Algorithm\u003c/strong\u003e: A smarter way to choose influencers, balancing fairness and efficiency. It adjusts seed selection dynamically to improve fairness while still reaching as many people as possible.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"2-mutual-fairness-a-new-metric\"\u003e\u003ca href=\"#mutual-fairness\"\u003e2. Mutual Fairness: A New Metric\u003c/a\u003e\u003c/h3\u003e\n\u003ch5 id=\"why-make-a-new-metric\"\u003e\u003cstrong\u003eWhy make a new metric?\u003c/strong\u003e\u003c/h5\u003e\n\u003cp\u003eTo understand why a new metric is necessary, let’s examine two situations where information propagation appears fair,we take the example of the introduction.\u003c/p\u003e\n\u003cp\u003eConsider two groups, C1 and C2, each with an outreach probability distribution:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig1\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure1.png\" alt=\"figure1\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 1 : Outreach Probability Distribution for Groups C1 and C2\u003c/i\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eδ\u003csub\u003e0\u003c/sub\u003e represents the case where the information is not transmitted.\u003c/li\u003e\n\u003cli\u003eδ\u003csub\u003e1\u003c/sub\u003e represents the case where the information is successfully received by the group.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOn average, both groups have a 50% chance of receiving the information, which might suggest a fair situation. However, this average hides significant differences in the actual distribution of information, as illustrated in the example below:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig2\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure2.png\" alt=\"figure2\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 2 : Comparison of Two Joint Probability Distributions: γa and γb\u003c/i\u003e\n\u003c/p\u003e\n\u003cp\u003eFirst case (γa) :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eδ\u003csub\u003e(0,0)\u003c/sub\u003e: Neither group receives the information (50%).\u003c/li\u003e\n\u003cli\u003eδ\u003csub\u003e(1,1)\u003c/sub\u003e: Both groups receive the information simultaneously (50%).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis configuration is fair because if one group does not receive the information, the other does not either.\u003c/p\u003e\n\u003cp\u003eSecond case (γb) :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eδ\u003csub\u003e(0,0)\u003c/sub\u003e: Neither group receives the information (25%).\u003c/li\u003e\n\u003cli\u003eδ\u003csub\u003e(1,1)\u003c/sub\u003e: Both groups receive the information simultaneously (25%).\u003c/li\u003e\n\u003cli\u003eδ\u003csub\u003e(0,1)\u003c/sub\u003e: Group 1 does not receive the information, but Group 2 does (25%).\u003c/li\u003e\n\u003cli\u003eδ\u003csub\u003e(1,0)\u003c/sub\u003e: Group 1 receives the information, but Group 2 does not (25%).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this case, there is a 50% chance that only one group will receive the information, creating an inequality. However, the marginal averages remain the same (µ\u003csub\u003e1\u003c/sub\u003e=µ\u003csub\u003e2\u003c/sub\u003e=50%).\u003c/p\u003e\n\u003cp\u003eThe distributions γ\u003csub\u003ea\u003c/sub\u003e and γ\u003csub\u003eb\u003c/sub\u003e are very different, yet the marginal averages do not capture these nuances. This highlights the need for a new metric to evaluate fairness more precisely, beyond simple averages.\u003c/p\u003e\n\u003ch4 id=\"proposed-fairness-metric\"\u003e\u003cstrong\u003eProposed fairness metric\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eThe idea is to start from a joint probability measure γ rather than the marginal averages μ\u003csub\u003ei\u003c/sub\u003e. From γ, we aim to measure the distance between it and an ideal distribution γ*, where each group receives the same amount of information in all cases.\u003c/p\u003e\n\u003cp\u003eTo illustrate this on a figure:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe x-axis represents the amount of information received by group 1.\u003c/li\u003e\n\u003cli\u003eThe y-axis represents the amount of information received by group 2.\u003c/li\u003e\n\u003cli\u003eThe diagonal signifies that both groups receive exactly the same amount of information, which corresponds to the ideal sought.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA movement perpendicular to the diagonal should be penalized, as it indicates an imbalance in the distribution of information between the groups. Similarly, we must penalize movement along the diagonal, as it affects efficiency. These costs are measured by Euclidean distance.\nIn the \u003ca href=\"#fig3\"\u003eFigure 3\u003c/a\u003e below, we observe the transition from a distribution γa to a distribution γb, broken down into two components. One is dashed, representing the efficiency, and the other is solid, representing the fairness.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig3\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure3.png\" alt=\"figure3\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 3 : Representation of the transport cost between two points (x1,x2) and(y1,y2) with an intermediate point z(x1,x2,y1,y2)\u003c/i\u003e\n\u003c/p\u003e\n\u003cp\u003eTo define this cost, we use the following formula:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig4\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure4.png\" alt=\"figure4\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 4 : Optimal Transport Cost Wcβ(γa,γb) Between Distributions γa and γb\u003c/i\u003e\n\u003c/p\u003e\n\u003cp\u003eWe can draw an analogy with a move, where the goal is to transport items from House A to House B at the lowest possible cost, while ensuring that each item reaches its final destination according to W\u003csub\u003ecβ\u003c/sub\u003e(γ\u003csub\u003ea\u003c/sub\u003e, γ\u003csub\u003eb\u003c/sub\u003e).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eγ\u003csub\u003ea\u003c/sub\u003e: represents the list of items present in House A, specifying where they are located and in what quantity.\u003c/li\u003e\n\u003cli\u003eγ\u003csub\u003eb\u003c/sub\u003e: represents the list of items expected in House B, indicating where they should arrive and in what quantity.\u003c/li\u003e\n\u003cli\u003ec: This is the function that specifies the cost of moving an item from one location to another. In \u003ca href=\"#fig5\"\u003efigure 5\u003c/a\u003e, we evaluate the transport cost between the point (x1, x2) and (y1, y2). We take z(x1, x2, y1, y2) as an intermediate point in the movement of an item to simplify the calculations. The transport cost is then defined as follows:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig5\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure5.png\" alt=\"figure5\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 5 : Cost Function cβ((x1,x2),(y1,y2)) Combining Fairness and Efficiency\u003c/i\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eβ: A coefficient used to weight the importance of fairness relative to efficiency.\u003c/li\u003e\n\u003cli\u003eπ: This is the \u0026ldquo;moving plan.\u0026rdquo; It indicates how many items are moved from each location in House A to each location in House B.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe goal is to find the plan π that minimizes the total moving cost while ensuring an equitable distribution of items between the destinations.\u003c/p\u003e\n\u003cp\u003eTo arrive at the final formula, we start with the W\u003csub\u003ecβ\u003c/sub\u003e distance, which measures the transport cost between the current distribution γ and the ideal distribution γ*. Using the Euclidean norm to measure the distance between the points (x1, x2) and (y1, y2), we factor and generalize the terms to obtain the following equality:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig6\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure6.png\" alt=\"figure6\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 6 : β-Fairness Metric: Combining Fairness and Efficiency in Distribution γ\u003c/i\u003e\n\u003c/p\u003e\n\u003cp\u003eThis formula combines both fairness (measured by |x1 - x2|) and efficiency (measured by |x1 + x2 - 2|), weighted by the parameter β. The term max{1, 2 - 2β} ensures that the metric remains normalized between 0 and 1.\u003c/p\u003e\n\u003ch4 id=\"short-example\"\u003e\u003cstrong\u003eShort example\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eTo better understand the impact of this metric, let\u0026rsquo;s examine several concrete cases of information distribution. We will compare different distributions and see how the mutual fairness metric allows us to evaluate them, considering both fairness and efficiency.\u003c/p\u003e\n\u003cp\u003eConsider the cases where γ = δ\u003csub\u003e(0,0)\u003c/sub\u003e and γ* = δ\u003csub\u003e(1,1)\u003c/sub\u003e. We also add an intermediate example γ\u003csub\u003eex\u003c/sub\u003e = δ\u003csub\u003e(0.8, 0.2)\u003c/sub\u003e, where 80% of group 1 has access to the information, compared to only 20% for group 2.\u003c/p\u003e\n\u003cp\u003eBy taking β = 0.6, which slightly favors fairness, we obtain the following results:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efairness(γ*) \u0026gt; fairness(γ), confirming that the information is better distributed when everyone receives it.\u003c/li\u003e\n\u003cli\u003efairness(γ\u003csub\u003eex\u003c/sub\u003e) ≈ fairness(γ), because although γ\u003csub\u003eex\u003c/sub\u003e is twice as efficient (|0.8 + 0.2 - 2| = 1 versus |0 + 0 - 2| = 2), its score is heavily penalized by the lack of fairness. Indeed, one group is significantly favored over the other, making it as unbalanced as γ.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig7\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure7.png\" alt=\"figure7\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 7 : Fairness Scores for Different Distributions: γ, γex, and γ∗\u003c/i\u003e\n\u003c/p\u003e\n\u003ch3 id=\"3-metric-in-practice\"\u003e\u003ca href=\"#fairness-evaluation\"\u003e3. Metric in practice\u003c/a\u003e\u003c/h3\u003e\n\u003ch4 id=\"mutual-fairness-in-practice\"\u003e\u003cstrong\u003eMutual fairness in practice\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eWe apply this metric to different datasets, which include social networks and communities partitioned into two groups. To do this, we load the dataset as a graph (V, E) and select a seedset S of size varying between 2 and 90. To diffuse the information, we use a probability p in the interval [0,1]. Since the process is stochastic, we repeat the operation 1000 times.\u003c/p\u003e\n\u003cp\u003eThe results in \u003ca href=\"#fig8\"\u003eFigure 8\u003c/a\u003e illustrate how information propagation varies depending on the probability p. In \u003ca href=\"#fig8\"\u003eFigure 8\u003c/a\u003e(a), with p = 0.5, the information is disseminated in a way that is both efficient and equitable, with the groups being highly connected. The behavior is deterministic, with similar results in each iteration. In \u003ca href=\"#fig8\"\u003eFigure 8\u003c/a\u003e(b), when p is reduced to 0.1, the propagation remains equitable but becomes less efficient, reaching on average only 20% of each group instead of nearly 100% as in (a).\u003c/p\u003e\n\u003cp\u003eIn \u003ca href=\"#fig8\"\u003eFigure 8\u003c/a\u003e(c), the outcomes are highly random: depending on the iteration, either one group receives all the information or the other, highlighting an inequity that is overlooked by traditional metrics. Finally, \u003ca href=\"#fig8\"\u003eFigure 8\u003c/a\u003e(d) reveals a slight bias, where the variance of the distribution extends but does not remain centered on the diagonal. Although some metrics might consider this situation fair, mutual fairness provides more insight by evaluating fairness in each realization rather than just averaging it.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig8\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure8.png\" alt=\"figure8\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 8 : Outreach Probability Distributions for Different Propagation Scenarios\u003c/i\u003e\n\u003c/p\u003e\n\u003ch4 id=\"impact-of--β\"\u003e\u003cstrong\u003eImpact of  β\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eThe results of the metric as a function of β are illustrated in Figure 9. Yellow indicates a low transport cost, while blue signals a deviation from the ideal.\u003c/p\u003e\n\u003cp\u003eWhen β = 0 \u003ca href=\"#fig9\"\u003eFigure 9\u003c/a\u003e(a), only efficiency is considered: the information reaches the maximum number of people, but without regard for fairness (low transport cost in (1,1)). In contrast, with β = 1 (\u003ca href=\"#fig9\"\u003eFigure 9\u003c/a\u003e(d)), only fairness is optimized, resulting in a perfectly equitable distribution but with reduced efficiency because the point (1,1) is no longer the only ideal one.\u003c/p\u003e\n\u003cp\u003eAn optimal balance is achieved for β = 0.66 (\u003ca href=\"#fig9\"\u003eFigure 9\u003c/a\u003e(c)), where the transport cost is minimized at the top-right of the plane. The further one moves away from this point, the higher the cost (blue areas), indicating a reduced level of optimization.\u003c/p\u003e\n\u003cp\u003eIn summary, as β increases, fairness is prioritized, and getting closer to the diagonal reduces the transport cost, ensuring an equitable and efficient diffusion of information. Thus, the formula dynamically adjusts the trade-off between fairness and efficiency, with β acting as the weighting parameter that influences the distribution of the cost.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig9\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure9.png\" alt=\"figure9\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 9 : Impact of β on Outreach Probability Distributions: Balancing Fairness and Efficiency\u003c/i\u003e\n\u003c/p\u003e\n\u003cp\u003eNow that we have our metric to determine if a distribution is fair and efficient, we want to focus on an algorithm to select the right seeds.\u003c/p\u003e\n\u003ch3 id=\"4-improving-fairness-with-s3d\"\u003e\u003ca href=\"#s3d-algorithm\"\u003e4. Improving Fairness with S3D\u003c/a\u003e\u003c/h3\u003e\n\u003ch4 id=\"stochastic-seed-selection-descent-s3d\"\u003eStochastic Seed Selection Descent (S3D)\u003c/h4\u003e\n\u003cp\u003eWhile Mutual Fairness provides a way to measure fairness, Influence Maximization (IM) algorithms still need a method to optimize it without sacrificing outreach. S3D addresses this by dynamically adjusting seed selection to ensure a balanced information spread across all communities.\u003c/p\u003e\n\u003cp\u003eInstead of selecting influential nodes solely based on popularity, S3D explores alternative seed sets, optimizing both fairness and outreach through an iterative process:\u003c/p\u003e\n\u003cp\u003eThe algorithm follows these key steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInitial Seed Selection\u003c/strong\u003e:\u003cbr\u003e\nInfluential nodes are chosen using traditional heuristics (e.g., degree centrality, community detection).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eExploration of Neighboring States\u003c/strong\u003e:\u003cbr\u003e\nThe algorithm tests alternative seed sets by adding, swapping, or removing nodes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFairness Evaluation\u003c/strong\u003e:\u003cbr\u003e\nEach set is scored using the β-Fairness metric, which balances fairness and efficiency.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAcceptance Criteria (Metropolis-Hastings Selection Rule)\u003c/strong\u003e:\u003cbr\u003e\nThe new seed set (S\u0026rsquo;) is accepted with a probability defined as:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure12.png\" alt=\"figure12\"\u003e\n\u003c/p\u003e\n\u003cp\u003eensuring fairness-improving modifications are favored while maintaining some randomness.\u003c/p\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e\u003cstrong\u003eConvergence\u003c/strong\u003e:\u003cbr\u003e\nThe process runs until the fairness score stabilizes, achieving an optimal trade-off between fairness and outreach.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"experiment\"\u003eExperiment\u003c/h4\u003e\n\u003cp\u003eTo evaluate the effectiveness of S3D, we compare it against traditional influence maximization algorithms on real-world social networks.\u003c/p\u003e\n\u003cp\u003eDataset : Here we can take for example job-matching networks where nodes represent individuals looking for job opportunities and edges represent connections between people (e.g., same school, same company, LinkedIn network).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraditional IM methods (bas_d, bas_g) → Select highly connected influencers, ignoring fairness.\u003c/li\u003e\n\u003cli\u003eFairness-aware heuristics (hrt_d, hrt_g) → Older fairness-based methods with static rules.\u003c/li\u003e\n\u003cli\u003eS3D (our approach) → Dynamically selects seeds to balance fairness and efficiency.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003eFairness (Mutual Fairness Score) – Measures how equally information is distributed across different groups.\u003c/li\u003e\n\u003cli\u003eEfficiency (Total Outreach) – Measures how many people receive job opportunities in total.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"does-s3d-improve-fairness\"\u003eDoes S3D Improve Fairness?\u003c/h4\u003e\n\u003cp\u003eImagine a job-matching platform that aims to spread job opportunities equally across university graduates and vocational school graduates.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraditional IM methods pick influencers mostly from highly connected elite universities, leaving out vocational school graduates.\u003c/li\u003e\n\u003cli\u003eS3D Solution: By dynamically adjusting seed selection, S3D ensures that both communities receive job postings more equitably.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca name=\"fig10\"\u003e\u003c/a\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure10.png\" alt=\"figure10\"\u003e\n  \u003cbr\u003e\n  \u003ci\u003eFigure 10 : S3D Improvement Across Datasets and Parameters\u003c/i\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eS3D (red points) significantly improves fairness compared to label-blind methods (blue).\u003c/li\u003e\n\u003cli\u003eThe outreach distribution shifts towards the diagonal, meaning both groups receive information more equally.\nWithout S3D, certain communities miss job postings entirely in some scenarios.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"how-does-s3d-balance-fairness-and-efficiency\"\u003eHow Does S3D Balance Fairness and Efficiency?\u003c/h4\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"\\images\\Burton_Elmo-MARIN_BERTIN\\images\\figure11.png\" alt=\"figure11\"\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eS3D achieves the highest fairness scores (y-axis).\u003c/li\u003e\n\u003cli\u003eMinimal efficiency loss (x-axis) → Proves that fairness gains do not come at a high cost.\u003c/li\u003e\n\u003cli\u003eInsight: S3D is most useful in moderately connected networks (e.g., workplaces, schools).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"impact-of-s3d\"\u003eImpact of S3D\u003c/h4\u003e\n\u003cp\u003eS3D improves information dissemination by ensuring a more equitable distribution between different groups, without sacrificing efficiency. In our job-matching example, it allows graduates of professional schools to access the same opportunities as those from prestigious universities, thus leading to inequalities linked to the social network. Thanks to its adaptive approach, S3D stands out as an effective solution to correct the biases of traditional algorithms while maintaining a broad reach.\u003c/p\u003e\n\u003ch3 id=\"5-conclusion\"\u003e\u003ca href=\"#conclusion\"\u003e5. Conclusion\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAs a conclusion, we can say that fairness in Social Influence Maximization is crucial to prevent systemic exclusion in information dissemination. Traditional methods often favor well-connected individuals, reinforcing existing inequalities. By introducing Mutual Fairness and the S3D algorithm, we provide a framework that balances fairness and efficiency, ensuring a more equitable outreach. Through our job-matching case study for example, we demonstrated that S3D significantly reduces bias while maintaining high influence spread. These results confirm that fairness-aware approaches can be both practical and impactful, making them essential for real-world applications such as hiring, education, or public awareness campaigns.\u003c/p\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eChowdhary, S., et al. (2024). Fairness in Social Influence Maximization via Optimal Transport. \u003cem\u003eNeurIPS 2024\u003c/em\u003e. Available here \u003ca href=\"https://neurips.cc/virtual/2024/poster/94521\"\u003ehttps://neurips.cc/virtual/2024/poster/94521\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/fairness-in-social-influence-maximization-via-optimal-transport/","date_published":"15036-15-09T327:1515:00+01:00","date_modified":"15036-15-09T327:1515:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"e90b79bcfa152b5f35973646267a48e5aa26225c","title":"Knowledge Distillation:  Boosting Interpretability in Deep Learning Models","summary":"","content_text":" \u003c!DOCTYPE html\u003e Interpretability, the hidden power of knowledge distillation Published March 15, 2025\nUpdate on GitHub bryanbradfo Bryan Chen\nremicsk Rémi Calvet\nKnowledge distillation is a powerful technique to transfer the knowledge from a large \u0026ldquo;teacher\u0026rdquo; model to a \u0026ldquo;student\u0026rdquo; model. While it\u0026rsquo;s commonly used to improve performance and reduce computational costs by compressing large models, this blog post explores a fascinating discovery: knowledge distillation can also enhance model interpretability. We\u0026rsquo;ll dive into the paper On the Impact of Knowledge Distillation for Model Interpretability\u0026quot; (ICML 2023) by H. Han et al., which sheds light on this novel perspective.\nInterpretability in AI allows researchers, engineers, and decision-makers to trust and control machine learning models. Recent models show impressive performance on many different tasks and often rely on deep learning models. Unfortunately, deep learning models are also know for the difficulty to interprete them and understand how they come to a result wich can be problematic in highly sensitive applications like autonomous driving or healthcare. The article we present in this blog shows that knowledge distillation can improve the interpretability of deep learning models.\nWhen AI is a black box, you\u0026rsquo;re just hoping for the best. But when you understand it, you become unstoppable.\n0. Table of Contents I. Crash Course on Knowledge Distillation and Label Smoothing II. Defining Interpretability Through Network Dissection III. Logit Distillation \u0026amp; Feature Distillation: A Powerful Duo for Interpretability IV. Why Knowledge Distillation Enhances Interpretability V. Experimental Results and Reproduction VI. Beyond Network Dissection: Other Interpretability Metrics Conclusion Join the Discussion I. Crash Course on Knowledge Distillation and Label Smoothing What is Knowledge Distillation? Knowledge distillation (KD) is a model compression technique introduced by Hinton et al. (2015) that transfers knowledge from a complex teacher model to a simpler student model. Unlike traditional training where models learn directly from hard labels (one-hot encodings), KD allows the student to learn from the teacher\u0026rsquo;s soft probability distributions.\nThe Key Mechanics of Knowledge Distillation The standard KD loss function combines the standard cross-entropy loss with a distillation loss term:\n$$\\mathcal{L}_{KD}=(1-\\alpha)\\mathrm{CE}(y,\\sigma(z_s))+\\alpha T^2 \\mathrm{CE}(\\sigma(z_t^T),\\sigma(z_s^T))$$\nWhere:\n$z_s$ and $z_t$ are the logits from the student and teacher models $T$ is the temperature parameter that controls softening of probability distributions $z_s^T \\mathrel{:}= \\frac{z_s}{T}$ and $z_t^T \\mathrel{:}= \\frac{z_t}{T}$ $\\sigma$ is the softmax function $\\sigma(z_s^T) \\mathrel{:}= \\frac{\\exp(z_s^T)}{\\sum_j \\exp(z_j^T)}$ and $\\sigma(z_t^T) \\mathrel{:}= \\frac{\\exp(z_t^T)}{\\sum_j \\exp(z_j^T)}$ $\\mathrm{CE}$ is cross-entropy loss $\\alpha$ balances the importance of each loss component. The first part of the loss $(1-\\alpha)\\mathrm{CE}(y,\\sigma(z_s))$ is to incitate the student model to learn from one hot encoded ground truth label.\nThe second part of the loss $\\alpha T^2 \\mathrm{CE}(\\sigma(z_t^T),\\sigma(z_s^T))$ is to incitate the student model to try to reproduce the ouputs of the teacher model. This is what permits the student to learn from the teacher. The larger $\\alpha$ is, the more the student will try to replicate the teacher model\u0026rsquo;s outputs and ignore the one hot encoded groundtruth and vice versa.\nLabel Smoothing Label smoothing (LS) is another technique that smooths hard targets by mixing them with a uniform distribution. In the cross entropy loss we replace the one hot encoded $y$ by $y_{LS} \\mathrel{:}= (1-\\alpha)y + \\frac{\\alpha}{K}$, where $K$ is the number of classes and $\\alpha$ the smoothing parameter:\nWe obtain a loss that is similar to knowledge diffusion but there is a key difference important for interpretability that we will discuss later. From the equation above, we get the label smoothing loss equation: $$L_{LS} = (1-\\alpha)\\mathrm{CE}(y,\\sigma(z)) + \\alpha\\mathrm{CE}(u,\\sigma(z)) $$ Where $u$ is a uniform distribution over all the possible $K$ classes.\nII. Defining Interpretability Through Network Dissection The first thing to know is that there are different approaches to define and measure interpretability in machine learning.\nFor image classification, the authors use network dissection to quantitatively measure interpretability. The idea is to compare activation maps and see if areas with high activation correspond to an object or a meaningful concept on the image.\nThe process can be better understood through the following illustration:\nFeed a neural network model an image, pick a deep layer and count the number of neurons that detects a concept like \u0026ldquo;cat\u0026rdquo; or \u0026ldquo;dog\u0026rdquo;. We call those neurons concept detectors and will define them more precisely. The number of concept detectors will be the primary metric to define the interpretability of a model, the higher the more we will consider it interpretable.\nThe easiest way to understand what is a concept detector is to look at the following pseudo code to compute the number of concept detectors:\n1. Selecting the Layer First, we need to choose a layer $\\mathcal{l}$ to dissect, typically deep in the network.\n2. Processing Each Image For each image x in the dataset:\nFeedforward Pass:\nInput an image x of shape $ (n,n) $ into the neural network. Activation Extraction:\nFor each neuron in layer $\\mathcal{l}$, collect the activation maps: \\[ A_i(x) \\in \\mathbb{R}^{d \\times d}, \\quad \\text{where } d \u003c n \\text{ and } i \\text{ is the neuron index.} \\] 3. Defining Activation Distribution For each neuron i in the layer $\\mathcal{l}$:\nDefine ai as the empirical distribution of activation values across different images x. 4. Computing Activation Threshold Compute a threshold Ti such that: \\[ P(a_i \\geq T_i) = 0.005 \\] - This ensures only the **top 0.5%** activations are retained. 5. Resizing Activation Maps Interpolate Ai to match the dimension $ (n,n) $ for direct comparison with input images. 6. Creating Binary Masks For each image x:\nGenerating Activation Masks:\nCreate a binary mask $ A_i^{\\text{mask}}(x) $ of shape $ (n,n) $: \\[ A_i^{\\text{mask}}(x)[j,k] = \\begin{cases} 1, \u0026 \\text{if } A_i(x)[j,k] \\geq T_i \\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\] This retains only the highest activations. Using Ground Truth Masks:\nGiven a ground truth mask $ M_c(x) $ of shape $ (n,n) $, where: $ M_c(x)[j,k] = 1 $ if the pixel in x belongs to class c, otherwise 0. Computing Intersection over Union (IoU):\nCalculate the IoU between Aimask(x) and Mc(x): \\[ \\text{IoU}_{i,c} = \\frac{|A_i^{\\text{mask}}(x) \\cap M_c(x)|}{|A_i^{\\text{mask}}(x) \\cup M_c(x)|} \\] If $\\text{IoU}_{i,c} \u0026gt; 0.05$, the neuron i is considered a concept detector for concept c. If you prefer to understand with code, here is an implementation of the procedure described above: def identify_concept_detectors(model, layer_name, dataset, concept_masks): \u0026#34;\u0026#34;\u0026#34; Identify neurons that act as concept detectors in a specific layer. Args: model: Neural network model layer_name: Name of the layer to analyze dataset: Dataset with images concept_masks: Dictionary mapping images to concept segmentation masks Returns: Dictionary mapping neurons to detected concepts \u0026#34;\u0026#34;\u0026#34; # Step 1: Collect activation maps for each image activation_maps = {} for image in dataset: # Forward pass and extract activation at specified layer activations = get_layer_activation(model, layer_name, image) for neuron_idx, activation in enumerate(activations): if neuron_idx not in activation_maps: activation_maps[neuron_idx] = [] activation_maps[neuron_idx].append(activation) # Step 2: Compute threshold for top 0.5% activations for each neuron thresholds = {} for neuron_idx, activations in activation_maps.items(): # Flatten all activations for this neuron all_activations = torch.cat([act.flatten() for act in activations]) # Compute threshold for top 0.5% threshold = torch.quantile(all_activations, 0.995) thresholds[neuron_idx] = threshold # Step 3: Create binary masks and compute IoU with concept masks concept_detectors = {} for image_idx, image in enumerate(dataset): image_concepts = concept_masks[image_idx] for neuron_idx, activations in activation_maps.items(): # Get activation for this neuron on this image activation = activations[image_idx] # Create binary mask using threshold binary_mask = (activation \u0026gt;= thresholds[neuron_idx]).float() # Resize to match image dimensions binary_mask = F.interpolate( binary_mask.unsqueeze(0).unsqueeze(0), size=image.shape[1:], mode=\u0026#39;bilinear\u0026#39; ).squeeze() # Compute IoU with each concept mask for concept, mask in image_concepts.items(): intersection = torch.sum(binary_mask * mask) union = torch.sum(binary_mask) + torch.sum(mask) - intersection iou = intersection / union if union \u0026gt; 0 else 0 # If IoU exceeds threshold (typically 0.05), consider it a detector if iou \u0026gt; 0.05: if neuron_idx not in concept_detectors: concept_detectors[neuron_idx] = set() concept_detectors[neuron_idx].add(concept) return concept_detectors III. Logit Distillation \u0026 Feature Distillation: A Powerful Duo for Interpretability Combining logit distillation with feature distillation not only boosts performance but also enhances the interpretability of student models. This improvement is measured by an increase in the number of concept detectors, which represent units aligned with human-interpretable concepts.\nwhere Attention Transfer (AT), Factor Transfer (FT), Contrastive Representation Distillation (CRD), and Self-Supervised Knowledge Distillation (SSKD) are all variations of knowledge distillation techniques, each designed to transfer knowledge from teacher models to student models in unique ways.\nHow they work together? Logit Distillation: Transfers class-similarity information from the teacher to the student through softened logits. Helps the student model understand the relationships between semantically similar classes, making activation maps more object-centric. Feature Distillation: Focuses on aligning intermediate layer features between the teacher and student. Improves the student model\u0026rsquo;s ability to replicate the teacher’s feature representations, supporting richer internal representations. IV. Why Knowledge Distillation Enhances Interpretability The key insight from the paper is that knowledge distillation transfers not just the ability to classify correctly, but also class-similarity information that makes the model focus on more interpretable features.\nTransfer of Class Similarities When a teacher model sees an image of a dog, it might assign:\n85% probability to \u0026ldquo;Golden Retriever\u0026rdquo; 10% probability to other dog breeds 5% probability to other animals and objects These \u0026ldquo;soft targets\u0026rdquo; (consequence of logit distillation) encode rich hierarchical information about how classes relate. The student model distilling this knowledge learns to focus on features that are common to similar classes (e.g., general \u0026ldquo;dog\u0026rdquo; features).\nLabel Smoothing vs. Knowledge Distillation By looking at the KD and label smoothing losses, we can see that they are similar. When $T=1$ they only differ in the second member where we have a $\\sigma(z_t^T)$ that contains class-similarity information instead of $u$ that doesn\u0026rsquo;t contain any information.\n$\\mathcal{L}_{KD}=(1-\\alpha)\\mathrm{CE}(y,\\sigma(z_s))+\\alpha T^2 \\mathrm{CE}(\\sigma(z_t^T),\\sigma(z_s^T))$ $L_{LS} = (1-\\alpha)\\mathrm{CE}(y,\\sigma(z)) + \\alpha\\mathrm{CE}(u,\\sigma(z)) $ So, if there is a difference in interpretability, it is likely that it comes from the fact that distillation permits to get class similarity knowledge from the teacher model. This is exactly what is shown in the figure below. Knowledge distillation guides student models to focus on more object-centric features rather than background or contextual features. This results in activation maps that better align with the actual objects in images.\nThe next figure also highlights the loss of interpretability (less concept detectors) when using label smoothing and the improvement of interpretability (more concept detectors) for KD:\nWhile label smoothing can improve accuracy, it often reduces interpretability by erasing valuable class relationships while KD keeps class relationship information and improves both accuracy and interpretability.\nV. Experimental Results and Reproduction Let\u0026rsquo;s implement a reproduction of one of the paper\u0026rsquo;s key experiments to see knowledge distillation\u0026rsquo;s effect on interpretability in action.\nSetting Up the Experiment We are going to replicate the experiment by using the GitHub repository provided by the authors. The repository contains the code to train the models, compute the concept detectors, and evaluate the interpretability of the models.\nAs it is often the case with a machine learning paper, running the code to reproduce results requires some struggle. To reproduce the results, you could use a virtual environment (e.g. SSP Cloud Datalab) and then do the following:\ngit clone https://github.com/Rok07/KD_XAI.git cd torchdistill pip install -e . cd .. bash script/dlbroden.sh nano torchdistill/torchdistill/models/custom/bottleneck/__init__.py ~ comment the first line pip install opencv-python pip install imageio sudo apt update sudo apt install -y libgl1-mesa-glx nano util/vecquantile.py ~ change NaN by nan nano loader/data_loader.py ~ add out[i] = rgb[:,:,0] + (rgb[:,:,1].astype(np.uint16) * 256) cd .. nano settings.py ~ change TEST_MODE = False to True cd dataset/broden1_224 cp index.csv index_sm.csv ~ keep the 4000 first lines cd ../.. nano visualize/bargraph.py ~ change parameter threshold of bar_graph_svg() to 0.001 python main.py Network Dissection quantifies the interpretability of hidden units by measuring their alignment with human-interpretable concepts. The following results reveal several interesting findings:\n1. Concept Distribution (from bargraph.svg): ~6 units detecting object concepts ~2 units detecting scene concepts 1 unit detecting material properties ~13 units detecting textures ~6 units detecting colors 2. Specific Units: (layer4-0xxx.jpg) Unit 330 has specialized in detecting grid and regular pattern textures Unit 202 detects sky regions in images The network dissection approach reveals interpretable neurons of a distilled ResNet18.\nVI. Beyond Network Dissection: Other Interpretability Metrics While the paper emphasizes the use of Network Dissection to measure model interpretability by quantifying concept detectors, it also explores several additional metrics to confirm the broader impact of Knowledge Distillation (KD) on interpretability:\nFive-Band Scores, proposed by Tjoah \u0026 Guan (2020): This metric assesses interpretability by evaluating pixel accuracy (accuracy of saliency maps in identifying critical features), precision (how well the saliency maps match the actual distinguishing features), recall, and false positive rates (FPR, lower FPR indicates better interpretability) using a synthesized dataset with heatmap ground truths. KD-trained models consistently show higher accuracy and lower FPR compared to other methods. DiffROAR Scores, proposed by Shah et al. (2021): This evaluates the difference in predictive power on a model trained on a dataset and a model trained on a version of the dataset where we removed top and bottom x% of the pixel according to their importance for the task. The authors find that KD has a higher DiffROAR score than a model trained from scratch. It means that KD makes the model use more relevant features and thus more interpretable in that sense. Loss Gradient Alignment: This metric measures the alignment of model gradients with human-perceived important features. KD models exhibit better alignment, indicating greater interpretability as we can see on this figure: These metrics collectively show that KD can enhance interpretability. The consistent results showing that knowledge distillation can enhance interpretability for different metrics of interpretability provide strong arguments to believe that KD could be broadly used for better interpretability of deep learning models. Conclusion The article showed that knowledge distillation can improve both accuracy and interpretability. They attribute the improvement in interpretability to the transfer of class similarity knowledge from the teacher to the student model. They compare label smoothing (LS) that is similar to KD but LS does not benefit from class-similarity information. The empirical experiments shows better accuracy for LS and KD but the interpretability of LS decreases whereas it increases for KD confirming the hypothesis that class similarity knowledge has a role in interpretability. The authors obtain consistent results when using other metrics than the number of concept detectors for interpretability showing that their approach is robust to different definitions of interpretability.\nThose encouraging results could lead to applications of knowledge distillation to improve the interpretability of deep learning models in highly sensitive areas like autonomous systems and healthcare.\nJoin the Discussion We’d love to hear your thoughts! What are your experiences with Knowledge Distillation (KD)? Have you found it to improve not just performance but also interpretability in your projects? Feel free to share your ideas, questions, or insights in the comments section or engage with us on GitHub!\nReferences Hinton, G., Vinyals, O., \u0026amp; Dean, J. (2015). Distilling the knowledge in a neural network. arXiv:1503.02531. Han, H., Kim, S., Choi, H.-S., \u0026amp; Yoon, S. (2023). On the Impact of Knowledge Distillation for Model Interpretability. arXiv:2305.15734. Bau, D., Zhou, B., Khosla, A., Oliva, A., \u0026amp; Torralba, A. (2017). Network dissection: Quantifying interpretability of deep visual representations. arXiv:1704.05796. Tjoa, E., \u0026amp; Guan, M. Y. (2020). Quantifying explainability of saliency methods in deep neural networks. arXiv:2009.02899. Shah, H., Jain, P., \u0026amp; Netrapalli, P. (2021). Do input gradients highlight discriminative features? arXiv:2102.12781, NeurIPS 2021. ","content_html":"\u003cstyle TYPE=\"text/css\"\u003e\n   code.has-jax {font:inherit;\n                  font-size:100%;\n                  background: inherit;\n                  border: inherit;}\n\u003c/style\u003e\n\u003cscript TYPE=\"text/x-mathjax-config\"\u003e\n   MathJax.Hub.Config({\n      tex2jax: {\n         inlineMath: [['$','$'], ['\\$','\\$']],\n         skipTags: ['script','noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n      }\n   });\n\n   MathJax.Hub.Queue(function() {\n      var all = MathJax.Hub.getAllJax(), i;\n      for(i = 0; i \u003c all.length; i += 1) {\n         all[i].SourceElement().parentNode.className += ' has-jax';\n      }\n   });\n\u003c/script\u003e\n\u003cscript TYPE=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\n\u003c/script\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"fr\"\u003e\n\u003chead\u003e\n   \u003cmeta charset=\"UTF-8\"\u003e\n   \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n\u003c/head\u003e\n\u003ch1 style=\"font-size: 28px;\"\u003eInterpretability, the hidden power of knowledge distillation\u003c/h1\u003e\n\u003cp\u003ePublished March 15, 2025\u003c/p\u003e\n\u003cdiv\u003e\n  \u003ca href=\"https://github.com/BryanBradfo/responsible-ai-datascience-ipParis.github.io\" class=\"btn\" style=\"text-decoration: none; display: inline-block; padding: 8px 16px; background-color: #f1f1f1; border: 1px solid #ddd; border-radius: 4px; color: black;\"\u003eUpdate on GitHub\u003c/a\u003e\n\u003c/div\u003e\n\u003cdiv style=\"display: flex; margin-top: 20px;\"\u003e\n   \u003cdiv style=\"display: flex; align-items: center; margin-right: 20px;\"\u003e\n      \u003cimg src=\"/images/Bryan_Remi/bryan.jpeg\" alt=\"Bryan Chen\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\"\u003e\n      \u003cdiv\u003e\n         \u003ca href=\"https://github.com/BryanBradfo\" style=\"text-decoration: none; color: #0366d6;\"\u003ebryanbradfo\u003c/a\u003e\n         \u003cp style=\"margin: 0;\"\u003eBryan Chen\u003c/p\u003e\n      \u003c/div\u003e\n   \u003c/div\u003e\n   \u003cdiv style=\"display: flex; align-items: center; margin-right: 20px;\"\u003e\n      \u003cimg src=\"/images/Bryan_Remi/remi.jpg\" alt=\"Rémi Calvet\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\"\u003e\n      \u003cdiv\u003e\n         \u003ca href=\"https://github.com/RemiCSK\" style=\"text-decoration: none; color: #0366d6;\"\u003eremicsk\u003c/a\u003e\n         \u003cp style=\"margin: 0;\"\u003eRémi Calvet\u003c/p\u003e\n      \u003c/div\u003e\n   \u003c/div\u003e\n\u003c/div\u003e\n\u003cp\u003eKnowledge distillation is a powerful technique to transfer the knowledge from a large \u0026ldquo;teacher\u0026rdquo; model to a \u0026ldquo;student\u0026rdquo; model. While it\u0026rsquo;s commonly used to improve performance and reduce computational costs by compressing large models, this blog post explores a fascinating discovery: knowledge distillation can also enhance model interpretability. We\u0026rsquo;ll dive into the paper \u003ca href=\"https://arxiv.org/abs/2305.15734\"\u003eOn the Impact of Knowledge Distillation for Model Interpretability\u0026quot;\u003c/a\u003e (ICML 2023) by H. Han et al., which sheds light on this novel perspective.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/better_meme.png\" alt=\"Introduction\" style=\"width: 60%; max-width: 500px; height: auto;\"\u003e\n\u003c/p\u003e\n\u003cp\u003eInterpretability in AI allows researchers, engineers, and decision-makers to trust and control machine learning models. Recent models show impressive performance on many different tasks and often rely on deep learning models. Unfortunately, deep learning models are also know for the difficulty to interprete them and understand how they come to a result wich can be problematic in highly sensitive applications like autonomous driving or healthcare. The article we present in this blog shows that knowledge distillation can improve the interpretability of deep learning models.\u003c/p\u003e\n\u003cp\u003eWhen AI is a black box, you\u0026rsquo;re just hoping for the best. But when you understand it, you become unstoppable.\u003c/p\u003e\n\u003ch2 style=\"font-size: 21px; display: flex; align-items: center;\"\u003e 0. Table of Contents \u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#i-crash-course-on-knowledge-distillation\"\u003eI. Crash Course on Knowledge Distillation and Label Smoothing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ii-defining-interpretability-through-network-dissection\"\u003eII. Defining Interpretability Through Network Dissection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#iii-logit-distillation-feature-distillation\"\u003eIII. Logit Distillation \u0026amp; Feature Distillation: A Powerful Duo for Interpretability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#iv-why-knowledge-distillation-enhances-interpretability\"\u003eIV. Why Knowledge Distillation Enhances Interpretability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#v-experimental-results-and-reproduction\"\u003eV. Experimental Results and Reproduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#vi-beyond-network-dissection-other-interpretability-metrics\"\u003eVI. Beyond Network Dissection: Other Interpretability Metrics\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#join-the-discussion\"\u003eJoin the Discussion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"i-crash-course-on-knowledge-distillation\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e I. Crash Course on Knowledge Distillation and Label Smoothing \u003c/h2\u003e\n\u003ch3 id=\"what-is-knowledge-distillation\"\u003eWhat is Knowledge Distillation?\u003c/h3\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/knowledge_distillation.png\" alt=\"Knowledge Distillation Overview\" style=\"width: 70%; max-width: 500px; height: auto;\"\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1503.02531\"\u003eKnowledge distillation (KD)\u003c/a\u003e is a model compression technique introduced by Hinton et al. (2015) that transfers knowledge from a complex teacher model to a simpler student model. Unlike traditional training where models learn directly from hard labels (one-hot encodings), KD allows the student to learn from the teacher\u0026rsquo;s soft probability distributions.\u003c/p\u003e\n\u003ch3 id=\"the-key-mechanics-of-knowledge-distillation\"\u003eThe Key Mechanics of Knowledge Distillation\u003c/h3\u003e\n\u003cp\u003eThe standard KD loss function combines the standard cross-entropy loss with a distillation loss term:\u003c/p\u003e\n\u003cp\u003e$$\\mathcal{L}_{KD}=(1-\\alpha)\\mathrm{CE}(y,\\sigma(z_s))+\\alpha T^2 \\mathrm{CE}(\\sigma(z_t^T),\\sigma(z_s^T))$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$z_s$ and $z_t$ are the logits from the student and teacher models\u003c/li\u003e\n\u003cli\u003e$T$ is the temperature parameter that controls softening of probability distributions\u003c/li\u003e\n\u003cli\u003e$z_s^T \\mathrel{:}= \\frac{z_s}{T}$ and $z_t^T \\mathrel{:}= \\frac{z_t}{T}$\u003c/li\u003e\n\u003cli\u003e$\\sigma$ is the softmax function\u003c/li\u003e\n\u003cli\u003e$\\sigma(z_s^T) \\mathrel{:}= \\frac{\\exp(z_s^T)}{\\sum_j \\exp(z_j^T)}$ and $\\sigma(z_t^T) \\mathrel{:}= \\frac{\\exp(z_t^T)}{\\sum_j \\exp(z_j^T)}$\u003c/li\u003e\n\u003cli\u003e$\\mathrm{CE}$ is cross-entropy loss\u003c/li\u003e\n\u003cli\u003e$\\alpha$ balances the importance of each loss component.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe first part of the loss $(1-\\alpha)\\mathrm{CE}(y,\\sigma(z_s))$ is to incitate the student model to learn from one hot encoded ground truth label.\u003c/p\u003e\n\u003cp\u003eThe second part of the loss $\\alpha T^2 \\mathrm{CE}(\\sigma(z_t^T),\\sigma(z_s^T))$ is to incitate the student model to try to reproduce the ouputs of the teacher model. This is what permits the student to learn from the teacher.\nThe larger $\\alpha$ is, the more the student will try to replicate the teacher model\u0026rsquo;s outputs and ignore the one hot encoded groundtruth and vice versa.\u003c/p\u003e\n\u003ch3 id=\"label-smoothing\"\u003eLabel Smoothing\u003c/h3\u003e\n\u003cp\u003eLabel smoothing (LS) is another technique that smooths hard targets by mixing them with a uniform distribution. In the cross entropy loss we replace the one hot encoded $y$ by $y_{LS} \\mathrel{:}= (1-\\alpha)y + \\frac{\\alpha}{K}$, where $K$ is the number of classes and $\\alpha$ the smoothing parameter:\u003c/p\u003e\n\u003cscript type=\"math/tex; mode=display\"\u003e\n\\begin{align}\nCE(y_{LS},\\sigma(z)) \u0026= - \\sum_{i=1}^{K} \\left( (1 - \\alpha) y_i + \\frac{\\alpha}{K} \\right) \\log \\sigma(z_i) \\\\\n\u0026= -(1 - \\alpha) \\sum_{i=1}^{K} y_i \\log \\sigma(z_i) - \\alpha \\sum_{i=1}^{K} \\frac{1}{K}\\log \\sigma(z_i) \\\\\n\\end{align}\n\u003c/script\u003e\n\u003cp\u003eWe obtain a loss that is similar to knowledge diffusion but there is a key difference important for interpretability that we will discuss later.\nFrom the equation above, we get the label smoothing loss equation:\n$$L_{LS} = (1-\\alpha)\\mathrm{CE}(y,\\sigma(z)) + \\alpha\\mathrm{CE}(u,\\sigma(z)) $$\nWhere $u$ is a uniform distribution over all the possible $K$ classes.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/label_smoothing.jpg\" alt=\"Label Smoothing\" width=\"700\"\u003e\n\u003c/p\u003e\n\u003ch2 id=\"ii-defining-interpretability-through-network-dissection\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e II. Defining Interpretability Through Network Dissection \u003c/h2\u003e\n\u003cp\u003eThe first thing to know is that there are different approaches to define and measure interpretability in machine learning.\u003c/p\u003e\n\u003cp\u003eFor image classification, the authors use \u003ca href=\"https://arxiv.org/pdf/1711.05611v2\"\u003enetwork dissection\u003c/a\u003e to quantitatively measure interpretability. The idea is to compare activation maps and see if areas with high activation correspond to an object or a meaningful concept on the image.\u003c/p\u003e\n\u003cp\u003eThe process can be better understood through the following illustration:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/network_dissection.png\" alt=\"Network Dissection Process\" width=\"700\"\u003e\n\u003c/p\u003e\n\u003cp\u003eFeed a neural network model an image, pick a deep layer and count the number of neurons that detects a concept like \u0026ldquo;cat\u0026rdquo; or \u0026ldquo;dog\u0026rdquo;.\nWe call those neurons concept detectors and will define them more precisely. The \u003cstrong\u003enumber of concept detectors will be the primary metric to define the interpretability of a model\u003c/strong\u003e, the higher the more we will consider it interpretable.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe easiest way to understand what is a concept detector is to look at the following pseudo code to compute the number of concept detectors:\u003c/strong\u003e\u003c/p\u003e\n\u003cstyle\u003e\n  body {\n      font-family: 'Arial', sans-serif;\n      line-height: 1.6;\n  }\n\n  .steps-container {\n      background: #f8f9fa;\n      border-left: 5px solid #007bff;\n      padding: 15px 20px;\n      margin: 20px 0;\n      border-radius: 5px;\n  }\n\n  .step {\n      font-weight: bold;\n      color: #007bff;\n      margin-top: 15px;\n  }\n\n  .math-expression {\n      font-family: 'Courier New', Courier, monospace;\n      background: #e9ecef;\n      padding: 5px;\n      border-radius: 3px;\n  }\n\n  .important {\n      background: #fff3cd;\n      color: #856404;\n      padding: 10px;\n      border-left: 4px solid #ffc107;\n      border-radius: 3px;\n      margin: 10px 0;\n  }\n\u003c/style\u003e\n\u003cdiv class=\"steps-container\"\u003e\n\u003ch3 id=\"1-selecting-the-layer\"\u003e\u003cspan class=\"step\"\u003e1. Selecting the Layer\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eFirst, we need to choose a layer $\\mathcal{l}$ to \u003cstrong\u003edissect\u003c/strong\u003e, typically deep in the network.\u003c/p\u003e\n\u003ch3 id=\"2-processing-each-image\"\u003e\u003cspan class=\"step\"\u003e2. Processing Each Image\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eFor each image \u003cstrong\u003ex\u003c/strong\u003e in the dataset:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFeedforward Pass\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInput an image \u003cstrong\u003ex\u003c/strong\u003e of shape $ (n,n) $ into the neural network.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eActivation Extraction\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor each neuron in layer $\\mathcal{l}$, collect the activation maps:\n\u003cdiv class=\"math-expression\"\u003e\n\\[ A_i(x) \\in \\mathbb{R}^{d \\times d}, \\quad \\text{where } d \u003c n \\text{ and } i \\text{ is the neuron index.} \\]\n\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"3-defining-activation-distribution\"\u003e\u003cspan class=\"step\"\u003e3. Defining Activation Distribution\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eFor each neuron \u003cstrong\u003ei\u003c/strong\u003e in the layer $\\mathcal{l}$:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine \u003cstrong\u003ea\u003csub\u003ei\u003c/sub\u003e\u003c/strong\u003e as the empirical distribution of activation values across different images \u003cstrong\u003ex\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"4-computing-activation-threshold\"\u003e\u003cspan class=\"step\"\u003e4. Computing Activation Threshold\u003c/span\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCompute a threshold \u003cstrong\u003eT\u003csub\u003ei\u003c/sub\u003e\u003c/strong\u003e such that:\n\u003cdiv class=\"math-expression\"\u003e\n\\[ P(a_i \\geq T_i) = 0.005 \\]\n\u003c/div\u003e\n- This ensures only the **top 0.5%** activations are retained.\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"5-resizing-activation-maps\"\u003e\u003cspan class=\"step\"\u003e5. Resizing Activation Maps\u003c/span\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eInterpolate \u003cstrong\u003eA\u003csub\u003ei\u003c/sub\u003e\u003c/strong\u003e to match the dimension $ (n,n) $ for direct comparison with input images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"6-creating-binary-masks\"\u003e\u003cspan class=\"step\"\u003e6. Creating Binary Masks\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eFor each image \u003cstrong\u003ex\u003c/strong\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGenerating Activation Masks\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreate a \u003cstrong\u003ebinary mask\u003c/strong\u003e $ A_i^{\\text{mask}}(x) $ of shape $ (n,n) $:\n\u003cdiv class=\"math-expression\"\u003e\n\\[ A_i^{\\text{mask}}(x)[j,k] = \\begin{cases} 1, \u0026 \\text{if } A_i(x)[j,k] \\geq T_i \\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\]\n\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003eThis retains only the highest activations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUsing Ground Truth Masks\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGiven a \u003cstrong\u003eground truth mask\u003c/strong\u003e $ M_c(x) $ of shape $ (n,n) $, where:\n\u003cul\u003e\n\u003cli\u003e$ M_c(x)[j,k] = 1 $ if the pixel in \u003cstrong\u003ex\u003c/strong\u003e belongs to class \u003cstrong\u003ec\u003c/strong\u003e, otherwise \u003cstrong\u003e0\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eComputing Intersection over Union (IoU)\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCalculate the IoU between \u003cstrong\u003eA\u003csub\u003ei\u003c/sub\u003e\u003csup\u003emask\u003c/sup\u003e(x)\u003c/strong\u003e and \u003cstrong\u003eM\u003csub\u003ec\u003c/sub\u003e(x)\u003c/strong\u003e:\n\u003cdiv class=\"math-expression\"\u003e\n\\[ \\text{IoU}_{i,c} = \\frac{|A_i^{\\text{mask}}(x) \\cap M_c(x)|}{|A_i^{\\text{mask}}(x) \\cup M_c(x)|} \\]\n\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003eIf  $\\text{IoU}_{i,c} \u0026gt; 0.05$, the neuron \u003cstrong\u003ei\u003c/strong\u003e is considered a \u003cstrong\u003econcept detector\u003c/strong\u003e for concept \u003cstrong\u003ec\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\u003ch3 id=\"if-you-prefer-to-understand-with-code-here-is-an-implementation-of-the-procedure-described-above\"\u003eIf you prefer to understand with code, here is an implementation of the procedure described above:\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eidentify_concept_detectors\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003econcept_masks\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    Identify neurons that act as concept detectors in a specific layer.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    Args:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e        model: Neural network model\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e        layer_name: Name of the layer to analyze\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e        dataset: Dataset with images\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e        concept_masks: Dictionary mapping images to concept segmentation masks\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    Returns:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e        Dictionary mapping neurons to detected concepts\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Step 1: Collect activation maps for each image\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eactivation_maps\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eimage\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# Forward pass and extract activation at specified layer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eactivations\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eget_layer_activation\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eimage\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivation\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eenumerate\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eactivations\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivation_maps\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eactivation_maps\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003eactivation_maps\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eactivation\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Step 2: Compute threshold for top 0.5% activations for each neuron\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003ethresholds\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivations\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivation_maps\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eitems\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# Flatten all activations for this neuron\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eall_activations\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecat\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eact\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eflatten\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eact\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivations\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# Compute threshold for top 0.5%\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003equantile\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eall_activations\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.995\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003ethresholds\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Step 3: Create binary masks and compute IoU with concept masks\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003econcept_detectors\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eimage_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eimage\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eenumerate\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eimage_concepts\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003econcept_masks\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eimage_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivations\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivation_maps\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eitems\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Get activation for this neuron on this image\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003eactivation\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eactivations\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eimage_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Create binary mask using threshold\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003ebinary_mask\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eactivation\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026gt;=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethresholds\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efloat\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Resize to match image dimensions\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003ebinary_mask\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eF\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003einterpolate\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ebinary_mask\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eunsqueeze\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eunsqueeze\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003esize\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eimage\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eshape\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emode\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#39;bilinear\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esqueeze\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Compute IoU with each concept mask\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003econcept\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emask\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eimage_concepts\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eitems\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eintersection\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esum\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebinary_mask\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emask\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eunion\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esum\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebinary_mask\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esum\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emask\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eintersection\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eiou\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eintersection\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eunion\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eunion\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#75715e\"\u003e# If IoU exceeds threshold (typically 0.05), consider it a detector\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eiou\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.05\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003econcept_detectors\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003econcept_detectors\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#111\"\u003econcept_detectors\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eneuron_idx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eadd\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003econcept\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003econcept_detectors\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"iii-logit-distillation-feature-distillation\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e III. Logit Distillation \u0026 Feature Distillation: A Powerful Duo for Interpretability \u003c/h2\u003e\n\u003cp\u003eCombining logit distillation with feature distillation not only boosts performance but also enhances the interpretability of student models. This improvement is measured by an increase in the number of concept detectors, which represent units aligned with human-interpretable concepts.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/feature_logit_distillation.png\" alt=\"Feature_Logit_Distillation\" width=\"700\"\u003e\n\u003c/p\u003e\n\u003cp\u003ewhere Attention Transfer (AT), Factor Transfer (FT), Contrastive Representation Distillation (CRD), and Self-Supervised Knowledge Distillation (SSKD) are all variations of knowledge distillation techniques, each designed to transfer knowledge from teacher models to student models in unique ways.\u003c/p\u003e\n\u003ch3 id=\"how-they-work-together\"\u003eHow they work together?\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLogit Distillation:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eTransfers class-similarity information from the teacher to the student through softened logits.\u003c/li\u003e\n\u003cli\u003eHelps the student model understand the relationships between semantically similar classes, making activation maps more object-centric.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eFeature Distillation:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eFocuses on aligning intermediate layer features between the teacher and student.\u003c/li\u003e\n\u003cli\u003eImproves the student model\u0026rsquo;s ability to replicate the teacher’s feature representations, supporting richer internal representations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"iv-why-knowledge-distillation-enhances-interpretability\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e IV. Why Knowledge Distillation Enhances Interpretability \u003c/h2\u003e\n\u003cp\u003eThe key insight from the paper is that knowledge distillation transfers not just the ability to classify correctly, but also class-similarity information that makes the model focus on more interpretable features.\u003c/p\u003e\n\u003ch3 id=\"transfer-of-class-similarities\"\u003eTransfer of Class Similarities\u003c/h3\u003e\n\u003cp\u003eWhen a teacher model sees an image of a dog, it might assign:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e85% probability to \u0026ldquo;Golden Retriever\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e10% probability to other dog breeds\u003c/li\u003e\n\u003cli\u003e5% probability to other animals and objects\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese \u0026ldquo;soft targets\u0026rdquo; (consequence of logit distillation) encode rich hierarchical information about how classes relate. The student model distilling this knowledge learns to focus on features that are common to similar classes (e.g., general \u0026ldquo;dog\u0026rdquo; features).\u003c/p\u003e\n\u003ch3 id=\"label-smoothing-vs-knowledge-distillation\"\u003eLabel Smoothing vs. Knowledge Distillation\u003c/h3\u003e\n\u003cp\u003eBy looking at the KD and label smoothing losses, we can see that they are similar. When $T=1$ they only differ in the second member where we have a $\\sigma(z_t^T)$ that contains class-similarity information instead of $u$ that doesn\u0026rsquo;t contain any information.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\mathcal{L}_{KD}=(1-\\alpha)\\mathrm{CE}(y,\\sigma(z_s))+\\alpha T^2 \\mathrm{CE}(\\sigma(z_t^T),\\sigma(z_s^T))$\u003c/li\u003e\n\u003cli\u003e$L_{LS} = (1-\\alpha)\\mathrm{CE}(y,\\sigma(z)) + \\alpha\\mathrm{CE}(u,\\sigma(z)) $\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, if there is a difference in interpretability, it is likely that it comes from the fact that distillation permits to get class similarity knowledge from the teacher model. This is exactly what is shown in the figure below. Knowledge distillation guides student models to focus on more object-centric features rather than background or contextual features. This results in activation maps that better align with the actual objects in images.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/comparisons_dog.png\" alt=\"ObjectCentricActivation\" width=\"700\"\u003e\n\u003c/p\u003e\n\u003cp\u003eThe next figure also highlights the loss of interpretability (less concept detectors) when using label smoothing and the improvement of interpretability (more concept detectors) for KD:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/NbConceptDetDiffModels.png\" alt=\"KD vs LS Distributions\" width=\"600\"\u003e\n\u003c/p\u003e\n\u003cp\u003eWhile label smoothing can improve accuracy, it often reduces interpretability by erasing valuable class relationships while KD keeps class relationship information and improves both accuracy and interpretability.\u003c/p\u003e\n\u003ch2 id=\"v-experimental-results-and-reproduction\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e V. Experimental Results and Reproduction \u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s implement a reproduction of one of the paper\u0026rsquo;s key experiments to see knowledge distillation\u0026rsquo;s effect on interpretability in action.\u003c/p\u003e\n\u003ch3 id=\"setting-up-the-experiment\"\u003eSetting Up the Experiment\u003c/h3\u003e\n\u003cp\u003eWe are going to replicate the experiment by using the \u003ca href=\"https://github.com/Rok07/KD_XAI\"\u003eGitHub repository provided by the authors\u003c/a\u003e. The repository contains the code to train the models, compute the concept detectors, and evaluate the interpretability of the models.\u003c/p\u003e\n\u003cp\u003eAs it is often the case with a machine learning paper, running the code to reproduce results requires some struggle.\nTo reproduce the results, you could use a virtual environment (e.g. \u003ca href=\"https://datalab.sspcloud.fr/\"\u003eSSP Cloud Datalab\u003c/a\u003e) and then do the following:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit clone https://github.com/Rok07/KD_XAI.git\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecd\u003c/span\u003e torchdistill\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install -e .\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecd\u003c/span\u003e ..\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebash script/dlbroden.sh\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enano torchdistill/torchdistill/models/custom/bottleneck/__init__.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e~ comment the first line\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install opencv-python\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install imageio\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt update\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt install -y libgl1-mesa-glx\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enano util/vecquantile.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e~ change NaN by nan\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enano loader/data_loader.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e~ add out\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003ei\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e rgb\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e:,:,0\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e + \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003ergb\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e:,:,1\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e.astype\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003enp.uint16\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e * 256\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecd\u003c/span\u003e ..\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enano settings.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e~ change \u003cspan style=\"color:#111\"\u003eTEST_MODE\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e False to True\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecd\u003c/span\u003e dataset/broden1_224\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecp index.csv index_sm.csv\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e~ keep the \u003cspan style=\"color:#ae81ff\"\u003e4000\u003c/span\u003e first lines\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecd\u003c/span\u003e ../..\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enano visualize/bargraph.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e~ change parameter threshold of bar_graph_svg\u003cspan style=\"color:#f92672\"\u003e()\u003c/span\u003e to 0.001\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNetwork Dissection quantifies the interpretability of hidden units by measuring their alignment with human-interpretable concepts. The following results reveal several interesting findings:\u003c/p\u003e\n\u003ch4 id=\"1-concept-distribution-from-bargraphsvg\"\u003e1. Concept Distribution (from bargraph.svg):\u003c/h4\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/class_distribution.png\" alt=\"Class Distribution\" width=\"600\"\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e~6 units detecting object concepts\u003c/li\u003e\n\u003cli\u003e~2 units detecting scene concepts\u003c/li\u003e\n\u003cli\u003e1 unit detecting material properties\u003c/li\u003e\n\u003cli\u003e~13 units detecting textures\u003c/li\u003e\n\u003cli\u003e~6 units detecting colors\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"2-specific-units-layer4-0xxxjpg\"\u003e2. Specific Units: (layer4-0xxx.jpg)\u003c/h4\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/unit_grid.png\" alt=\"Unit Grid\" width=\"1600\"\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUnit 330\u003c/strong\u003e has specialized in detecting grid and regular pattern textures\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/unit_sky.png\" alt=\"Unit Sky\" width=\"1600\"\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUnit 202\u003c/strong\u003e detects sky regions in images\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe network dissection approach reveals interpretable neurons of a distilled ResNet18.\u003c/p\u003e\n\u003ch2 id=\"vi-beyond-network-dissection-other-interpretability-metrics\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e VI. Beyond Network Dissection: Other Interpretability Metrics \u003c/h2\u003e\n\u003cp\u003eWhile the paper emphasizes the use of \u003cstrong\u003eNetwork Dissection\u003c/strong\u003e to measure model interpretability by quantifying concept detectors, it also explores several additional metrics to confirm the broader impact of \u003cstrong\u003eKnowledge Distillation (KD)\u003c/strong\u003e on interpretability:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003ca href=\"https://arxiv.org/pdf/2009.02899\"\u003eFive-Band Scores\u003c/a\u003e, proposed by Tjoah \u0026 Guan (2020):\u003c/strong\u003e This metric assesses interpretability by evaluating pixel accuracy (accuracy of saliency maps in identifying critical features), precision (how well the saliency maps match the actual distinguishing features), recall, and false positive rates (FPR, lower FPR indicates better interpretability) using a synthesized dataset with heatmap ground truths. KD-trained models consistently show higher accuracy and lower FPR compared to other methods.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003ca href=\"https://arxiv.org/pdf/2102.12781\"\u003eDiffROAR Scores\u003c/a\u003e, proposed by Shah et al. (2021):\u003c/strong\u003e This evaluates the difference in predictive power on a model trained on a dataset and a model trained on a version of the dataset where we removed top and bottom x% of the pixel according to their importance for the task. The authors find that KD has a higher DiffROAR score than a model trained from scratch. It means that KD makes the model use more relevant features and thus more interpretable in that sense.\n  \u003cli\u003e\u003cstrong\u003eLoss Gradient Alignment:\u003c/strong\u003e This metric measures the alignment of model gradients with human-perceived important features. KD models exhibit better alignment, indicating greater interpretability as we can see on this figure:\n  \u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/Bryan_Remi/gradient_interpre.png\" alt=\"ObjectCentricActivation\" width=\"700\"\u003e\n\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n\u003cp\u003eThese metrics collectively show that KD can enhance interpretability. The consistent results showing that knowledge distillation can enhance interpretability for different metrics of interpretability provide strong arguments to believe that KD could be broadly used for better interpretability of deep learning models. \u003c/p\u003e\n\u003ch2 id=\"conclusion\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e Conclusion \u003c/h2\u003e\n\u003cp align=\"center\"\u003e \u003cimg src=\"/images/Bryan_Remi/pinguins_studying.gif\" alt=\"Feeling strong with interpretable AI\" style=\"width: 30%; max-width: 500px; height: auto;\"\u003e \u003c/p\u003e\n\u003cp\u003eThe article showed that knowledge distillation can improve both accuracy and interpretability. They attribute the improvement in interpretability to the transfer of class similarity knowledge from the teacher to the student model. They compare label smoothing (LS) that is similar to KD but LS does not benefit from class-similarity information. The empirical experiments shows better accuracy for LS and KD but the interpretability of LS decreases whereas it increases for KD confirming the hypothesis that class similarity knowledge has a role in interpretability. The authors obtain consistent results when using other metrics than the number of concept detectors for interpretability showing that their approach is robust to different definitions of interpretability.\u003c/p\u003e\n\u003cp\u003eThose encouraging results could lead to applications of knowledge distillation to improve the interpretability of deep learning models in highly sensitive areas like autonomous systems and healthcare.\u003c/p\u003e\n\u003ch2 id=\"join-the-discussion\" style=\"font-size: 21px; display: flex; align-items: center;\"\u003e Join the Discussion \u003c/h2\u003e\n\u003cp\u003eWe’d love to hear your thoughts! What are your experiences with Knowledge Distillation (KD)? Have you found it to improve not just performance but also interpretability in your projects? Feel free to share your ideas, questions, or insights in the comments section or engage with us on \u003ca href=\"https://github.com/BryanBradfo/responsible-ai-datascience-ipParis.github.io\"\u003eGitHub\u003c/a\u003e!\u003c/p\u003e\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eHinton, G., Vinyals, O., \u0026amp; Dean, J. (2015). \u003ca href=\"https://arxiv.org/abs/1503.02531\"\u003eDistilling the knowledge in a neural network.\u003c/a\u003e arXiv:1503.02531.\u003c/li\u003e\n\u003cli\u003eHan, H., Kim, S., Choi, H.-S., \u0026amp; Yoon, S. (2023). \u003ca href=\"https://arxiv.org/pdf/2305.15734\"\u003eOn the Impact of Knowledge Distillation for Model Interpretability.\u003c/a\u003e arXiv:2305.15734.\u003c/li\u003e\n\u003cli\u003eBau, D., Zhou, B., Khosla, A., Oliva, A., \u0026amp; Torralba, A. (2017). \u003ca href=\"https://arxiv.org/pdf/1704.05796\"\u003eNetwork dissection: Quantifying interpretability of deep visual representations.\u003c/a\u003e arXiv:1704.05796.\u003c/li\u003e\n\u003cli\u003eTjoa, E., \u0026amp; Guan, M. Y. (2020). \u003ca href=\"https://arxiv.org/pdf/2009.02899\"\u003e Quantifying explainability of saliency methods in deep neural networks.\u003c/a\u003e arXiv:2009.02899.\u003c/li\u003e\n\u003cli\u003eShah, H., Jain, P., \u0026amp; Netrapalli, P. (2021). \u003ca href=\"https://arxiv.org/pdf/2102.12781\"\u003eDo input gradients highlight discriminative features?\u003c/a\u003e arXiv:2102.12781, NeurIPS 2021.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/html\u003e","url":"https://responsible-ai-datascience-ipParis.github.io/posts/impact-knowledge-distillation-model-interpretability/","date_published":"15036-15-09T316:1515:00+01:00","date_modified":"15036-15-09T316:1515:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"dda35d04e18530bb17257e43629e47ccc8837f0a","title":"Understanding Visual Feature Reliance Through the Lens of Complexity","summary":"","content_text":" Understanding Visual Feature Reliance through the Lens of Complexity Authors: DIB Caren, SABA Jean Paul, WANG Romain\nArticle: Understanding Visual Feature Reliance through the Lens of Complexity\nTable of Contents Introduction Feature Extraction via Dictionary Learning Detecting Complexity Relations with Redundancy Relations with Robustness Importance Measure Feature Flow and Information Theory Experiment Summary: Exploring Feature Complexity in ResNet18 How the Code Was Structured and What Was Done Experiment Results and Interpretation Conclusion 1. Introduction In this blog, we’ll take a deep dive into an insightful and thought-provoking paper authored by Thomas Fel, Louis Béthune, Andrew Kyle Lampinen, Thomas Serre, and Katherine Hermann. Their work explores the intricate mechanisms underlying how deep neural networks—specifically ResNet50—learn and represent complex features. This research, rooted in both theoretical and empirical analysis, investigates the nature of feature complexity, how features evolve over the course of training, and the computational structures that enable neural networks to generalize effectively.\nThe authors\u0026rsquo; central motivation is to understand how models balance computational efficiency with representational richness. They explore why deep networks exhibit a preference for simpler features (simplicity bias), how complex features are supported within a network, and the trade-offs between redundancy, robustness, and importance of these features. V-information serves as the main complexity metric used throughout their study, offering a principled approach to quantifying how computationally accessible features are. In addition to V-information, they employ several other quantitative measures—such as importance scores derived from Gradient × Input, and redundancy and robustness metrics informed by prior work—to provide an exhaustive and structured analysis of feature learning dynamics.\nTheir findings have implications for model interpretability, robustness, and generalization, offering deep insights into the practical and theoretical aspects of modern deep learning systems. In this blog, we break down their study into a comprehensive guide for easier understanding.\n2. Feature Extraction and Visualization In this section, the authors explore the nature and diversity of features learned by the network. First, some general information:\nSimple Features: These are easy-to-decode, frequently occurring concepts like sky, grass, and watermarks. They typically emerge early in the network and are transported through residual connections with little modification. Such features are aligned with simplicity bias and often serve as shortcuts for the model.\nMedium Complexity Features: These include concepts like human-related elements, low-pixel quality detectors, and trademarks. They often represent slightly more abstract or nuanced properties and require more layers and computational effort to emerge.\nComplex Features: Highly intricate concepts like insect legs, whiskers, and filament structures represent the most complex features. These require extensive processing across multiple layers and involve both the main and residual network branches to form progressively.\nTo be able to extract those features, the authors introduced an overcomplete dictionary as a solution to a key challenge in understanding deep neural networks: the superposition problem, where multiple features are entangled within single neurons, making it difficult to isolate and analyze individual features. In standard neural networks, activations $f_n(x)$ in the penultimate layer represent complex, often overlapping features, and the number of distinct features may far exceed the number of neurons $|A_\\ell|$. To address this, the authors leveraged dictionary learning to build an overcomplete dictionary $D^*$, where $k \\gg |A_\\ell|$, with $k$ representing the number of atoms (or basis elements) in the dictionary. This allowed them to extract a richer set of disentangled features—up to 10,000, far more than the neuron count.\nEach activation $f_n(x)$ is approximated as a linear combination of atoms from the overcomplete dictionary $D^*$, weighted by sparse coefficients $z$:\n$$ f_n(x) \\approx z D^* = \\sum_{i=1}^k z_i d_i $$\nThis overcomplete setup allows for disentangling features beyond what individual neurons can represent. The dictionary $D^*$ is learned using Non-Negative Matrix Factorization (NMF), which aligns with the non-negative nature of ReLU activations. The optimization minimizes reconstruction error with non-negativity constraints:\n$$ (Z, D^*) = \\arg \\min_{Z \\ge 0, D^* \\ge 0} | f_n(X) - Z D^* |_F $$\nTrained on ImageNet with 58 million samples, the dictionary preserves over 99% of the model’s predictive accuracy. Once $D^*$ is fixed, features for new inputs are extracted by solving:\n$$ z = \\arg \\min_{z \\ge 0} | f_n(x) - z D^* |_F $$\nThe authors used MACO, an advanced feature visualization technique, to produce clearer and more realistic images of the network’s learned features. These visualizations are sorted by complexity, from the simplest to the most complex features, highlighting the increasing detail and intricacy as complexity grows.\nCaption: Feature Viz using MACO\nCaption: Feature complexity using UMAP\n3. Detecting Complexity The authors propose V-information as their primary metric to quantify feature complexity. They focus on a setting where the predictive family $V$ consists of linear classifiers with Gaussian posteriors. In this context, V-information measures how much information a representation $x$ provides about a feature $z$ under computational constraints. The authors derive a closed-form solution for V-information when $V$ consists of these linear Gaussian models:\n$$ I_V(x \\to z) = H_V(z) - H_V(z|x) $$\nHere, $H_V(z)$ represents the V-entropy, which measures the uncertainty about $z$ when using the best possible predictor from the restricted family $V$. Similarly, $H_V(z|x)$ is the V-conditional entropy, which measures the remaining uncertainty about $z$ after observing $x$, again under the constraint of using predictors from $V$.\nWhich leads to having:\n$$ 0 \\le I_V(x \\to z) \\le \\text{Var}(z) $$\nSince the input data are centered and scaled, $\\text{Var}(z)$ is typically close to 1. The authors define feature complexity $K(z, x)$ as the inverse of the average V-information across network layers, quantifying how difficult it is to decode a feature as it propagates through the network:\n$$ K(z, x) = 1 - \\frac{1}{n} \\sum_{\\ell=1}^{n} I_V(f_\\ell(x) \\to z) $$\nThey note that a higher $K(z, x)$ score indicates a more complex feature, harder to decode until later in the model. Empirically, they observed that $K(z, x)$ generally falls within $[0, 1]$, with 1 representing high complexity.\n3.1. Relations with Redundancy To explore the relationship between complexity and redundancy, the authors employed a redundancy measure based on Centered Kernel Alignment (CKA). In their analysis, they compared the similarity between a feature $z$ and the network activations $f_n(X)$, both before and after masking parts of the activations. A binary mask $m$ was applied to the activations, where $m \\in {0,1}^{|A_\\ell|}$ selects which neurons remain active (1) and which are deactivated (0). If masking neurons didn’t change the similarity much, it meant the feature was redundant, spread over many neurons. If the similarity dropped a lot, the feature was localized, relying on fewer neurons. The redundancy score was calculated as:\n$$ \\text{Redundancy} = \\mathbb{E}_m \\left[ \\frac{CKA(f_n(X) \\odot m, z)}{CKA(f_n(X), z)} \\right] $$\nWhere:\n$$ \\text{CKA}(A, B) = \\frac{|K_A K_B|_F^2}{|K_A K_A|_F \\cdot |K_B K_B|_F} $$\nThey found that complex features are less redundant, meaning they depend on specific neurons and are more fragile.\n3.2. Complexity and Robustness The authors also investigated how feature complexity relates to robustness. They found that complex features are less robust, meaning they are more sensitive to input perturbations.\nThey quantified robustness by measuring the variance in a feature’s response $z(x)$ when the input $x$ was perturbed with Gaussian noise. For each input, they generated perturbed versions:\n$$ \\tilde{x} = x + \\mathcal{N}(0, \\sigma^2 I) $$\nand computed the sensitivity score as the variance of $z(\\tilde{x})$:\n$$ \\text{Sensitivity}(z) = \\text{Var}(z(\\tilde{x})) $$\nThey tested this over 100 noise samples and three noise levels $\\sigma \\in {0.01, 0.1, 0.5}$ across 2,000 validation images. Regardless of the metric used (variance or range), complex features consistently showed higher sensitivity to noise. This suggests complex features are more fragile and less robust compared to simpler ones.\n3.3. Importance Measure By trying to find a relation with the importance of the features, the authors focused on the penultimate layer of the network, where the extracted features $z$ are directly connected to the logits $y$. A logit is the raw output of the model before applying softmax to get class probabilities. It represents the model’s confidence for each class.\nTo measure how much each feature $z_i$ influences the logit $y$, the authors used the Gradient × Input method. Specifically, they computed:\n$$ \\Gamma(z_i) = \\mathbb{E} \\left[ \\left| \\frac{\\partial y}{\\partial z_i} \\cdot z_i \\right| \\right] $$\nThis gives the importance score for each feature, showing how sensitive the model\u0026rsquo;s output is to changes in $z_i$. A higher score means the feature has a bigger impact on the prediction.\nThe authors found that simple features often have higher importance scores. These features are more directly used by the network to make decisions. On the other hand, complex features tend to have lower direct importance but may still play supporting roles in the model’s reasoning.\n4. Feature Flow and Information Theory The authors validated their hypothesis on how simple and complex features propagate through neural networks by replicating their earlier analysis with a different complexity measure. While they initially used Centered Kernel Alignment (CKA), they later confirmed their findings with V-Information, their primary complexity metric. The results were consistent: simple features showed high V-Information early and were efficiently passed through residual connections, while complex features accumulated V-Information gradually, being constructed layer by layer. This replication, done with a different ResNet50 model (Keras) and on ImageNet validation data, reinforced the idea that complex features are built progressively rather than carried intact through the network.\nThe authors then connected their empirical findings to concepts from algorithmic information theory, particularly Kolmogorov Complexity and Levin Complexity, to offer a theoretical foundation for their observations.\nKolmogorov Complexity measures the length of the shortest program capable of producing a given output sequence $(u_n)$ over some finite alphabet $\\Sigma$:\n$$ K^{(\\infty)}_L(u_n) = \\min_{P(n) = u_n} |P| $$\nIntuitively, sequences that follow simple patterns (like $1, 2, 3, 4, \u0026hellip;$) have low Kolmogorov complexity because they can be described by a short program. Conversely, truly random sequences have high complexity because they lack shorter descriptions. While Kolmogorov complexity captures an idealized notion of simplicity, it is not computable—there’s no general algorithm that can calculate it.\nTo make this concept tractable, Levin Complexity was introduced. Levin adds a penalty based on the runtime of the program, balancing the program’s length and the time it takes to run:\n$$ K^{(T)}_L(u_n) = \\min_{P(n) = u_n} |P| + \\log |\\Sigma| \\cdot T(P, n) $$\nHere, $T(P, n)$ represents the time the program $P$ takes to compute $u_n$. This makes it possible to compute Levin complexity through Levin Universal Search, an algorithm that runs all programs of increasing length in parallel, for one step at a time, until one halts and produces the output:\nAlgorithm 1 : Levin Universal Search Input: sequence (u_n) ∈ Σ* Output: program P minimizing K(T)_L 1: S ← ∅ 2: for i ∈ N do 3: for each program P ∈ (Σ^i ∩ L) do 4: S ← S ∪ {P} 5: for each P ∈ S in parallel do 6: Run P for exactly 1 step 7: if P halts on u_n then 8: return P This algorithm tends to find the simplest and fastest programs first, illustrating a simplicity bias: simpler solutions are discovered before more complex ones.\nThe authors argue that deep learning models exhibit a similar simplicity bias. Neural networks are effectively programs composed of weights and computations. Features that are decoded early in the network—those requiring fewer layers and simpler computation—are analogous to low Kolmogorov or Levin complexity. Features that require deeper, more complex computations align with higher complexity measures.\nTheir V-Information metric formalizes this intuition. It quantifies how much useful information about a feature $z$ can be extracted from an input $x$ at different layers of the network:\n$$ I_V(x \\rightarrow z) = H_V(z) - H_V(z | x) $$\nA higher V-Information indicates a feature is easier to decode (simpler), while lower V-Information implies a feature is harder to access and thus more complex. This mirrors the relationship between program length and runtime in Kolmogorov and Levin complexities.\n5. Experiment Summary: Exploring Feature Complexity in ResNet18 In this experiment, we set out to explore how complex and simple features are learned and represented in a deep convolutional neural network, specifically using ResNet18 trained on CIFAR-10. The goal was to understand feature complexity, redundancy, robustness, and importance, similar to the methods and insights presented in the research paper.\n5.1. How the Code Was Structured and What Was Done In this experiment, we worked with the CIFAR-10 dataset, which contains 60,000 color images across 10 classes, such as airplanes, cars, and birds. We split the dataset into training and validation sets and normalized the images for consistent input.\nWe trained a ResNet18 model from scratch on CIFAR-10 over 5 epochs using Stochastic Gradient Descent (SGD) and cross-entropy loss. The trained model achieved reasonable accuracy, making it suitable for analyzing feature representations.\nAfter training, we extracted features from the penultimate layer and pre-pooling feature maps, which capture high-level concepts learned by the network. To further analyze these features, we applied Non-Negative Matrix Factorization (NMF), creating an overcomplete dictionary that helped disentangle individual features and overcome the issue of feature superposition.\nWe computed V-Information scores to measure the complexity of each feature, showing how difficult they are to decode at different layers of the network. In addition, we analyzed feature importance through logistic regression, assessed redundancy by examining feature correlations, and measured robustness by testing feature stability under noise perturbations.\nFinally, we visualized the features using UMAP, projecting them into 2D space to reveal clusters based on complexity and semantic similarity. HDBSCAN clustering helped identify groups of related features, offering insights into their complexity and roles in the network’s decision-making process.\n5.2. Experiment Results and Interpretation We plotted the features using UMAP and colored them by their complexity scores (based on V-Information). Caption: UMAP representation of the Features\u0026rsquo; Complexity on CIFAR10 Dataset\nCaption: Clusters that are labeled on the UMAP Map\nBy applying V-Information, we clustered features based on their complexity, and visualized them using UMAP. In the second UMAP plot (Figure 2), we see clear groupings: clusters with images of planes, cars, and sky are associated with low complexity. These images are simple, with uniform regions and clear shapes, making them easier to process for the model.\nIn contrast, clusters labeled Animals, Deers, and Horses contain high complexity images. These images feature more visual detail, like fur, leaves, and branches, which require deeper processing and more complex feature representations.\nEven with CIFAR-10’s low-resolution, the network distinguishes simple from complex features effectively. Images dominated by sky are less complex, while those with animals and rich textures show up as more complex in both the V-Information measure and the UMAP clustering.\nAfter analyzing the UMAP, we plotted how feature complexity relates to feature importance for classification. In our experiment, we plotted the relationship between feature complexity and importance to investigate how these two properties interact within the network. The scatter plot shows each feature as a blue dot, with complexity on the x-axis (measured by V-Information) and importance on the y-axis (derived from a logistic regression model trained on the features). We also added a red regression line to highlight the overall trend.\nFrom this visualization, we found a slight negative correlation between complexity and importance. This means that simpler features, which are easier for the network to decode, tend to have higher importance in the model’s decisions. On the other hand, more complex features, which require deeper computation and are harder to extract, tend to have lower importance on their own. This result supports the findings in the paper, which argue that neural networks exhibit a simplicity bias, relying more heavily on simpler, easily accessible features during inference.\n6. Conclusion In this work, we explored the concept of feature complexity in deep neural networks, based on the paper \u0026ldquo;Understanding Visual Feature Reliance through the Lens of Complexity.\u0026rdquo; The authors introduced V-Information as a metric to measure how accessible and complex a feature is within a model. Their findings highlight a simplicity bias in networks, where simpler features are prioritized due to their ease of extraction, robustness, and importance for decision-making.\nIn our experiment with ResNet18 on CIFAR-10, we replicated key aspects of their methodology. Using NMF and V-Information, we analyzed feature complexity and visualized the feature space with UMAP and HDBSCAN clustering. Our results showed that simple features, such as planes and skies, clustered together and had lower complexity, while more detailed images, like animals, exhibited higher complexity. We also observed a slight negative correlation between feature complexity and importance, reinforcing the idea that networks rely more on simple features.\nOverall, our experiments support the paper’s conclusions: deep networks tend to favor simple, robust features while complex ones play a secondary, less stable role in the decision process.\n","content_html":"\u003chr\u003e\u003c/hr\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\$','\\$']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eUnderstanding Visual Feature Reliance through the Lens of Complexity\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors: DIB Caren, SABA Jean Paul, WANG Romain\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArticle: \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2024/file/819977c0a95458911bbfd9e5b5115018-Paper-Conference.pdf\"\u003eUnderstanding Visual Feature Reliance through the Lens of Complexity\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#feature-extraction\"\u003eFeature Extraction via Dictionary Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#detecting-complexity\"\u003eDetecting Complexity\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#redundancy\"\u003eRelations with Redundancy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#robustness\"\u003eRelations with Robustness\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#importance\"\u003eImportance Measure\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#replication\"\u003eFeature Flow and Information Theory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#experiment\"\u003eExperiment Summary: Exploring Feature Complexity in ResNet18\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#experiment_1\"\u003eHow the Code Was Structured and What Was Done\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#experiment_2\"\u003eExperiment Results and Interpretation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003cp\u003eIn this blog, we’ll take a deep dive into an insightful and thought-provoking paper authored by Thomas Fel, Louis Béthune, Andrew Kyle Lampinen, Thomas Serre, and Katherine Hermann. Their work explores the intricate mechanisms underlying how deep neural networks—specifically ResNet50—learn and represent complex features. This research, rooted in both theoretical and empirical analysis, investigates the nature of feature complexity, how features evolve over the course of training, and the computational structures that enable neural networks to generalize effectively.\u003c/p\u003e\n\u003cp\u003eThe authors\u0026rsquo; central motivation is to understand how models balance computational efficiency with representational richness. They explore why deep networks exhibit a preference for simpler features (simplicity bias), how complex features are supported within a network, and the trade-offs between redundancy, robustness, and importance of these features. V-information serves as the main complexity metric used throughout their study, offering a principled approach to quantifying how computationally accessible features are. In addition to V-information, they employ several other quantitative measures—such as importance scores derived from Gradient × Input, and redundancy and robustness metrics informed by prior work—to provide an exhaustive and structured analysis of feature learning dynamics.\u003c/p\u003e\n\u003cp\u003eTheir findings have implications for model interpretability, robustness, and generalization, offering deep insights into the practical and theoretical aspects of modern deep learning systems. In this blog, we break down their study into a comprehensive guide for easier understanding.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"feature-extraction\"\u003e2. Feature Extraction and Visualization\u003c/h1\u003e\n\u003cp\u003eIn this section, the authors explore the nature and diversity of features learned by the network. First, some general information:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSimple Features\u003c/strong\u003e: These are easy-to-decode, frequently occurring concepts like \u003cem\u003esky\u003c/em\u003e, \u003cem\u003egrass\u003c/em\u003e, and \u003cem\u003ewatermarks\u003c/em\u003e. They typically emerge early in the network and are transported through residual connections with little modification. Such features are aligned with simplicity bias and often serve as shortcuts for the model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMedium Complexity Features\u003c/strong\u003e: These include concepts like \u003cem\u003ehuman-related elements\u003c/em\u003e, \u003cem\u003elow-pixel quality detectors\u003c/em\u003e, and \u003cem\u003etrademarks\u003c/em\u003e. They often represent slightly more abstract or nuanced properties and require more layers and computational effort to emerge.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eComplex Features\u003c/strong\u003e: Highly intricate concepts like \u003cem\u003einsect legs\u003c/em\u003e, \u003cem\u003ewhiskers\u003c/em\u003e, and \u003cem\u003efilament structures\u003c/em\u003e represent the most complex features. These require extensive processing across multiple layers and involve both the main and residual network branches to form progressively.\u003c/p\u003e\n\u003cp\u003eTo be able to extract those features, the authors introduced an overcomplete dictionary as a solution to a key challenge in understanding deep neural networks: \u003cstrong\u003ethe superposition problem\u003c/strong\u003e, where multiple features are entangled within single neurons, making it difficult to isolate and analyze individual features. In standard neural networks, activations $f_n(x)$ in the penultimate layer represent complex, often overlapping features, and the number of distinct features may far exceed the number of neurons $|A_\\ell|$. To address this, the authors leveraged dictionary learning to build an overcomplete dictionary $D^*$, where $k \\gg |A_\\ell|$, with $k$ representing the number of atoms (or basis elements) in the dictionary. This allowed them to extract a richer set of disentangled features—up to 10,000, far more than the neuron count.\u003c/p\u003e\n\u003cp\u003eEach activation $f_n(x)$ is approximated as a linear combination of atoms from the overcomplete dictionary $D^*$, weighted by sparse coefficients $z$:\u003c/p\u003e\n\u003cp\u003e$$\nf_n(x) \\approx z D^* = \\sum_{i=1}^k z_i d_i\n$$\u003c/p\u003e\n\u003cp\u003eThis overcomplete setup allows for disentangling features beyond what individual neurons can represent. The dictionary $D^*$ is learned using Non-Negative Matrix Factorization (NMF), which aligns with the non-negative nature of ReLU activations. The optimization minimizes reconstruction error with non-negativity constraints:\u003c/p\u003e\n\u003cp\u003e$$\n(Z, D^*) = \\arg \\min_{Z \\ge 0, D^* \\ge 0} | f_n(X) - Z D^* |_F\n$$\u003c/p\u003e\n\u003cp\u003eTrained on ImageNet with 58 million samples, the dictionary preserves over 99% of the model’s predictive accuracy. Once $D^*$ is fixed, features for new inputs are extracted by solving:\u003c/p\u003e\n\u003cp\u003e$$\nz = \\arg \\min_{z \\ge 0} | f_n(x) - z D^* |_F\n$$\u003c/p\u003e\n\u003cp\u003eThe authors used MACO, an advanced feature visualization technique, to produce clearer and more realistic images of the network’s learned features. These visualizations are sorted by complexity, from the simplest to the most complex features, highlighting the increasing detail and intricacy as complexity grows.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/JeanPaul_Saba/VIZ.png\"\n  alt=\"Meta-Feature Visualization\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Feature Viz using MACO\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/JeanPaul_Saba/UMAP.png\"\n  alt=\"UMAP Visualization\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Feature complexity using UMAP\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"detecting-complexity\"\u003e3. Detecting Complexity\u003c/h1\u003e\n\u003cp\u003eThe authors propose \u003cstrong\u003eV-information\u003c/strong\u003e as their primary metric to quantify feature complexity. They focus on a setting where the predictive family $V$ consists of linear classifiers with Gaussian posteriors. In this context, V-information measures how much information a representation $x$ provides about a feature $z$ under computational constraints. The authors derive a closed-form solution for V-information when $V$ consists of these linear Gaussian models:\u003c/p\u003e\n\u003cp\u003e$$\nI_V(x \\to z) = H_V(z) - H_V(z|x)\n$$\u003c/p\u003e\n\u003cp\u003eHere, $H_V(z)$ represents the V-entropy, which measures the uncertainty about $z$ when using the best possible predictor from the restricted family $V$. Similarly, $H_V(z|x)$ is the V-conditional entropy, which measures the remaining uncertainty about $z$ after observing $x$, again under the constraint of using predictors from $V$.\u003c/p\u003e\n\u003cp\u003eWhich leads to having:\u003c/p\u003e\n\u003cp\u003e$$\n0 \\le I_V(x \\to z) \\le \\text{Var}(z)\n$$\u003c/p\u003e\n\u003cp\u003eSince the input data are centered and scaled, $\\text{Var}(z)$ is typically close to 1. The authors define feature complexity $K(z, x)$ as the inverse of the average V-information across network layers, quantifying how difficult it is to decode a feature as it propagates through the network:\u003c/p\u003e\n\u003cp\u003e$$\nK(z, x) = 1 - \\frac{1}{n} \\sum_{\\ell=1}^{n} I_V(f_\\ell(x) \\to z)\n$$\u003c/p\u003e\n\u003cp\u003eThey note that a higher $K(z, x)$ score indicates a more complex feature, harder to decode until later in the model. Empirically, they observed that $K(z, x)$ generally falls within $[0, 1]$, with 1 representing high complexity.\u003c/p\u003e\n\u003ch2 id=\"redundancy\"\u003e3.1. Relations with Redundancy\u003c/h2\u003e\n\u003cp\u003eTo explore the relationship between complexity and redundancy, the authors employed a redundancy measure based on \u003cstrong\u003eCentered Kernel Alignment (CKA)\u003c/strong\u003e. In their analysis, they compared the similarity between a feature $z$ and the network activations $f_n(X)$, both before and after masking parts of the activations. A \u003cstrong\u003ebinary mask $m$\u003c/strong\u003e was applied to the activations, where $m \\in {0,1}^{|A_\\ell|}$ selects which neurons remain active (1) and which are deactivated (0). If masking neurons didn’t change the similarity much, it meant the feature was redundant, spread over many neurons. If the similarity dropped a lot, the feature was localized, relying on fewer neurons. The redundancy score was calculated as:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Redundancy} = \\mathbb{E}_m \\left[ \\frac{CKA(f_n(X) \\odot m, z)}{CKA(f_n(X), z)} \\right]\n$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{CKA}(A, B) = \\frac{|K_A K_B|_F^2}{|K_A K_A|_F \\cdot |K_B K_B|_F}\n$$\u003c/p\u003e\n\u003cp\u003eThey found that \u003cstrong\u003ecomplex features are less redundant\u003c/strong\u003e, meaning they depend on specific neurons and are more fragile.\u003c/p\u003e\n\u003ch2 id=\"robustness\"\u003e3.2. Complexity and Robustness\u003c/h2\u003e\n\u003cp\u003eThe authors also investigated how feature complexity relates to robustness. They found that complex features are less robust, meaning they are more sensitive to input perturbations.\u003c/p\u003e\n\u003cp\u003eThey quantified robustness by measuring the variance in a feature’s response $z(x)$ when the input $x$ was perturbed with Gaussian noise. For each input, they generated perturbed versions:\u003c/p\u003e\n\u003cp\u003e$$\n\\tilde{x} = x + \\mathcal{N}(0, \\sigma^2 I)\n$$\u003c/p\u003e\n\u003cp\u003eand computed the sensitivity score as the variance of $z(\\tilde{x})$:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Sensitivity}(z) = \\text{Var}(z(\\tilde{x}))\n$$\u003c/p\u003e\n\u003cp\u003eThey tested this over 100 noise samples and three noise levels $\\sigma \\in {0.01, 0.1, 0.5}$ across 2,000 validation images. Regardless of the metric used (variance or range), complex features consistently showed higher sensitivity to noise. This suggests complex features are more fragile and less robust compared to simpler ones.\u003c/p\u003e\n\u003ch2 id=\"importance\"\u003e3.3. Importance Measure\u003c/h2\u003e\n\u003cp\u003eBy trying to find a relation with the importance of the features, the authors focused on the penultimate layer of the network, where the extracted features $z$ are directly connected to the logits $y$. A logit is the raw output of the model before applying softmax to get class probabilities. It represents the model’s confidence for each class.\u003c/p\u003e\n\u003cp\u003eTo measure how much each feature $z_i$ influences the logit $y$, the authors used the Gradient × Input method. Specifically, they computed:\u003c/p\u003e\n\u003cp\u003e$$\n\\Gamma(z_i) = \\mathbb{E} \\left[ \\left| \\frac{\\partial y}{\\partial z_i} \\cdot z_i \\right| \\right]\n$$\u003c/p\u003e\n\u003cp\u003eThis gives the importance score for each feature, showing how sensitive the model\u0026rsquo;s output is to changes in $z_i$. A higher score means the feature has a bigger impact on the prediction.\u003c/p\u003e\n\u003cp\u003eThe authors found that simple features often have higher importance scores. These features are more directly used by the network to make decisions. On the other hand, complex features tend to have lower direct importance but may still play \u003cstrong\u003esupporting roles\u003c/strong\u003e in the model’s reasoning.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"replication\"\u003e4. Feature Flow and Information Theory\u003c/h1\u003e\n\u003cp\u003eThe authors validated their hypothesis on how simple and complex features propagate through neural networks by replicating their earlier analysis with a different complexity measure. While they initially used Centered Kernel Alignment (CKA), they later confirmed their findings with \u003cstrong\u003eV-Information\u003c/strong\u003e, their primary complexity metric.\nThe results were consistent: simple features showed high V-Information early and were efficiently passed through residual connections, while complex features accumulated V-Information gradually, being constructed layer by layer. This replication, done with a different ResNet50 model (Keras) and on ImageNet validation data, reinforced the idea that complex features are built progressively rather than carried intact through the network.\u003c/p\u003e\n\u003cp\u003eThe authors then connected their empirical findings to concepts from algorithmic information theory, particularly \u003cstrong\u003eKolmogorov Complexity\u003c/strong\u003e and \u003cstrong\u003eLevin Complexity\u003c/strong\u003e, to offer a theoretical foundation for their observations.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKolmogorov Complexity\u003c/strong\u003e measures the length of the shortest program capable of producing a given output sequence $(u_n)$ over some finite alphabet $\\Sigma$:\u003c/p\u003e\n\u003cp\u003e$$\nK^{(\\infty)}_L(u_n) = \\min_{P(n) = u_n} |P|\n$$\u003c/p\u003e\n\u003cp\u003eIntuitively, sequences that follow simple patterns (like $1, 2, 3, 4, \u0026hellip;$) have low Kolmogorov complexity because they can be described by a short program. Conversely, truly random sequences have high complexity because they lack shorter descriptions. While Kolmogorov complexity captures an idealized notion of simplicity, it is not computable—there’s no general algorithm that can calculate it.\u003c/p\u003e\n\u003cp\u003eTo make this concept tractable, \u003cstrong\u003eLevin Complexity\u003c/strong\u003e was introduced. Levin adds a penalty based on the runtime of the program, balancing the program’s length and the time it takes to run:\u003c/p\u003e\n\u003cp\u003e$$\nK^{(T)}_L(u_n) = \\min_{P(n) = u_n} |P| + \\log |\\Sigma| \\cdot T(P, n)\n$$\u003c/p\u003e\n\u003cp\u003eHere, $T(P, n)$ represents the time the program $P$ takes to compute $u_n$. This makes it possible to compute Levin complexity through Levin Universal Search, an algorithm that runs all programs of increasing length in parallel, for one step at a time, until one halts and produces the output:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAlgorithm 1 : Levin Universal Search\nInput: sequence (u_n) ∈ Σ*\nOutput: program P minimizing K(T)_L\n1: S ← ∅\n2: for i ∈ N do\n3:     for each program P ∈ (Σ^i ∩ L) do\n4:         S ← S ∪ {P}\n5:     for each P ∈ S in parallel do\n6:         Run P for exactly 1 step\n7:         if P halts on u_n then\n8:             return P\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis algorithm tends to find the simplest and fastest programs first, illustrating a simplicity bias: simpler solutions are discovered before more complex ones.\u003c/p\u003e\n\u003cp\u003eThe authors argue that deep learning models exhibit a similar simplicity bias. Neural networks are effectively programs composed of weights and computations. Features that are decoded early in the network—those requiring fewer layers and simpler computation—are analogous to low Kolmogorov or Levin complexity. Features that require deeper, more complex computations align with higher complexity measures.\u003c/p\u003e\n\u003cp\u003eTheir \u003cstrong\u003eV-Information\u003c/strong\u003e metric formalizes this intuition. It quantifies how much useful information about a feature $z$ can be extracted from an input $x$ at different layers of the network:\u003c/p\u003e\n\u003cp\u003e$$\nI_V(x \\rightarrow z) = H_V(z) - H_V(z | x)\n$$\u003c/p\u003e\n\u003cp\u003eA higher V-Information indicates a feature is easier to decode (simpler), while lower V-Information implies a feature is harder to access and thus more complex. This mirrors the relationship between program length and runtime in Kolmogorov and Levin complexities.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"experiment\"\u003e5. Experiment Summary: Exploring Feature Complexity in ResNet18\u003c/h1\u003e\n\u003cp\u003eIn this experiment, we set out to explore how complex and simple features are learned and represented in a deep convolutional neural network, specifically using \u003cstrong\u003eResNet18\u003c/strong\u003e trained on \u003cstrong\u003eCIFAR-10\u003c/strong\u003e. The goal was to understand feature complexity, redundancy, robustness, and importance, similar to the methods and insights presented in the research paper.\u003c/p\u003e\n\u003ch2 id=\"experiment_1\"\u003e5.1. How the Code Was Structured and What Was Done\u003c/h2\u003e\n\u003cp\u003eIn this experiment, we worked with the CIFAR-10 dataset, which contains 60,000 color images across 10 classes, such as airplanes, cars, and birds. We split the dataset into training and validation sets and normalized the images for consistent input.\u003c/p\u003e\n\u003cp\u003eWe trained a ResNet18 model from scratch on CIFAR-10 over 5 epochs using Stochastic Gradient Descent (SGD) and cross-entropy loss. The trained model achieved reasonable accuracy, making it suitable for analyzing feature representations.\u003c/p\u003e\n\u003cp\u003eAfter training, we extracted features from the penultimate layer and pre-pooling feature maps, which capture high-level concepts learned by the network. To further analyze these features, we applied Non-Negative Matrix Factorization (NMF), creating an overcomplete dictionary that helped disentangle individual features and overcome the issue of feature superposition.\u003c/p\u003e\n\u003cp\u003eWe computed V-Information scores to measure the complexity of each feature, showing how difficult they are to decode at different layers of the network. In addition, we analyzed feature importance through logistic regression, assessed redundancy by examining feature correlations, and measured robustness by testing feature stability under noise perturbations.\u003c/p\u003e\n\u003cp\u003eFinally, we visualized the features using UMAP, projecting them into 2D space to reveal clusters based on complexity and semantic similarity. HDBSCAN clustering helped identify groups of related features, offering insights into their complexity and roles in the network’s decision-making process.\u003c/p\u003e\n\u003ch2 id=\"experiment_2\"\u003e5.2. Experiment Results and Interpretation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eWe plotted the features using \u003cstrong\u003eUMAP\u003c/strong\u003e and colored them by their \u003cstrong\u003ecomplexity scores\u003c/strong\u003e (based on V-Information).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/JeanPaul_Saba/experimentUMAPlabeled.png\"\n  alt=\"Feature Complexity UMAP\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: UMAP representation of the Features\u0026rsquo; Complexity on CIFAR10 Dataset\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/JeanPaul_Saba/clusters.png\"\n  alt=\"Examples Of Clusters\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Clusters that are labeled on the UMAP Map\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eBy applying V-Information, we clustered features based on their complexity, and visualized them using UMAP. In the second UMAP plot (Figure 2), we see clear groupings: clusters with images of planes, cars, and sky are associated with low complexity. These images are simple, with uniform regions and clear shapes, making them easier to process for the model.\u003c/p\u003e\n\u003cp\u003eIn contrast, clusters labeled Animals, Deers, and Horses contain high complexity images. These images feature more visual detail, like fur, leaves, and branches, which require deeper processing and more complex feature representations.\u003c/p\u003e\n\u003cp\u003eEven with CIFAR-10’s low-resolution, the network distinguishes simple from complex features effectively. Images dominated by sky are less complex, while those with animals and rich textures show up as more complex in both the V-Information measure and the UMAP clustering.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eAfter analyzing the UMAP, we plotted how feature complexity relates to feature importance for classification.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/JeanPaul_Saba/complexityVSimportance.png\"\n  alt=\"Complexity vs Importance\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn our experiment, we plotted the relationship between feature complexity and importance to investigate how these two properties interact within the network. The scatter plot shows each feature as a blue dot, with complexity on the x-axis (measured by V-Information) and importance on the y-axis (derived from a logistic regression model trained on the features). We also added a red regression line to highlight the overall trend.\u003c/p\u003e\n\u003cp\u003eFrom this visualization, we found a slight negative correlation between complexity and importance. This means that simpler features, which are easier for the network to decode, tend to have higher importance in the model’s decisions. On the other hand, more complex features, which require deeper computation and are harder to extract, tend to have lower importance on their own. This result supports the findings in the paper, which argue that neural networks exhibit a simplicity bias, relying more heavily on simpler, easily accessible features during inference.\u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003e6. Conclusion\u003c/h1\u003e\n\u003cp\u003eIn this work, we explored the concept of feature complexity in deep neural networks, based on the paper \u003cem\u003e\u0026ldquo;Understanding Visual Feature Reliance through the Lens of Complexity.\u0026rdquo;\u003c/em\u003e The authors introduced V-Information as a metric to measure how accessible and complex a feature is within a model. Their findings highlight a simplicity bias in networks, where simpler features are prioritized due to their ease of extraction, robustness, and importance for decision-making.\u003c/p\u003e\n\u003cp\u003eIn our experiment with ResNet18 on CIFAR-10, we replicated key aspects of their methodology. Using NMF and V-Information, we analyzed feature complexity and visualized the feature space with UMAP and HDBSCAN clustering. Our results showed that simple features, such as planes and skies, clustered together and had lower complexity, while more detailed images, like animals, exhibited higher complexity. We also observed a slight negative correlation between feature complexity and importance, reinforcing the idea that networks rely more on simple features.\u003c/p\u003e\n\u003cp\u003eOverall, our experiments support the paper’s conclusions: deep networks tend to favor simple, robust features while complex ones play a secondary, less stable role in the decision process.\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/understanding_visual_feature_reliance_through_the_lens_of_complexity/","date_published":"12036-12-09T328:1212:00+01:00","date_modified":"12036-12-09T328:1212:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"2c8de60e180b13998893fad3dacfa289a5f66271","title":"When Fairness Meets Privacy","summary":"","content_text":" When Fairness Meets Privacy: A Double-Edged Sword in Machine Learning This blog is based on and aims to present the key insights from the paper: When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks by Tian et al. (2023) 1. The study investigates how fairness-aware models can introduce new privacy risks, specifically through membership inference attacks. By summarizing the main findings and implications, this blog provides an accessible overview of the paper’s contributions and their significance for machine learning security and ethical AI development. For full details, refer to the original publication here.\nAuthors: Lagarde Vincent, Boyenval Thibaut, Leurquin Daniel This image was generated using artificial intelligence.\nTable of Contents Introduction: Fairness or Privacy—Pick Your Poison? Algorithmic Fairness: A Noble Goal That Cuts Both Ways Membership Inference Attacks: The Silent Thief of Privacy The Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA) Reproducible Code Experiments: Illustrating FD-MIA Experimental Findings: How Fairness Opens the Door to Attackers The Future of Fairness and Privacy: Can We Have Both? 1. Introduction: Fairness or Privacy Pick Your Poison It is double pleasure to deceive the deceiver. — Niccolò Machiavelli.\nThis paradox of attack and defense perfectly applies to the interplay between fairness and privacy in machine learning.\nImagine stepping into a high-tech courtroom. The AI judge, designed to be perfectly fair, renders unbiased decisions. But then, a hacker in the back row smirks—because that same fairness-enhancing mechanism just leaked private data about every case it trained on.\nFairness and privacy in AI are like the two ends of a seesaw: push too hard on one side, and the other rises uncontrollably. Recent research reveals a disturbing paradox: making a model fairer can also make it leak more private information.\nThis blog explores how fairness in machine learning, despite its good intentions, can introduce Membership Inference Attacks (MIAs). Worse still, it uncovers a devastating new attack—Fairness Discrepancy Membership Inference Attack (FD-MIA)—that exploits fairness interventions to make privacy breaches even more effective.\n2. Algorithmic Fairness: A Noble Goal That Cuts Both Ways Fairness in AI is like forging a perfect sword—it must be balanced, precise, and just. Researchers have developed in-processing fairness interventions, which modify the training process to remove biases in model predictions. These methods act like master swordsmiths, hammering out the unwanted imperfections in AI decision-making.\nHowever, every sword has two edges. These fairness techniques do not just eliminate biases—they also alter how models respond to data. This change in behavior can create exploitable patterns that adversaries can use to infer whether a specific individual was part of the training data. In short, while fairness dulls one blade (bias), it sharpens another (privacy risk).\nMathematically, fairness interventions often involve introducing constraints into the loss function:\n$$L_\\text{fair} = L_\\text{orig} + \\lambda \\cdot \\mathcal{L}_{\\text{fairness}}$$\nwhere\n$\\mathcal{L}_{\\text{orig}}$ is the original loss function (e.g., cross-entropy loss for classification tasks). $\\mathcal{L}_{\\text{fairness}}$ is a fairness penalty term, which ensures that predictions are balanced across different demographic groups. $\\lambda$ is a hyperparameter controlling the trade-off between accuracy and fairness. Common fairness constraints include Equalized Odds, which ensures that true positive and false positive rates are equal across groups:\n$$P(\\hat{Y} = 1 | Y = 1, S = s_0) = P(\\hat{Y} = 1 | Y = 1, S = s_1)$$\nwhere $S$ represents a sensitive attribute (e.g., gender or race).\nThe fairness penalty can be incorporated during model training by adding it to the loss function, as shown in this training function. A penalty coefficient can be specified to control the impact of the fairness term. Setting the coefficient to 0 results in no penalty being applied to the loss:\ndef train_model(model, data_loader, optimizer, criterion, fairness_weight=0.0): model.train() total_loss = 0.0 for X_batch, y_batch, sensitive_batch in data_loader: optimizer.zero_grad() logits = model(X_batch) loss = criterion(logits, y_batch) # Fairness regularization: penalize different confidence (softmax probabilities) if fairness_weight \u0026gt; 0: probs = nn.functional.softmax(logits, dim=1) # Compute average probability for each sensitive group group0_mask = (sensitive_batch == 0) group1_mask = (sensitive_batch == 1) if group0_mask.sum() \u0026gt; 0 and group1_mask.sum() \u0026gt; 0: avg_prob0 = probs[group0_mask].mean(dim=0) avg_prob1 = probs[group1_mask].mean(dim=0) # L2 difference between average prediction distributions fairness_penalty = torch.norm(avg_prob0 - avg_prob1, p=2) loss += fairness_weight * fairness_penalty loss.backward() optimizer.step() total_loss += loss.item() return total_loss / len(data_loader) While these interventions improve fairness, they also alter the confidence distribution of model predictions—a fact that attackers can exploit.\n3. Membership Inference Attacks: The Silent Thief of Privacy 🔐 A Parallel with Cryptography Membership inference attacks (MIAs) are to privacy what brute-force attacks are to passwords. Instead of guessing a password, they test thousands of combinations to see which one is good.\nMIAs work the same way: they analyze a model’s outputs to determine if a given data point was part of its training set.\nA traditional MIA exploits confidence scores—the probabilities that a model assigns to different predictions. The intuition is simple: models tend to be more confident on data they have seen during training. Given a target model $T$ and a queried sample $x$, an attacker computes:\n$$M(x) = 1 \\text{ if } A(T(x)) \u0026gt; \\tau$$\nwhere:\n$A(T(x))$ is a decision function (often a threshold on the confidence score). $\\tau$ is a predefined threshold. Why Traditional MIAs Fail on Fair Models Fairness interventions introduce more uncertainty into the model’s predictions. This causes:\nLower confidence scores overall, making it harder for attackers to distinguish between training and non-training samples. More uniform confidence distributions, which means attackers lose their key signal. Thus, fairness-enhanced models resist traditional MIAs. But this protection is not foolproof—a new, more dangerous attack lurks in the shadows.\n4. The Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA) If traditional MIAs are blunt weapons, FD-MIA is a scalpel. It exploits the discrepancies between a biased model and a fairness-enhanced one.\nHow does FD-MIA work? Fairness interventions shift model predictions differently for training and non-training data. This creates a gap between how biased and fair models behave for the same inputs. An attacker, armed with knowledge of both models, can exploit this difference to infer membership with high accuracy.\nMathematically, FD-MIA extends membership prediction by comparing prediction shifts between biased and fair models:\n$$M(x) = 1 \\text{ if } |T_{\\text{bias}}(x) - T_{\\text{fair}}(x)| \u0026gt; \\tau$$\nwhere:\n$T_{\\text{bias}}(x)$ and $T_{\\text{fair}}(x)$ are the predictions from the biased and fair models, respectively. $\\tau$ is a threshold chosen by the attacker. Figure 1: FD-MIA exploits the predictions from both models to achieve efficient attacks. From original paper.\nThe key insight is that fairness interventions cause systematic shifts in model confidence, creating a measurable pattern that attackers can exploit.\nHere is an example implementation of the FD-MIA attack using a function that compares the prediction difference between two models against a user-defined threshold:\ndef fd_mia_attack(sample, biased_model, fair_model, threshold=0.1): \u0026#34;\u0026#34;\u0026#34; Given a sample, compute the absolute difference between the biased and fair model\u0026#39;s softmax outputs for the positive class. If the difference exceeds the threshold, predict membership. \u0026#34;\u0026#34;\u0026#34; biased_model.eval() fair_model.eval() with torch.no_grad(): logits_b = biased_model(sample.unsqueeze(0)) logits_f = fair_model(sample.unsqueeze(0)) prob_b = nn.functional.softmax(logits_b, dim=1)[0, 1] prob_f = nn.functional.softmax(logits_f, dim=1)[0, 1] diff = abs(prob_b - prob_f) # In practice, the threshold can be tuned via shadow models or validation return 1 if diff \u0026gt; threshold else 0, diff.item() 5. Reproducible Code Experiments: Illustrating FD-MIA In this section, we reproduce the FD-MIA attack described in the original paper:\nData Generation and Splitting\nThe process starts with generating a synthetic dataset where a binary sensitive attribute influences the feature distribution. The target labels are computed using a logistic function, and the dataset is then split into training (member) and testing (non-member) sets to simulate membership inference scenarios.\nClassifier Architectures\nTwo identical neural network architectures are defined:\nA biased baseline model trained without fairness constraints. A fairness-enhanced model, which incorporates a fairness penalty to balance prediction distributions across sensitive groups. Training with Fairness Regularization\nThe function train_model allows the inclusion of a fairness penalty during training. For the fair model, this penalty—weighted by fairness_weight—is added to the standard cross-entropy loss to encourage prediction consistency across sensitive groups.\nFD-MIA Attack Implementation\nThe attack, implemented in fd_mia_attack, exploits the absolute difference in predicted probabilities (for the positive class) between the biased and fair models. If this difference exceeds a given threshold, the sample is inferred as a training member. This approach leverages the core principle of FD-MIA: fairness interventions create prediction discrepancies that can be used for membership inference.\nEvaluation and Visualization\nThe attack is evaluated by comparing prediction differences between member and non-member data. We visualize the prediction difference distributions to highlight how fairness-driven adjustments can unintentionally expose membership information.\nimport torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader, random_split import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import roc_auc_score # Set random seeds for reproducibility torch.manual_seed(42) np.random.seed(42) # ------------------------------- # 1. Create a synthetic dataset with a sensitive attribute # ------------------------------- class SyntheticFairDataset(Dataset): def __init__(self, n_samples=1000): # Features: two-dimensional points drawn from different distributions self.n_samples = n_samples self.X = [] self.y = [] self.sensitive = [] # sensitive attribute: 0 or 1 for i in range(n_samples): # Randomly assign a sensitive group (imbalance can be introduced here) s = np.random.choice([0, 1], p=[0.7, 0.3]) # Generate features from group-dependent distributions if s == 0: x = np.random.normal(loc=0.0, scale=1.0, size=2) else: x = np.random.normal(loc=1.5, scale=1.0, size=2) # Label is determined by a linear rule (with some noise) y_prob = 1 / (1 + np.exp(- (x[0] + x[1] - 0.5))) y_label = np.random.binomial(1, y_prob) self.X.append(x) self.y.append(y_label) self.sensitive.append(s) self.X = torch.tensor(self.X, dtype=torch.float32) self.y = torch.tensor(self.y, dtype=torch.long) self.sensitive = torch.tensor(self.sensitive, dtype=torch.long) def __len__(self): return self.n_samples def __getitem__(self, idx): return self.X[idx], self.y[idx], self.sensitive[idx] dataset = SyntheticFairDataset(n_samples=2000) # Split into training (for model training) and attack evaluation (simulate member vs. non-member) train_size = int(0.5 * len(dataset)) test_size = len(dataset) - train_size train_dataset, test_dataset = random_split(dataset, [train_size, test_size]) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) # ------------------------------- # 2. Define the classifier architectures # ------------------------------- class SimpleClassifier(nn.Module): def __init__(self, input_dim=2, hidden_dim=16, output_dim=2): super(SimpleClassifier, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): out = self.relu(self.fc1(x)) return self.fc2(out) # raw logits # Baseline (biased) model biased_model = SimpleClassifier() # Fairness-enhanced model: we add a fairness penalty to the loss (for demonstration) fair_model = SimpleClassifier() # ------------------------------- # 3. Train both models # ------------------------------- criterion = nn.CrossEntropyLoss() # Train biased model without fairness penalty optimizer_biased = optim.Adam(biased_model.parameters(), lr=0.01) for epoch in range(20): loss = train_model(biased_model, train_loader, optimizer_biased, criterion, fairness_weight=0.0) # Uncomment to print loss: print(f\u0026#34;Biased Model Epoch {epoch+1}: Loss {loss:.4f}\u0026#34;) # Train fair model with a fairness penalty (fairness_weight \u0026gt; 0) optimizer_fair = optim.Adam(fair_model.parameters(), lr=0.01) fairness_weight = 1.0 # adjust to control trade-off for epoch in range(20): loss = train_model(fair_model, train_loader, optimizer_fair, criterion, fairness_weight=fairness_weight) # Uncomment to print loss: print(f\u0026#34;Fair Model Epoch {epoch+1}: Loss {loss:.4f}\u0026#34;) # ------------------------------- # 4. Membership Inference Attack using FD-MIA # ------------------------------- # Evaluate the attack on both training (members) and test (non-members) samples def evaluate_attack(dataset, biased_model, fair_model, threshold=0.1): attack_labels = [] attack_scores = [] # Assuming samples in train_dataset are members and test_dataset non-members for sample, _, _ in DataLoader(dataset, batch_size=1, shuffle=False): pred, diff = fd_mia_attack(sample[0], biased_model, fair_model, threshold) attack_labels.append(pred) attack_scores.append(diff) return np.array(attack_labels), np.array(attack_scores) # For demonstration, we use the entire training set as member data and test set as non-member data. member_labels, member_diffs = evaluate_attack(train_dataset, biased_model, fair_model, threshold=0.1) nonmember_labels, nonmember_diffs = evaluate_attack(test_dataset, biased_model, fair_model, threshold=0.1) 6. Experimental Findings: How Fairness Opens the Door to Attackers The study conducted extensive experiments across six datasets, three attack methods, and five fairness approaches, testing over 160 models. To do this, they performed a comprehensive set of experiments involving:\nMultiple datasets with different sensitive attributes (like gender or race),\nBiased vs. fair model variants,\nThree types of MIAs (including their novel FD-MIA),\nMultiple fairness interventions\nDataset Number of Fairness Settings Target Tasks FD-MIA Effectiveness CelebA 3 settings (smile, hair, makeup) 3 binary targets × 2 sensitive attributes FD-MIA consistently outperformed other methods. UTKFace 2 settings (race prediction, gender prediction) Race or gender, sensitive to the other FD-MIA revealed privacy leaks even with balanced groups. FairFace 2 settings (race prediction, gender prediction) Same as UTKFace Most vulnerable dataset—biggest fairness shift = biggest privacy leak. The results were shocking:\nFair models were significantly harder to attack using traditional MIAs. FD-MIA, however, dramatically increased attack success rates—fairness actually made models more vulnerable! The greater the fairness intervention, the wider the discrepancy between biased and fair models, making FD-MIA even more effective. Our synthetic experiment further supports these findings. The histogram below illustrates the absolute difference between the predictions of the biased and fair models for both members (training data) and non-members (test data).\nFigure 2: FD-MIA: Prediction Difference Distribution.\nAs expected, members exhibit a significantly higher prediction discrepancy compared to non-members. This clear separation highlights how fairness constraints alter model confidence differently for training and test samples—providing an exploitable signal for membership inference. In simple terms: making a model fairer may paradoxically make it leak more private information. A cruel irony for those trying to do the right thing.\n7. The Future of Fairness and Privacy: Can We Have Both? The million-dollar question: Can we balance fairness and privacy without sacrificing one for the other?\nIf you know the enemy and know yourself, you need not fear the result of a hundred battles. — Sun Tzu.\nA better understanding of the link between fairness and privacy, as well as the potential and new attacks introduced by unbiased models, is already a solid step toward defending against threats. By understanding the underlying mechanisms, it becomes possible to counteract them.\nThe researchers propose two key defenses:\nRestricting Information Access\nLimiting confidence score outputs reduces the information available to attackers. Differential Privacy (DP)\nBy injecting noise into model training, DP-SGD (Differentially Private Stochastic Gradient Descent) helps obscure membership information:\n$$\\tilde{g}_t = g_t + \\mathcal{N}(0, \\sigma^2 I)$$\nwhere $g_t$ is the original gradient and $\\mathcal{N}(0, \\sigma^2 I)$ is Gaussian noise added to prevent membership inference.\nWhile these methods help, they come with trade-offs: too much privacy protection can lower fairness and accuracy, while too little leaves models vulnerable. The challenge ahead is designing AI systems that can balance both.\n💡 Alternative approach: Fairness-Aware Differential Privacy (FADP) adapts noise levels based on protected groups, balancing privacy and fairness.\nReferences H. Tian, G. Zhang, B. Liu, T. Zhu, M. Ding, and W. Zhou, \u0026ldquo;When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks\u0026rdquo;, arXiv e-prints, Art. no. arXiv.2311.03865.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","content_html":"\u003cstyle TYPE=\"text/css\"\u003e\n code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n \u003c/style\u003e\n \u003cscript type=\"text/x-mathjax-config\"\u003e\n MathJax.Hub.Config({\n     tex2jax: {\n         inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n         displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n     }\n });\n MathJax.Hub.Queue(function() {\n     var all = MathJax.Hub.getAllJax(), i;\n     for(i = 0; i \u003c all.length; i += 1) {\n         all[i].SourceElement().parentNode.className += ' has-jax';\n     }\n });\n \u003c/script\u003e\n \u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eWhen Fairness Meets Privacy: A Double-Edged Sword in Machine Learning\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eThis blog is based on and aims to present the key insights from the paper: \u003cstrong\u003eWhen Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks\u003c/strong\u003e by Tian et al. (2023) \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. The study investigates how fairness-aware models can introduce new privacy risks, specifically through membership inference attacks. By summarizing the main findings and implications, this blog provides an accessible overview of the paper’s contributions and their significance for machine learning security and ethical AI development. For full details, refer to the original publication \u003ca href=\"https://arxiv.org/pdf/2311.03865\"\u003ehere\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Lagarde Vincent, Boyenval Thibaut, Leurquin Daniel\u003c/h1\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n  \u003cimg src=\"/images/image_fairness_privacy/figure2.webp\" alt=\"FD-MIA: Prediction Difference Distribution\" style=\"width:80%; display:block; margin:auto;\"\u003e\n    \u003cp style=\"font-style: italic; font-size: 14px;\"\u003eThis image was generated using artificial intelligence.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#1-introduction-fairness-or-privacy-pick-your-poison\"\u003eIntroduction: Fairness or Privacy—Pick Your Poison?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#2-algorithmic-fairness-a-noble-goal-that-cuts-both-ways\"\u003eAlgorithmic Fairness: A Noble Goal That Cuts Both Ways\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#3-membership-inference-attacks-the-silent-thief-of-privacy\"\u003eMembership Inference Attacks: The Silent Thief of Privacy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#4-the-birth-of-a-new-threat-fairness-discrepancy-membership-inference-attacks-fd-mia\"\u003eThe Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#5-reproducible-code-experiments-illustrating-fd-mia\"\u003eReproducible Code Experiments: Illustrating FD-MIA\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#6-experimental-findings-how-fairness-opens-the-door-to-attackers\"\u003eExperimental Findings: How Fairness Opens the Door to Attackers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#7-the-future-of-fairness-and-privacy-can-we-have-both\"\u003eThe Future of Fairness and Privacy: Can We Have Both?\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-introduction-fairness-or-privacy-pick-your-poison\"\u003e1. Introduction: Fairness or Privacy Pick Your Poison\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eIt is double pleasure to deceive the deceiver.\u003c/em\u003e — Niccolò Machiavelli.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThis paradox of attack and defense perfectly applies to the interplay between fairness and privacy in machine learning.\u003c/p\u003e\n\u003cp\u003eImagine stepping into a high-tech courtroom. The AI judge, designed to be perfectly fair, renders unbiased decisions. But then, a hacker in the back row smirks—because that same fairness-enhancing mechanism just leaked private data about every case it trained on.\u003c/p\u003e\n\u003cp\u003eFairness and privacy in AI are like the two ends of a seesaw: push too hard on one side, and the other rises uncontrollably. \u003cstrong\u003eRecent research reveals a disturbing paradox: making a model fairer can also make it leak more private information.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis blog explores how fairness in machine learning, despite its good intentions, can introduce \u003cstrong\u003eMembership Inference Attacks (MIAs)\u003c/strong\u003e. Worse still, it uncovers a devastating new attack—\u003cstrong\u003eFairness Discrepancy Membership Inference Attack (FD-MIA)\u003c/strong\u003e—that exploits fairness interventions to \u003cstrong\u003emake privacy breaches even more effective\u003c/strong\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"2-algorithmic-fairness-a-noble-goal-that-cuts-both-ways\"\u003e2. Algorithmic Fairness: A Noble Goal That Cuts Both Ways\u003c/h2\u003e\n\u003cp\u003eFairness in AI is like forging a perfect sword—it must be balanced, precise, and just. Researchers have developed \u003cstrong\u003ein-processing fairness interventions\u003c/strong\u003e, which modify the training process to remove biases in model predictions. These methods act like master swordsmiths, hammering out the unwanted imperfections in AI decision-making.\u003c/p\u003e\n\u003cp\u003eHowever, every sword has two edges. These fairness techniques do not just eliminate biases—they also alter how models respond to data. This change in behavior can create exploitable patterns that adversaries can use to infer whether a specific individual was part of the training data. In short, while fairness dulls one blade (bias), it sharpens another (privacy risk).\u003c/p\u003e\n\u003cp\u003eMathematically, fairness interventions often involve introducing constraints into the loss function:\u003c/p\u003e\n\u003cp\u003e$$L_\\text{fair} = L_\\text{orig} + \\lambda \\cdot \\mathcal{L}_{\\text{fairness}}$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\mathcal{L}_{\\text{orig}}$ is the original loss function (e.g., cross-entropy loss for classification tasks).\u003c/li\u003e\n\u003cli\u003e$\\mathcal{L}_{\\text{fairness}}$ is a fairness penalty term, which ensures that predictions are balanced across different demographic groups.\u003c/li\u003e\n\u003cli\u003e$\\lambda$ is a hyperparameter controlling the trade-off between accuracy and fairness.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCommon fairness constraints include \u003cstrong\u003eEqualized Odds\u003c/strong\u003e, which ensures that true positive and false positive rates are equal across groups:\u003c/p\u003e\n\u003cp\u003e$$P(\\hat{Y} = 1 | Y = 1, S = s_0) = P(\\hat{Y} = 1 | Y = 1, S = s_1)$$\u003c/p\u003e\n\u003cp\u003ewhere $S$ represents a sensitive attribute (e.g., gender or race).\u003c/p\u003e\n\u003cp\u003eThe fairness penalty can be incorporated during model training by adding it to the loss function, as shown in this training function. A penalty coefficient can be specified to control the impact of the fairness term. Setting the coefficient to 0 results in no penalty being applied to the loss:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003etrain_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptimizer\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecriterion\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efairness_weight\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etrain\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003etotal_loss\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eX_batch\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ey_batch\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esensitive_batch\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eoptimizer\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ezero_grad\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003elogits\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX_batch\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecriterion\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elogits\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ey_batch\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# Fairness regularization: penalize different confidence (softmax probabilities)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efairness_weight\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003eprobs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efunctional\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esoftmax\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elogits\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Compute average probability for each sensitive group\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003egroup0_mask\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esensitive_batch\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003egroup1_mask\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esensitive_batch\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003egroup0_mask\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esum\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e \u003cspan style=\"color:#111\"\u003egroup1_mask\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esum\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eavg_prob0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eprobs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003egroup0_mask\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emean\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eavg_prob1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eprobs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003egroup1_mask\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emean\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#75715e\"\u003e# L2 difference between average prediction distributions\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003efairness_penalty\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enorm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eavg_prob0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eavg_prob1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ep\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efairness_weight\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efairness_penalty\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebackward\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eoptimizer\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003estep\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003etotal_loss\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eitem\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etotal_loss\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elen\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhile these interventions improve fairness, they also alter the confidence distribution of model predictions—\u003cstrong\u003ea fact that attackers can exploit\u003c/strong\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"3-membership-inference-attacks-the-silent-thief-of-privacy\"\u003e3. Membership Inference Attacks: The Silent Thief of Privacy\u003c/h2\u003e\n\u003cp\u003e🔐 A Parallel with Cryptography\n\u003cstrong\u003eMembership inference attacks (MIAs)\u003c/strong\u003e are to privacy what brute-force attacks are to passwords. Instead of guessing a password, they test thousands of combinations to see which one is good.\u003c/p\u003e\n\u003cp\u003eMIAs work the same way: they analyze a model’s outputs to determine if a given data point was part of its training set.\u003c/p\u003e\n\u003cp\u003eA traditional MIA exploits \u003cstrong\u003econfidence scores\u003c/strong\u003e—the probabilities that a model assigns to different predictions. The intuition is simple: models tend to be more confident on data they have seen during training. Given a target model $T$ and a queried sample $x$, an attacker computes:\u003c/p\u003e\n\u003cp\u003e$$M(x) = 1 \\text{ if } A(T(x)) \u0026gt; \\tau$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$A(T(x))$ is a decision function (often a threshold on the confidence score).\u003c/li\u003e\n\u003cli\u003e$\\tau$ is a predefined threshold.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"why-traditional-mias-fail-on-fair-models\"\u003eWhy Traditional MIAs Fail on Fair Models\u003c/h3\u003e\n\u003cp\u003eFairness interventions introduce \u003cstrong\u003emore uncertainty\u003c/strong\u003e into the model’s predictions. This causes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLower confidence scores overall\u003c/strong\u003e, making it harder for attackers to distinguish between training and non-training samples.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMore uniform confidence distributions\u003c/strong\u003e, which means attackers lose their key signal.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus, fairness-enhanced models resist traditional MIAs. But this protection is not foolproof—\u003cstrong\u003ea new, more dangerous attack lurks in the shadows\u003c/strong\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"4-the-birth-of-a-new-threat-fairness-discrepancy-membership-inference-attacks-fd-mia\"\u003e4. The Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA)\u003c/h2\u003e\n\u003cp\u003eIf traditional MIAs are blunt weapons, \u003cstrong\u003eFD-MIA is a scalpel.\u003c/strong\u003e It exploits the discrepancies between a biased model and a fairness-enhanced one.\u003c/p\u003e\n\u003ch3 id=\"how-does-fd-mia-work\"\u003eHow does FD-MIA work?\u003c/h3\u003e\n\u003cp\u003eFairness interventions shift model predictions differently for training and non-training data. This creates a \u003cstrong\u003egap\u003c/strong\u003e between how biased and fair models behave for the same inputs. An attacker, armed with knowledge of both models, can exploit this difference to infer membership with high accuracy.\u003c/p\u003e\n\u003cp\u003eMathematically, FD-MIA extends membership prediction by comparing prediction shifts between biased and fair models:\u003c/p\u003e\n\u003cp\u003e$$M(x) = 1 \\text{ if } |T_{\\text{bias}}(x) - T_{\\text{fair}}(x)| \u0026gt; \\tau$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$T_{\\text{bias}}(x)$ and $T_{\\text{fair}}(x)$ are the predictions from the biased and fair models, respectively.\u003c/li\u003e\n\u003cli\u003e$\\tau$ is a threshold chosen by the attacker.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n  \u003cimg src=\"/images/image_fairness_privacy/figure3.png\" alt=\"FD-MIA: Prediction Difference Distribution\" style=\"width:80%; display:block; margin:auto;\"\u003e\n    \u003cp style=\"font-style: italic; font-size: 14px;\"\u003eFigure 1: FD-MIA exploits the predictions from both models to achieve efficient attacks. From original paper.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eThe key insight is that \u003cstrong\u003efairness interventions cause systematic shifts\u003c/strong\u003e in model confidence, creating a measurable pattern that attackers can exploit.\u003c/p\u003e\n\u003cp\u003eHere is an example implementation of the FD-MIA attack using a function that compares the prediction difference between two models against a user-defined threshold:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003efd_mia_attack\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esample\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    Given a sample, compute the absolute difference between the\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    biased and fair model\u0026#39;s softmax outputs for the positive class.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    If the difference exceeds the threshold, predict membership.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eeval\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eeval\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003ewith\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eno_grad\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003elogits_b\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esample\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eunsqueeze\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003elogits_f\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esample\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eunsqueeze\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eprob_b\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efunctional\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esoftmax\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elogits_b\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eprob_f\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efunctional\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esoftmax\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elogits_f\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003ediff\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eabs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eprob_b\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eprob_f\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# In practice, the threshold can be tuned via shadow models or validation\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ediff\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ediff\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eitem\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003chr\u003e\n\u003ch2 id=\"5-reproducible-code-experiments-illustrating-fd-mia\"\u003e5. Reproducible Code Experiments: Illustrating FD-MIA\u003c/h2\u003e\n\u003cp\u003eIn this section, we reproduce the FD-MIA attack described in the original paper:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData Generation and Splitting\u003c/strong\u003e\u003cbr\u003e\nThe process starts with generating a synthetic dataset where a binary sensitive attribute influences the feature distribution. The target labels are computed using a logistic function, and the dataset is then split into training (member) and testing (non-member) sets to simulate membership inference scenarios.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eClassifier Architectures\u003c/strong\u003e\u003cbr\u003e\nTwo identical neural network architectures are defined:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003ebiased baseline model\u003c/strong\u003e trained without fairness constraints.\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003efairness-enhanced model\u003c/strong\u003e, which incorporates a fairness penalty to balance prediction distributions across sensitive groups.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTraining with Fairness Regularization\u003c/strong\u003e\u003cbr\u003e\nThe function \u003ccode\u003etrain_model\u003c/code\u003e allows the inclusion of a fairness penalty during training. For the fair model, this penalty—weighted by \u003ccode\u003efairness_weight\u003c/code\u003e—is added to the standard cross-entropy loss to encourage prediction consistency across sensitive groups.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFD-MIA Attack Implementation\u003c/strong\u003e\u003cbr\u003e\nThe attack, implemented in \u003ccode\u003efd_mia_attack\u003c/code\u003e, exploits the absolute difference in predicted probabilities (for the positive class) between the biased and fair models. If this difference exceeds a given threshold, the sample is inferred as a training member. This approach leverages the core principle of FD-MIA: fairness interventions create prediction discrepancies that can be used for membership inference.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation and Visualization\u003c/strong\u003e\u003cbr\u003e\nThe attack is evaluated by comparing prediction differences between member and non-member data. We visualize the prediction difference distributions to highlight how fairness-driven adjustments can unintentionally expose membership information.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.nn\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.optim\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptim\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.utils.data\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eDataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003erandom_split\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enumpy\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eplt\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esklearn.metrics\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroc_auc_score\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Set random seeds for reproducibility\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emanual_seed\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003erandom\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eseed\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 1. Create a synthetic dataset with a sensitive attribute\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eSyntheticFairDataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eDataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e__init__\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003en_samples\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1000\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# Features: two-dimensional points drawn from different distributions\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003en_samples\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003en_samples\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ey\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esensitive\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# sensitive attribute: 0 or 1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ei\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003erange\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003en_samples\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Randomly assign a sensitive group (imbalance can be introduced here)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003es\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003erandom\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003echoice\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e],\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ep\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.7\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Generate features from group-dependent distributions\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003es\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ex\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003erandom\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enormal\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eloc\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003escale\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esize\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#00a8c8\"\u003eelse\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ex\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003erandom\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enormal\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eloc\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003escale\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esize\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Label is determined by a linear rule (with some noise)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003ey_prob\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eexp\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ex\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ex\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003ey_label\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003erandom\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebinomial\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ey_prob\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ex\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ey\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ey_label\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esensitive\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003es\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edtype\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efloat32\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ey\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ey\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edtype\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elong\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esensitive\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esensitive\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edtype\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elong\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e__len__\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003en_samples\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e__getitem__\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eidx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eidx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e],\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ey\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eidx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e],\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esensitive\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eidx\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eSyntheticFairDataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003en_samples\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2000\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Split into training (for model training) and attack evaluation (simulate member vs. non-member)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etrain_size\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elen\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etest_size\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elen\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etrain_size\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etrain_dataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etest_dataset\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003erandom_split\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etrain_size\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etest_size\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etrain_loader\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etrain_dataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e64\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eshuffle\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etest_loader\u003c/span\u003e  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etest_dataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e64\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eshuffle\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eFalse\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 2. Define the classifier architectures\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eSimpleClassifier\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e__init__\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einput_dim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ehidden_dim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoutput_dim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003esuper\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eSimpleClassifier\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e__init__\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efc1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eLinear\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003einput_dim\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ehidden_dim\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003erelu\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eReLU\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efc2\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eLinear\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ehidden_dim\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoutput_dim\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eforward\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ex\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eout\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003erelu\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efc1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ex\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efc2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eout\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# raw logits\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Baseline (biased) model\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eSimpleClassifier\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Fairness-enhanced model: we add a fairness penalty to the loss (for demonstration)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eSimpleClassifier\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 3. Train both models\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecriterion\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Train biased model without fairness penalty\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eoptimizer_biased\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eAdam\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eparameters\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elr\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.01\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eepoch\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003erange\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etrain_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etrain_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptimizer_biased\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecriterion\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efairness_weight\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Uncomment to print loss: print(f\u0026#34;Biased Model Epoch {epoch+1}: Loss {loss:.4f}\u0026#34;)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Train fair model with a fairness penalty (fairness_weight \u0026gt; 0)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eoptimizer_fair\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eAdam\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eparameters\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elr\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.01\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003efairness_weight\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# adjust to control trade-off\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eepoch\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003erange\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etrain_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etrain_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptimizer_fair\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecriterion\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efairness_weight\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efairness_weight\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Uncomment to print loss: print(f\u0026#34;Fair Model Epoch {epoch+1}: Loss {loss:.4f}\u0026#34;)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 4. Membership Inference Attack using FD-MIA\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Evaluate the attack on both training (members) and test (non-members) samples\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eevaluate_attack\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eattack_labels\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eattack_scores\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Assuming samples in train_dataset are members and test_dataset non-members\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esample\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e_\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e_\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eshuffle\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eFalse\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003epred\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ediff\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efd_mia_attack\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esample\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e],\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eattack_labels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epred\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eattack_scores\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ediff\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003earray\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eattack_labels\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enp\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003earray\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eattack_scores\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# For demonstration, we use the entire training set as member data and test set as non-member data.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003emember_labels\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emember_diffs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eevaluate_attack\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etrain_dataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003enonmember_labels\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enonmember_diffs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eevaluate_attack\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etest_dataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebiased_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003efair_model\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ethreshold\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003chr\u003e\n\u003ch3 id=\"6-experimental-findings-how-fairness-opens-the-door-to-attackers\"\u003e\u003cstrong\u003e6. Experimental Findings: How Fairness Opens the Door to Attackers\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe study conducted extensive experiments across \u003cstrong\u003esix datasets, three attack methods, and five fairness approaches\u003c/strong\u003e, testing over \u003cstrong\u003e160 models\u003c/strong\u003e.\nTo do this, they performed a comprehensive set of experiments involving:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMultiple datasets with different sensitive attributes (like gender or race),\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBiased vs. fair model variants,\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThree types of MIAs (including their novel FD-MIA),\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMultiple fairness interventions\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDataset\u003c/th\u003e\n          \u003cth\u003eNumber of Fairness Settings\u003c/th\u003e\n          \u003cth\u003eTarget Tasks\u003c/th\u003e\n          \u003cth\u003eFD-MIA Effectiveness\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCelebA\u003c/td\u003e\n          \u003ctd\u003e3 settings (smile, hair, makeup)\u003c/td\u003e\n          \u003ctd\u003e3 binary targets × 2 sensitive attributes\u003c/td\u003e\n          \u003ctd\u003eFD-MIA consistently outperformed other methods.\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eUTKFace\u003c/td\u003e\n          \u003ctd\u003e2 settings (race prediction, gender prediction)\u003c/td\u003e\n          \u003ctd\u003eRace or gender, sensitive to the other\u003c/td\u003e\n          \u003ctd\u003eFD-MIA revealed privacy leaks even with balanced groups.\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFairFace\u003c/td\u003e\n          \u003ctd\u003e2 settings (race prediction, gender prediction)\u003c/td\u003e\n          \u003ctd\u003eSame as UTKFace\u003c/td\u003e\n          \u003ctd\u003eMost vulnerable dataset—biggest fairness shift = biggest privacy leak.\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe results were shocking:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFair models were significantly harder to attack using traditional MIAs.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFD-MIA, however, dramatically increased attack success rates—fairness actually made models more vulnerable!\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe greater the fairness intervention, the wider the discrepancy between biased and fair models, making FD-MIA even more effective.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOur synthetic experiment further supports these findings. The histogram below illustrates the absolute difference between the predictions of the biased and fair models for both \u003cem\u003emembers\u003c/em\u003e (training data) and \u003cem\u003enon-members\u003c/em\u003e (test data).\u003c/p\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cimg src=\"/images/image_fairness_privacy/figure1.png\" alt=\"FD-MIA: Prediction Difference Distribution\"\u003e\n    \u003cp style=\"font-style: italic; font-size: 14px;\"\u003eFigure 2: FD-MIA: Prediction Difference Distribution.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eAs expected, members exhibit a significantly higher prediction discrepancy compared to non-members. This clear separation highlights how fairness constraints alter model confidence differently for training and test samples—providing an exploitable signal for membership inference. \u003cstrong\u003eIn simple terms: making a model fairer may paradoxically make it leak more private information.\u003c/strong\u003e A cruel irony for those trying to do the right thing.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"7-the-future-of-fairness-and-privacy-can-we-have-both\"\u003e7. The Future of Fairness and Privacy: Can We Have Both?\u003c/h2\u003e\n\u003cp\u003eThe million-dollar question: \u003cstrong\u003eCan we balance fairness and privacy without sacrificing one for the other?\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eIf you know the enemy and know yourself, you need not fear the result of a hundred battles.\u003c/em\u003e — Sun Tzu.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eA better understanding of the link between fairness and privacy, as well as the potential and new attacks introduced by unbiased models, is already a solid step toward defending against threats. By understanding the underlying mechanisms, it becomes possible to counteract them.\u003c/p\u003e\n\u003cp\u003eThe researchers propose two key defenses:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRestricting Information Access\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLimiting confidence score outputs reduces the information available to attackers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDifferential Privacy (DP)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBy injecting noise into model training, DP-SGD (Differentially Private Stochastic Gradient Descent) helps obscure membership information:\u003c/p\u003e\n\u003cp\u003e$$\\tilde{g}_t = g_t + \\mathcal{N}(0, \\sigma^2 I)$$\u003c/p\u003e\n\u003cp\u003ewhere $g_t$ is the original gradient and $\\mathcal{N}(0, \\sigma^2 I)$ is Gaussian noise added to prevent membership inference.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWhile these methods help, they come with trade-offs: \u003cstrong\u003etoo much privacy protection can lower fairness and accuracy, while too little leaves models vulnerable.\u003c/strong\u003e The challenge ahead is designing AI systems that can balance both.\u003c/p\u003e\n\u003cp\u003e💡 Alternative approach: Fairness-Aware Differential Privacy (FADP) adapts noise levels based on protected groups, balancing privacy and fairness.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eH. Tian, G. Zhang, B. Liu, T. Zhu, M. Ding, and W. Zhou, \u0026ldquo;When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks\u0026rdquo;, \u003cem\u003earXiv e-prints\u003c/em\u003e, Art. no. \u003ca href=\"https://arxiv.org/pdf/2311.03865\"\u003earXiv.2311.03865\u003c/a\u003e.\u0026#160;\u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/when_fairness_meets_privacy/","date_published":"10036-10-09T322:1010:00+01:00","date_modified":"10036-10-09T322:1010:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"10cefc68955c83516a168e5a0e37efd85920115d","title":"Get a calibrated and efficient model with tailored data augmentation.","summary":"","content_text":" Authors : Tristan Waddington, Fabien Lagnieu \u0026amp; Dimitri Henrard-Iratchet Comment on the research paper: Tailoring Mixup to Data for Calibration, written by Quentin Bouniot, Pavlo Mozharovskyi \u0026amp; Florence d’Alché-Buc, from LTCI, Télécom Paris, Institut Polytechnique de Paris, France Table of contents Existing Data Augmentation Methods Understanding Calibration Best of both worlds: Tailoring Mixup to Data for Calibration Introduction \u0026ldquo;But it works well on the training set!\u0026rdquo; is the machine learning equivalent to the classic \u0026ldquo;But it works on my computer!\u0026rdquo;\nThe basic workflow of machine learning has two steps:\nFirst, train your model to perform a task from an available dataset. Second, generalize and predict the results from unseen data. How can data scientists be sure and confident that their model will infer a correct result on this new data?\nWe know that deep learning models need vast amounts of data to be efficient. So, when there is not enough, researchers simply… create more data: this is the concept of data augmentation.\nHowever, this technique tends to exarcerbate the models\u0026rsquo; overconfidence in their predictions. Discrepancies between confidence and prediction accuracy are acceptable in domains such as e-commerce recommendations, but high stake applications such as medical diagnosis or nuclear safety require an accurate confidence score, where robustness and reliability are critical.\nThis is the idea behind calibration: the model\u0026rsquo;s confidence in its prediction must truly reflect its own prediction accuracy. Well-calibrated models do not just inspire trust; they also contribute to fairer decision-making, by ensuring that predictions are accompanied by reliable confidence estimates, which can help reduce biased or unjust outcomes.\nFigure: Illustration of the confidence of a calibrated classifier (right) with accurate prediction probabilities and a more brutal and under-confident one (left).\nMerging data augmentation and calibration is challenging. The first is prone to create manifold intrusion, where synthetic data with a given label conflicts with original data of another class. The rise of the size of the dataset also increases the computational cost of the training. This contradicts the potential objective of frugality. The second is known to constrain the accuracy.\nTo handle these challenges, Quentin Bouniot and Pavlo Mozharovskyi have conducted under the direction of Florence d\u0026rsquo;Alché-Buc an extensive study on one of the technique of data augmentation, the linear interpolation of training samples, also called Mixup. They have found an efficient way to tune this process to both improve the performance and the calibration of models, while being much more efficient than previous methods.\nLet\u0026rsquo;s dig step by step into it.\n1. Existing Data Augmentation Methods Deep learning methods rely on vast amounts of data, so if you do not have enough, make it yourself. This is the first conclusion of the study of a Microsoft Research team, lead by Patrice Simard in 2003 aimed to list the current best practices of neural networks training:\n\u0026ldquo;The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data.\u0026rdquo; [Simard et al. 2003] 1\nThe good results of subsequent models have proved them right. And numerous techniques have been developed since. Let\u0026rsquo;s review some of them.\n1.1. Create new images for classification The most visual example of data augmentation is the way image classifiers are trained. To make them more robust and efficient, scientists have transformed the input images to drastically increase the size of the training set (by up to 2048 times).\nThe most commonly used transformations are random cropping and resizing, flipping, and color distortion. This is now so common that it can be done in a few lines in pytorch (see next code snippet), and automatic recipes such as AutoAugment2 are readily available to augment common datasets.\nfrom torchvision import transforms # Definition of transformations for an image dataset transformTrain = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ColorJitter(brightness=.5, hue=.3) ]) Additional transformations for images are illustrated on the next figure. We expect the neural networks to \u0026ldquo;see\u0026rdquo; these 10 new images as being close in their latent space. With this example, one original labeled image is processed 10 times in different versions during the training of the model.\nFigure: Illustration of different data augmentation operators, taken from the paper of Chen 2020 3\n1.2. Linear interpolation or Mixup Another idea is to create a virtual sample from a vicinity around the true training data—like we did in high school when we added epsilon to a number to see in witch direction the function is moving. This principle has been demonstrated to help models generalize. However, the method of creation is often hand-crafted and only mimic natural perturbations.\nTo scale up this process, [Zhang et al., 2018]4 imagined the Mixup process, which is a linear interpolation, or mixing, of two or more training datapoints.\nFigure: Illustration of a linear interpolation of Mixup. New points $\\tilde{\\mathrm{x}}_1$ and $\\tilde{\\mathrm{x}}_2$ are respectively drawn from the segment $[\\mathrm{x}_1, \\mathrm{x}_2]$ and $[\\mathrm{x}_1, \\mathrm{x}_3]$\nThe process of data augmentation during training with Mixup consists of three phases:\nselecting tuples (most often pairs) of points to mix together, sampling coefficients that will govern the interpolation to generate synthetic points, applying a specific interpolation procedure between the points weighted by the coefficients sampled. However, the literature explores the drawbacks of this process:\nMixing carelessly different points can result in incorrect labels and hurt generalization [Guo et al., 2019]5, while mixing similar points helps in diversity [Dablain et al., 2022]. Furthermore, several previous work have highlighted a trade-off between performance and calibration in Mixup [Wang et al., 2023]. Before digging further into the Mixup process, it is time to understand what exactly is the calibration of a model and why it can be worth of a trade-off with performance.\n2. Understanding Calibration Modern (post 2016) neural networks have a high accuracy but are overconfident in their predictions, outputting softmax scores of above 99.9% for the dominant class, hence misleading the user into a false sense of confidence. This is why we need calibration.\nFigure: Meme about the overconfidence of an AI agent (obviously uncalibrated) over a failed prediction.\nCalibration is a metric to quantify uncertainty, measuring the difference between a model’s confidence in its predictions and the actual probability of those predictions being correct.6\nIn other words, if a calibrated model predicts the image as a cat with a confidence of 0.3, this prediction has a 30% chance of being correct. The actual aim is not exactly to explain the results, but confidence calibration prevents some mistakes by associating a prediction with its confidence score.\nLet\u0026rsquo;s explore further the motivations of calibration and the way to measure it and the potential draw backs.\n2.1. Importance of calibration The gap in confidence calibration has been spotted by [Guo et al. (2017)]5, and is linked to the actual use cases of neural networks, where the calibration is not crucial. In LLMs or online recommender systems, a 90% quality of predictions is enough and occasional mistakes are acceptable. For further use however, like in medical diagnosis prediction or in defense systems, an overconfident model can lead to tragic consequences.\nWhat would be the benefits of a well calibrated model ?\nIt can filter out the poor predictions, and not provide a wrong prediction to the user. It can reinforce the continuous training, by asking for the actual label of the low confidence prediction. It can detect outliers and warn the user that something strange is happening. It can improve the robustness of the model by ensuring that prediction confidence accurately reflects the underlying uncertainty, leading to more reliable decisions in critical situations. To sum it up, a well calibrated model is a reliable coworker aware of its own capacities.\n2.2. Calibration Metrics To compare the calibration of models, specific metrics are required. Unlike simple accuracy on a dataset, various metrics have been proposed in the literature, each focusing on different characteristics.\n2.2.1. The Brier Score The Brier score [Brier, 1050] is the mean square error between predicted confidence and target. Here the target has the form of a one-hot encoded vector.\nFigure: Computing the Brier Score on classification task (image by Wolfe)7.\nIntuitively, the Brier Score measures the accuracy of predicted probabilities. It can be decomposed into three components — uncertainty (marginal uncertainty over labels), resolution (deviations of individual predictions against the marginal), and reliability (average violation of true label frequencies)\nBrier Score = uncertainty - resolution + reliability\nThe Brier Score is insensitive to the low frequencies events, hence it can be used in combination with one of the other following metrics to provide useful insights. Basically, the score is low when the predictions reflect the confidence, i.e. when the model is calibrated.\nThe following code is a dummy example of Brier score computation of a single classification probabilities over 3 classes. The same probabilities will be used on different metrics.\nimport torch from sklearn.metrics import brier_score_loss # Example of prediction outputs prob_u = torch.Tensor([0.34, 0.33, 0.33]) prob_l = torch.Tensor([0.5, 0.25, 0.25]) prob_h = torch.Tensor([0.9, 0.07, 0.03]) target = torch.Tensor([1, 0, 0]) # Compute brier score print(f\u0026#34;Brier score for uniform:\\t {brier_score_loss(target, prob_u):.4f}\u0026#34;) print(f\u0026#34;Brier score for low confidence:\\t {brier_score(target, prob_l):.4f}\u0026#34;) print(f\u0026#34;Brier score for high confidence: {brier_score(target, prob_h):.4f}\u0026#34;) Output\nBrier score for uniform:\t0.2178 Brier score for low confidence:\t0.1250 Brier score for high confidence:\t0.0053 2.2.2. The Expected Calibration Error (ECE) The Expected Calibration Error [Guo et al, 2017] approximates the difference between accuracy and confidence by grouping samples into equally spaced bins with respect to their confidence scores. Because it is both simple and interpretable, ECE is a popular metric to evaluate calibration on classification tasks in practice. ECE computes the difference between average confidence and accuracy within each bin, then takes a weighted average of these values based upon the relative size of each bin.\nFigure: Computing ECE over a group of prediction, (image by Wolfe)7.\nECE measures how well a model’s estimated \u0026ldquo;probabilities\u0026rdquo; match the observed probabilities by taking a weighted average over the absolute difference between accuracy and estimated probabilities (confidence). This measure involves splitting the predictions into $M$ equally spaced bins.\n$$ECE = \\sum_{bins}^M \\frac{\\text{bin size}}{\\text{nb samples}} | \\text{accuracy per bin} - \\text{average bin probability}| $$\nA very good example on how to compute ECE by hand can be found in the article of Maja Pavlovic on the blog TowardsDataScience8.\nVariants: Adaptative ECE (AECE) is simmilar to ECE, but with each bin having the same number of samples. Other extensions of ECE can be used to estimate the variance over the bins, the Uncertainty Calibration Error (UCE) or the Expected Normalize Calibration Error (ENCE). They will not be detailed further here.\n2.2.3. The Negative Log Likelihood (NLL) The Negative Log Likelihood (NLL) is the typical objective function for training neural networks in multi-class classification. It characterizes the disparity between the predicted and the actual confidence for the true label. It reaches a perfect score of $0$ when all data is correctly predicted with 100% confidence, and rises as soon as some are misclassified. Hence lower scores correspond to better calibration.\nDummy example of NLL computation of a single prediction over 3 classes\nimport torch import torch.nn as nn log_softmax = nn.LogSoftmax(dim=1) loss_fn = nn.NLLLoss() # input to NLLLoss is of size (batch_size x nb_classes) = 1 x 3 target = torch.Tensor([0]).long() # correct class is at index O # different examples of logits from a classifier logits_u = torch.Tensor([[1, 1, 1]]) # uniform prediction logits_l = torch.Tensor([[1, 0.2, 0.1]]) # low confidence prediction logits_h = torch.Tensor([[10, 0.1, 0.1]]) # high confidence prediction print(f\u0026#34;nll uniform: \\t\\t{loss_fn(log_softmax(logits_u), target):.4f}\u0026#34;) print(f\u0026#34;nll low confidence: \\t{loss_fn(log_softmax(logits_l), target):.4f}\u0026#34;) print(f\u0026#34;nll high confidence: \\t{loss_fn(log_softmax(logits_h), target):.4f}\u0026#34;) Output\nnll uniform: 1.0986 nll low confidence: 0.6184 nll high confidence: 0.0001 However, NNL also causes overconfidence in modern neural networks. They are purposely trained to minimize it by making high confidence predictions, which actually lowers the exponential sum of the soft max, as in our high_confidence example above.\nThis kind of behavior can be exhibited by drawing the calibration curve of the predictor.\n2.3. Calibration Curves - Reliability diagrams The calibration curves [Wilks, 1995] compare how well the probabilistic predictions of a binary classifier are calibrated. It shows the frequency of the predicted label against the predicted probability. It is easily drawn with the method model.predict_proba() of scikit-learn.\nScikit-Learn\u0026rsquo;s documentation provides a very insightful illustration to better understand these curves. They have fitted 4 different classifiers on a very small training set and plot the calibration curve along with the histogram showing the related distribution of the predicted probabilities on each of the 10 bins. On this specific example, we can observe the following behaviors:\nLogistic Regression: not perfect, but well calibrated because the optimized log loss is also the scoring rule (as seen in previous section). Gaussian Naive Bayes: its tendency to push probabilities to 0 or 1 is well shown on histogram orange. This means an overconfident model. Support vector Classifier displays a typical sigmoid calibration curve. This under-confident result is typical of maximum-margin methods. Random Forest averages the predictions over a set of models, meaning exact predictions of 0 or 1 are rare, hence the shift towards 0.2 and 0.9. The whole model seems under-confident, but since each tree is fitted in minimizing a scoring rule (Brier score of log-loss) the pink calibration curve is pretty close to the dot line. Figure: Behavior of regular classifier on a standard task. Upper: Calibration curves with sklearn.calibration.CalibrationDisplay.from_estimator. Bottom: Histograms of the number of samples per bins of predict_proba values.\nRemember: a perfectly calibrated estimator will get the doted diagonal line and its histogram will be flat.\n2.4 Drawbacks To provide an insight of the side effects of the calibration, we will study the impact of the provided method CalibratedClassifierCV in Scikit-Learn. It uses cross-validation to obtain unbiased predictions, which are then used for calibration. The sigmoid method here is a simple logistic regression model. We experiment the effect of the calibration on the accuracy and the Brier Score of 4 classifiers fitted on the titanic dataset. We will only display the code to instantiate and calibrate the models.\nfrom sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.calibration import CalibratedClassifierCV # [...] load titanic dataset, split data, Skub automatically the data X_train, X_test, y_train, y_test = train_test_split(...) # Initialize classifiers classifiers = { \u0026#34;Logistic Regression\u0026#34;: LogisticRegression(), \u0026#34;Naive Bayes\u0026#34;: GaussianNB(), \u0026#34;Support Vector Classifier\u0026#34;: SVC(probability=True), \u0026#34;Random Forest\u0026#34;: RandomForestClassifier(), } # Calibrate with the simgmoid regression calibrated_classifiers = {} calibrated_proba = {} for name, classifier in classifiers.items(): calibrated_classifiers[name] = CalibratedClassifierCV( classifier, cv=3, method=\u0026#34;sigmoid\u0026#34; ).fit(X_train, y_train) calibrated_proba[name] = classifier.predict_proba(X_test)[:, 1] # [...] Compute metrics and display as bar plots Figure: effect of calibration on classifiers\u0026rsquo; metrics. We expect a small drop in accuracy and a reduction (improvement) of the Brier Score. But our method is not efficient here.\nWe can see that this method has moderate and sometimes counterintuitive effects. This suggests that the training set is not sufficient to fit a calibrated estimator.\n3. Best of Both Worlds: Tailoring Mixup to Data for Calibration We have reached the core of the paper of Bouniot et al. They show that, by taking the distance of points into account when sampling the coefficients in the second phase of Mixup, we can (i) avoid a loss in diversity, and (ii) reduce manifold intrusion and label noise.\nFigure: Illustration of a Blouniot et al.\u0026rsquo;s process of similarity in interpolation. New point $\\tilde{\\mathrm{x}}_2$ have to be closer to $\\mathrm{x}_1$ because $\\mathrm{x}_3$ has a different label but still preserve diversity.\nIf we had only used a selection of samples with similar labels, we would have lost the possible exploration of new directions of the latent space. With this similarity process, at the end of the day, we have avoided restricting possible direction of mixing while staying in the vicinity of original points, hence preventing manifold intrusion.\n3.1 Linear interpolation of training samples: Mixup Mixing samples through linear interpolation is the easiest and most efficient way to create new data from a computational point of view. Combining data from the same batch also avoids additional sampling during training.\nSpecific techniques have been proposed since 2018 to compute linear interpolation but often at the cost of more complex training or loss of diversity. The selection process of samples to interpolate from may be computationally expensive. Furthermore such studies have been conducted with the aim of improving models\u0026rsquo; generalization, not their calibration, and will not solve our issue.\nIn the original mixup method of [Zhang et al. 2018]4, at each training iteration of the model, each input is mixed with another input randomly selected from the same batch, with a random strength drawn form a Beta law.\nBut how can we be sure of the label of these new datapoints?\n3.2 Weighting to prevent manifold intrusion The real danger of mixup is manifold intrusion, where the interpolated sample between two identical label points falls into an other class.\nThe likelihood of conflict in the synthetic label increases with the distance between the two points. As data live in manifolds of the representation space, the linear combination of two points far from each other can lie in a different manifold than the linear combination of the labels. The further away the points are, the more manifolds can exist in between.\nFigure: Illustration from [Baena, 2022]9 of a manifold intrusion (right) when drawing samples as mixup existing points. The linear interpolation (red line) crosses the blue classe leading to conflict.\nBouniot et al. have conduct extensive experiments to show that there is a trade-off between adding diversity by increasing the proportion of elements to mix, and uncertainty by mixing elements far from each other. Furthermore, it shows that we cannot restrict pairs to mix by selecting data solely based on distance, as it can degrade performance by reducing diversity of synthetic samples.\nTo better control this trade-off with Mixup, they suggest to tailor interpolation coefficients based on the distance of training data. The final part will detail this process.\n3.3 The power of the similarity kernel Bouniot et al. used a similarity kernel to mix more strongly similar data and avoid mixing less similar ones, to preserve label quality and confidence of the network.\nTo do so, they needed to change the interpolation coefficient depending on the similarity between the points. They have found a way to preserve the type of distribution of samples by warping these coefficients at every iteration to govern the strength and direction of the mixup. Curious readers can refer to section 3.2 of 6 for technical details. In summary, they only need the parameter $\\tau$ of a Beta distribution $B(\\tau, \\tau)$ that behaves logarithmically with this parameter. Hence, $\\tau$ should be exponentially correlated with the distance of points to finally obtain a linear interpolation.\nTo this end, they define a class of similarity kernels, based on a normalized and centered Gaussian kernel, that outputs the correct warping parameter $\\tau$ for the given pair of points. This similarity kernel is defined by the amplitude and the standard derivation of the Gaussian, two additional parameters to tune separately. The computation of $\\tau$ also depends on the average distance of samples in the same batch. More specifically, for classification tasks, they use the $L_2$ distance between embeddings, while for regression tasks, they use the distance between labels.\nThe algorithm to compute this parameter is described bellow in pseudo-code:\nInput: (Batch (x_i, y_i) of size n, kernel similarity parameters, current model parameters) Sample random permutation sigma For i in [1, n], do # Compute the interpolated points from x_i and x_sigma(i): Compute warping parameter tau using a Beta coefficient and the similarity kernel Generate new point x_tilde as a linear interpolation of x_i and x_sigma(i), weighted by tau Generate new label y_tilde as a linear interpolation of y_i and y_sigma(i), weighted by tau Aggregate new data to batch Optimize loss over this augmented batch Output: the updated model parameters Figure: Illustration of the effect of the similarity kernel on two points $x_i$ and $x_{\\sigma{(i)}}$, additional description bellow. Figure from [Bouniot, 2024]6.\nThe motivation behind this kernel is to have $\\tau \u0026gt;1$ when the two points to mix are similar, i.e., the distance is lower than average, to increase the mixing effect, and $\\tau \u0026lt; 1$ otherwise, to reduce the mixing. Above Figure illustrates the evolution of the density of warped interpolation coefficients $ω_τ(λ)$, depending on the distance between the points to mix. Close distances (left part of the heatmap) induce strong interpolations, while far distances (right part of the heatmap) reduce interpolation. Using this similarity kernel to find the correct τ to parameterize the Beta distribution defines our full Similarity Kernel Mixup framework.\n3.4 Going further Extensive experiments have been conducted by the authors on image classification and regression tasks. They have reproduced the protocol of the literature and their framework displays an improvement in both accuracy and calibration across the 3 metrics described above (ECE, Brier and NLL).\nIt is important to note, however, that the hyper-parameters have been tuned, and the best results across different metrics do not share the same values for the kernel standard deviation.\nDuring the experiment process, the authors have compared the final results after temperature scaling, following [Guo, 2017]5 process. This temperature is also a learnable parameter that have been optimized during the training of the models.\nIn addition to improving calibration and performance, this approach demonstrates greater frugality by reducing computational overhead compared to other calibration-driven data augmentation methods. By efficiently tailoring the interpolation process, it lowers the number of unnecessary computations and memory usage, contributing to more sustainable and energy-efficient machine learning practices.\nConclusion With similarity kernel, we get a more accurate and better calibrated model because the coefficients governing the interpolation are warped to change their underlying distribution depending on the similarity between the points to mix, so that similar datapoints are mixed more strongly than less similar ones, preserving calibration by avoiding manifold intrusion and label noise.\nAs seen in the pseudo-code, this provides a more efficient data augmentation approach than Calibration-driven Mixup methods, both in terms of time and memory, offering a better trade-off between performance and calibration improvement, while promoting frugality by reducing unnecessary computational resources.\nConcurrently, [Verma et al. 2018]10 have proposed the Manifold Mixup framework that encourages neural networks to predict less confidently on interpolations of hidden representation via a simple regularizer. This training method leads to class-representations with fewer directions of variance. But even with the actual purpose the reduce the over confidence, the word \u0026ldquo;Calibration\u0026rdquo; never occurs in the paper\u0026hellip;\nThis highlights the necessity of raising awareness about calibration and establishing a standard process for evaluating models.\nP.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEkin D. Cubuk, Barret Zoph , Dandelion Mané, Vijay Vasudevan, Quoc V. Le, Google Brain (2018). AutoAugment: Learning Augmentation Strategies from Data arXiv\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTing Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, (2020). A Simple Framework for Contrastive Learning of Visual Representations arXiv\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2018). mixup: Beyond empirical risk minimization. In International Conference on Learning Representations.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGuo, Chuan, et al. “On calibration of modern neural networks.” International Conference on Machine Learning. PMLR, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBouniot, Q., Mozharovskyi P., d\u0026rsquo;Alché-Buc, F. (2023). Tailoring Mixup to Data for Calibration arXiv\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCameron R. Wolfe, Ph.D. Confidence Calibration for Deep Networks: Why and How? medium/TowardsDataScience blogpost\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMaja Pavlovicic, Expected Calibration Error (ECE): A Step-by-Step Visual Explanation link\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRaphael Baena, Lucas Drumetz, Vincent Gripon (2022) Preventing Manifold Intrusion with Locality: Local Mixup arXiv\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVerma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D., and Bengio, Y. (2019). Manifold mixup: Better representations by interpolating hidden states. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6438–6447. PMLR.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","content_html":"\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch2 id=\"authors--tristan-waddington-fabien-lagnieu--dimitri-henrard-iratchet\"\u003eAuthors : \u003cem\u003eTristan Waddington, Fabien Lagnieu \u0026amp; Dimitri Henrard-Iratchet\u003c/em\u003e\u003c/h2\u003e\n\u003ch2 id=\"comment-on-the-research-paper-tailoring-mixup-to-data-for-calibration-written-by-quentin-bouniot-pavlo-mozharovskyi--florence-dalché-buc-from-ltci-télécom-paris-institut-polytechnique-de-paris-france\"\u003eComment on the research paper: \u003ca href=\"https://arxiv.org/abs/2311.01434\"\u003e\u003cstrong\u003eTailoring Mixup to Data for Calibration\u003c/strong\u003e\u003c/a\u003e, written by \u003cem\u003eQuentin Bouniot, Pavlo Mozharovskyi \u0026amp; Florence d’Alché-Buc\u003c/em\u003e, from LTCI, Télécom Paris, Institut Polytechnique de Paris, France\u003c/h2\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of contents\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#1-existing-data-augmentation-methods\"\u003eExisting Data Augmentation Methods\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#2-understanding-calibration\"\u003eUnderstanding Calibration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#3-best-of-both-worlds-tailoring-mixup-to-data-for-calibration\"\u003eBest of both worlds: Tailoring Mixup to Data for Calibration\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;But it works well on the training set!\u0026rdquo; is the machine learning equivalent to the classic \u0026ldquo;But it works on my computer!\u0026rdquo;\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe basic workflow of machine learning has two steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirst, \u003cstrong\u003etrain\u003c/strong\u003e your model to perform a task from an available dataset.\u003c/li\u003e\n\u003cli\u003eSecond, \u003cstrong\u003egeneralize\u003c/strong\u003e and predict the results from unseen data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHow can data scientists be sure and \u003cem\u003econfident\u003c/em\u003e that their model will infer a correct result on this new data?\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/WoMM.jpg\" \n    alt=\"Does not know why it (don't) works.\"\n    width=300\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eWe know that deep learning models need vast amounts of data to be efficient.\nSo, when there is not enough, researchers simply… create more data:\nthis is the concept of \u003cstrong\u003edata augmentation\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eHowever, this technique tends to exarcerbate the models\u0026rsquo; \u003cstrong\u003eoverconfidence\u003c/strong\u003e in their\npredictions.\nDiscrepancies between confidence and prediction accuracy are acceptable in domains such as e-commerce recommendations,\nbut high stake applications such as medical diagnosis or nuclear safety require an accurate \u003cstrong\u003econfidence score\u003c/strong\u003e, where robustness and reliability are critical.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is the idea behind calibration: the model\u0026rsquo;s confidence in its prediction must truly reflect its own prediction accuracy.\u003c/strong\u003e\nWell-calibrated models do not just inspire trust; they also contribute to \u003cstrong\u003efairer decision-making\u003c/strong\u003e, by ensuring that predictions are accompanied by reliable confidence estimates, which can help reduce biased or unjust outcomes.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/CalibratedClassifier.png\" \n    alt=\"Difference between a calibrated classifier (right) and a bad one (left).\"\n    width=600\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Illustration of the confidence of a calibrated classifier (right) with\naccurate prediction probabilities and a more brutal and under-confident one (left).\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eMerging data augmentation and calibration is challenging. The first is prone to\ncreate \u003cstrong\u003emanifold intrusion\u003c/strong\u003e, where synthetic data with a given label conflicts\nwith original data of another class. The rise of the size of the dataset also\nincreases the computational cost of the training. This contradicts the potential objective of frugality. The second is known to \u003cstrong\u003econstrain the accuracy\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eTo handle these challenges, Quentin Bouniot and Pavlo Mozharovskyi have conducted\nunder the direction of Florence d\u0026rsquo;Alché-Buc an extensive study on one of the\ntechnique of data augmentation, the \u003cstrong\u003elinear interpolation of training samples\u003c/strong\u003e,\nalso called \u003cstrong\u003eMixup\u003c/strong\u003e. They have found an efficient way to tune this process to\nboth improve the performance \u003cstrong\u003eand\u003c/strong\u003e the calibration of models, while being\nmuch more efficient than previous methods.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s dig step by step into it.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"1-existing-data-augmentation-methods\"\u003e1. Existing Data Augmentation Methods\u003c/h1\u003e\n\u003cp\u003eDeep learning methods rely on vast amounts of data, so if you do not have enough, make\nit yourself. This is the first conclusion of the\nstudy of a Microsoft Research team, lead by Patrice Simard in 2003 aimed to list the\ncurrent best practices of neural networks training:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003e\u0026ldquo;The most important practice is getting a training set as large as possible: we\nexpand the training set by adding a new form of distorted data.\u0026rdquo;\u003c/em\u003e [Simard et al. 2003] \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe good results of subsequent models have proved them right. And numerous\ntechniques have been developed since. Let\u0026rsquo;s review some of them.\u003c/p\u003e\n\u003ch2 id=\"11-create-new-images-for-classification\"\u003e1.1. Create new images for classification\u003c/h2\u003e\n\u003cp\u003eThe most visual example of data augmentation is the way image classifiers are trained.\nTo make them more robust and efficient, scientists have transformed the input images\nto drastically increase the size of the training set (by up to 2048 times).\u003c/p\u003e\n\u003cp\u003eThe most commonly used transformations\nare random cropping and resizing, flipping, and color distortion.\nThis is now so common that it can be done in a few lines in \u003ccode\u003epytorch\u003c/code\u003e (see next code\nsnippet), and automatic recipes\nsuch as \u003ccode\u003eAutoAugment\u003c/code\u003e\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e are readily available to augment common datasets.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Definition of transformations for an image dataset\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etransformTrain\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCompose\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eRandomResizedCrop\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e224\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eRandomHorizontalFlip\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eColorJitter\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebrightness\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ehue\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e.3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         \u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAdditional transformations for images are illustrated\non the next figure. We expect the neural networks to \u0026ldquo;see\u0026rdquo; these 10 new images\nas being close in their latent space. With this example, one original labeled image\nis processed 10 times in different versions during the training of the model.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/Data-Augmentation.png\" \n    alt=\"Different methods of data augmentation operators.\"\n    width=600\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Illustration of different data augmentation operators, taken from the paper of Chen 2020 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"12-linear-interpolation-or-mixup\"\u003e1.2. Linear interpolation or Mixup\u003c/h2\u003e\n\u003cp\u003eAnother idea is to create a virtual sample from a \u003cstrong\u003evicinity\u003c/strong\u003e around the true\ntraining data—like we did in high school when we added epsilon to a number to see in witch direction\nthe function is moving. This principle has been demonstrated to help models\ngeneralize. However, the method of creation is often hand-crafted and only mimic\nnatural perturbations.\u003c/p\u003e\n\u003cp\u003eTo scale up this process, [Zhang et al., 2018]\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e imagined the \u003cstrong\u003eMixup\u003c/strong\u003e process,\nwhich is a linear interpolation, or mixing, of two or more training datapoints.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/mixup_figures-vanilla.png\"\n    alt=\"Example of vanilla Mixup\"\n    width=300\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Illustration of a linear interpolation of Mixup. New points $\\tilde{\\mathrm{x}}_1$ and $\\tilde{\\mathrm{x}}_2$ are respectively drawn from the segment $[\\mathrm{x}_1, \\mathrm{x}_2]$ and $[\\mathrm{x}_1, \\mathrm{x}_3]$\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe process of data augmentation during training with Mixup consists of three phases:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eselecting tuples\u003c/strong\u003e (most often pairs) of points to mix together,\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esampling coefficients\u003c/strong\u003e that will govern the interpolation to generate synthetic points,\u003c/li\u003e\n\u003cli\u003eapplying a specific \u003cstrong\u003einterpolation procedure\u003c/strong\u003e between the points weighted by the coefficients sampled.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHowever, the literature explores the drawbacks of this process:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMixing carelessly different points can result in\n\u003cstrong\u003eincorrect labels and hurt generalization\u003c/strong\u003e [Guo et al., 2019]\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e, while\nmixing similar points helps in diversity [Dablain et al., 2022].\u003c/li\u003e\n\u003cli\u003eFurthermore, several previous work have highlighted a \u003cstrong\u003etrade-off between performance and calibration\u003c/strong\u003e in Mixup [Wang et al., 2023].\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBefore digging further into the Mixup process, it is time to understand what\nexactly is the calibration of a model and why it can be worth of a trade-off with performance.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"2-understanding-calibration\"\u003e2. Understanding Calibration\u003c/h1\u003e\n\u003cp\u003eModern (post 2016) neural networks have a high accuracy but are overconfident\nin their predictions, outputting softmax scores of\nabove 99.9% for the dominant class, hence misleading the user into a false sense of confidence.\nThis is why we need \u003cstrong\u003ecalibration\u003c/strong\u003e.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/Over_confident_IA.jpg\" \n    alt=\"Failed object detection with high confidence\"\n    width=400\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Meme about the overconfidence of an AI agent (obviously uncalibrated) over a failed prediction.\u003c/em\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eCalibration\u003c/em\u003e is a metric to quantify uncertainty, measuring the difference between a model’s confidence\nin its predictions and the actual probability of those predictions being correct.\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eIn other words, if a \u003cem\u003ecalibrated model\u003c/em\u003e predicts the image as a cat with a confidence\nof 0.3, this prediction has a 30% chance of being correct.\n\u003cstrong\u003eThe actual aim is not exactly to explain the results, but confidence calibration prevents\nsome mistakes by associating a prediction with its confidence score.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s explore further the motivations of calibration and the way to measure\nit and the potential draw backs.\u003c/p\u003e\n\u003ch3 id=\"21-importance-of-calibration\"\u003e2.1. Importance of calibration\u003c/h3\u003e\n\u003cp\u003eThe gap in confidence calibration has been spotted by [Guo et al. (2017)]\u003csup id=\"fnref1:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e,\nand is linked to the actual use cases of neural networks, where the calibration is not crucial.\nIn LLMs or online recommender systems, a 90% quality of predictions is enough and occasional mistakes are acceptable. For further use however, like in medical diagnosis prediction or\nin defense systems, an overconfident model can lead to tragic consequences.\u003c/p\u003e\n\u003cp\u003eWhat would be the benefits of a well calibrated model ?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt can \u003cstrong\u003efilter out the poor predictions\u003c/strong\u003e, and not provide a wrong prediction to the user.\u003c/li\u003e\n\u003cli\u003eIt can \u003cstrong\u003ereinforce the continuous training\u003c/strong\u003e, by asking for the actual label of the low confidence prediction.\u003c/li\u003e\n\u003cli\u003eIt can \u003cstrong\u003edetect outliers\u003c/strong\u003e and warn the user that something strange is happening.\u003c/li\u003e\n\u003cli\u003eIt can improve the \u003cstrong\u003erobustness of the model\u003c/strong\u003e by ensuring that prediction confidence accurately reflects the underlying uncertainty, leading to more reliable decisions in critical situations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTo sum it up, a well calibrated model is a reliable coworker aware of its own capacities.\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"22-calibration-metrics\"\u003e2.2. Calibration Metrics\u003c/h3\u003e\n\u003cp\u003eTo compare the calibration of models, specific metrics are required. Unlike simple accuracy on a dataset, various metrics have been proposed in the literature, each focusing on different characteristics.\u003c/p\u003e\n\u003ch4 id=\"221-the-brier-score\"\u003e2.2.1. The Brier Score\u003c/h4\u003e\n\u003cp\u003eThe Brier score [Brier, 1050] is the mean square error between predicted confidence and target.\nHere the target has the form of a one-hot encoded vector.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/BrierScore_Wolfe.png\" \n    alt=\"Brier Score illustration\"\n    width=600\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Computing the Brier Score on classification task (image by Wolfe)\u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIntuitively, the \u003cstrong\u003eBrier Score measures the accuracy of predicted probabilities\u003c/strong\u003e. It can be decomposed into three components — uncertainty (marginal uncertainty over labels), resolution (deviations of individual predictions against the marginal), and reliability (average violation of true label frequencies)\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eBrier Score = uncertainty - resolution + reliability\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe Brier Score is insensitive to the low frequencies events, hence it can be used in combination\nwith one of the other following metrics to provide useful insights.\nBasically, the score is low when the predictions reflect the confidence, i.e. when the model is calibrated.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThe following code is a dummy example of Brier score computation of a single classification probabilities over 3 classes. The same probabilities will be used on different metrics.\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esklearn.metrics\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebrier_score_loss\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Example of prediction outputs\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprob_u\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.34\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.33\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.33\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprob_l\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.25\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.25\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprob_h\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.9\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.07\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.03\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Compute brier score\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;Brier score for uniform:\u003c/span\u003e\u003cspan style=\"color:#8045ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e \u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebrier_score_loss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eprob_u\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;Brier score for low confidence:\u003c/span\u003e\u003cspan style=\"color:#8045ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e \u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebrier_score\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eprob_l\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;Brier score for high confidence: \u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebrier_score\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eprob_h\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003eOutput\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBrier score \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e uniform:\t      \t0.2178\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBrier score \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e low confidence:\t\t0.1250\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBrier score \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e high confidence:\t0.0053\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"222-the-expected-calibration-error-ece\"\u003e2.2.2. The Expected Calibration Error (ECE)\u003c/h4\u003e\n\u003cp\u003eThe Expected Calibration Error [Guo et al, 2017] approximates the difference between \u003cstrong\u003eaccuracy\u003c/strong\u003e and \u003cstrong\u003econfidence\u003c/strong\u003e by grouping samples into equally spaced \u003cstrong\u003ebins\u003c/strong\u003e with respect to their confidence scores.\nBecause it is both simple and interpretable, ECE is a popular metric to evaluate calibration on classification tasks in practice.\nECE computes the difference between average confidence and accuracy within each\nbin, then takes a \u003cstrong\u003eweighted average of these values based upon the relative\nsize of each bin.\u003c/strong\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/ECE_Wolfe.png\" \n    alt=\"ECE illustration \"\n    width=600\u003e\n    \u003c/img\u003e\n  \u003cfigcaption\u003e\u003c/figcaption\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Computing ECE over a group of prediction, (image by Wolfe)\u003csup id=\"fnref1:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eECE measures how well a model’s estimated \u0026ldquo;probabilities\u0026rdquo; match the observed\nprobabilities by taking a weighted average over the absolute difference between\naccuracy and estimated probabilities (confidence). This measure involves splitting\nthe predictions into $M$ equally spaced bins.\u003c/p\u003e\n\u003cp\u003e$$ECE  = \\sum_{bins}^M \\frac{\\text{bin size}}{\\text{nb samples}} | \\text{accuracy per bin} - \\text{average bin probability}| $$\u003c/p\u003e\n\u003cp\u003eA very good example on how to compute ECE by hand can be found in the article\nof \u003ca href=\"https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/\"\u003eMaja Pavlovic\u003c/a\u003e\non the blog TowardsDataScience\u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVariants\u003c/strong\u003e: \u003cem\u003eAdaptative ECE\u003c/em\u003e (AECE) is simmilar to ECE, but with each bin having the same number of samples. Other extensions of ECE can\nbe used to estimate the variance over the bins, the \u003cem\u003eUncertainty Calibration Error\u003c/em\u003e\n(UCE) or the \u003cem\u003eExpected Normalize Calibration Error\u003c/em\u003e (ENCE). They will not be\ndetailed further here.\u003c/p\u003e\n\u003ch4 id=\"223-the-negative-log-likelihood-nll\"\u003e2.2.3. The Negative Log Likelihood (NLL)\u003c/h4\u003e\n\u003cp\u003eThe Negative Log Likelihood (NLL) is the typical objective function for training neural networks in multi-class classification. It characterizes the disparity\nbetween the predicted and the actual confidence for the true label.\nIt reaches a perfect score of $0$ when all data is correctly predicted with 100% confidence,\nand rises as soon as some are misclassified. Hence lower scores correspond to better calibration.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eDummy example of NLL computation of a single prediction over 3 classes\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.nn\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003elog_softmax\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eLogSoftmax\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eloss_fn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eNLLLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# input to NLLLoss is of size (batch_size x nb_classes) = 1 x 3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e])\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elong\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# correct class is at index O\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# different examples of logits from a classifier\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003elogits_u\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]])\u003c/span\u003e      \u003cspan style=\"color:#75715e\"\u003e# uniform prediction\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003elogits_l\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]])\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# low confidence prediction\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003elogits_h\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTensor\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e([[\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]])\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# high confidence prediction\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;nll uniform: \u003c/span\u003e\u003cspan style=\"color:#8045ff\"\u003e\\t\\t\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eloss_fn\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elog_softmax\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elogits_u\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;nll low confidence: \u003c/span\u003e\u003cspan style=\"color:#8045ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eloss_fn\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elog_softmax\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elogits_l\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;nll high confidence: \u003c/span\u003e\u003cspan style=\"color:#8045ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eloss_fn\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elog_softmax\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elogits_h\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003eOutput\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enll uniform: \t\t  1.0986\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enll low confidence:   0.6184\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enll high confidence:  0.0001\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eHowever, NNL also causes overconfidence in modern neural networks.\u003c/strong\u003e\nThey are purposely trained to minimize it by making high confidence predictions, which actually lowers the exponential sum of the soft max, as in our\nhigh_confidence example above.\u003c/p\u003e\n\u003cp\u003eThis kind of behavior can be exhibited by drawing the calibration curve of the predictor.\u003c/p\u003e\n\u003ch4 id=\"23-calibration-curves---reliability-diagrams\"\u003e2.3. Calibration Curves - Reliability diagrams\u003c/h4\u003e\n\u003cp\u003eThe calibration curves [Wilks, 1995] compare how well the probabilistic predictions of a\nbinary classifier are calibrated. It shows the frequency of the predicted label against the\npredicted probability. It is easily drawn with the method \u003ccode\u003emodel.predict_proba()\u003c/code\u003e of scikit-learn.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html#calibration-curves\"\u003eScikit-Learn\u0026rsquo;s documentation\u003c/a\u003e\nprovides a very insightful illustration to better understand these curves. They\nhave fitted 4 different classifiers on a very small training set and plot the\ncalibration curve along with the histogram showing the related distribution of\nthe predicted probabilities on each of the 10 bins. On this specific example,\nwe can observe the following behaviors:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLogistic Regression\u003c/strong\u003e: not perfect, but well calibrated because the\noptimized log loss is also the scoring rule (as seen in previous section).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGaussian Naive Bayes\u003c/strong\u003e: its tendency to push probabilities to 0 or 1 is well\nshown on histogram orange. This means an overconfident model.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupport vector Classifier\u003c/strong\u003e displays a typical sigmoid calibration curve.\nThis under-confident result is typical of maximum-margin methods.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRandom Forest\u003c/strong\u003e averages the predictions over a set of models, meaning exact predictions of 0 or 1 are rare, hence the shift towards 0.2 and 0.9. The whole\nmodel seems under-confident, but since each tree is fitted in minimizing a scoring\nrule (Brier score of log-loss) the pink calibration curve is pretty close to the dot line.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/sphx_glr_plot_compare_calibration_001.png\" \n    alt=\"Calibration plot comparison\"\n    width=600\u003e\n    \u003c/img\u003e\n  \u003cfigcaption\u003e\n    \u003c/figcaption\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Behavior of regular classifier on a standard task.\nUpper: Calibration curves with \u003ccode\u003esklearn.calibration.CalibrationDisplay.from_estimator\u003c/code\u003e.\nBottom: Histograms of the number of samples per bins of \u003ccode\u003epredict_proba\u003c/code\u003e values.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eRemember: a \u003cstrong\u003eperfectly calibrated estimator\u003c/strong\u003e will get the doted diagonal line\nand its histogram will be flat.\u003c/p\u003e\n\u003ch3 id=\"24-drawbacks\"\u003e2.4 Drawbacks\u003c/h3\u003e\n\u003cp\u003eTo provide an insight of the side effects of the calibration, we will study the\nimpact of the provided method \u003ccode\u003eCalibratedClassifierCV\u003c/code\u003e in Scikit-Learn.\nIt uses cross-validation to obtain unbiased predictions, which are then used for calibration.\nThe sigmoid method here is a simple logistic regression model. We experiment\nthe effect of the calibration on the accuracy and the Brier Score of 4 classifiers\nfitted on the titanic dataset. We will only display the code to instantiate and\ncalibrate the models.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esklearn.linear_model\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eLogisticRegression\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esklearn.naive_bayes\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eGaussianNB\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esklearn.svm\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eSVC\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esklearn.ensemble\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eRandomForestClassifier\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esklearn.calibration\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCalibratedClassifierCV\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# [...] load titanic dataset, split data, Skub automatically the data\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eX_train\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eX_test\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ey_train\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ey_test\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etrain_test_split\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Initialize classifiers\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eclassifiers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;Logistic Regression\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eLogisticRegression\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;Naive Bayes\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eGaussianNB\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;Support Vector Classifier\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eSVC\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eprobability\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;Random Forest\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eRandomForestClassifier\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Calibrate with the simgmoid regression\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecalibrated_classifiers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ecalibrated_proba\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ename\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eclassifier\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eclassifiers\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eitems\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003ecalibrated_classifiers\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ename\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCalibratedClassifierCV\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eclassifier\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecv\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emethod\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;sigmoid\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efit\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX_train\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ey_train\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003ecalibrated_proba\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ename\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eclassifier\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epredict_proba\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eX_test\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)[:,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# [...] Compute metrics and display as bar plots\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/Calibration_side_effect.png\" \n    alt=\"Calibration effect on metrics\"\n    width=600\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: effect of calibration on classifiers\u0026rsquo; metrics. We expect a small drop in\naccuracy and a reduction (improvement) of the Brier Score. But our method is not\nefficient here.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe can see that this method has moderate and sometimes counterintuitive effects.\n\u003cstrong\u003eThis suggests that the training set is not sufficient to\nfit a calibrated estimator.\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"3-best-of-both-worlds-tailoring-mixup-to-data-for-calibration\"\u003e3. Best of Both Worlds: Tailoring Mixup to Data for Calibration\u003c/h1\u003e\n\u003cp\u003eWe have reached the core of the paper of Bouniot et al. They show that, by\n\u003cstrong\u003etaking the distance of points into account when sampling the coefficients\u003c/strong\u003e in the\nsecond phase of Mixup, we can (i) avoid a loss in diversity, and (ii) reduce manifold intrusion and label noise.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/Mixup-ours.png\"\n    alt=\"Similarity Mixup\"\n    width=300\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Illustration of a Blouniot et al.\u0026rsquo;s process of \u003cstrong\u003esimilarity\u003c/strong\u003e in interpolation. New point $\\tilde{\\mathrm{x}}_2$ have to be closer to $\\mathrm{x}_1$ because $\\mathrm{x}_3$ has a different label but still preserve diversity.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIf we had only used a selection of samples with similar labels, we would have\nlost the possible exploration of new directions of the latent space.\n\u003cstrong\u003eWith this similarity process, at the end of the day, we have avoided restricting\u003c/strong\u003e\n\u003cstrong\u003epossible direction of mixing while staying in the vicinity of original points,\u003c/strong\u003e\n\u003cstrong\u003ehence preventing manifold intrusion.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"31-linear-interpolation-of-training-samples-mixup\"\u003e3.1 Linear interpolation of training samples: Mixup\u003c/h2\u003e\n\u003cp\u003eMixing samples through linear interpolation is the easiest and most efficient way\nto create new data from a computational point of view. Combining data from the same\nbatch also avoids additional sampling during training.\u003c/p\u003e\n\u003cp\u003eSpecific techniques have been proposed since 2018 to compute linear interpolation\nbut often at the cost of more complex training or loss of diversity.\nThe selection process of samples to interpolate from may be computationally\nexpensive.\nFurthermore\nsuch studies have been conducted with the aim of improving models\u0026rsquo; generalization, not their calibration, and will not solve our issue.\u003c/p\u003e\n\u003cp\u003eIn the original mixup method of [Zhang et al. 2018]\u003csup id=\"fnref1:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e, at each training iteration\nof the model, each input is mixed with another input randomly selected from the\nsame batch, with a random strength drawn form a Beta law.\u003c/p\u003e\n\u003cp\u003eBut how can we be sure of the label of these new datapoints?\u003c/p\u003e\n\u003ch2 id=\"32-weighting-to-prevent-manifold-intrusion\"\u003e3.2 Weighting to prevent manifold intrusion\u003c/h2\u003e\n\u003cp\u003eThe real danger of mixup is \u003cstrong\u003emanifold intrusion\u003c/strong\u003e, where the interpolated\nsample between two identical label points falls into an other class.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe likelihood of conflict in the synthetic label\nincreases with the distance between the two points.  As\ndata live in manifolds of the representation space, the linear combination of\ntwo points far from each other can lie in a different manifold than the linear\ncombination of the labels. The further away the\npoints are, the more manifolds can exist in between.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/Manifold_mixup.png\"\n    alt=\"Risk of manifold intrusion when mixing samples\"\n    width=600\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Illustration from [Baena, 2022]\u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e of a \u003cstrong\u003emanifold intrusion\u003c/strong\u003e (right) when drawing samples as mixup existing points. The linear interpolation (red line) crosses the blue classe leading to conflict.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eBouniot et al. have conduct extensive experiments to show that there is a\n\u003cstrong\u003etrade-off between adding diversity\u003c/strong\u003e by increasing the proportion\nof elements to mix, \u003cstrong\u003eand uncertainty\u003c/strong\u003e by mixing elements far from each other.\nFurthermore, it shows that we cannot restrict pairs to mix by selecting data\nsolely based on distance, as \u003cstrong\u003eit can degrade performance by reducing diversity of synthetic samples\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eTo better control this trade-off with Mixup, they suggest to tailor interpolation\ncoefficients based on the distance of training data. The final part will detail this process.\u003c/p\u003e\n\u003ch2 id=\"33-the-power-of-the-similarity-kernel\"\u003e3.3 The power of the similarity kernel\u003c/h2\u003e\n\u003cp\u003eBouniot et al. \u003cstrong\u003eused a similarity kernel to mix more strongly\nsimilar data and avoid mixing less similar ones\u003c/strong\u003e, to preserve label\nquality and confidence of the network.\u003c/p\u003e\n\u003cp\u003eTo do so, \u003cstrong\u003ethey needed to change the interpolation coefficient depending on the\nsimilarity between the points\u003c/strong\u003e. They have found a way to preserve the type of\ndistribution of samples by warping these coefficients at every iteration to\ngovern the strength and direction of the mixup. Curious readers can refer\nto section 3.2 of \u003csup id=\"fnref1:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e for technical details. In summary, they only\nneed the parameter $\\tau$ of a Beta distribution $B(\\tau, \\tau)$ that behaves\nlogarithmically with this parameter. Hence, $\\tau$ should be exponentially\ncorrelated with the distance of points to finally obtain a linear interpolation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTo this end, they define a class of similarity kernels, based on a normalized and\ncentered Gaussian kernel, that outputs the correct warping parameter $\\tau$ for the given pair of points.\u003c/strong\u003e\nThis similarity kernel is defined by the amplitude and the standard derivation\nof the Gaussian, two additional parameters to tune separately. The computation\nof $\\tau$ also depends on the average distance of samples in the same batch.\nMore specifically, for classification\ntasks, they use the $L_2$ distance between embeddings, while for regression\ntasks, they use the distance between labels.\u003c/p\u003e\n\u003cp\u003eThe algorithm to compute this parameter is described bellow in pseudo-code:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-code\" data-lang=\"code\"\u003eInput: (Batch (x_i, y_i) of size n, kernel similarity parameters, current model parameters)\nSample random permutation sigma\nFor i in [1, n], do\n  # Compute the interpolated points from x_i and x_sigma(i):\n  Compute warping parameter tau using a Beta coefficient and the similarity kernel\n  Generate new point x_tilde as a linear interpolation of x_i and x_sigma(i), weighted by tau\n  Generate new label y_tilde as a linear interpolation of y_i and y_sigma(i), weighted by tau\n  Aggregate new data to batch\nOptimize loss over this augmented batch\nOutput: the updated model parameters\n\u003c/code\u003e\u003c/pre\u003e\u003cp align=\"center\"\u003e\n  \u003cfigure\u003e\n  \u003cimg src=\"/images/MixUpDataCalibration/heatmap_density_dist_inverse_warp_v2.png\"\n    alt=\"Evolution of the similarity kernel\"\n    width=300\u003e\n    \u003c/img\u003e\n  \u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Illustration of the effect of the similarity kernel on two points $x_i$ and $x_{\\sigma{(i)}}$, additional description bellow. Figure from [Bouniot, 2024]\u003csup id=\"fnref2:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe motivation behind this kernel is to have $\\tau \u0026gt;1$ when the two points to mix are similar, i.e., the\ndistance is lower than average, to increase the mixing effect, and $\\tau \u0026lt; 1$ otherwise, to reduce the\nmixing. Above Figure illustrates the evolution of the density of warped interpolation coefficients $ω_τ(λ)$,\ndepending on the distance between the points to mix. Close distances (left part of the heatmap)\ninduce strong interpolations, while far distances (right part of the heatmap) reduce interpolation.\n\u003cstrong\u003eUsing this similarity kernel to find the correct τ to parameterize the Beta distribution defines our full\u003c/strong\u003e\n\u003cstrong\u003eSimilarity Kernel Mixup framework.\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"34-going-further\"\u003e3.4 Going further\u003c/h2\u003e\n\u003cp\u003eExtensive experiments have been conducted by the authors on image classification and regression tasks.\nThey have reproduced the protocol of the literature and \u003cstrong\u003etheir framework displays an improvement in\nboth accuracy and calibration across the 3 metrics described above (ECE, Brier and NLL)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIt is important to note, however, that the hyper-parameters have been tuned, and\nthe best results across different metrics do not share the same values for the kernel standard deviation.\u003c/p\u003e\n\u003cp\u003eDuring the experiment process, the authors have compared the final results after\n\u003cem\u003etemperature scaling\u003c/em\u003e, following [Guo, 2017]\u003csup id=\"fnref2:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e process. This temperature\nis also a learnable parameter that have been optimized during\nthe training of the models.\u003c/p\u003e\n\u003cp\u003eIn addition to improving calibration and performance, this approach demonstrates \u003cstrong\u003egreater frugality\u003c/strong\u003e by reducing computational overhead compared to other calibration-driven data augmentation methods. By efficiently tailoring the interpolation process, it lowers the number of unnecessary computations and memory usage, contributing to \u003cstrong\u003emore sustainable and energy-efficient\u003c/strong\u003e machine learning practices.\u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eWith similarity kernel, we get a more accurate and better calibrated model because\nthe coefficients governing the interpolation are warped to change their underlying distribution\ndepending on the similarity between the points to mix, so that\n\u003cstrong\u003esimilar datapoints are mixed more strongly than less similar ones\u003c/strong\u003e,\n\u003cstrong\u003epreserving calibration by avoiding manifold intrusion and label noise\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAs seen in the pseudo-code, this provides a more efficient data augmentation\napproach than Calibration-driven Mixup methods, both in\nterms of time and memory, offering a \u003cstrong\u003ebetter trade-off between performance and calibration improvement\u003c/strong\u003e, while promoting frugality by reducing unnecessary computational resources.\u003c/p\u003e\n\u003cp\u003eConcurrently, [Verma et al. 2018]\u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e have proposed the \u003cstrong\u003eManifold Mixup\u003c/strong\u003e\nframework that encourages neural networks to predict less confidently on\ninterpolations of hidden representation via a simple regularizer. This\ntraining method leads to class-representations with fewer directions of variance.\nBut even with the actual purpose the reduce the over confidence, the word\n\u0026ldquo;Calibration\u0026rdquo; never occurs in the paper\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis highlights the necessity of raising awareness about calibration and establishing a standard process for evaluating models.\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eP.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to\nvisual document analysis. In Proceedings of the Seventh International Conference on Document Analysis\nand Recognition, volume 2, pages 958–962, 2003.\u0026#160;\u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eEkin D. Cubuk, Barret Zoph , Dandelion Mané, Vijay Vasudevan, Quoc V. Le, Google Brain (2018).\nAutoAugment: Learning Augmentation Strategies from Data \u003ca href=\"https://arxiv.org/abs/1805.09501\"\u003earXiv\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eTing Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, (2020).\nA Simple Framework for Contrastive Learning of Visual Representations \u003ca href=\"https://arxiv.org/abs/2002.05709\"\u003earXiv\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2018). mixup: Beyond empirical risk\nminimization. In International Conference on Learning Representations.\u0026#160;\u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref1:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eGuo, Chuan, et al. “On calibration of modern neural networks.” International Conference on Machine Learning. PMLR, 2017.\u0026#160;\u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref1:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref2:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eBouniot, Q., Mozharovskyi P., d\u0026rsquo;Alché-Buc, F. (2023).\nTailoring Mixup to Data for Calibration \u003ca href=\"https://arxiv.org/abs/2311.01434\"\u003earXiv\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref1:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref2:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eCameron R. Wolfe, Ph.D. Confidence Calibration for Deep Networks: Why and How? \u003ca href=\"https://medium.com/towards-data-science/confidence-calibration-for-deep-networks-why-and-how-e2cd4fe4a086\"\u003emedium/TowardsDataScience blogpost\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref1:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eMaja Pavlovicic,  Expected Calibration Error (ECE): A Step-by-Step Visual Explanation \u003ca href=\"https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/\"\u003elink\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eRaphael Baena, Lucas Drumetz, Vincent Gripon (2022)\nPreventing Manifold Intrusion with Locality: Local Mixup \u003ca href=\"https://arxiv.org/abs/2201.04368\"\u003earXiv\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003eVerma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D., and Bengio, Y. (2019).\nManifold mixup: Better representations by interpolating hidden states. In Chaudhuri, K. and\nSalakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning,\nvolume 97 of Proceedings of Machine Learning Research, pages 6438–6447. PMLR.\u0026#160;\u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/mixupdatacalibration/","date_published":"9036-09-09T33:99:00+01:00","date_modified":"9036-09-09T33:99:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"f9ffecbbe90ebc49d769a117e08344bf73970d20","title":"BitFit: BIas-Term FIne-Tuning","summary":"","content_text":" BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers Authors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich 1. Introduction Fine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?\nThrough this blog post, we will discuss BitFit — a novel parameter-efficient fine-tuning technique proposed in the paper BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers (Ben Zaken et al., 2022).\n2. Why Fine-tuning Needs Optimization Fine-tuning NLP models typically involves updating all model parameters, but this poses some major challenges:\nComputational Cost: Training a full BERT-large model requires high-end GPUs and significant memory. Deployment Complexity: Every task-specific fine-tuned model requires a separate copy of the large model. Transfer Learning Issues: Modifying too many parameters can lead to overfitting on small datasets. Wouldn\u0026rsquo;t it be great if we could fine-tune just a small subset of the parameters and get similar results? This is exactly where BitFit comes in.\n3. How BitFit Works Traditional fine-tuning updates all the parameters in a Transformer model, which is computationally expensive. BitFit, on the other hand, only updates the bias terms in the model while keeping all other weights frozen.\nBias terms are small but essential parameters in neural networks. They adjust activations before applying transformations, helping models adapt to new tasks with minimal updates.\n3.1 Why Focus on Bias Terms ? Bias terms b play a crucial role in neural networks because:\nThey allow neurons to fire (activate) even when inputs are zero. Adjusting bias values can shift outputs without requiring full retraining. Updating biases is a lightweight operation, meaning less memory and faster adaptation. A key finding in the BitFit paper is that bias terms contribute uniquely to fine-tuning. When researchers randomly selected the same number of non-bias parameters for fine-tuning, the model performed significantly worse than BitFit → This suggests that bias parameters are not just a small subset but play an important role in model adaptation.\n3.2 What Layers Does BitFit Modify ? BitFit updates the bias terms in key layers of Transformer models like BERT:\nSelf-Attention Layers Transformers use self-attention to focus on important words in a sentence. Each attention head contains three transformations: Query (Q), Key (K), Value (V). And each transformation has its own bias term (bQ, bK, bV).\n→ BitFit updates only these biases (bQ, bK, bV), while the main attention weights (WQ, WK, WV) remain unchanged.\nFeedforward Layers (MLPs) Transformers contain fully connected layers that transform intermediate representations. These layers consist of two main weight matrices (W1, W2) and bias terms (b1, b2).\n→ BitFit updates only the bias terms (b1, b2), keeping the weight matrices (W1, W2) frozen.\nLayer Normalization (LN) Layers Transformers use Layer Normalization to stabilize training and prevent exploding gradients. Each LN layer has two learnable parameters: the scale factor $\\gamma$ to control the output scaling and the bias $\\beta$ to adjust the mean shift.\n→ BitFit modifies only the bias term $\\beta$, while keeping the scale factor $\\gamma$ fixed.\n3.3 Mathematical Explanation 3.3.1 Standard Fine-Tuning In traditional fine-tuning, we update both the weights ($W$) and bias terms ($b$):\n$$ W_{\\text{fine-tuned}} = W_{\\text{pretrained}} + \\Delta W $$\n$$ b_{\\text{fine-tuned}} = b_{\\text{pretrained}} + \\Delta b $$\nwhere:\n$W_{\\text{pretrained}}$ is the original weight matrix from the pre-trained model. $\\Delta W$ is the learned update during fine-tuning. $b_{\\text{pretrained}}$ is the original bias vector. $\\Delta b$ is the learned bias update. 3.3.2 BitFit: BIas-Term FIne-Tuning Instead of updating all weights, BitFit freezes $W$ and only updates $b$:\n$$ W_{\\text{fine-tuned}} = W_{\\text{pretrained}} $$\n$$ b_{\\text{fine-tuned}} = b_{\\text{pretrained}} + \\Delta b $$\nHere, $\\Delta b$ represents the learned adjustments needed for the new task.\n3.4 Are all Bias Terms Equal ? Not all bias parameters contribute equally to fine-tuning. Researchers found that two types of bias terms are especially important:\nQuery Biases bQ: Found in self-attention layers, responsible for selecting relevant words. Middle-Layer MLP Biases b2: Found in feedforward layers, responsible for transforming representations. By only fine-tuning these two subsets, performance remained almost identical to full BitFit while updating half the parameters.\nBitFit typically fine-tunes 0.08% of model parameters, but using only bQ and b2, this number drops to 0.04% with no major accuracy loss!\nThis means fine-tuning can be made even more efficient by selecting only the most impactful bias terms.\n4. How Well Does BitFit Perform? Compared to other parameter-efficient fine-tuning techniques such as Diff-Pruning and Adapters, BitFit achieves competitive performance with significantly fewer trainable parameters.\nBitFit outperforms Diff-Pruning on 4 of the 9 tasks of the GLUE benchmark using the BERTLARGE model and with 6 times fewer trainable parameters. On the test set, BitFit decisively beats Diff-Pruning over two tasks and Adapters over four tasks with 45 times fewer trainable parameters.\nThe performance trends of BitFit remain consistent across different base models, e.g., BERTBASE and RoBERTaBASE. The performance of BitFit is not simply due to its adaptation of a collection of parameters, but rather the specific choice of bias parameters. Random selection of an identical number of parameters yields significantly poorer performance, which means that bias parameters have a unique critical contribution to fine-tuning. Moreover, further analysis reveals that not all bias parameters are equally important as some of them contribute more to the model\u0026rsquo;s performance than others.\nBitFit also demonstrates a smaller generalization gap compared to full fine-tuning, suggesting better generalization capabilities. In token-level tasks such as POS-tagging, BitFit achieves comparable results to full fine-tuning.\nFinally, BitFit\u0026rsquo;s performance also appears to rely on training set size. In experiment with the Stanford Question Answering Dataset, BitFit outperforms full fine-tuning in small-data regimes, but the trend reverses as the training set size increases. What that means is that BitFit is particularly useful when it comes to targeted fine-tuning under small-to-mid-sized data conditions.\n5. Why Does BitFit Work? BitFit\u0026rsquo;s success can be attributed to several key factors that challenge traditional assumptions about fine-tuning large language models. Rather than retraining all parameters, BitFit selectively updates only the bias terms, leading to efficient adaptation without sacrificing performance. But why is this approach effective?\n5.1 Fine-Tuning as Knowledge Exposure, Not Learning A crucial insight is that fine-tuning large pre-trained transformers is often less about \u0026ldquo;learning new knowledge\u0026rdquo; and more about \u0026ldquo;exposing\u0026rdquo; the knowledge already embedded in the model. Since transformer-based models like BERT have already learned a vast range of linguistic patterns during their unsupervised pre-training phase, adjusting a small number of parameters—specifically the bias terms—can be enough to bring out task-specific information without reworking the entire model.\n5.2 Bias Terms and Their Unique Role Bias terms in neural networks serve as offset values, allowing neurons to activate even when input features are zero. Unlike weights, which define relationships between features, bias terms shift outputs in a task-specific manner.\nBitFit leverages the fact that bias terms interact across layers in a way that can subtly adjust how information flows through the model without needing to modify the main weight matrices. This enables significant changes in task-specific performance with minimal modifications to the model structure.\n5.3 A Targeted and Structured Approach Not all bias parameters contribute equally to model adaptation. The study found that:\nQuery Biases (bQ): Found in self-attention layers, crucial for determining which words receive attention.\nMiddle-Layer MLP Biases (b2): Found in feedforward layers, responsible for transforming hidden representations.\nBy focusing updates on these specific biases, BitFit achieves near full fine-tuning performance while modifying only 0.04% of model parameters.\n5.4 The Generalization Advantage Another reason BitFit works well is its effect on generalization. Traditional fine-tuning tends to overfit on small datasets because it updates many parameters, potentially memorizing noise rather than learning transferable patterns. BitFit, by contrast, updates only a fraction of the parameters, leading to a smaller generalization gap.\nIn fact, experiments show that BitFit performs better than full fine-tuning in small-data regimes. This suggests that limiting parameter updates can sometimes lead to better robustness and generalization.\n6. Implications and Future Directions BitFit opens new avenues for efficient fine-tuning, making it particularly relevant in scenarios where computational resources are limited, such as:\n6.1 Efficient Deployment in Real-World Applications Low-Resource AI Systems: BitFit’s lightweight approach is ideal for deploying NLP models in mobile applications, IoT devices, and embedded AI systems where computational efficiency is critical.\nMulti-Task Learning: Since only bias terms need updating, multiple tasks can share the same base model, reducing memory overhead and increasing flexibility in production systems.\nScalable NLP Services: Cloud-based NLP services that handle multiple tasks (e.g., chatbots, automated translations) can benefit from BitFit by reducing the need to store and load separate models for each task.\n6.2 Understanding Model Adaptation The success of BitFit raises deeper questions about the nature of transfer learning and fine-tuning:\nDo large transformers truly need full fine-tuning, or is most of their knowledge already latent?\nCould bias-only tuning be the key to unlocking efficient continual learning strategies?\nAre there other small but critical subsets of parameters that can be updated to achieve similar efficiency gains?\n6.3 Potential Enhancements While BitFit is a promising step forward, future research could explore:\nSelective bias tuning: Further analyzing which specific bias terms contribute most to adaptation and whether additional optimization can reduce the number of updates even further.\nHybrid approaches: Combining BitFit with methods like adapters or LoRA (Low-Rank Adaptation) to achieve even better efficiency.\nApplication to other architectures: Investigating whether BitFit’s principles extend beyond BERT to models like GPT, T5, and multimodal transformers.\n7. Conclusion In conclusion, BitFit offers a desirable compromise between effectiveness and efficiency, making it a valuable tool for fine-tuning transformer-based models, especially in resource-constrained environments or with limited amounts of training data. Having the capability to achieve competitive performance using significantly fewer trainable parameters, coupled with its achievement in low data regimes, bodes well for other NLP tasks and applications.\nBitFit defies the usual wisdom concerning universal fine-tuning by illustrating how slight tweaks in only a very small percentage of model parameters yield high-performance. This efficient approach makes AI models more accessible, scalable, and cost-effective.\nReferences BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models (Ben Zaken et al., ACL 2022) ","content_html":"\u003cstyle\nTYPE=\"text/css\"\u003e\n\ncode.has-jax {font:\ninherit;\nfont-size:\n100%; \nbackground: \ninherit; \nborder: \ninherit;}\n\n\u003c/style\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eBitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers\u003c/h1\u003e\n\u003ch3 id=\"authors--abdoul-r-zeba-nour-yahya-nourelhouda-klich\"\u003eAuthors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich\u003c/h3\u003e\n\u003ch2 style=\"font-size: 20px;\"\u003e 1. Introduction \u003c/h2\u003e\n\u003cp\u003eFine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?\u003c/p\u003e\n\u003cp\u003eThrough this blog post, we will discuss \u003ci\u003e\u003cstrong\u003eBitFit\u003c/strong\u003e\u003c/i\u003e —  a novel parameter-efficient fine-tuning technique proposed in the paper BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers (\u003ca href=\"#benzaken\"\u003eBen Zaken et al., 2022\u003c/a\u003e).\u003c/p\u003e\n\u003ch2 style=\"font-size: 20px;\"\u003e 2. Why Fine-tuning Needs Optimization \u003c/h2\u003e\n\u003cp\u003eFine-tuning NLP models typically involves updating all model parameters, but this poses some major challenges:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eComputational Cost: Training a full BERT-large model requires high-end GPUs and significant memory.\u003c/li\u003e\n\u003cli\u003eDeployment Complexity: Every task-specific fine-tuned model requires a separate copy of the large model.\u003c/li\u003e\n\u003cli\u003eTransfer Learning Issues: Modifying too many parameters can lead to overfitting on small datasets.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWouldn\u0026rsquo;t it be great if we could fine-tune just a small subset of the parameters and get similar results?  This is exactly where \u003ci\u003e\u003cstrong\u003eBitFit\u003c/strong\u003e\u003c/i\u003e comes in.\u003c/p\u003e\n\u003ch2 style=\"font-size: 20px;\"\u003e 3. How BitFit Works  \u003c/h2\u003e\n\u003cp\u003eTraditional fine-tuning updates \u003cstrong\u003eall\u003c/strong\u003e the parameters in a Transformer model, which is computationally expensive. BitFit, on the other hand, \u003cstrong\u003eonly updates the bias terms\u003c/strong\u003e in the model while keeping all other weights \u003cstrong\u003efrozen\u003c/strong\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eBias terms\u003c/strong\u003e are small but essential parameters in neural networks. They adjust activations before applying transformations, helping models adapt to new tasks with minimal updates.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 3.1 Why Focus on Bias Terms ? \u003c/h3\u003e\n\u003cp\u003eBias terms \u003cstrong\u003e\u003ci\u003eb\u003c/i\u003e\u003c/strong\u003e play a crucial role in neural networks because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThey allow neurons to fire (activate) even when inputs are zero.\u003c/li\u003e\n\u003cli\u003eAdjusting bias values can shift outputs without requiring full retraining.\u003c/li\u003e\n\u003cli\u003eUpdating biases is a lightweight operation, meaning less memory and faster adaptation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA key finding in the BitFit paper is that bias terms contribute uniquely to fine-tuning. When researchers randomly selected the \u003cstrong\u003esame number\u003c/strong\u003e of non-bias parameters for fine-tuning, the model performed significantly worse than BitFit → This suggests that bias parameters are not just a small subset but play an important role in model adaptation.\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 3.2 What Layers Does BitFit Modify ? \u003c/h3\u003e\n\u003cp\u003eBitFit updates the bias terms in \u003cstrong\u003ekey layers\u003c/strong\u003e of Transformer models like BERT:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-Attention Layers\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTransformers use self-attention to focus on important words in a sentence. Each attention head contains three transformations: \u003cstrong\u003eQuery\u003c/strong\u003e (\u003ccode\u003eQ\u003c/code\u003e), \u003cstrong\u003eKey\u003c/strong\u003e (\u003ccode\u003eK\u003c/code\u003e), \u003cstrong\u003eValue\u003c/strong\u003e (\u003ccode\u003eV\u003c/code\u003e). And each transformation has its own bias term (\u003ccode\u003ebQ\u003c/code\u003e, \u003ccode\u003ebK\u003c/code\u003e, \u003ccode\u003ebV\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003e→ BitFit \u003cstrong\u003eupdates only these biases\u003c/strong\u003e (\u003ccode\u003ebQ\u003c/code\u003e, \u003ccode\u003ebK\u003c/code\u003e, \u003ccode\u003ebV\u003c/code\u003e), while the main attention weights (\u003ccode\u003eWQ\u003c/code\u003e, \u003ccode\u003eWK\u003c/code\u003e, \u003ccode\u003eWV\u003c/code\u003e) remain \u003cstrong\u003eunchanged\u003c/strong\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFeedforward Layers (MLPs)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTransformers contain fully connected layers that transform intermediate representations. These layers consist of two main \u003cstrong\u003eweight matrices\u003c/strong\u003e (\u003ccode\u003eW1\u003c/code\u003e, \u003ccode\u003eW2\u003c/code\u003e) and \u003cstrong\u003ebias terms\u003c/strong\u003e (\u003ccode\u003eb1\u003c/code\u003e, \u003ccode\u003eb2\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003e→ BitFit \u003cstrong\u003eupdates only the bias terms\u003c/strong\u003e (\u003ccode\u003eb1\u003c/code\u003e, \u003ccode\u003eb2\u003c/code\u003e), keeping the weight matrices (\u003ccode\u003eW1\u003c/code\u003e, \u003ccode\u003eW2\u003c/code\u003e) \u003cstrong\u003efrozen\u003c/strong\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLayer Normalization (LN) Layers\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTransformers use Layer Normalization to stabilize training and prevent exploding gradients. Each LN layer has two learnable parameters: the \u003cstrong\u003escale factor\u003c/strong\u003e \u003ccode\u003e$\\gamma$\u003c/code\u003e to control the output scaling and the \u003cstrong\u003ebias\u003c/strong\u003e \u003ccode\u003e$\\beta$\u003c/code\u003e to adjust the mean shift.\u003c/p\u003e\n\u003cp\u003e→ BitFit \u003cstrong\u003emodifies only the bias term\u003c/strong\u003e \u003ccode\u003e$\\beta$\u003c/code\u003e, while keeping the scale factor \u003ccode\u003e$\\gamma$\u003c/code\u003e \u003cstrong\u003efixed\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 3.3 Mathematical Explanation \u003c/h3\u003e\n\u003ch4 style=\"font-size: 16px;\"\u003e 3.3.1  Standard Fine-Tuning \u003c/h4\u003e\n\u003cp\u003eIn traditional fine-tuning, we update \u003cstrong\u003eboth\u003c/strong\u003e the weights ($W$) and bias terms ($b$):\u003c/p\u003e\n\u003cp\u003e$$\nW_{\\text{fine-tuned}} = W_{\\text{pretrained}} + \\Delta W\n$$\u003c/p\u003e\n\u003cp\u003e$$\nb_{\\text{fine-tuned}} = b_{\\text{pretrained}} + \\Delta b\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$W_{\\text{pretrained}}$ is the original weight matrix from the pre-trained model.\u003c/li\u003e\n\u003cli\u003e$\\Delta W$ is the learned update during fine-tuning.\u003c/li\u003e\n\u003cli\u003e$b_{\\text{pretrained}}$ is the original bias vector.\u003c/li\u003e\n\u003cli\u003e$\\Delta b$ is the learned bias update.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 style=\"font-size: 16px;\"\u003e 3.3.2  BitFit: BIas-Term FIne-Tuning \u003c/h4\u003e\n\u003cp\u003eInstead of updating all weights, BitFit \u003cstrong\u003efreezes\u003c/strong\u003e $W$ and \u003cstrong\u003eonly updates\u003c/strong\u003e $b$:\u003c/p\u003e\n\u003cp\u003e$$\nW_{\\text{fine-tuned}} = W_{\\text{pretrained}}\n$$\u003c/p\u003e\n\u003cp\u003e$$\nb_{\\text{fine-tuned}} = b_{\\text{pretrained}} + \\Delta b\n$$\u003c/p\u003e\n\u003cp\u003eHere, $\\Delta b$ represents the learned adjustments needed for the new task.\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 3.4 Are all Bias Terms Equal ? \u003c/h3\u003e\n\u003cp\u003eNot all bias parameters contribute equally to fine-tuning. Researchers found that two types of bias terms are especially important:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eQuery Biases\u003c/strong\u003e \u003ccode\u003ebQ\u003c/code\u003e: Found in self-attention layers, responsible for selecting relevant words.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMiddle-Layer MLP Biases\u003c/strong\u003e \u003ccode\u003eb2\u003c/code\u003e: Found in feedforward layers, responsible for transforming representations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy only fine-tuning these two subsets, performance remained almost identical to full BitFit while updating half the parameters.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBitFit typically fine-tunes \u003cstrong\u003e0.08%\u003c/strong\u003e of model parameters, but using only \u003ccode\u003ebQ\u003c/code\u003e and \u003ccode\u003eb2\u003c/code\u003e, this number drops to \u003cstrong\u003e0.04%\u003c/strong\u003e with no major accuracy loss!\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThis means fine-tuning can be made even more efficient by selecting only the most impactful bias terms.\u003c/p\u003e\n\u003ch2 style=\"font-size: 20px;\"\u003e 4. How Well Does BitFit Perform? \u003c/h2\u003e\n\u003cp\u003eCompared to other parameter-efficient fine-tuning techniques such as Diff-Pruning and Adapters, BitFit achieves competitive performance with significantly fewer trainable parameters.\u003c/p\u003e\n\u003cp\u003eBitFit outperforms Diff-Pruning on 4 of the 9 tasks of the GLUE benchmark using the BERTLARGE model and with 6 times fewer trainable parameters. On the test set, BitFit decisively beats Diff-Pruning over two tasks and Adapters over four tasks with 45 times fewer trainable parameters.\u003c/p\u003e\n\u003cp\u003eThe performance trends of BitFit remain consistent across different base models, e.g., BERTBASE and RoBERTaBASE. The performance of BitFit is not simply due to its adaptation of a collection of parameters, but rather the specific choice of bias parameters. Random selection of an identical number of parameters yields significantly poorer performance, which means that bias parameters have a unique critical contribution to fine-tuning.\nMoreover, further analysis reveals that not all bias parameters are equally important as some of them contribute more to the model\u0026rsquo;s performance than others.\u003c/p\u003e\n\u003cp\u003eBitFit also demonstrates a smaller generalization gap compared to full fine-tuning, suggesting better generalization capabilities. In token-level tasks such as POS-tagging, BitFit achieves comparable results to full fine-tuning.\u003c/p\u003e\n\u003cp\u003eFinally, BitFit\u0026rsquo;s performance also appears to rely on training set size. In experiment with the Stanford Question Answering Dataset, BitFit outperforms full fine-tuning in small-data regimes, but the trend reverses as the training set size increases. What that means is that BitFit is particularly useful when it comes to targeted fine-tuning under small-to-mid-sized data conditions.\u003c/p\u003e\n\u003ch2 style=\"font-size: 20px;\"\u003e 5. Why Does BitFit Work? \u003c/h2\u003e\n\u003cp\u003eBitFit\u0026rsquo;s success can be attributed to several key factors that challenge traditional assumptions about fine-tuning large language models. Rather than retraining all parameters, BitFit selectively updates only the bias terms, leading to efficient adaptation without sacrificing performance. But why is this approach effective?\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 5.1 Fine-Tuning as Knowledge Exposure, Not Learning \u003c/h3\u003e\n\u003cp\u003eA crucial insight is that fine-tuning large pre-trained transformers is often less about \u0026ldquo;learning new knowledge\u0026rdquo; and more about \u0026ldquo;exposing\u0026rdquo; the knowledge already embedded in the model. Since transformer-based models like BERT have already learned a vast range of linguistic patterns during their unsupervised pre-training phase, adjusting a small number of parameters—specifically the bias terms—can be enough to bring out task-specific information without reworking the entire model.\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 5.2 Bias Terms and Their Unique Role \u003c/h3\u003e\n\u003cp\u003eBias terms in neural networks serve as offset values, allowing neurons to activate even when input features are zero. Unlike weights, which define relationships between features, bias terms shift outputs in a task-specific manner.\u003c/p\u003e\n\u003cp\u003eBitFit leverages the fact that bias terms interact across layers in a way that can subtly adjust how information flows through the model without needing to modify the main weight matrices. This enables significant changes in task-specific performance with minimal modifications to the model structure.\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 5.3 A Targeted and Structured Approach \u003c/h3\u003e\n\u003cp\u003eNot all bias parameters contribute equally to model adaptation. The study found that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eQuery Biases (bQ)\u003c/strong\u003e: Found in self-attention layers, crucial for determining which words receive attention.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMiddle-Layer MLP Biases (b2)\u003c/strong\u003e: Found in feedforward layers, responsible for transforming hidden representations.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy focusing updates on these specific biases, BitFit achieves near full fine-tuning performance while modifying only 0.04% of model parameters.\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 5.4 The Generalization Advantage \u003c/h3\u003e\n\u003cp\u003eAnother reason BitFit works well is its effect on generalization. Traditional fine-tuning tends to overfit on small datasets because it updates many parameters, potentially memorizing noise rather than learning transferable patterns. BitFit, by contrast, updates only a fraction of the parameters, leading to a smaller generalization gap.\u003c/p\u003e\n\u003cp\u003eIn fact, experiments show that BitFit performs better than full fine-tuning in small-data regimes. This suggests that limiting parameter updates can sometimes lead to better robustness and generalization.\u003c/p\u003e\n\u003ch2 style=\"font-size: 20px;\"\u003e 6. Implications and Future Directions \u003c/h2\u003e\n\u003cp\u003eBitFit opens new avenues for efficient fine-tuning, making it particularly relevant in scenarios where computational resources are limited, such as:\u003c/p\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 6.1 Efficient Deployment in Real-World Applications \u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLow-Resource AI Systems\u003c/strong\u003e: BitFit’s lightweight approach is ideal for deploying NLP models in mobile applications, IoT devices, and embedded AI systems where computational efficiency is critical.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMulti-Task Learning\u003c/strong\u003e: Since only bias terms need updating, multiple tasks can share the same base model, reducing memory overhead and increasing flexibility in production systems.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eScalable NLP Services\u003c/strong\u003e: Cloud-based NLP services that handle multiple tasks (e.g., chatbots, automated translations) can benefit from BitFit by reducing the need to store and load separate models for each task.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 6.2 Understanding Model Adaptation \u003c/h3\u003e\n\u003cp\u003eThe success of BitFit raises deeper questions about the nature of transfer learning and fine-tuning:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDo large transformers truly need full fine-tuning, or is most of their knowledge already latent?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCould bias-only tuning be the key to unlocking efficient continual learning strategies?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAre there other small but critical subsets of parameters that can be updated to achieve similar efficiency gains?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 style=\"font-size: 18px;\"\u003e 6.3 Potential Enhancements \u003c/h3\u003e\n\u003cp\u003eWhile BitFit is a promising step forward, future research could explore:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSelective bias tuning\u003c/strong\u003e: Further analyzing which specific bias terms contribute most to adaptation and whether additional optimization can reduce the number of updates even further.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHybrid approaches\u003c/strong\u003e: Combining BitFit with methods like adapters or LoRA (Low-Rank Adaptation) to achieve even better efficiency.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eApplication to other architectures\u003c/strong\u003e: Investigating whether BitFit’s principles extend beyond BERT to models like GPT, T5, and multimodal transformers.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 style=\"font-size: 20px;\"\u003e 7. Conclusion \u003c/h2\u003e\n\u003cp\u003eIn conclusion, BitFit offers a desirable compromise between effectiveness and efficiency, making it a valuable tool for fine-tuning transformer-based models, especially in resource-constrained environments or with limited amounts of training data. Having the capability to achieve competitive performance using significantly fewer trainable parameters, coupled with its achievement in low data regimes, bodes well for other NLP tasks and applications.\u003c/p\u003e\n\u003cp\u003eBitFit defies the usual wisdom concerning universal fine-tuning by illustrating how slight tweaks in only a very small percentage of model parameters yield high-performance. This efficient approach makes AI models more accessible, scalable, and cost-effective.\u003c/p\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca id=\"#benzaken\"\u003e\u003c/a\u003e \u003ca href=\"https://aclanthology.org/2022.acl-short.1/\"\u003eBitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models\u003c/a\u003e (Ben Zaken et al., ACL 2022)\u003c/li\u003e\n\u003c/ul\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/bitfit/","date_published":"19026-19-09T220:1919:00+01:00","date_modified":"19026-19-09T220:1919:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"ae2f64cac6ac9f6d89cf77b7ba0c9541c49ea196","title":"Axiomatic Explanations for Visual Search, Retrieval and Similarity Learning","summary":"","content_text":" \u003c!DOCTYPE html\u003e Styled Table AXIOMATIC EXPlanATIONS FOR VISUAL SEARCh, RETRIEVAL, AND SIMILARITY LEARNING Authors:Mark Hamilton ${ }^{1,2}$, Scott Lundberg ${ }^{2}$, Stephanie Fu ${ }^{1}$, Lei Zhang ${ }^{2}$, William T. Freeman ${ }^{1,3}$\n${ }^{1}$ MIT, ${ }^{2}$ Microsoft, ${ }^{3}$ Google\nmarkth@mit.edu **Authors of the blogpost**: Yassine Beniguemim and Noureddine BOULLAM. Table of Contents Abstract Introduction Exploring Visual Search Algorithm Explanations First-Order Explanations Unifying First-Order Search Interpretation Techniques Second-Order Explanations A Fast Shapley-Taylor Approximation Kernel Second-Order Search Activation Maps Implementing Second-Order Explanations in Practice Conclusion Abstract Visual search, recommendation, and contrastive similarity learning are pivotal technologies shaping user experiences in the digital age. However, the complexity of modern model architectures often obscures their inner workings, making them challenging to interpret. In our blog, we delve into a groundbreaking paper titled \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al. This paper introduces a novel framework grounded in the theory of fair credit assignment, providing axiomatic solutions that generalize existing explanation techniques and address fairness concerns in recommendation systems. Through our exploration, we aim to demystify the complexities of visual search algorithms, offering readers insights into their operation and implications for various domains.\nFigure 1: Architectures for search engine interpretability. Like classifier explanations, First-order search explanations yield heatmaps of important pixels for similarity (bottom row third column). Second order search interpretation methods yield a dense correspondence between image locations (last two columns). CAM (second column) is a particular case of Shapley value approximation, and we generalize it to yield dense correspondences (last column).\nIntroduction Welcome to our blog, where we embark on a journey to demystify the intricate world of visual search technology. In today\u0026rsquo;s digital age, recommendation systems play a pivotal role in guiding users through a vast sea of information, aiding in everything from online shopping to content discovery.\nYet, behind the scenes, these recommendation engines operate using sophisticated algorithms that can seem like a black box to many users. How do they decide which products to suggest, or which images are most similar to a given query? These questions lie at the heart of our exploration.\nInspired by the groundbreaking paper \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al., we embark on a quest to unravel the inner workings of these recommendation systems. By delving into the concepts of fair credit assignment, Shapley values, and Harsanyi Dividends, we aim to shed light on the underlying principles that govern visual search algorithms.\n1. Exploring Visual Search Algorithm Explanations In our exploration of visual search algorithm explanations, we delve into the fundamental concepts introduced in the paper by Mark Hamilton et al. Our journey begins with an examination of the two distinct classes of explanation methods: \u0026ldquo;first order\u0026rdquo; and \u0026ldquo;second order.\u0026rdquo; First-order approaches focus on highlighting important pixels contributing to object similarity, while second-order explanations provide a comprehensive correspondence between query and retrieved images.\n1.1 First-Order Explanations First-order interpretations are rooted in classifier explainability theory, offering insights into the importance of individual pixels or features in determining object similarity. We explore the theoretical underpinnings of these explanations, drawing parallels to existing techniques such as Class Activation Maps (CAM), GradCAM, and LIME.\nFormalizing First-Order Interpretations The core of first-order explanations lies in the formalization of the value function, typically represented as $v_1(S)$, where $S$ represents subsets of features or pixels. This function allows us to quantify the contribution of each subset to the overall similarity score between query and retrieved images.\n$$ v_1(S): 2^N \\rightarrow \\mathbb{R} := d(x, \\text{mask}(y, S)) $$\n1.2 Unifying First-Order Search Interpretation Techniques Building upon existing classifier explainability methods, we introduce an approach to transform opaque and grey-box classification explainers into search engine explainers. By formalizing the value function and leveraging concepts like Shapley values, we extend existing approaches such as LIME and SHAP to the realm of visual search.\nLeveraging Shapley Values Shapley values provide a principled way to assign credit to individual features or pixels based on their contribution to the similarity function. By applying Shapley values to the search engine context, we can identify the most influential elements in both query and retrieved images.\n$$ \\phi_{v_1}(S) = \\sum_{T: S \\subset T} \\frac{d_v(T)}{\\binom{|T|}{|S|}} $$\n1.3 Second-Order Explanations Moving beyond pixel-level interpretations, we delve into second-order explanations that capture the interactions between areas of query and retrieved images. Drawing inspiration from Harsanyi Dividends and Shapley-Taylor indices, we explore how these concepts generalize to provide richer insights into image similarity.\nUnderstanding Second-Order Interpretations Second-order explanations go beyond individual features to capture the interaction strength between different parts of query and retrieved images. We introduce the concept of Harsanyi Dividends, which provide a detailed view of the function\u0026rsquo;s behavior at every coalition of features.\n$$ d_v(S) = \\begin{cases} v(S) \u0026amp; \\text{if } |S|=1 \\ v(S) - \\sum_{T \\subsetneq S} d_v(T) \u0026amp; \\text{if } |S| \u0026gt; 1 \\end{cases} $$\n1.4 A Fast Shapley-Taylor Approximation Kernel While Harsanyi Dividends and Shapley-Taylor indices offer robust credit assignment mechanisms, their computation can be challenging. We introduce a novel weighting kernel for second-order Shapley-Taylor indices, significantly reducing computational complexity while maintaining accuracy.\nFigure 5: Convergence of Shapley-Taylor estimation schemes with respect to the Mean Squared Error (MSE) on randomly initialized deep networks with 15 dimensional input. Our strategies (Kernel) converge with significantly fewer function evaluations.\nFigure 6: Our Second-order explanation evaluation strategy. A good method should project query objects (top left and middle) to corresponding objects in the retrieved image (bottom left and middle). When censoring all but these shared objects (right column) the search engine should view these images as similar.\nEfficient Computation with Kernel Approximation The proposed weighting kernel allows for efficient approximation of Shapley-Taylor indices, enabling faster computation without sacrificing accuracy. By sampling random coalitions and aggregating information into a weighted quadratic model, we achieve a significant reduction in computational overhead.\n1.5 Second-Order Search Activation Maps Applying the Shapley-Taylor framework, we derive second-order search activation maps, offering dense correspondences between query and retrieved image locations. These maps provide a deeper understanding of image similarity, facilitating more nuanced interpretations of visual search results.\nVisualizing Second-Order Explanations Using the derived Shapley-Taylor indices, we construct matrices representing the interaction strength between query and retrieved image locations. These matrices allow us to visualize how different parts of the query image correspond to parts of the retrieved image, providing intuitive insights into the similarity judgments made by the search algorithm.\nTable 1: Comparison of performance of first- and second-order search explanation methods.\n2. Implementing Second-Order Explanations in Practice With a solid theoretical foundation in place, we now turn our attention to practical implementation steps for incorporating second-order explanations into existing visual search systems.\n2.1 Data Preparation and Preprocessing Before integrating second-order explanations, it\u0026rsquo;s crucial to preprocess and structure the data appropriately. This involves organizing the dataset into query-image pairs, ensuring consistency in image format, resolution, and preprocessing steps such as normalization and resizing.\nData Pipeline Overview We design a robust data pipeline encompassing data loading, preprocessing, and augmentation stages. Leveraging popular libraries like TensorFlow and PyTorch, we streamline the process of preparing the dataset for training and evaluation.\n2.2 Model Modification and Integration To enable the computation of second-order explanations, we modify the existing visual search model architecture. This adaptation involves incorporating additional layers or modules to capture the interactions between query and retrieved images.\nArchitectural Adjustments We introduce novel components such as interaction modules or attention mechanisms to facilitate the computation of second-order explanations. These architectural adjustments enable the model to learn and represent the complex relationships between different regions of query and retrieved images.\n2.3 Training and Evaluation Procedures Training a visual search model with second-order explanations requires careful consideration of training objectives, loss functions, and evaluation metrics. We devise training procedures that optimize both the primary search task and the secondary objective of generating accurate explanations.\nObjective Function Formulation We define a composite objective function that combines the primary search task loss with a regularization term for encouraging meaningful second-order explanations. This formulation ensures that the model learns to balance between search accuracy and explanation fidelity during training.\n2.4 Validation and Interpretation Once trained, we validate the effectiveness of the model\u0026rsquo;s second-order explanations through comprehensive evaluation procedures. This involves qualitative analysis of explanation maps, quantitative assessment of explanation quality, and user studies to gauge the interpretability of the generated explanations.\nEvaluation Metrics We define metrics such as explanation fidelity, coherence, and relevance to quantitatively evaluate the quality of second-order explanations. By comparing against baseline methods and human annotations, we assess the model\u0026rsquo;s ability to capture meaningful interactions between query and retrieved images.\n2.5 Deployment Considerations Deploying a visual search system with second-order explanations requires careful planning and integration into existing infrastructure. We address scalability, latency, and user experience considerations to ensure seamless deployment in real-world applications.\nScalable Inference Architecture We design an inference pipeline optimized for efficient computation of second-order explanations in production environments. This involves leveraging distributed computing frameworks and model optimization techniques to minimize latency and maximize throughput.\n3. Conclusion By following these implementation steps, we bridge the gap between theoretical insights and practical deployment of second-order explanations in visual search systems. Our approach empowers users to gain deeper insights into the underlying mechanisms driving search results, paving the way for more transparent and interpretable AI systems.\nAdditional Resources Video Description: Dive deeper into the concepts with a detailed video overview available here. Code Repository: Access the training and evaluation code to explore the implementation details here. For a comprehensive exploration of the technical details and experimental results, refer to the full paper.\nREFERENCES Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels. Technical report, 2010.\nJiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation with inter-pixel relations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2209-2218, 2019.\nMarco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. In International Conference on Machine Learning, pp. 272-281. PMLR, 2019.\nRobert J Aumann and Lloyd S Shapley. Values of non-atomic games. Princeton University Press, 2015.\nSebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.\nBing. Beyond text queries: Searching with bing visual search, Jun 2017. URL https://aka. ms/AAas 7 jg.\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 12091218, 2018.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.\nHila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 782-791, 2021.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020 b.\nYun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang. Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020c.\n","content_html":"\u003cstyle\nTYPE=\"text/css\"\u003e\n\ncode.has-jax {font:\ninherit;\nfont-size:\n100%; \nbackground: \ninherit; \nborder: \ninherit;}\n\n\u003c/style\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"UTF-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n\u003ctitle\u003eStyled Table\u003c/title\u003e\n\u003cstyle\u003e\n    table {\n        border-collapse: collapse;\n        width: 100%;\n    }\n    th, td {\n        padding: 8px;\n        text-align: center;\n        border-bottom: 1px solid #ddd;\n    }\n    th {\n        background-color: #f2f2f2;\n    }\n    tr:hover {\n        background-color: #f5f5f5;\n    }\n\u003c/style\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAXIOMATIC EXPlanATIONS FOR VISUAL SEARCh, RETRIEVAL, AND SIMILARITY LEARNING \u003c/h1\u003e\n\u003ch1 style=\"font-size: 13px;\"\u003eAuthors:Mark Hamilton ${ }^{1,2}$, Scott Lundberg ${ }^{2}$, Stephanie Fu ${ }^{1}$, Lei Zhang ${ }^{2}$, William T. Freeman ${ }^{1,3}$\u003cbr\u003e${ }^{1}$ MIT, ${ }^{2}$ Microsoft, ${ }^{3}$ Google\u003cbr\u003emarkth@mit.edu\n\u003cbr/\u003e\n**Authors of the blogpost**: Yassine Beniguemim and Noureddine BOULLAM.\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0.0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-0.1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eExploring Visual Search Algorithm Explanations\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1.1\"\u003eFirst-Order Explanations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.2\"\u003eUnifying First-Order Search Interpretation Techniques\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.3\"\u003eSecond-Order Explanations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.4\"\u003eA Fast Shapley-Taylor Approximation Kernel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.5\"\u003eSecond-Order Search Activation Maps\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eImplementing Second-Order Explanations in Practice\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0.0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eVisual search, recommendation, and contrastive similarity learning are pivotal technologies shaping user experiences in the digital age. However, the complexity of modern model architectures often obscures their inner workings, making them challenging to interpret. In our blog, we delve into a groundbreaking paper titled \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al. This paper introduces a novel framework grounded in the theory of fair credit assignment, providing axiomatic solutions that generalize existing explanation techniques and address fairness concerns in recommendation systems. Through our exploration, we aim to demystify the complexities of visual search algorithms, offering readers insights into their operation and implications for various domains.\u003c/p\u003e\n\u003cdiv style=\"display: inline-block; width:\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-02.jpg?height=600\u0026width=1354\u0026top_left_y=282\u0026top_left_x=382\" alt=\"Figure 5\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eFigure 1: Architectures for search engine interpretability. Like classifier explanations, First-order search explanations yield heatmaps of important pixels for similarity (bottom row third column). Second order search interpretation methods yield a dense correspondence between image locations (last two columns). CAM (second column) is a particular case of Shapley value approximation, and we generalize it to yield dense correspondences (last column).\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"section-0.1\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to our blog, where we embark on a journey to demystify the intricate world of visual search technology. In today\u0026rsquo;s digital age, recommendation systems play a pivotal role in guiding users through a vast sea of information, aiding in everything from online shopping to content discovery.\u003c/p\u003e\n\u003cp\u003eYet, behind the scenes, these recommendation engines operate using sophisticated algorithms that can seem like a black box to many users. How do they decide which products to suggest, or which images are most similar to a given query? These questions lie at the heart of our exploration.\u003c/p\u003e\n\u003cp\u003eInspired by the groundbreaking paper \u0026ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING\u0026rdquo; authored by Mark Hamilton et al., we embark on a quest to unravel the inner workings of these recommendation systems. By delving into the concepts of fair credit assignment, Shapley values, and Harsanyi Dividends, we aim to shed light on the underlying principles that govern visual search algorithms.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003e1. Exploring Visual Search Algorithm Explanations\u003c/h2\u003e\n\u003cp\u003eIn our exploration of visual search algorithm explanations, we delve into the fundamental concepts introduced in the paper by Mark Hamilton et al. Our journey begins with an examination of the two distinct classes of explanation methods: \u0026ldquo;first order\u0026rdquo; and \u0026ldquo;second order.\u0026rdquo; First-order approaches focus on highlighting important pixels contributing to object similarity, while second-order explanations provide a comprehensive correspondence between query and retrieved images.\u003c/p\u003e\n\u003ch3 id=\"section-1.1\"\u003e1.1 First-Order Explanations\u003c/h3\u003e\n\u003cp\u003eFirst-order interpretations are rooted in classifier explainability theory, offering insights into the importance of individual pixels or features in determining object similarity. We explore the theoretical underpinnings of these explanations, drawing parallels to existing techniques such as Class Activation Maps (CAM), GradCAM, and LIME.\u003c/p\u003e\n\u003ch4 id=\"formalizing-first-order-interpretations\"\u003eFormalizing First-Order Interpretations\u003c/h4\u003e\n\u003cp\u003eThe core of first-order explanations lies in the formalization of the value function, typically represented as $v_1(S)$, where $S$ represents subsets of features or pixels. This function allows us to quantify the contribution of each subset to the overall similarity score between query and retrieved images.\u003c/p\u003e\n\u003cp\u003e$$\nv_1(S): 2^N \\rightarrow \\mathbb{R} := d(x, \\text{mask}(y, S))\n$$\u003c/p\u003e\n\u003ch3 id=\"section-1.2\"\u003e1.2 Unifying First-Order Search Interpretation Techniques\u003c/h3\u003e\n\u003cp\u003eBuilding upon existing classifier explainability methods, we introduce an approach to transform opaque and grey-box classification explainers into search engine explainers. By formalizing the value function and leveraging concepts like Shapley values, we extend existing approaches such as LIME and SHAP to the realm of visual search.\u003c/p\u003e\n\u003ch4 id=\"leveraging-shapley-values\"\u003eLeveraging Shapley Values\u003c/h4\u003e\n\u003cp\u003eShapley values provide a principled way to assign credit to individual features or pixels based on their contribution to the similarity function. By applying Shapley values to the search engine context, we can identify the most influential elements in both query and retrieved images.\u003c/p\u003e\n\u003cp\u003e$$\n\\phi_{v_1}(S) = \\sum_{T: S \\subset T} \\frac{d_v(T)}{\\binom{|T|}{|S|}}\n$$\u003c/p\u003e\n\u003ch3 id=\"section-1.3\"\u003e1.3 Second-Order Explanations\u003c/h3\u003e\n\u003cp\u003eMoving beyond pixel-level interpretations, we delve into second-order explanations that capture the interactions between areas of query and retrieved images. Drawing inspiration from Harsanyi Dividends and Shapley-Taylor indices, we explore how these concepts generalize to provide richer insights into image similarity.\u003c/p\u003e\n\u003ch4 id=\"understanding-second-order-interpretations\"\u003eUnderstanding Second-Order Interpretations\u003c/h4\u003e\n\u003cp\u003eSecond-order explanations go beyond individual features to capture the interaction strength between different parts of query and retrieved images. We introduce the concept of Harsanyi Dividends, which provide a detailed view of the function\u0026rsquo;s behavior at every coalition of features.\u003c/p\u003e\n\u003cp\u003e$$\nd_v(S) = \\begin{cases} v(S) \u0026amp; \\text{if } |S|=1 \\\nv(S) - \\sum_{T \\subsetneq S} d_v(T) \u0026amp; \\text{if } |S| \u0026gt; 1 \\end{cases}\n$$\u003c/p\u003e\n\u003ch3 id=\"section-1.4\"\u003e1.4 A Fast Shapley-Taylor Approximation Kernel\u003c/h3\u003e\n\u003cp\u003eWhile Harsanyi Dividends and Shapley-Taylor indices offer robust credit assignment mechanisms, their computation can be challenging. We introduce a novel weighting kernel for second-order Shapley-Taylor indices, significantly reducing computational complexity while maintaining accuracy.\u003c/p\u003e\n\u003cdiv style=\"display: inline-block; width: 45%;\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-07.jpg?height=455\u0026width=610\u0026top_left_y=282\u0026top_left_x=405\" alt=\"Figure 5\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eFigure 5: Convergence of Shapley-Taylor estimation schemes with respect to the Mean Squared Error (MSE) on randomly initialized deep networks with 15 dimensional input. Our strategies (Kernel) converge with significantly fewer function evaluations.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv style=\"display: inline-block; width: 45%;\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-07.jpg?height=455\u0026width=656\u0026top_left_y=282\u0026top_left_x=1079\" alt=\"Figure 6\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eFigure 6: Our Second-order explanation evaluation strategy. A good method should project query objects (top left and middle) to corresponding objects in the retrieved image (bottom left and middle). When censoring all but these shared objects (right column) the search engine should view these images as similar.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch4 id=\"efficient-computation-with-kernel-approximation\"\u003eEfficient Computation with Kernel Approximation\u003c/h4\u003e\n\u003cp\u003eThe proposed weighting kernel allows for efficient approximation of Shapley-Taylor indices, enabling faster computation without sacrificing accuracy. By sampling random coalitions and aggregating information into a weighted quadratic model, we achieve a significant reduction in computational overhead.\u003c/p\u003e\n\u003ch3 id=\"section-1.5\"\u003e1.5 Second-Order Search Activation Maps\u003c/h3\u003e\n\u003cp\u003eApplying the Shapley-Taylor framework, we derive second-order search activation maps, offering dense correspondences between query and retrieved image locations. These maps provide a deeper understanding of image similarity, facilitating more nuanced interpretations of visual search results.\u003c/p\u003e\n\u003ch4 id=\"visualizing-second-order-explanations\"\u003eVisualizing Second-Order Explanations\u003c/h4\u003e\n\u003cp\u003eUsing the derived Shapley-Taylor indices, we construct matrices representing the interaction strength between query and retrieved image locations. These matrices allow us to visualize how different parts of the query image correspond to parts of the retrieved image, providing intuitive insights into the similarity judgments made by the search algorithm.\u003c/p\u003e\n\u003cdiv style=\"display: inline-block; width:\"\u003e\n  \u003cimg src=\"https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-08.jpg?height=1003\u0026width=1312\u0026top_left_y=377\u0026top_left_x=404\" alt=\"Figure 5\" width=\"100%\"\u003e\n  \u003cp style=\"text-align: center; font-size: 10px;\"\u003eTable 1: Comparison of performance of first- and second-order search explanation methods.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"section-2\"\u003e2. Implementing Second-Order Explanations in Practice\u003c/h2\u003e\n\u003cp\u003eWith a solid theoretical foundation in place, we now turn our attention to practical implementation steps for incorporating second-order explanations into existing visual search systems.\u003c/p\u003e\n\u003ch3 id=\"section-2.1\"\u003e2.1 Data Preparation and Preprocessing\u003c/h3\u003e\n\u003cp\u003eBefore integrating second-order explanations, it\u0026rsquo;s crucial to preprocess and structure the data appropriately. This involves organizing the dataset into query-image pairs, ensuring consistency in image format, resolution, and preprocessing steps such as normalization and resizing.\u003c/p\u003e\n\u003ch4 id=\"data-pipeline-overview\"\u003eData Pipeline Overview\u003c/h4\u003e\n\u003cp\u003eWe design a robust data pipeline encompassing data loading, preprocessing, and augmentation stages. Leveraging popular libraries like TensorFlow and PyTorch, we streamline the process of preparing the dataset for training and evaluation.\u003c/p\u003e\n\u003ch3 id=\"section-2.2\"\u003e2.2 Model Modification and Integration\u003c/h3\u003e\n\u003cp\u003eTo enable the computation of second-order explanations, we modify the existing visual search model architecture. This adaptation involves incorporating additional layers or modules to capture the interactions between query and retrieved images.\u003c/p\u003e\n\u003ch4 id=\"architectural-adjustments\"\u003eArchitectural Adjustments\u003c/h4\u003e\n\u003cp\u003eWe introduce novel components such as interaction modules or attention mechanisms to facilitate the computation of second-order explanations. These architectural adjustments enable the model to learn and represent the complex relationships between different regions of query and retrieved images.\u003c/p\u003e\n\u003ch3 id=\"section-2.3\"\u003e2.3 Training and Evaluation Procedures\u003c/h3\u003e\n\u003cp\u003eTraining a visual search model with second-order explanations requires careful consideration of training objectives, loss functions, and evaluation metrics. We devise training procedures that optimize both the primary search task and the secondary objective of generating accurate explanations.\u003c/p\u003e\n\u003ch4 id=\"objective-function-formulation\"\u003eObjective Function Formulation\u003c/h4\u003e\n\u003cp\u003eWe define a composite objective function that combines the primary search task loss with a regularization term for encouraging meaningful second-order explanations. This formulation ensures that the model learns to balance between search accuracy and explanation fidelity during training.\u003c/p\u003e\n\u003ch3 id=\"section-2.4\"\u003e2.4 Validation and Interpretation\u003c/h3\u003e\n\u003cp\u003eOnce trained, we validate the effectiveness of the model\u0026rsquo;s second-order explanations through comprehensive evaluation procedures. This involves qualitative analysis of explanation maps, quantitative assessment of explanation quality, and user studies to gauge the interpretability of the generated explanations.\u003c/p\u003e\n\u003ch4 id=\"evaluation-metrics\"\u003eEvaluation Metrics\u003c/h4\u003e\n\u003cp\u003eWe define metrics such as explanation fidelity, coherence, and relevance to quantitatively evaluate the quality of second-order explanations. By comparing against baseline methods and human annotations, we assess the model\u0026rsquo;s ability to capture meaningful interactions between query and retrieved images.\u003c/p\u003e\n\u003ch3 id=\"section-2.5\"\u003e2.5 Deployment Considerations\u003c/h3\u003e\n\u003cp\u003eDeploying a visual search system with second-order explanations requires careful planning and integration into existing infrastructure. We address scalability, latency, and user experience considerations to ensure seamless deployment in real-world applications.\u003c/p\u003e\n\u003ch4 id=\"scalable-inference-architecture\"\u003eScalable Inference Architecture\u003c/h4\u003e\n\u003cp\u003eWe design an inference pipeline optimized for efficient computation of second-order explanations in production environments. This involves leveraging distributed computing frameworks and model optimization techniques to minimize latency and maximize throughput.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e3. Conclusion\u003c/h2\u003e\n\u003cp\u003eBy following these implementation steps, we bridge the gap between theoretical insights and practical deployment of second-order explanations in visual search systems. Our approach empowers users to gain deeper insights into the underlying mechanisms driving search results, paving the way for more transparent and interpretable AI systems.\u003c/p\u003e\n\u003ch2 id=\"additional-resources\"\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eVideo Description\u003c/strong\u003e: Dive deeper into the concepts with a detailed video overview available \u003ca href=\"https://aka.ms/axiomatic-video\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCode Repository\u003c/strong\u003e: Access the training and evaluation code to explore the implementation details \u003ca href=\"https://aka.ms/axiomatic-code\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor a comprehensive exploration of the technical details and experimental results, refer to the \u003ca href=\"https://arxiv.org/pdf/2103.00370.pdf\"\u003efull paper\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"references\"\u003eREFERENCES\u003c/h2\u003e\n\u003cp\u003eRadhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels. Technical report, 2010.\u003c/p\u003e\n\u003cp\u003eJiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation with inter-pixel relations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2209-2218, 2019.\u003c/p\u003e\n\u003cp\u003eMarco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. In International Conference on Machine Learning, pp. 272-281. PMLR, 2019.\u003c/p\u003e\n\u003cp\u003eRobert J Aumann and Lloyd S Shapley. Values of non-atomic games. Princeton University Press, 2015.\u003c/p\u003e\n\u003cp\u003eSebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\u003c/p\u003e\n\u003cp\u003eYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.\u003c/p\u003e\n\u003cp\u003eBing. Beyond text queries: Searching with bing visual search, Jun 2017. URL https://aka. ms/AAas 7 jg.\u003c/p\u003e\n\u003cp\u003eHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 12091218, 2018.\u003c/p\u003e\n\u003cp\u003eMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.\u003c/p\u003e\n\u003cp\u003eHila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 782-791, 2021.\u003c/p\u003e\n\u003cp\u003eTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.\u003c/p\u003e\n\u003cp\u003eXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020 b.\u003c/p\u003e\n\u003cp\u003eYun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang. Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020c.\u003c/p\u003e\n\u003chr\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/axiomatic_explanations/","date_published":"28036-28-09T358:2828:00+01:00","date_modified":"28036-28-09T358:2828:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"6f60dbb8442851a1134c3a218d91373cc4dfd1a8","title":"Privacy Amplification by Decentralization","summary":"","content_text":"Privacy Amplification by Decentralization Author: Sarah ABBANA BENNANI Table of Contents Introduction - the challenge of data privacy Theoretical Aspects on Differential Privacy First Case: walk on a ring Generalisation: walk on a complete graph Experiments Perspectives This is a blogpost about the paper Privacy Amplification by Decentralization, published by E. Cyffers et al. in 2022 and available here.\nIntroduction - the challenge of data privacy In recent years, the concept of privacy has gained significant attention due to the proliferation of data collection practices and the need to safeguard individuals\u0026rsquo; personal information. There has been a notable shift towards implementing regulations to govern the gathering of data from individuals, underscoring the pressing demand for privacy measures that are not only effective and robust against potential attacks but also transparent and firmly grounded in logic and mathematics.\nA current way to define privacy in the context of data sharing is the promess of the dataholder (the person or entity managing the data) towards the users, that there will be no consequences (positive or negative) induced by their consent to sharing their data.\nLet us take a small example to illustrate and to understand the underlying complexity of this notion: we consider an entity that desires to conduct a study on the correlation between smoking and cancer risks. Should a smoker participate, and the study concludes that smoking indeed increases the likelihood of cancer, the repercussions for the smoker could vary.\nSome negative impacts: insurance premiums could increase\nSome positive impacts: motivation to quit smoking\nWe could therefore think that privacy in this case is broken for the participant, however there is a subtility which is one of the keys to capture the nuance between the privacy of an individiual and that of a group. In this case, crucially, we cannot say privacy is breached, as the participation of the smoker should not alter the study\u0026rsquo;s outcome, i.e. from a probabilistic standpoint, whether or not the individual participates in the study will not significantly change the likelihood of the conclusion of the study. Formally, and to introduce some probabilities, which we will delve into further later on:\n$\\mathbb{P}(result=smoking\\ bad | individual\\ participates) \\approx \\mathbb{P}(result=smoking\\ bad | individual\\ does\\ not\\ participate) $ Privacy has become a real challenge for all parties, as it is necesssary to find a balance between the utility of the data and the privacy guarantees of the users.\nFor the dataholder, the aim is to retain the wealth of data to derive useful insights. They must be able to analyse enough of the data to learn about the population without revealing any individual-specific information. For the users, they must believe that their data will be protected and that they will not be hurt by giving them. This trust in the dataholder is important to incite the users to give their data. In this paper, the aim of the authors, E. Cyffers and A. Bellet, was to show some algorithms and methods that allow to improve the privacy-utility trade-offs and therefore reinforce privacy around the data, while keeping scalability.\nThe proposed algorithms are based on full decentralization, and newtork differential privacy (DP), two notions that we will explain right below.\nTheoretical Aspects on Differential Privacy Mathematical context We must introduce some key mathematical definitions to understand the problem we want to tackle.\nUsers space We consider a set of $n$ users (e.g. a population responding to a survey), each holding a private dataset that we note $D_u$ (e.g. their answer to the questions of the survey).\nNeighboring relation We write $D=D_1 \\cup \\cdots \\cup D_n$ the union of all users datasets.\nWe can define a neighboring relation over these datasets, that we call user-level Differential Privacy: For two datasets $D$ and $D\u0026rsquo;$ of the same size, we denote by $D \\sim_u D^{\\prime}$ the fact that $D$ and $D\u0026rsquo;$ are neighbors, in the sense that they only differ on user $u$\u0026rsquo;s data.\nFor example, $D$ and $D\u0026rsquo;$ could be two datasets corresponding to the answers of a survey from 10 users. For nine of these users the answers are the same for the two datasets. But for one user $u$, the answers are different (e.g. in $D$ user $u$ smokes, in $D\u0026rsquo;$ he doesn\u0026rsquo;t smoke).\nThe inuition between this definition relatively to privacy is that compared to traditional differential privacy, which considers changes in individual data points, user-level DP provides stronger privacy guarantees. By hiding the influence of an entire user\u0026rsquo;s dataset, rather than just a single data point, it ensures that individual user contributions are not discernible, thus enhancing overall privacy protection.\nDecentralization We will set ourselves in a fully decentralized system. In this configuration, each user only communicates with a small number of other users at each step, and there is no central coordinator processing all the data. The aim of this setting is to limit the concentration of sensitive information in one place, reducing the risk of data breaches and unauthorized access.\nThe users and their communications are represented by a network (directed or undirected) graph $G = (V, E)$, where $V$ is the users ensemble defined above, and $E$ is the set of edges: $(u, v) \\in E$ indicates that user $u$ can send messages to user $v$.\nIn this case, a randomised decentralized algorithm is defined as a mapping that from a dataset, returns that transcript of all messages exchanges between the users over the network. In formal terms, $A: D \\longmapsto {(u, m, v): u \\text{ sent message with content } m \\text{ to } v }$.\nThe aim of decentralization in this representation, is to give users the fewer information possible, i.e. only the messages they are involved in, and not the full transcript $A(D)$.\nWe introduce this view of a user $u$: $\\mathcal{O}_u(\\mathcal{A}(D))=\\left(\\left(v, m, v^{\\prime}\\right) \\in \\mathcal{A}(D): v=u \\text { or } v^{\\prime}=u\\right)$\nDifferential Privacy We will take a step back on this representation to introduce in a more global way the mathematical notion of Differential Privacy (DP).\nLet us consider a randomised algorithm $M$. $M$ is said to be \u0026ldquo;$\\alpha$-differentially private\u0026rdquo; if, for any event $A$:\n$$\\mathbb{P}[M(D)\\in A]\\leq e^{\\alpha} \\cdot \\mathbb{P}[M(D\u0026rsquo;)\\in A]$$\nwhere $D$ and $D\u0026rsquo;$ are two datasets differing on a single element.\nTo make this more intuitive, a randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm must treat the data so that the output is not overly depend on the data of any one individual.\nLet\u0026rsquo;s consider the event \u0026ldquo;Smoking is correlated to cancer\u0026rdquo;, and $D$ and $D\u0026rsquo;$ differing on the user $u$\u0026rsquo;s data, whether or not that individual that has cancer smokes or not.\nWe can rewrite the definition as: $\\frac{\\mathbb{P}\\left[M\\left(D\\right) \\in A\\right]}{\\mathbb{P}\\left[M\\left(D\u0026rsquo;\\right) \\in A\\right]} \\leq e^{\\alpha}$\nWe can see that $\\alpha$, the privacy factor, represents the lost of privacy:\nWhen $\\alpha \\rightarrow 0$: the two probabilities are equal, meaning that whether user $u$ participates or not to the survey, the result is the same, i.e. privacy is at its maximum, but the statistical utility is null.\nWhen $\\alpha \\rightarrow+\\infty$: there are no constraints on the probabilities and therefore no constraints on privacy.\nThus it is the intermediate case for $\\alpha$ that is the most interesting and that can allow a good trade-off between privacy and utility.\nNetwork Differential Privacy In this paper, the definition used for Differential Privacy is a bit different, actually relaxed as the algorithm is decentralized.\nAn algorithm $A$ is said to be $(\\varepsilon, \\delta)$-network Differentially Private if for all pairs of distinct user $u, v \\in V$ and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\n$$ \\mathbb{P}\\left(\\mathcal{O}_v(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_v\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\nWe can interpret this as the need that the information gathered by $v$ during the execution of $A$ must not depend too much on $u$\u0026rsquo;s data.\nFurthermore, the definition can be extended in the case of collusion between the users, i.e. if multiple individuals collaborate or conspire to exploit or manipulate a system or process for their collective benefit.\nAn algorithm $A$ is $(c, \\varepsilon, \\delta)$-network DP if for each user $u$, all subsets $W \\subset V$ such that $\\left|W\\right| \\leq c$, and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\n$$ \\mathbb{P}\\left(\\mathcal{O}_W(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_W\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\nHere $\\mathcal{O}_W$ represents the aggregated information of the collusion: $\\mathcal{O}_W = \\cup _{w \\in W} \\mathcal{O}_w$.\nDecentralized computation model The algorithms studied in this paper are meant to perform computations by using a token that will walk through the nodes of the network graph. The purpose of the token is to facilitate sequential updates across the nodes in the network. As it traverses through the nodes following the edges of the graph, it carries information and updates its states based on local computations performed at each node from the values obtainable from the corresponding user.\nIf the token $\\tau$ resides at some node $u$, it will be:\nUpdated by: $\\tau \\leftarrow \\tau+x_u^k, \\quad$ with $x_u^k=g^k\\left(\\tau ; D_u\\right)$\nSent to another user $v$ with $(u, v) \\in E$\nHere, $x_u^k$ denotes the contribution of user $u$ to the computation. It depends both on the current value of $\\tau$ and on the number of times $k$ that the token visited $u$ so far.\nThis model of computation allows to optimize the combination of local costs within the network, which is useful for tasks like training machine learning models. The token holds the model\u0026rsquo;s parameters and is updated based on the local information at each point it visits. This decentralized approach can also be used to calculate summaries of data contributed by users, such as finding totals or averages.\nThe idea of the following parts is to study different graph achitectures and computation protocols, based on the formalization explained above, to achieve good utility-privacy trade-offs\nFirst case: walk on a ring We consider here a graph architecture of a directed ring, i.e. $E = {(u, u+1)}_{u=1}^{n-1} \\cup{(n, 1)}$, meaning that the token, starting from the first user, will travel around the ring multiple times, and more precisely go through every user $K$ times.\nThis is a simple case that is meant to show how we can achieve suitable results without relying on a centralised agregator.\nWe are going to explain how this architecture can perform for privacy guarantees on the task of Real Summation, and then on Discrete Histogram Computation.\nReal Summation Each user will contribute a value during each round of the token\u0026rsquo;s journey. The task of real summation aims to estimate the sum of all contributions made by users.\nFor example, we can imagine a scenario where users of a health monitoring app report their daily step counts. The app\u0026rsquo;s goal is to calculate the total number of steps taken by all users, without revealing individual step counts. Each user\u0026rsquo;s daily step count is considered a contribution, and the app needs to aggregate these contributions while preserving user privacy.\nIndeed to preserve privacy in this case, a common method is to add random noise, an abstract perturbation mechanism, which usually consist in a standard Gaussian or Laplace deviation to the contribution. We won\u0026rsquo;t go into further details on the perturbation, but we assume that it satisfies traditional local differntial privacy (LDP).\nFurthermore, here the decentralized protocol proposes to add this noise only once every few hops of the token, and in fact every $n-1$ hops of the token as shown in the algorithm below:\nThey prove the following theorem:\nTheorem: Let $\\varepsilon, \\delta\u0026gt;0$. Algorithm 1 outputs an unbiased estimate of $\\bar{x}$ with standard deviation $\\sqrt{\\left\\lfloor \\frac{Kn}{n - 1} \\right\\rfloor} \\sigma_{\\text{loc}}$, and is $\\sqrt{2K \\ln\\left(\\frac{1}{\\delta\u0026rsquo;}\\right)\\varepsilon}$ $+ K\\epsilon(e^\\varepsilon - 1), K\\delta + \\delta\u0026rsquo;$-network DP for any $\\delta\u0026rsquo; \u0026gt; 0$\nThe Algorithm 1 proposed actually provides a gain on the error of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ compared to a LDP achieving the same privacy guarantees. This means it achieves a similar balance between privacy and utility as a centralized aggregator would, if they itratively aggregated user contributions then perturb the results before sending it to the users, buy here without the need for this centralized party.\nDiscrete Histogram Computation Here we focus on another task that is computing histograms over a discrete domain.\nWith the same example as above, it could be such as counting the frequency of steps in different ranges for a health monitoring app.\nTraditional local differential privacy (LDP) methods use L-ary randomized response, where each user submits their true value with probability $1-\\gamma$ and a random value with probability $\\gamma$. However, in the decentralized approach with a ring network, they propose Algorithm 2. This algorithm randomizes each user\u0026rsquo;s contribution using L-ary randomized response before adding it to the token, which maintains a partial histogram representing the shuffled contributions, thus enhancing privacy through shuffling, as demonstrated in previous studies.\nAs for the case of real summation, a theorem proves that to achieve the same privacy in LDP, it would need $\\sqrt n$ times more random responses, and when achieving the same utility (meaning to fix $\\gamma$), Algorithm 2 provides a gain of privacy of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$.\nWe see that decentralized computation over a ring enables comparable utility to a trusted aggregator by sequentially hiding previous users\u0026rsquo; contributions, without relying on a central server or requiring costly multi-party computation protocols.\nHowever this simple topology presents limitations including vulnerability to collusions, which compromises differential privacy guarantees, and inadequacy for extensions to gradient descent due to the lack of privacy amplification between users with fixed positions in the ring.\nThis is why we shall now consider random walks over a complete graph.\nGeneralisation: walk on a complete graph Random walk on a complete graph assumes the token is randommly sent to a user at each step. The walk consists of fixed-length random walks, ensuring that each user\u0026rsquo;s contributions are random, and their path is concealed, allowing only the messages sent and received to be known by a user.\nReal Summation Algorithm 3 shows the protocol, naturally extended from the ring topology, where each user updates the token with its contribution and a perturbation. The secrecy of the path taken by the token and the aggregations of the contributions between two visits of the token guarantee the network DP property.\nAgain, a theorem proves that asymptotically, network DP offers a privacy amplification of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ over LDP for the same conditions, which aligns with the privacy-utility trade-off of a trusted aggregtor.\nThe same analysis can be done for the discrete histogram computation case.\nStochastic Gradient Descent In this section, we address the challenge of private convex optimization using stochastic gradient descent (SGD). We consider a convex set $(W \\subseteq \\mathbb{R}^d)$ and a collection of convex functions $(f(\\cdot; D_1), \\ldots, f(\\cdot; D_n))$, each associated with a user, being L-Lipschitz and $(\\beta)$-smooth over $(W)$. Our goal is to privately solve the optimization problem to find $(w^*)$ minimizing the average of these functions over $(W)$:\n$$w^* \\in \\arg \\min_{w \\in \\mathcal{W}} \\left( F(w):=\\frac{1}{n} \\sum_{u=1}^n f\\left(w ; D_u\\right) \\right)$$\nThis equation encapsulates various machine learning tasks, such as ridge and logistic regression, and others. This is significant because it addresses the need for private optimization in machine learning, ensuring that sensitive data remains protected while training models on distributed datasets.\nThe algorithm below proposes a method to privately approximate $w^*$, where the token represents the current iterate. At each step, the user $u$ holding the token performs a projected noisy gradient step and sends the updated token to a random user. The variance in the Gaussian mechanism of line 4 is deduced from the Lipschitz property of the functions.\nA theorem based on the evolution of the privacy loss proves the differential privacy guarantees, and again the results are satisfactory. Compared to traditional local differential privacy methods, we obtain a privacy amplification of $O\\left(\\frac{\\ln n}{\\sqrt{n}}\\right)$ for a specific number of iterations, with the same level of privacy-utility trade-off.\nWith a fixed privacy budget and a large number of iteration, the expected error of this algorithm is smaller with this network DP than with LDP.\nCompared to the ring case, this random walk approach has better robustness to collusion, as colluding users can be treated as a single node with adjusted transition probabilities, leading to equivalent privacy guarantees as for non-colluding users.\nExperiments To show the efficiency of the privacy amplification methods explained in this article, some experiments have been made on the complete graph, first for the Real Summation task, and then for Machine Learning with Stochastic Gradient Descent (SGD).\nThe code is available here: Github Link\nReal Summation We reproduced the first experiment from the paper, comparing th analytical bounds of LDP and NDP on the real summation task.\nTo do so, we only need to run the main_a.py and main_b.py files with python from the fig1 folder to display the corresponding figures (a) and (b). It works, for instance, with Python version 3.8, with the prerequisite of having installed the packages numpy and matplotlib, only taking a few seconds to execute.\nIt gives the following results:\nAs we may see from the theoretical bounds, privacy is amplified with network differential privacy over LDP when the numer of users $n$ is greater or equal to 20, with increaingly substancial improvements as $n$ grows.\nIn practice by making some simulations, the gains are even more significant and even for a smaller number of users, as we see in figure (b).\nMachine Learning with SGD For this second experiment, the task is to train a logistic regression model in this decentralized context.\nThe setting of the experiment is:\nUCI Housing dataset (binarized version) Standardized features and normalized data point (to have unit L2 norm and Lipschitz property of the logistic loss) Train/test split of 80% uniformly at random Training set split between $n = 2000$ users (each user has a local dataset of size $8$) The experiment compares three settings for Stochastic Gradient Descent with perturbation:\nCentralized DP-SGD, requiring a trusted curator Local DP-SGD, corresponding to Algorithm 4 with LDP method Network DP-SGD, corresponding to Algorithm 4 with Network DP method, the one of interest We must run the main.py file of folder fig2 with Python to display the results.\nIt is possible to use the command python main.py \u0026ndash;help to show the list of parameters that can be tuned to modify the context of the experiment (the default ones are for $\\varepsilon = 10$ and $\\varepsilon = 1$):\nI had some issues to run this program with my settings (same as for the first experiment).\nThe typer module was missing therefore I had to install it : pip install typer The _intercept_dot function from sklearn.linear_model._logistic couldn\u0026rsquo;t be found either. By checking the sklearn.linear_model.LogisticRegression (which is the public class corresponding to the import here), this function doesn\u0026rsquo;t appear. I wanted to change it with the intercept_ attribute but it didn\u0026rsquo;t fit either. Then by checking the usage of this function in the case, it seemed that it computes a dot product between the model parameters and the input data, taking into account whether an intercept term is included. Therefore I tried to manually code this functionality but unfortunately it didn\u0026rsquo;t give coherent results compared to the paper. Here are the original results from the paper:\nHere, although the number of contributions per user doesn\u0026rsquo;t align with the optimal regime for network DP, the observed privacy amplification surpasses theoretical expectations. By numerically determining the minimum noise level required for theoretical proofs, they demonstrated that Network DP-SGD achieves a privacy-utility trade-off comparable to Centralized DP-SGD across various privacy levels, showcasing significant privacy amplification benefits over Local DP-SGD, especially in scenarios with fewer iterations than typically recommended.\nPerspectives The work presented suggests numerous avenues for exploration. Generalizations to diverse graph structures, incorporating dynamic topologies to reinforce resilience against collusion, and investigating decentralized models beyond our current scope are key directions. Exploring the potential of multiple tokens traversing the graph simultaneously and delving into randomized gossip algorithms offer promising avenues for advancing privacy-preserving techniques. Finally, probing the theoretical limits of network DP and exploring scenarios where users trust nearby peers more could provide insights into refining privacy mechanisms.\n","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003ePrivacy Amplification by Decentralization\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Sarah ABBANA BENNANI \u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction - the challenge of data privacy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eTheoretical Aspects on Differential Privacy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eFirst Case: walk on a ring\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eGeneralisation: walk on a complete graph\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eExperiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003ePerspectives\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr /\u003e\n\u003cp\u003eThis is a blogpost about the paper  Privacy Amplification by Decentralization, published by E. Cyffers et al. in 2022 and available \u003ca href=\"https://proceedings.mlr.press/v151/cyffers22a/cyffers22a.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"section-1\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eIntroduction - the challenge of data privacy\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eIn recent years, the concept of privacy has gained significant attention due to the proliferation of data collection practices and the need to safeguard individuals\u0026rsquo; personal information. \u003cbr\u003e\nThere has been a notable shift towards implementing regulations to govern the gathering of data from individuals, underscoring the pressing demand for privacy measures that are not only effective and robust against potential attacks but also transparent and firmly grounded in logic and mathematics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA current way to define privacy in the context of data sharing is the promess of the dataholder (the person or entity managing the data) towards the users, that there will be no consequences (positive or negative) induced by their consent to sharing their data.\u003c/strong\u003e\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003e\u003cem\u003eLet us take a small example to illustrate and to understand the underlying complexity of this notion: we consider an entity that desires to conduct a study on the correlation between smoking and cancer risks. \u003cbr\u003e\nShould a smoker participate, and the study concludes that smoking indeed increases the likelihood of cancer, the repercussions for the smoker could vary.\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eSome negative impacts: insurance premiums could increase\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eSome positive impacts: motivation to quit smoking\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eWe could therefore think that privacy in this case is broken for the participant, however there is a subtility which is one of the keys to capture the nuance between the privacy of an individiual and that of a group. In this case, crucially, we cannot say privacy is breached, as the participation of the smoker should not alter the study\u0026rsquo;s outcome, i.e. from a probabilistic standpoint, whether or not the individual participates in the study will not significantly change the likelihood of the conclusion of the study. \u003cbr\u003e\nFormally, and to introduce some probabilities, which we will delve into further later on:\u003c/em\u003e\u003c/p\u003e\n\u003ch1 style=\"font-size: 13px;\"\u003e$\\mathbb{P}(result=smoking\\ bad | individual\\ participates) \\approx \\mathbb{P}(result=smoking\\ bad | individual\\ does\\ not\\ participate) $\u003c/h1\u003e\n\u003cbr /\u003e \n\u003cp\u003ePrivacy has become a real challenge for all parties, as \u003cstrong\u003eit is necesssary to find a balance between the utility of the data and the privacy guarantees of the users\u003c/strong\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor the dataholder, the aim is to retain the wealth of data to derive useful insights. They must be able to analyse enough of the data to learn about the population without revealing any individual-specific information.\u003c/li\u003e\n\u003cli\u003eFor the users, they must believe that their data will be protected and that they will not be hurt by giving them. This trust in the dataholder is important to incite the users to give their data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr /\u003e \n\u003cp\u003eIn this paper, the aim of the authors, E. Cyffers and A. Bellet, was to show some algorithms and methods that allow to improve the privacy-utility trade-offs and therefore reinforce privacy around the data, while keeping scalability.\u003c/p\u003e\n\u003cp\u003eThe proposed algorithms are based on \u003cstrong\u003efull decentralization\u003c/strong\u003e, and \u003cstrong\u003enewtork differential privacy (DP)\u003c/strong\u003e, two notions that we will explain right below.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch1 id=\"section-2\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eTheoretical Aspects on Differential Privacy\u003c/h1\u003e\u003c/h1\u003e\n\u003ch2 id=\"mathematical-context\"\u003e\u003ch1 style=\"font-size: 18px;\"\u003eMathematical context\u003c/h1\u003e\u003c/h2\u003e\n\u003cp\u003eWe must introduce some key mathematical definitions to understand the problem we want to tackle.\u003c/p\u003e\n\u003ch2 id=\"users-space\"\u003eUsers space\u003c/h2\u003e\n\u003cp\u003eWe consider a set of $n$ users (e.g. a population responding to a survey), each holding a private dataset that we note $D_u$ (e.g. their answer to the questions of the survey).\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/users-space.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch2 id=\"neighboring-relation\"\u003eNeighboring relation\u003c/h2\u003e\n\u003cp\u003eWe write $D=D_1 \\cup \\cdots \\cup D_n$ the union of all users datasets.\u003c/p\u003e\n\u003cp\u003eWe can define a \u003cstrong\u003eneighboring relation\u003c/strong\u003e over these datasets, that we call user-level Differential Privacy: \u003cbr\u003e\nFor two datasets $D$ and $D\u0026rsquo;$ of the same size, we denote by $D \\sim_u D^{\\prime}$ the fact that $D$ and $D\u0026rsquo;$ are neighbors, in the sense that they only differ on user $u$\u0026rsquo;s data.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFor example, $D$ and $D\u0026rsquo;$ could be two datasets corresponding to the answers of a survey from 10 users. For nine of these users the answers are the same for the two datasets. But for one user $u$, the answers are different (e.g. in $D$ user $u$ smokes, in $D\u0026rsquo;$ he doesn\u0026rsquo;t smoke).\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe inuition between this definition relatively to privacy is that compared to traditional differential privacy, which considers changes in individual data points, user-level DP provides stronger privacy guarantees. By hiding the influence of an entire user\u0026rsquo;s dataset, rather than just a single data point, it ensures that individual user contributions are not discernible, thus enhancing overall privacy protection.\u003c/p\u003e\n\u003ch2 id=\"decentralization\"\u003eDecentralization\u003c/h2\u003e\n\u003cp\u003eWe will set ourselves in a fully decentralized system. In this configuration, each user only communicates with a small number of other users at each step, and there is no central coordinator processing all the data. The aim of this setting is to limit the concentration of sensitive information in one place, reducing the risk of data breaches and unauthorized access.\u003c/p\u003e\n\u003cp\u003eThe users and their communications are represented by a network (directed or undirected) graph $G = (V, E)$, where $V$ is the users ensemble defined above, and $E$ is the set of edges: $(u, v) \\in E$ indicates that user $u$ can send messages to user $v$.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eIn this case, a randomised decentralized algorithm is defined as a mapping that from a dataset, returns that transcript of all messages exchanges between the users over the network. In formal terms, $A: D \\longmapsto {(u, m, v): u \\text{ sent message with content } m \\text{ to } v }$.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eThe aim of decentralization in this representation, is to give users the fewer information possible, i.e. only the messages they are involved in, and not the full transcript $A(D)$.\u003c/p\u003e\n\u003cp\u003eWe introduce this view of a user $u$: $\\mathcal{O}_u(\\mathcal{A}(D))=\\left(\\left(v, m, v^{\\prime}\\right) \\in \\mathcal{A}(D): v=u \\text { or } v^{\\prime}=u\\right)$\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"differential-privacy\"\u003e\u003ch1 style=\"font-size: 18px;\"\u003eDifferential Privacy\u003c/h1\u003e\u003c/h2\u003e\n\u003cp\u003eWe will take a step back on this representation to introduce in a more global way the mathematical notion of Differential Privacy (DP).\u003c/p\u003e\n\u003cp\u003eLet us consider a randomised algorithm $M$. $M$ is said to be \u0026ldquo;$\\alpha$-differentially private\u0026rdquo; if, for any event $A$:\u003c/p\u003e\n\u003cp\u003e$$\\mathbb{P}[M(D)\\in A]\\leq e^{\\alpha} \\cdot \\mathbb{P}[M(D\u0026rsquo;)\\in A]$$\u003c/p\u003e\n\u003cp\u003ewhere $D$ and $D\u0026rsquo;$ are two datasets differing on a single element.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eTo make this more intuitive, a randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm must treat the data so that the output is not overly depend on the data of any one individual.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider the event \u0026ldquo;Smoking is correlated to cancer\u0026rdquo;, and $D$ and $D\u0026rsquo;$ differing on the user $u$\u0026rsquo;s data, whether or not that individual that has cancer smokes or not.\u003c/p\u003e\n\u003cp\u003eWe can rewrite the definition as: $\\frac{\\mathbb{P}\\left[M\\left(D\\right) \\in A\\right]}{\\mathbb{P}\\left[M\\left(D\u0026rsquo;\\right) \\in A\\right]} \\leq e^{\\alpha}$\u003c/p\u003e\n\u003cp\u003eWe can see that $\\alpha$, the privacy factor, represents the lost of privacy:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eWhen $\\alpha \\rightarrow 0$: the two probabilities are equal, meaning that whether user $u$ participates or not to the survey, the result is the same, i.e. privacy is at its maximum, but the statistical utility is null.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhen $\\alpha \\rightarrow+\\infty$: there are no constraints on the probabilities and therefore no constraints on privacy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus it is the intermediate case for $\\alpha$ that is the most interesting and that can allow a good trade-off between privacy and utility.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"network-differential-privacy\"\u003eNetwork Differential Privacy\u003c/h2\u003e\n\u003cp\u003eIn this paper, the definition used for Differential Privacy is a bit different, actually relaxed as the algorithm is decentralized.\u003c/p\u003e\n\u003cp\u003eAn algorithm $A$ is said to be $(\\varepsilon, \\delta)$-network Differentially Private if for all pairs of distinct user $u, v \\in V$ and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\u003c/p\u003e\n\u003cp\u003e$$ \\mathbb{P}\\left(\\mathcal{O}_v(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_v\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\u003c/p\u003e\n\u003cp\u003eWe can interpret this as the need that the information gathered by $v$ during the execution of $A$ must not depend too much on $u$\u0026rsquo;s data.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eFurthermore, the definition can be extended in the case of collusion between the users, i.e. if multiple individuals collaborate or conspire to exploit or manipulate a system or process for their collective benefit.\u003c/p\u003e\n\u003cp\u003eAn algorithm $A$ is $(c, \\varepsilon, \\delta)$-network DP if for each user $u$, all subsets $W \\subset V$ such that $\\left|W\\right| \\leq c$, and all pairs of neighboring datasets $D \\sim_u D^{\\prime}$, we have:\u003c/p\u003e\n\u003cp\u003e$$ \\mathbb{P}\\left(\\mathcal{O}_W(\\mathcal{A}(D))\\right) \\leq e^{\\varepsilon} \\mathbb{P}\\left(\\mathcal{O}_W\\left(\\mathcal{A}\\left(D^{\\prime}\\right)\\right)\\right)+\\delta $$\u003c/p\u003e\n\u003cp\u003eHere $\\mathcal{O}_W$ represents the aggregated information of the collusion: $\\mathcal{O}_W = \\cup _{w \\in W} \\mathcal{O}_w$.\u003c/p\u003e\n \u003cbr /\u003e\n\u003ch2 id=\"decentralized-computation-model\"\u003eDecentralized computation model\u003c/h2\u003e\n\u003cp\u003eThe algorithms studied in this paper are meant to perform computations by using a token that will walk through the nodes of the network graph. The purpose of the token is to facilitate sequential updates across the nodes in the network. As it traverses through the nodes following the edges of the graph, it carries information and updates its states based on local computations performed at each node from the values obtainable from the corresponding user.\u003c/p\u003e\n\u003cp\u003eIf the token $\\tau$ resides at some node $u$, it will be:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUpdated by: $\\tau \\leftarrow \\tau+x_u^k, \\quad$ with $x_u^k=g^k\\left(\\tau ; D_u\\right)$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSent to another user $v$ with $(u, v) \\in E$\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHere, $x_u^k$ denotes the contribution of user $u$ to the computation. It depends both on the current value of $\\tau$ and on the number of times $k$ that the token visited $u$ so far.\u003c/p\u003e\n\u003cp\u003eThis model of computation allows to optimize the combination of local costs within the network, which is useful for tasks like training machine learning models. The token holds the model\u0026rsquo;s parameters and is updated based on the local information at each point it visits. This decentralized approach can also be used to calculate summaries of data contributed by users, such as finding totals or averages.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003e\u003cem\u003eThe idea of the following parts is to study different graph achitectures and computation protocols, based on the formalization explained above, to achieve good utility-privacy trade-offs\u003c/em\u003e\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch1 id=\"section-3\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eFirst case: walk on a ring\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eWe consider here a graph architecture of a directed ring, i.e. $E = {(u, u+1)}_{u=1}^{n-1} \\cup{(n, 1)}$, meaning that the token, starting from the first user, will travel around the ring multiple times, and more precisely go through every user $K$ times.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/walk-on-ring.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis is a simple case that is meant to show how we can achieve suitable results without relying on a centralised agregator.\u003c/p\u003e\n\u003cp\u003eWe are going to explain how this architecture can perform for privacy guarantees on the task of \u003cem\u003eReal Summation\u003c/em\u003e, and then on \u003cem\u003eDiscrete Histogram Computation\u003c/em\u003e.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"real-summation\"\u003eReal Summation\u003c/h2\u003e\n\u003cp\u003eEach user will contribute a value during each round of the token\u0026rsquo;s journey. The task of \u003cem\u003ereal summation\u003c/em\u003e aims to estimate the sum of all contributions made by users.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFor example, we can imagine a scenario where users of a health monitoring app report their daily step counts. The app\u0026rsquo;s goal is to calculate the total number of steps taken by all users, without revealing individual step counts. Each user\u0026rsquo;s daily step count is considered a contribution, and the app needs to aggregate these contributions while preserving user privacy.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIndeed to preserve privacy in this case, a common method is to add random noise, an abstract perturbation mechanism, which usually consist in a standard Gaussian or Laplace deviation to the contribution. We won\u0026rsquo;t go into further details on the perturbation, but we assume that it satisfies traditional local differntial privacy (LDP).\u003c/p\u003e\n\u003cp\u003eFurthermore, here the decentralized protocol proposes to add this noise only once every few hops of the token, and in fact every $n-1$ hops of the token as shown in the algorithm below:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo1.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThey prove the following theorem:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem:\u003c/strong\u003e Let $\\varepsilon, \\delta\u0026gt;0$. Algorithm 1 outputs an unbiased estimate of $\\bar{x}$ with standard deviation $\\sqrt{\\left\\lfloor \\frac{Kn}{n - 1} \\right\\rfloor} \\sigma_{\\text{loc}}$, and is $\\sqrt{2K \\ln\\left(\\frac{1}{\\delta\u0026rsquo;}\\right)\\varepsilon}$ $+ K\\epsilon(e^\\varepsilon - 1), K\\delta + \\delta\u0026rsquo;$-network DP for any $\\delta\u0026rsquo; \u0026gt; 0$\u003c/p\u003e\n\u003cp\u003eThe Algorithm 1 proposed actually provides a gain on the error of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ compared to a LDP achieving the same privacy guarantees. This means it achieves a similar balance between privacy and utility as a centralized aggregator would, if they itratively aggregated user contributions then perturb the results before sending it to the users, buy here without the need for this centralized party.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"discrete-histogram-computation\"\u003eDiscrete Histogram Computation\u003c/h2\u003e\n\u003cp\u003eHere we focus on another task that is computing histograms over a discrete domain.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eWith the same example as above, it could be such as counting the frequency of steps in different ranges for a health monitoring app.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eTraditional local differential privacy (LDP) methods use L-ary randomized response, where each user submits their true value with probability $1-\\gamma$ and a random value with probability $\\gamma$. However, in the decentralized approach with a ring network, they propose Algorithm 2. This algorithm randomizes each user\u0026rsquo;s contribution using L-ary randomized response before adding it to the token, which maintains a partial histogram representing the shuffled contributions, thus enhancing privacy through shuffling, as demonstrated in previous studies.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo2.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs for the case of real summation, a theorem proves that to achieve the same privacy in LDP, it would need $\\sqrt n$ times more random responses, and when achieving the same utility (meaning to fix $\\gamma$), Algorithm 2 provides a gain of privacy of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eWe see that decentralized computation over a ring enables comparable utility to a trusted aggregator by sequentially hiding previous users\u0026rsquo; contributions, without relying on a central server or requiring costly multi-party computation protocols.\u003c/p\u003e\n\u003cp\u003eHowever this simple topology presents limitations including vulnerability to collusions, which compromises differential privacy guarantees, and inadequacy for extensions to gradient descent due to the lack of privacy amplification between users with fixed positions in the ring.\u003c/p\u003e\n\u003cp\u003eThis is why we shall now consider random walks over a complete graph.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch1 id=\"section-4\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eGeneralisation: walk on a complete graph\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eRandom walk on a complete graph assumes the token is randommly sent to a user at each step. The walk consists of fixed-length random walks, ensuring that each user\u0026rsquo;s contributions are random, and their path is concealed, allowing only the messages sent and received to be known by a user.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"real-summation-1\"\u003eReal Summation\u003c/h2\u003e\n\u003cp\u003eAlgorithm 3 shows the protocol, naturally extended from the ring topology, where each user updates the token with its contribution and a perturbation. The secrecy of the path taken by the token and the aggregations of the contributions between two visits of the token guarantee the network DP property.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo3.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAgain, a theorem proves that asymptotically, network DP offers a privacy amplification of $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ over LDP for the same conditions, which aligns with the privacy-utility trade-off of a trusted aggregtor.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eThe same analysis can be done for the discrete histogram computation case.\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"stochastic-gradient-descent\"\u003eStochastic Gradient Descent\u003c/h2\u003e\n\u003cp\u003eIn this section, we address the challenge of private convex optimization using stochastic gradient descent (SGD). We consider a convex set $(W \\subseteq \\mathbb{R}^d)$ and a collection of convex functions $(f(\\cdot; D_1), \\ldots, f(\\cdot; D_n))$, each associated with a user, being L-Lipschitz and $(\\beta)$-smooth over $(W)$. Our goal is to privately solve the optimization problem to find $(w^*)$ minimizing the average of these functions over $(W)$:\u003c/p\u003e\n\u003cp\u003e$$w^* \\in \\arg \\min_{w \\in \\mathcal{W}} \\left( F(w):=\\frac{1}{n} \\sum_{u=1}^n f\\left(w ; D_u\\right) \\right)$$\u003c/p\u003e\n\u003cp\u003eThis equation encapsulates various machine learning tasks, such as ridge and logistic regression, and others. This is significant because it addresses the need for private optimization in machine learning, ensuring that sensitive data remains protected while training models on distributed datasets.\u003c/p\u003e\n\u003cp\u003eThe algorithm below proposes a method to privately approximate $w^*$, where the token represents the current iterate. At each step, the user $u$ holding the token performs a projected noisy gradient step and sends the updated token to a random user. The variance in the Gaussian mechanism of line 4 is deduced from the Lipschitz property of the functions.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/algo4.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eA theorem based on the evolution of the privacy loss proves the differential privacy guarantees, and again the results are satisfactory. Compared to traditional local differential privacy methods, we obtain a privacy amplification of $O\\left(\\frac{\\ln n}{\\sqrt{n}}\\right)$ for a specific number of iterations, with the same level of privacy-utility trade-off.\u003c/p\u003e\n\u003cp\u003eWith a fixed privacy budget and a large number of iteration, the expected error of this algorithm is smaller with this network DP than with LDP.\u003c/p\u003e\n\u003cbr /\u003e\n\u003cp\u003eCompared to the ring case, this random walk approach has better robustness to collusion, as colluding users can be treated as a single node with adjusted transition probabilities, leading to equivalent privacy guarantees as for non-colluding users.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch1 id=\"section-5\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003eExperiments\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eTo show the efficiency of the privacy amplification methods explained in this article, some experiments have been made on the complete graph, first for the Real Summation task, and then for Machine Learning with Stochastic Gradient Descent (SGD).\u003c/p\u003e\n\u003cp\u003eThe code is available here: \u003ca href=\"https://github.com/totilas/privacy-amplification-by-decentralization/tree/main\"\u003eGithub Link\u003c/a\u003e\u003c/h1\u003e\u003c/p\u003e\n\u003cbr /\u003e\n\u003ch2 id=\"real-summation-2\"\u003eReal Summation\u003c/h2\u003e\n\u003cp\u003eWe reproduced the first experiment from the paper, comparing th analytical bounds of LDP and NDP on the real summation task.\u003c/p\u003e\n\u003cp\u003eTo do so, we only need to run the \u003cem\u003emain_a.py\u003c/em\u003e and \u003cem\u003emain_b.py\u003c/em\u003e files with python from the \u003cem\u003efig1\u003c/em\u003e folder to display the corresponding figures (a) and (b). It works, for instance, with Python version 3.8, with the prerequisite of having installed the packages \u003cem\u003enumpy\u003c/em\u003e and \u003cem\u003ematplotlib\u003c/em\u003e, only taking a few seconds to execute.\u003c/p\u003e\n\u003cp\u003eIt gives the following results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/results1.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs we may see from the theoretical bounds, privacy is amplified with network differential privacy over LDP when the numer of users $n$ is greater or equal to 20, with increaingly substancial improvements as $n$ grows.\u003c/p\u003e\n\u003cp\u003eIn practice by making some simulations, the gains are even more significant and even for a smaller number of users, as we see in figure (b).\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch2 id=\"machine-learning-with-sgd\"\u003eMachine Learning with SGD\u003c/h2\u003e\n\u003cp\u003eFor this second experiment, the task is to train a logistic regression model in this decentralized context.\u003c/p\u003e\n\u003cp\u003eThe setting of the experiment is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUCI Housing dataset (binarized version)\u003c/li\u003e\n\u003cli\u003eStandardized features and normalized data point (to have unit L2 norm and Lipschitz property of the logistic loss)\u003c/li\u003e\n\u003cli\u003eTrain/test split of 80% uniformly at random\u003c/li\u003e\n\u003cli\u003eTraining set split between $n = 2000$ users (each user has a local dataset of size $8$)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe experiment compares three settings for Stochastic Gradient Descent with perturbation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCentralized DP-SGD, requiring a trusted curator\u003c/li\u003e\n\u003cli\u003eLocal DP-SGD, corresponding to Algorithm 4 with LDP method\u003c/li\u003e\n\u003cli\u003eNetwork DP-SGD, corresponding to Algorithm 4 with Network DP method, the one of interest\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr/\u003e\n\u003cp\u003eWe must run the \u003cem\u003emain.py\u003c/em\u003e file of folder \u003cem\u003efig2\u003c/em\u003e with Python to display the results.\u003c/p\u003e\n\u003cp\u003eIt is possible to use the command \u003cem\u003epython main.py \u0026ndash;help\u003c/em\u003e to show the list of parameters that can be tuned to modify the context of the experiment (the default ones are for $\\varepsilon = 10$ and $\\varepsilon = 1$):\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/options.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eI had some issues to run this program with my settings (same as for the first experiment).\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe \u003cem\u003etyper\u003c/em\u003e module was missing therefore I had to install it : \u003cem\u003epip install typer\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eThe \u003cem\u003e_intercept_dot\u003c/em\u003e function from \u003cem\u003esklearn.linear_model._logistic\u003c/em\u003e couldn\u0026rsquo;t be found either. By checking the \u003cem\u003esklearn.linear_model.LogisticRegression\u003c/em\u003e (which is the public class corresponding to the import here), this function doesn\u0026rsquo;t appear. I wanted to change it with the \u003cem\u003eintercept_\u003c/em\u003e attribute but it didn\u0026rsquo;t fit either. Then by checking the usage of this function in the case, it seemed that it computes a dot product between the model parameters and the input data, taking into account whether an intercept term is included. Therefore I tried to manually code this functionality but unfortunately it didn\u0026rsquo;t give coherent results compared to the paper.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHere are the original results from the paper:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Sarah_Abbana/results2.png\"\n  alt=\"\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere, although the number of contributions per user doesn\u0026rsquo;t align with the optimal regime for network DP, the observed privacy amplification surpasses theoretical expectations. By numerically determining the minimum noise level required for theoretical proofs, they demonstrated that Network DP-SGD achieves a privacy-utility trade-off comparable to Centralized DP-SGD across various privacy levels, showcasing significant privacy amplification benefits over Local DP-SGD, especially in scenarios with fewer iterations than typically recommended.\u003c/p\u003e\n\u003cbr/\u003e\n\u003ch1 id=\"section-6\"\u003e\u003ch1 style=\"font-size: 20px;\"\u003ePerspectives\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe work presented suggests numerous avenues for exploration. Generalizations to diverse graph structures, incorporating dynamic topologies to reinforce resilience against collusion, and investigating decentralized models beyond our current scope are key directions. Exploring the potential of multiple tokens traversing the graph simultaneously and delving into randomized gossip algorithms offer promising avenues for advancing privacy-preserving techniques. Finally, probing the theoretical limits of network DP and exploring scenarios where users trust nearby peers more could provide insights into refining privacy mechanisms.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e","url":"https://responsible-ai-datascience-ipParis.github.io/posts/privacy-amplification/","date_published":"27036-27-09T35:2727:00+01:00","date_modified":"27036-27-09T35:2727:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"9f7cea94a1198e99d2efbbf0a65bb95637f7f7b0","title":"Robust or Fair","summary":"","content_text":"To be Robust or to be Fair: Towards Fairness in Adversarial Training Authors: Maryem Hajji \u0026 Cément Teulier Table of Contents Abstract Introduction Initial Analysis Previous Studies Theoretical Demonstration Model Fairness Requirements Practical Algorithms Experimentation Conclusion References Abstract This blog post retraces the study conducted in the paper \u0026ldquo;To be Robust or to be Fair: Towards Fairness in Adversarial Training\u0026rdquo; and written by Han Xu, Xiaorui Liu, Yaxin Li, Yaxin Li, Anil K. Jain and Jiliang Tang.\nTheir study is based on a simple observation: while adversarial training has been shown to improve model\u0026rsquo;s robustness, it also introduces several performances disparities among different data groups.\nTo address this issue, the authors present the Fair-Robust-Learning (FRL) framework that aims to reduce such unfairness.\nIntroduction Nowadays, Machine Learning algorithms and Artificial Intelligence are becoming more and more omnipresent in all kinds of jobs. If many of these models are developed to replace human tasks, it is of key importance that they do not reproduce the same mistakes. In fact, human decision making can sometimes be considered \u0026ldquo;unfair\u0026rdquo;, a trait that must not be present in Machine Learning. But as we push our models to be as precise as possible, one question stands out: can we find the good balance between accuracy and equity ?\nDiving into this topic, we focus our study on adversarial training algorithms. Indeed, it has been shown that there is a significant issue in adversarial training for deep neural networks: while such training boosts the model\u0026rsquo;s defenses against adversarial attacks, it unfortunately leads to significant differences in how well the model performs across various types of data. For instance, detailed observations on CIFAR-10 dataset show a non-negligeable difference in the model\u0026rsquo;s performance between \u0026ldquo;car\u0026rdquo; and \u0026ldquo;cat\u0026rdquo; classes (details of this example in our section 1.1).\nThis phenomenon raises concern on concrete topics like the safety of autonomous driving vehicules or facial recognition while also creating ethical problems by discriminating certain classes. To put a word on it, the authors have identified this issue as the robust-fairness problem of adversarial training.\n1. Initial Analysis We recall here the previous studies conducted by the authors that allowed them to identify the existence of the robust-fairness problem.\n1.1 Previous Studies For their first analysis, the authors have decided to study algorithms like the PGD ( Projected Gradient Descent) adversarial training and TRADES ( Theoretically Principled Trade-off between Robustness and Accuracy for Deep Learning ) on the CIFAR-10 dataset. The investigation is made using a PreAct-ResNet18 model structure under specific adversarial attack constraints. The results they obtained are as follows:\nAs we can see, natural training maintains a consistent standard error across classes and a consistent robust error rate when faced with 8/255 PGD attacks. However, in the case of adversarial training, some huge disparities appear. Going back to our introduction\u0026rsquo;s example with \u0026ldquo;cats\u0026rdquo; and \u0026ldquo;cars\u0026rdquo;, we observe that the standard and robust errors for \u0026ldquo;car\u0026rdquo; class ( respectively 6% and 34% ) are significantly lower than those of the \u0026ldquo;cat\u0026rdquo; class ( respectively 33% and 82% ). The results on the TRADES, altough not depicted here, also show some great disparities between certain classes.\nTo support this graphical study, the authors also present statistical evidence of this phenomenom throughout metrics like the Standard Deviation (SD) or the Normalized SD (NSD) of class-wide error. Once again, these metrics reveal that adversarial training indeed results in greater disparities across classes in both standard and robust performance compared to natural training.\nPotential Causes While the authors succeeded in identifying the problem of fairness, they also aimed to understand where it was coming from. From what they observed, it seems that the fairness issue particularly disadvantages classes that are inherently more challenging to classify. Adversarial training in fact tends to increase the standard errors for \u0026ldquo;harder\u0026rdquo; classes (like \u0026ldquo;cat\u0026rdquo;) significantly more than for \u0026ldquo;easier\u0026rdquo; classes (such as \u0026ldquo;car\u0026rdquo;).\n1.2 Theoretical Demonstration From the experiments on the potential causes of the fairness issue, the authors made the following hypotetis: Adversarial training makes hard classes even harder to classify or classify robustly. In this section, we review the theoretical proof of this hypothesis.\nFor this analysis, we place ourselves in the case of a binary classification task, using a mixed Gaussian distribution to create two classes with distinct levels of classification difficulty. Thus, adversarial training does not notably lower the average standard error but it shifts the decision boundary in a way that favours the \u0026rsquo;easier\u0026rsquo; class at the expense of the \u0026lsquo;harder\u0026rsquo; class.\nPrerequisites The classification model, denoted $f$, is a mapping $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ from input data space $\\mathcal{X}$ and output labels $\\mathcal{Y}$ defined as $f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$ with parameters $\\mathbf{w}$ and $b$ The standard error for a classifier $f$ generally is: $R_{\\text{nat}}(f) = \\Pr(f(\\mathbf{x}) \\neq y)$ The robust error for a classifier $f$ generally is: $R_{\\text{rob}}(f) = \\Pr(\\exists \\delta, |\\delta| \\leq \\epsilon, \\text{s.t. } f(\\mathbf{x} + \\delta) \\neq y)$ (the probability of a perturbation existing that would cause the model to produce an incorrect prediction) The standard error conditional on a specific class $\\{Y = y\\}$ is represented by $R_{\\text{nat}}(f; y)$ Theoretical Experiment We generate a simple example of the binary classification task that we presented at the beginning of section 1.2. The data therefore comes from two classes $\\mathcal{Y} = { \\{-1, +1\\}}$, with each class\u0026rsquo; data following a Gaussian distribution $\\mathcal{D}$ centered on $-\\theta$ and $\\theta$ respectively. It is important to specify that there is a $K$-factor difference between the variance of the two classes defined as follows: $\\sigma_{+1} : \\sigma_{-1} = K : 1$ and $K \u0026gt; 1$.\nThe authors then use the theorem stating that:\nTheorem: In the case of a data distribution $D$ like the one above, the optimal linear classifier $f_{\\text{nat}}$ which minimizes the average standard classification error is: $$ f_{\\text{nat}} = \\arg\\min_f \\Pr(f(\\mathbf{x}) \\neq y) $$.\nWith that theorem and after computations, the authors prove that the class \u0026ldquo;$+1$\u0026rdquo; as a larger standard error than the class \u0026ldquo;$-1$\u0026rdquo;.\nOverall, this result shows well that the class \u0026ldquo;$+1$\u0026rdquo;, characterized by a larger variance, tends to be more challenging to classify than the class\u0026quot;$-1$\u0026quot;; a result confirming the hypothesis initially made.\n2. Model In this section, we present the Fair Robust Learning model (FRL).\n2.1 Fairness Requirements The authors introduced the concepts of Equalized Accuracy and Equalized Robustness, emphasizing the importance of providing equal prediction quality and resilience against adversarial attacks across different groups. To achieve this balance, the authors propose a Fair Robust Learning (FRL) strategy. This framework addresses fairness issues in adversarial training by aiming to minimize overall robust error while ensuring fairness constraints are met. They separate robust error into standard error and boundary error, allowing independent solving of the unfairness of both errors. [ref 7]\nThe training objective thus becomes minimizing the sum of standard error and boundary error while adhering to fairness constraints that ensure no significant disparities in error rates among classes. Techniques from prior research are leveraged to optimize boundary errors during training.\n2.2 Practical Algorithms This section explores effective methods to implement and address the challenges outlined in the training objective, such as the Reweight strategy. In order to implement it, Lagrange multipliers are introduced, denoted as $φ = (φ_{nat}^{\\text{i}}, φ_{bndy}^{\\text{i}})$ where each multiplier corresponds to a fairness constraint. These multipliers are non-negative and play a crucial role in the optimization process.\nThe approach involves forming a Lagrangian, represented by the function $L(f, φ)$, which combines the standard error ($R_{\\text{nat}}(f)$) and boundary error ($R_{\\text{bndy}}(f)$) terms along with the fairness constraints. The Lagrangian acts as a guide for the optimization process, helping to balance the trade-off between minimizing errors and satisfying fairness requirements.\n$$ \\scriptsize{ L(f, \\phi) = R_{\\text{nat}}(f) + R_{\\text{bndy}}(f) + \\sum_{i=1}^{Y} \\phi_{\\text{nat}}^i \\left( R_{\\text{nat}}(f, i) - R_{\\text{nat}}(f) - \\tau_1 \\right)^+ + \\sum_{i=1}^{Y} \\phi_{\\text{bndy}}^i \\left( R_{\\text{bndy}}(f, i) - R_{\\text{bndy}}(f) - \\tau_2 \\right)^+ } $$\nThe optimization problem is then framed as a max-min game between the classifier $f$ and the Lagrange multipliers $φ$. The objective is to maximize the fairness constraints while minimizing the Lagrangian function, which encapsulates both standard and boundary errors.\nOn the other hand, the Reweight strategy presents a limitation particularly in mitigating boundary errors for specific classes. While upweighting the cost for standard errors ($R_{\\text{nat}}(f, i)$) can penalize large errors and improve performance for disadvantaged groups, solely upweighting the boundary error ($R_{\\text{bndy}}(f, i)$) for a class doesn\u0026rsquo;t effectively reduce its boundary error.\nTo overcome this challenge, the Remargin strategy introduces an alternative approach by enlarging the perturbation margin ($\\epsilon$) during adversarial training. This strategy is inspired by previous research showing that increasing the margin during adversarial training can enhance a model\u0026rsquo;s robustness against attacks under the current intensity.[ref 8]\nSpecifically, the Remargin strategy involves adjusting the adversarial margin for generating adversarial examples during training, focusing on specific classes where boundary errors are significant. This adjustment aims to improve the robustness of these classes and reduce their large boundary errors ($R_{\\text{bndy}}(f, i)$).\n3. Experimentation In this section, we reproduce the experimental methodology and setup used to evaluate the effectiveness of the proposed Fair Robust Learning (FRL) framework in constructing robust deep neural network (DNN) models.\nFirstly, we train a fairly simple model on the Fashion MNIST dataset, then we test out torchattack\u0026rsquo;s PGD on our naturally trained model, Then we will adversarially train the same architecture to see if we can identify this unfairness.\nAs we can see above, the naturally trained model has low standard error, but high PGD error. The adversarially trained model, in contrast, has a much lower PGD error, but higher standard error, and higher disparity between the classes.\nSecond, we implement the FRL algorithm (Reweight strategy) which formulates the learning problem as a cost-sensitive classification that penalizes those classes which violate fairness. Essentially, we create multipliers that up or down weight the loss of classes based on how fair or unfair they are with respect to the average across all classes.\nThe following is the FRL Algorithm outlined in the paper:\nWe made a setup to run the process 3 times: once with equal alpha values, once with an alpha ratio that favors the natural error, and one with an alpha ratio that favors the boundary error.\nIn accordance with the authors of the paper, we find that the alpha ratio that favors the natural error is successful in preventing the unfairness of the standard error in the model, and does help somewhat with the unfairness of the PGD error. On the other hand, we notice that the algorithm struggles to improve the worst-case boundary error, leading to disparities in robustness performance across different classes.\nConclusion In conclusion, the studied article discusses the development and implementation of Fair Robust Learning (FRL) strategies to address fairness concerns in adversarial training of deep neural networks. The objective of these strategies is to achieve both equalized accuracy and robustness across different classes.\nThe Reweight strategy aims to minimize overall robust error while adhering to fairness constraints by adjusting training weights based on class-wise errors while the Remargin strategy enlarges the perturbation margin during adversarial training to improve robustness and reduce boundary errors.\nFinally, The FRL framework combines these strategies to mitigate fairness issues and improve model performance across various classes. These approaches represent promising steps towards achieving fairness in robust deep learning models.\nReferences [1] Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang1. To be Robust or to be Fair: Towards Fairness in Adversarial Training. 2021.\n[2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. 2014.\n[3] Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y. Fooling a real car with adversarial traffic signs. 2019.\n[4] Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pp. 1528–1540, 2016.\n[5] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n[6] He, H. and Garcia, E. A. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263–1284. 2009.\n[7] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. 2019.\n[8] Tramer, F., Behrmann, J., Carlini, N., Papernot, N., and Ja- ` cobsen, J.-H. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561–9571. PMLR. 2020.\n","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eTo be Robust or to be Fair: Towards Fairness in Adversarial Training\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Maryem Hajji \u0026 Cément Teulier\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eInitial Analysis\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-2.1\"\u003ePrevious Studies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.2\"\u003eTheoretical Demonstration\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eModel\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-3.1\"\u003eFairness Requirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3.2\"\u003ePractical Algorithms\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eExperimentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThis blog post retraces the study conducted in the \u003ca href=\"http://proceedings.mlr.press/v139/xu21b.html\"\u003epaper\u003c/a\u003e \u0026ldquo;To be Robust or to be Fair: Towards Fairness in Adversarial Training\u0026rdquo; and written by Han Xu, Xiaorui Liu, Yaxin Li, Yaxin Li, Anil K. Jain and Jiliang Tang.\u003c/p\u003e\n\u003cp\u003eTheir study is based on a simple observation: while adversarial training has been shown to improve model\u0026rsquo;s robustness, it also introduces several performances disparities among different data groups.\u003c/p\u003e\n\u003cp\u003eTo address this issue, the authors present the Fair-Robust-Learning (FRL) framework that aims to reduce such unfairness.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNowadays, Machine Learning algorithms and Artificial Intelligence are becoming more and more omnipresent in all kinds of jobs. If many of these models are developed to replace human tasks, it is of key importance that they do not reproduce the same mistakes. In fact, human decision making can sometimes be considered \u0026ldquo;unfair\u0026rdquo;, a trait that must not be present in Machine Learning. But as we push our models to be as precise as possible, one question stands out: can we find the good balance between accuracy and equity ?\u003c/p\u003e\n\u003cp\u003eDiving into this topic, we focus our study on adversarial training algorithms.\nIndeed, it has been shown that there is a significant issue in adversarial training for deep neural networks: while such training boosts the model\u0026rsquo;s defenses against adversarial attacks, it unfortunately leads to significant differences in how well the model performs across various types of data.\nFor instance, detailed observations on CIFAR-10 dataset show a non-negligeable difference in the model\u0026rsquo;s performance between \u0026ldquo;car\u0026rdquo; and \u0026ldquo;cat\u0026rdquo; classes (details of this example in our section 1.1).\u003c/p\u003e\n\u003cp\u003eThis phenomenon raises concern on concrete topics like the safety of autonomous driving vehicules or facial recognition while also creating ethical problems by discriminating certain classes.\nTo put a word on it, the authors have identified this issue as the \u003cstrong\u003erobust-fairness\u003c/strong\u003e problem of adversarial training.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003e1. Initial Analysis\u003c/h2\u003e\n\u003cp\u003eWe recall here the previous studies conducted by the authors that allowed them to identify the existence of the robust-fairness problem.\u003c/p\u003e\n\u003ch2 id=\"section-2.1\"\u003e1.1 Previous Studies\u003c/h2\u003e\n\u003cp\u003eFor their first analysis, the authors have decided to study algorithms like the PGD ( Projected Gradient Descent) adversarial training and TRADES ( Theoretically Principled Trade-off between Robustness and Accuracy for Deep Learning ) on the CIFAR-10 dataset.\nThe investigation is made using a PreAct-ResNet18 model structure under specific adversarial attack constraints.\nThe results they obtained are as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Hajji_Teulier/cat_car.png\"\n  alt=\"Paper Initial Results\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs we can see, natural training maintains a consistent standard error across classes and a consistent robust error rate when faced with 8/255 PGD attacks. However, in the case of adversarial training, some huge disparities appear. Going back to our introduction\u0026rsquo;s example with \u0026ldquo;cats\u0026rdquo; and \u0026ldquo;cars\u0026rdquo;, we observe that the standard and robust errors for \u0026ldquo;car\u0026rdquo; class ( respectively 6% and 34% ) are significantly lower than those of the \u0026ldquo;cat\u0026rdquo; class ( respectively 33% and 82% ). The results on the TRADES, altough not depicted here, also show some great disparities between certain classes.\u003c/p\u003e\n\u003cp\u003eTo support this graphical study, the authors also present statistical evidence of this phenomenom throughout metrics like the Standard Deviation (SD) or the Normalized SD (NSD) of class-wide error. Once again, these metrics reveal that adversarial training indeed results in greater disparities across classes in both standard and robust performance compared to natural training.\u003c/p\u003e\n\u003ch3 id=\"potential-causes\"\u003ePotential Causes\u003c/h3\u003e\n\u003cp\u003eWhile the authors succeeded in identifying the problem of fairness, they also aimed to understand where it was coming from. From what they observed, it seems that the fairness issue particularly disadvantages classes that are inherently more challenging to classify. Adversarial training in fact tends to increase the standard errors for \u0026ldquo;harder\u0026rdquo; classes (like \u0026ldquo;cat\u0026rdquo;) significantly more than for \u0026ldquo;easier\u0026rdquo; classes (such as \u0026ldquo;car\u0026rdquo;).\u003c/p\u003e\n\u003ch2 id=\"section-2.2\"\u003e1.2 Theoretical Demonstration\u003c/h2\u003e\n\u003cp\u003eFrom the experiments on the potential causes of the fairness issue, the authors made the following hypotetis: Adversarial training makes hard classes even harder to classify or classify robustly.\nIn this section, we review the theoretical proof of this hypothesis.\u003c/p\u003e\n\u003cp\u003eFor this analysis, we place ourselves in the case of a binary classification task, using a mixed Gaussian distribution to create two classes with distinct levels of classification difficulty. Thus, adversarial training does not notably lower the average standard error but it shifts the decision boundary in a way that favours the \u0026rsquo;easier\u0026rsquo; class at the expense of the \u0026lsquo;harder\u0026rsquo; class.\u003c/p\u003e\n\u003ch3 id=\"prerequisites\"\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003eclassification model\u003c/strong\u003e, denoted $f$, is a mapping  $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ from input data space $\\mathcal{X}$ and output labels $\\mathcal{Y}$ defined as $f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$ with parameters $\\mathbf{w}$ and $b$\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003estandard error\u003c/strong\u003e for a classifier $f$ generally is: $R_{\\text{nat}}(f) = \\Pr(f(\\mathbf{x}) \\neq y)$\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003erobust error\u003c/strong\u003e for a classifier $f$ generally is: $R_{\\text{rob}}(f) = \\Pr(\\exists \\delta, |\\delta| \\leq \\epsilon, \\text{s.t. } f(\\mathbf{x} + \\delta) \\neq y)$ (the probability of a perturbation existing that would cause the model to produce an incorrect prediction)\u003c/li\u003e\n\u003cli\u003eThe standard error \u003cstrong\u003econditional\u003c/strong\u003e on a specific class $\\{Y = y\\}$ is represented by $R_{\\text{nat}}(f; y)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"theoretical-experiment\"\u003eTheoretical Experiment\u003c/h3\u003e\n\u003cp\u003eWe generate a simple example of the binary classification task that we presented at the beginning of section 1.2.\nThe data therefore comes from two classes $\\mathcal{Y} = { \\{-1, +1\\}}$, with each class\u0026rsquo; data following a Gaussian distribution $\\mathcal{D}$ centered on $-\\theta$ and $\\theta$ respectively.\nIt is important to specify that there is a $K$-factor difference between the variance of the two classes defined as follows: $\\sigma_{+1} : \\sigma_{-1} = K : 1$ and $K \u0026gt; 1$.\u003c/p\u003e\n\u003cp\u003eThe authors then use the theorem stating that:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem:\u003c/strong\u003e In the case of a data distribution $D$ like the one above, the optimal linear classifier $f_{\\text{nat}}$ which minimizes the average standard classification error is:\n$$ f_{\\text{nat}} = \\arg\\min_f \\Pr(f(\\mathbf{x}) \\neq y) $$.\u003c/p\u003e\n\u003cp\u003eWith that theorem and after computations, the authors prove that the class \u0026ldquo;$+1$\u0026rdquo; as a larger standard error than the class \u0026ldquo;$-1$\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eOverall, this result shows well that the class \u0026ldquo;$+1$\u0026rdquo;, characterized by a larger variance, tends to be more challenging to classify than the class\u0026quot;$-1$\u0026quot;; a result confirming the hypothesis initially made.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e2. Model\u003c/h2\u003e\n\u003cp\u003eIn this section, we present the Fair Robust Learning model (FRL).\u003c/p\u003e\n\u003ch2 id=\"section-3.1\"\u003e2.1 Fairness Requirements\u003c/h2\u003e\n\u003cp\u003eThe authors introduced the concepts of Equalized Accuracy and Equalized Robustness, emphasizing the importance of providing equal prediction quality and resilience against adversarial attacks across different groups.\nTo achieve this balance, the authors propose a Fair Robust Learning (FRL) strategy.\nThis framework addresses fairness issues in adversarial training by aiming to minimize overall robust error while ensuring fairness constraints are met. They separate robust error into standard error and boundary error, allowing independent  solving of the unfairness of both errors. [ref 7]\u003c/p\u003e\n\u003cp\u003eThe training objective thus becomes minimizing the sum of standard error and boundary error while adhering to fairness constraints that ensure no significant disparities in error rates among classes. Techniques from prior research are leveraged to optimize boundary errors during training.\u003c/p\u003e\n\u003ch2 id=\"section-3.2\"\u003e2.2 Practical Algorithms\u003c/h2\u003e\n\u003cp\u003eThis section explores effective methods to implement and address the challenges outlined in the training objective, such as the Reweight strategy.\nIn order to implement it, Lagrange multipliers are introduced, denoted as $φ = (φ_{nat}^{\\text{i}}, φ_{bndy}^{\\text{i}})$ where each multiplier corresponds to a fairness constraint. These multipliers are non-negative and play a crucial role in the optimization process.\u003c/p\u003e\n\u003cp\u003eThe approach involves forming a Lagrangian, represented by the function $L(f, φ)$, which combines the standard error ($R_{\\text{nat}}(f)$) and boundary error ($R_{\\text{bndy}}(f)$) terms along with the fairness constraints. The Lagrangian acts as a guide for the optimization process, helping to balance the trade-off between minimizing errors and satisfying fairness requirements.\u003c/p\u003e\n\u003cp\u003e$$\n\\scriptsize{\nL(f, \\phi) = R_{\\text{nat}}(f) + R_{\\text{bndy}}(f) + \\sum_{i=1}^{Y} \\phi_{\\text{nat}}^i \\left( R_{\\text{nat}}(f, i) - R_{\\text{nat}}(f) - \\tau_1 \\right)^+ + \\sum_{i=1}^{Y} \\phi_{\\text{bndy}}^i \\left( R_{\\text{bndy}}(f, i) - R_{\\text{bndy}}(f) - \\tau_2 \\right)^+\n}\n$$\u003c/p\u003e\n\u003cp\u003eThe optimization problem is then framed as a max-min game between the classifier $f$ and the Lagrange multipliers $φ$. The objective is to maximize the fairness constraints while minimizing the Lagrangian function, which encapsulates both standard and boundary errors.\u003c/p\u003e\n\u003cp\u003eOn the other hand, the Reweight strategy presents a limitation particularly in mitigating boundary errors for specific classes. While upweighting the cost for standard errors ($R_{\\text{nat}}(f, i)$) can penalize large errors and improve performance for disadvantaged groups, solely upweighting the boundary error ($R_{\\text{bndy}}(f, i)$) for a class doesn\u0026rsquo;t effectively reduce its boundary error.\u003c/p\u003e\n\u003cp\u003eTo overcome this challenge, the Remargin strategy introduces an alternative approach by enlarging the perturbation margin ($\\epsilon$) during adversarial training. This strategy is inspired by previous research showing that increasing the margin during adversarial training can enhance a model\u0026rsquo;s robustness against attacks under the current intensity.[ref 8]\u003c/p\u003e\n\u003cp\u003eSpecifically, the Remargin strategy involves adjusting the adversarial margin for generating adversarial examples during training, focusing on specific classes where boundary errors are significant. This adjustment aims to improve the robustness of these classes and reduce their large boundary errors ($R_{\\text{bndy}}(f, i)$).\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e3. Experimentation\u003c/h2\u003e\n\u003cp\u003eIn this section, we reproduce the experimental methodology and setup used to evaluate the effectiveness of the proposed Fair Robust Learning (FRL) framework in constructing robust deep neural network (DNN) models.\u003c/p\u003e\n\u003cp\u003eFirstly, we train a fairly simple model on the Fashion MNIST dataset, then we test out torchattack\u0026rsquo;s PGD on our naturally trained model, Then we will adversarially train the same architecture to see if we can identify this unfairness.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Hajji_Teulier/result1.png\"\n  alt=\"Paper Initial Results\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs we can see above, the naturally trained model has low standard error, but high PGD error. The adversarially trained model, in contrast, has a much lower PGD error, but higher standard error, and higher disparity between the classes.\u003c/p\u003e\n\u003cp\u003eSecond, we implement the FRL algorithm (Reweight strategy) which formulates the learning problem as a cost-sensitive classification that penalizes those classes which violate fairness. Essentially, we create multipliers that up or down weight the loss of classes based on how fair or unfair they are with respect to the average across all classes.\u003c/p\u003e\n\u003cp\u003eThe following is the FRL Algorithm outlined in the paper:\u003c/p\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n  \u003cimg src=\"/images/Hajji_Teulier/algo1.png\" alt=\"Paper Initial Results\" width=\"400\" /\u003e\n\u003c/div\u003e\n\u003cp\u003eWe made a setup to run the process 3 times: once with equal alpha values, once with an alpha ratio that favors the natural error, and one with an alpha ratio that favors the boundary error.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Hajji_Teulier/result2.png\"\n  alt=\"Paper Initial Results\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn accordance with the authors of the paper, we find that the alpha ratio that favors the natural error is successful in preventing the unfairness of the standard error in the model, and does help somewhat with the unfairness of the PGD error. On the other hand, we notice that the algorithm struggles to improve the worst-case boundary error, leading to disparities in robustness performance across different classes.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn conclusion, the studied article discusses the development and implementation of Fair Robust Learning (FRL) strategies to address fairness concerns in adversarial training of deep neural networks. The objective of these strategies is to achieve both equalized accuracy and robustness across different classes.\u003c/p\u003e\n\u003cp\u003eThe Reweight strategy aims to minimize overall robust error while adhering to fairness constraints by adjusting training weights based on class-wise errors while the Remargin strategy enlarges the perturbation margin during adversarial training to improve robustness and reduce boundary errors.\u003c/p\u003e\n\u003cp\u003eFinally, The FRL framework combines these strategies to mitigate fairness issues and improve model performance across various classes. These approaches represent promising steps towards achieving fairness in robust deep learning models.\u003c/p\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1]  Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang1. To be Robust or to be Fair: Towards Fairness in Adversarial Training. 2021.\u003c/p\u003e\n\u003cp\u003e[2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. 2014.\u003c/p\u003e\n\u003cp\u003e[3]  Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y. Fooling a real car with adversarial traffic signs. 2019.\u003c/p\u003e\n\u003cp\u003e[4]  Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pp. 1528–1540, 2016.\u003c/p\u003e\n\u003cp\u003e[5] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\u003c/p\u003e\n\u003cp\u003e[6] He, H. and Garcia, E. A. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263–1284. 2009.\u003c/p\u003e\n\u003cp\u003e[7] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. 2019.\u003c/p\u003e\n\u003cp\u003e[8] Tramer, F., Behrmann, J., Carlini, N., Papernot, N., and Ja- ` cobsen, J.-H. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561–9571. PMLR. 2020.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/robust-or-fair/","date_published":"27036-27-09T337:2727:00+01:00","date_modified":"27036-27-09T337:2727:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"a5cd2d184b6a0acff2b0c7ff52e9a7dbf67c5d3b","title":"XCM, an explainable CNN for MTS classficiation","summary":"","content_text":" XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification Authors : Nicolas SAINT \u0026 Matthis Guérin Table of Contents 1. Introduction 2. Related Work 3. XCM 4. Evaluation 5. Results 6. Implementation 7. Conclusion Appendix References This is a blog post about the article \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo; published by Kevin Fauvel et al. in 2021 and available here.\n1. Introduction The classification of multivariate time series (MTS) has emerged as an increasingly important research area over the last decade, driven by the exponential growth of temporal data across various domains such as finance, healthcare, mobility, and natural disaster prediction . A time series is a sequence of real values ordered in time, and when a set of co-evolving series is recorded simultaneously by a set of sensors, it is referred to as an MTS. MTS classification, which involves learning the relationship between an MTS and its label, presents a significant challenge due to the inherent complexity of the multivariate and temporal nature of the data.\nTraditional approaches to MTS classification, while effective on large datasets, encounter significant limitations such as poor generalization on small datasets and a lack of explainability, which can limit their adoption in sensitive applications where understanding the model\u0026rsquo;s decisions is crucial . For example, the European GDPR regulation highlights the importance of providing meaningful explanations for automated decisions, emphasizing the need for approaches capable of reconciling performance and explainability .\n2. Related Work The existing literature on MTS classification can be broadly grouped into three main categories: similarity-based methods, feature-based methods, and deep learning approaches.\nSimilarity-based methods: These methods utilize similarity measures to compare two MTS. Dynamic Time Warping (DTW) combined with the nearest neighbor rule (k-NN) has shown impressive performance, although it is not without limitations, particularly in terms of computational cost and the absence of an explicit feature representation.\nFeature-based methods: Approaches such as shapelets and Bag-of-Words (BoW) models transform time series into a more manageable feature space. WEASEL+MUSE, for instance, uses a symbolic Fourier approximation to create a BoW representation of MTS, enabling efficient classification using logistic regression.\nDeep learning approaches: The advent of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks has opened new avenues for MTS classification, thanks to their ability to automatically learn complex data representations. MLSTM-FCN, combining LSTM and CNN, has been identified as one of the top-performing models, despite its complexity and difficulty in providing explanations for its decisions.\nExplainability of MTS classification models has become a major concern, particularly for critical applications. Post-hoc methods, such as LIME and SHAP, offer ways to generate explanations for black-box models, but these explanations may lack fidelity to the model\u0026rsquo;s internal workings. This underscores the need for approaches that inherently integrate explainability into the model design.\nIn this context, our work presents XCM, an innovative convolutional neural network architecture for MTS classification, that not only outperforms existing approaches in terms of performance but also provides reliable and intuitive explanations for its predictions, directly addressing the challenges of performance and explainability in MTS classification. This approach is grounded on the foundational work presented in the paper \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo;, which offers a novel solution to the pressing needs in the domain of MTS classification.\n3. XCM Architecture\nXCM\u0026rsquo;s architecture is specifically designed to efficiently address the challenge of multivariate time series (MTS) classification by simultaneously extracting relevant information about observed variables and time directly from the input data. This unique approach allows XCM to capture the complexity and inherent interactions within MTS, thereby enhancing its generalization capability across different datasets and its applicability in various application contexts.\nTo achieve this, XCM employs a combination of parallel 2D and 1D convolution filters. The 2D filters focus on extracting features related to observed variables at each time instant, while the 1D filters capture temporal dynamics across all variables.\n2D Convolution Formula for Observed Variables: $$A^{(k)} = f(W^{(k)} * X + b^{(k)})$$\n$A^{(k)}$: représente la carte des caractéristiques activées pour le k-ème filtre. $f$: denotes the activation function, often ReLU, to introduce non-linearity. $W^{(k)}$, $b^{(k)}$: weights and bias of the $k$-th 2D convolution filter. $X$: the input MTS data. $*$: the convolution operation. By extracting features in this manner, XCM is able to detect complex patterns in MTS that are crucial for precise series classification.\n1D Convolution Formula for Temporal Information: $$M^{(k)} = f(W^{(k)} \\circledast X + b^{(k)})$$\n$M^{(k)}$: the activated feature map resulting from 1D filters. $\\circledast$: the 1D convolution operation focusing on the temporal dimension. This dual convolution approach enables XCM to maintain high accuracy while offering a better understanding of the contributions of different variables and temporal dynamics to the final decision.\nExplainability\nOne of the hallmark features of the XCM architecture is its inherent capability to provide explainable predictions, leveraging the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. Grad-CAM produces heatmaps that highlight the regions of the input data that most significantly contribute to a specific class prediction. This feature is crucial for applications where understanding the model\u0026rsquo;s reasoning is as important as the prediction accuracy itself.\nGrad-CAM Calculation\nGrad-CAM utilizes the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the input for predicting the concept. This method allows the visualization of which parts of the input are considered important by the CNN for classification tasks.\nThe calculation involves the following steps:\nFeature Map Extraction: Firstly, the feature maps $A^{(k)}$ are extracted from the last convolutional layer. These feature maps are essentially the output of the convolution operations and contain the spatial information that the network has learned to identify.\nGradient Calculation: The gradients of the score for class $c$, denoted as $y^c$ , with respect to the feature map activations $A^{(k)}$ of a convolutional layer, are computed. These gradients are pooled across the width and height dimensions (indexed by $i$ and $j$) to obtain the neuron importance weights $\\alpha_k^c$.\nThe weights for the feature map activations are computed as follows:\n$$\\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^{(k)}}$$ where $Z$ is the number of pixels in the feature map, and $y^c$ is the score for class $c$, before the softmax layer.\nWeighted Combination of Feature Maps: The weighted combination of feature maps, followed by a ReLU, gives the Grad-CAM heatmap $L_{\\text{Grad-CAM}}^c$ : $$L_{\\text{Grad-CAM}}^c = \\text{ReLU}\\left(\\sum_k \\alpha_k^c A^{(k)}\\right)$$\nThis equation combines the feature maps of the last convolutional layer of the network with the neuron importance weights to produce a heatmap for each class. The ReLU function is applied to the linear combination of maps to only consider the features that have a positive influence on the class of interest, effectively highlighting the regions of the input that are important for predicting class $c$.\nThis process elucidates how certain input features contribute to the model\u0026rsquo;s predictions, offering a layer of transparency that can aid in the validation and trust-building of machine learning models in critical applications. The ability to generate such visual explanations not only helps in understanding the model\u0026rsquo;s behavior but also in identifying potential biases or errors in the learning process.\nIn summary, the explainability aspect of XCM, powered by Grad-CAM, stands out as a significant advancement in making deep learning models more interpretable and trustworthy, especially in domains where decision-making processes need to be transparent and justifiable.\n4. Evaluation The evaluation of the XCM model focuses on its performance across various datasets from the UEA multivariate time series classification archive. The datasets are diverse, spanning different types such as motion, ECG, HAR (Human Activity Recognition), AS (Audio Spectra), and EEG/MEG (Electroencephalogram/Magnetoencephalogram), with varying lengths, dimensions, and number of classes. This diversity presents a rigorous challenge and a comprehensive platform to assess the capabilities of XCM.\nHere\u0026rsquo;s an exemple of datasets table used ine the paper:\nTable: Datasets Overview from UEA Archive\nDatasets Type Train Test Length Dimensions Classes Articulary Word Recognition Motion 275 300 144 9 25 Atrial Fibrillation ECG 15 15 640 2 3 Basic Motions HAR 40 40 100 6 4 Character Trajectories Motion 1422 1436 182 3 20 Cricket HAR 108 72 1197 6 12 Duck Duck Geese AS 60 40 270 1345 5 Eigen Worms Motion 128 131 17984 6 5 Epilepsy HAR 137 138 206 3 4 Ering HAR 30 30 65 4 6 Ethanol Concentration Other 261 263 1751 3 4 Face Detection EEG/MEG 5890 3524 62 144 2 Finger Movements EEG/MEG 316 100 50 28 2 Interpretation and Results:\nEach dataset presents unique challenges for MTS classification:\nArticulary Word Recognition: With a substantial number of classes (25), the model must discern between intricate motion patterns. A high accuracy score here would indicate XCM\u0026rsquo;s ability to manage high-dimensional, complex pattern recognition tasks.\nAtrial Fibrillation: Given the high length of the time series (640) and fewer instances for training and testing, the model\u0026rsquo;s performance can signal its efficiency in overfitting prevention and extracting meaningful information from lengthy sequences with minimal data.\nBasic Motions: A dataset like this with a shorter length and moderate dimensionality can showcase XCM\u0026rsquo;s quick learning capability for simple temporal patterns and basic human activities.\nCharacter Trajectories: This dataset, with a large training set and many classes, is an excellent test of XCM\u0026rsquo;s scalability and classification robustness in handling motion data.\nCricket: Long sequences (1197) and a fair number of classes (12) make this dataset suited for evaluating XCM\u0026rsquo;s temporal pattern learning and generalization over longer periods.\nDuck Duck Geese: An Audio Spectrum dataset with a high dimensionality challenges the model to process and classify complex audio patterns, testing XCM\u0026rsquo;s ability in handling non-motion data.\nEigen Worms: With the longest sequences in the given datasets (17,984), XCM\u0026rsquo;s performance can be interpreted as its capability in modeling highly intricate temporal behaviors.\nEpilepsy: Human activity recognition data like this one requires the model to be sensitive to subtle variations, a good indicator of XCM\u0026rsquo;s precision in critical classification scenarios.\nEring: Small datasets with higher class counts test the model\u0026rsquo;s overfitting resilience and classification dexterity.\nEthanol Concentration: An \u0026lsquo;Other\u0026rsquo; type dataset with long sequences will challenge any classifier\u0026rsquo;s ability to handle diverse, non-standard data.\nFace Detection: This EEG/MEG dataset has a significant number of instances for both training and testing, focusing on XCM\u0026rsquo;s performance in biometric pattern recognition scenarios.\nFinger Movements: Another EEG/MEG dataset, but with shorter sequences and fewer dimensions, this can highlight how well XCM captures rapid, subtle changes in electrical activity related to movements.\nHyperparameters and Metrics\nIn the evaluation of XCM, a systematic approach was taken to optimize hyperparameters for each dataset. A grid search was employed, where the hyperparameters were fine-tuned to achieve the best average accuracy. This process was underpinned by a stratified 5-fold cross-validation on the training set, ensuring a robust estimation of the model\u0026rsquo;s performance.\nTo benchmark against other classifiers, the primary metric used was classification accuracy. This metric is standard for evaluating MTS classifiers on the public UEA datasets. Furthermore, classifiers were ranked based on their performance, with the number of wins or ties noted to establish a comparative landscape of classifier effectiveness.\nBeyond accuracy, a critical difference diagram was used to provide a visual statistical comparison of multiple classifiers across multiple datasets. This method uses the nonparametric Friedman test to highlight performance disparities. For the implementation of this statistical test, the R package scmamp was utilized, which is a recognized tool for such analyses in the machine learning community.\nThese rigorous evaluation methods ensure that the performance assessment of XCM is both comprehensive and reliable, offering clear insights into its classification capabilities and its standing relative to existing MTS classifiers.\nFor our research paper based on the XCM method and its performance on various datasets, here’s how we could approach Section 5, which covers the analysis and interpretation of results:\n5. Results The performance of the XCM method was rigorously evaluated across a comprehensive set of UEA datasets with a focus on multivariate time series classification. Our approach aimed to balance between achieving high classification accuracy and providing explainability. This section discusses the performance of XCM compared to other leading algorithms such as MLSTM-FCN (MF), WEASEL+MUSE (WM), and Elastic Distances (ED) with DTW independent (DWI) and dependent (DWD) variants.\nTable: Performance Comparison on UEA Datasets\nDatasets XC XC Seq MC MF WM ED (n) DWI DWD (XC Params) Batch Win % Articulary Word Recognition 98.3 92.7 92.3 98.6 99.3 97.0 98.0 98.7 32 80 Atrial Fibrillation 46.7 33.3 33.3 20.0 26.7 26.7 26.7 20.0 1 60 Basic Motions 100.0 100.0 100.0 100.0 100.0 67.6 100.0 97.5 32 20 Character Trajectories 99.5 98.8 97.4 99.3 99.0 96.4 96.9 99.0 32 80 Cricket 100.0 93.1 90.3 98.6 98.6 98.6 100.0 94.4 32 100 Duck Duck Geese 70.0 52.5 65.0 67.5 57.5 27.5 55.0 60.0 8 80 Eigen Worms 43.5 45.0 41.9 80.9 89.0 55.0 60.3 61.8 32 40 Epilepsy 99.3 93.5 94.9 96.4 99.3 66.7 97.8 96.4 32 20 Ering 13.3 13.3 13.3 13.3 13.3 13.3 13.3 13.3 32 80 Ethanol Concentration 34.6 31.6 30.8 31.6 29.3 29.3 30.4 32.3 32 80 Face Detection 63.9 63.8 50.0 57.4 54.5 51.9 51.3 52.9 32 60 Finger Movements 60.0 60.0 49.0 61.0 54.0 55.0 52.0 53.0 32 40 (Note: \u0026ldquo;XC\u0026rdquo; denotes the accuracy of XCM, \u0026ldquo;XC Seq\u0026rdquo; denotes the accuracy of XCM with sequential layers, \u0026ldquo;MC\u0026rdquo; represents MTEX-CNN, \u0026ldquo;MF\u0026rdquo; denotes MLSTM-FCN, \u0026ldquo;WM\u0026rdquo; stands for WEASEL+MUSE, \u0026ldquo;ED (n)\u0026rdquo; represents Elastic Distance (normalized), \u0026ldquo;DWI\u0026rdquo; and \u0026ldquo;DWD\u0026rdquo; refer to Dynamic Time Warping independent and dependent, respectively. \u0026ldquo;Win %\u0026rdquo; indicates the percentage of times XCM achieved the highest accuracy across all folds.)\nInterpretation of Results\nArticulary Word Recognition: XCM achieved a high accuracy of 98.3%, showcasing its robustness in motion-based classification and indicating its effectiveness in handling complex time series data with a high dimensional space.\nAtrial Fibrillation: This dataset posed a challenge with lower accuracy across all methods. XCM\u0026rsquo;s performance at 46.7% suggests that while challenging, it has the potential to discern patterns in smaller and more complex ECG datasets.\nBasic Motions: XCM perfected the score, highlighting its proficiency in recognizing basic human activity patterns, a crucial capability for HAR applications.\nCharacter Trajectories: The high score of 99.5% reflects XCM\u0026rsquo;s strength in managing datasets with numerous classes, reinforcing its scalability for extensive data.\nCricket: A perfect score of 100.0% emphasizes XCM\u0026rsquo;s ability to capture intricate temporal patterns, suggesting its suitability for complex HAR scenarios.\nDuck Duck Geese: XCM\u0026rsquo;s performance at 70.0% accuracy indicates a significant capability in audio spectrum data classification, a testament to its adaptability to different data types.\nEigen Worms: Despite the lower score, XCM\u0026rsquo;s handling of the longest sequences among the datasets indicates its potential to model complex temporal behaviors in motion data.\nEpilepsy: An accuracy of 99.3% portrays XCM\u0026rsquo;s precision and reliability in critical classification scenarios, essential for medical applications.\nEring: The universally low scores across methods reflect the dataset\u0026rsquo;s complexity, underscoring a need for specialized approaches or additional features to aid classification.\nEthanol Concentration: Although challenging, XCM\u0026rsquo;s relatively higher score suggests its capacity to filter meaningful information from noisy data.\nFace Detection: XCM\u0026rsquo;s ability to handle biometric patterns is evidenced by its performance, indicating its utility in EEG/MEG data interpretation.\nFinger Movements: The moderate score reflects the complexity of the task but also suggests XCM\u0026rsquo;s capability to capture rapid changes in EEG/MEG datasets associated with movements.\nThe \u0026ldquo;Win %\u0026rdquo; column indicates the superiority of XCM in most datasets, which combined with its explainability features, positions it as a preferred choice for MTS classification in practical applications. This comprehensive analysis not only confirms the effectiveness of the XCM approach but also guides future advancements and potential improvements.\nDiscussion\nThe results underscore the effectiveness of XCM in multivariate time series classification across a variety of domains, highlighting its capability to maintain high accuracy even in datasets with challenging characteristics. Moreover, the high win percentage indicates XCM\u0026rsquo;s robustness as it frequently outperforms other methods. It is crucial to note that beyond accuracy, XCM\u0026rsquo;s design enables it to offer a layer of explainability which is not captured by accuracy metrics alone but is invaluable in practical applications.\n6. Implementation We decided to implement ourselves the XCM model using this GitHub Repository on a dataset used in the original paper : BasiMotions.\nThe code of the XCM model is shown in the Appendix.\nHere are the results we obtained for a 5 fold training :\nDataset Model_Name Batch_Size Window_Size Fold Accuracy_Train Accuracy_Validation Accuracy_Test Accuracy_Test_Full_Train BasicMotions XCM 32 20 1 0.90625 0.75 0.825 1.0 BasicMotions XCM 32 20 2 1.0 1.0 0.925 1.0 BasicMotions XCM 32 20 3 1.0 1.0 0.925 1.0 BasicMotions XCM 32 20 4 1.0 0.875 0.9 1.0 BasicMotions XCM 32 20 5 0.78125 0.875 0.825 1.0 We then analyzed with a graph the evolution of both accuaries with regard to the epochs. The model is thus perfoming really well as explained in the paper.\nOne of the main improvment of XCM is his explainalibily of the features which can be explicitly shown with layer activations features map. Here is the one we extracted from the model we trained on BasicMotions dataset.\n7. Conclusion The XCM approach signifies a substantial step forward in MTS classification, achieving high accuracy while providing explainability of features which is indispensable for applications demanding transparency in AI decision-making. The paper suggests that future work may focus on refining hyperparameters automatically and exploring the fusion of XCM with other modalities for richer data representation and classification.\nAppendix Implementation of the XCM model with Keras\nReferences Fauvel, K.; Lin, T.; Masson, V.; Fromont, É.; Termier, A. XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification. Mathematics 2021, 9, 3137. DOI: 10.3390/math9233137\nLi, J.; Rong, Y.; Meng, H.; Lu, Z.; Kwok, T.; Cheng, H. TATC: Predicting Alzheimer’s Disease with Actigraphy Data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, London, UK, 19–23 August 2018.\nJiang, R.; Song, X.; Huang, D.; Song, X.; Xia, T.; Cai, Z.; Wang, Z.; Kim, K.; Shibasaki, R. DeepUrbanEvent: A System for Predicting Citywide Crowd Dynamics at Big Events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Anchorage, AK, USA, 4–8 August 2019.\nFauvel, K.; Balouek-Thomert, D.; Melgar, D.; Silva, P.; Simonet, A.; Antoniu, G.; Costan, A.; Masson, V.; Parashar, M.; Rodero, I.; et al. A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, NY, USA, 7–12 February 2020.\nKarim, F.; Majumdar, S.; Darabi, H.; Harford, S. Multivariate LSTM-FCNs for Time Series Classification. Neural Netw. 2019, 116, 237–245. [CrossRef] [PubMed]\nSchäfer, P.; Leser, U. Multivariate Time Series Classification with WEASEL+MUSE. arXiv 2017, arXiv:1711.11343.\nBagnall, A.; Lines, J.; Keogh, E. The UEA Multivariate Time Series Classification Archive, 2018. arXiv 2018, arXiv:1811.00075.\n","content_html":"\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eXCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u003c/h1\u003e\n\u003ch3 style=\"font-size: 24px;\"\u003eAuthors : Nicolas SAINT \u0026 Matthis Guérin\u003c/h3\u003e\n\u003ch4 style=\"font-size: 22px;\"\u003eTable of Contents\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#1-introduction\"\u003e1. Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#2-related-work\"\u003e2. Related Work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#3-xcm\"\u003e3. XCM\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#4-evaluation\"\u003e4. Evaluation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#5-results\"\u003e5. Results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#6-implementation\"\u003e6. Implementation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#7-conclusion\"\u003e7. Conclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#appendix\"\u003eAppendix\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the article \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo; published by Kevin Fauvel et al. in 2021 and available \u003ca href=\"https://www.mdpi.com/2227-7390/9/23/3137\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"1-introduction\"\u003e1. Introduction\u003c/h3\u003e\n\u003cp\u003eThe classification of multivariate time series (MTS) has emerged as an increasingly important research area over the last decade, driven by the exponential growth of temporal data across various domains such as finance, healthcare, mobility, and natural disaster prediction . A time series is a sequence of real values ordered in time, and when a set of co-evolving series is recorded simultaneously by a set of sensors, it is referred to as an MTS. MTS classification, which involves learning the relationship between an MTS and its label, presents a significant challenge due to the inherent complexity of the multivariate and temporal nature of the data.\u003c/p\u003e\n\u003cp\u003eTraditional approaches to MTS classification, while effective on large datasets, encounter significant limitations such as poor generalization on small datasets and a lack of explainability, which can limit their adoption in sensitive applications where understanding the model\u0026rsquo;s decisions is crucial . For example, the European GDPR regulation highlights the importance of providing meaningful explanations for automated decisions, emphasizing the need for approaches capable of reconciling performance and explainability .\u003c/p\u003e\n\u003ch3 id=\"2-related-work\"\u003e2. Related Work\u003c/h3\u003e\n\u003cp\u003eThe existing literature on MTS classification can be broadly grouped into three main categories: similarity-based methods, feature-based methods, and deep learning approaches.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSimilarity-based methods\u003c/strong\u003e: These methods utilize similarity measures to compare two MTS. Dynamic Time Warping (DTW) combined with the nearest neighbor rule (k-NN) has shown impressive performance, although it is not without limitations, particularly in terms of computational cost and the absence of an explicit feature representation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFeature-based methods\u003c/strong\u003e: Approaches such as shapelets and Bag-of-Words (BoW) models transform time series into a more manageable feature space. WEASEL+MUSE, for instance, uses a symbolic Fourier approximation to create a BoW representation of MTS, enabling efficient classification using logistic regression.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDeep learning approaches\u003c/strong\u003e: The advent of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks has opened new avenues for MTS classification, thanks to their ability to automatically learn complex data representations. MLSTM-FCN, combining LSTM and CNN, has been identified as one of the top-performing models, despite its complexity and difficulty in providing explanations for its decisions.\u003c/p\u003e\n\u003cp\u003eExplainability of MTS classification models has become a major concern, particularly for critical applications. Post-hoc methods, such as LIME and SHAP, offer ways to generate explanations for black-box models, but these explanations may lack fidelity to the model\u0026rsquo;s internal workings. This underscores the need for approaches that inherently integrate explainability into the model design.\u003c/p\u003e\n\u003cp\u003eIn this context, our work presents XCM, an innovative convolutional neural network architecture for MTS classification, that not only outperforms existing approaches in terms of performance but also provides reliable and intuitive explanations for its predictions, directly addressing the challenges of performance and explainability in MTS classification. This approach is grounded on the foundational work presented in the paper \u0026ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\u0026rdquo;, which offers a novel solution to the pressing needs in the domain of MTS classification.\u003c/p\u003e\n\u003ch3 id=\"3-xcm\"\u003e3. XCM\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eXCM\u0026rsquo;s architecture is specifically designed to efficiently address the challenge of multivariate time series (MTS) classification by simultaneously extracting relevant information about observed variables and time directly from the input data. This unique approach allows XCM to capture the complexity and inherent interactions within MTS, thereby enhancing its generalization capability across different datasets and its applicability in various application contexts.\u003c/p\u003e\n\u003cp\u003eTo achieve this, XCM employs a combination of parallel 2D and 1D convolution filters. The 2D filters focus on extracting features related to observed variables at each time instant, while the 1D filters capture temporal dynamics across all variables.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2D Convolution Formula for Observed Variables\u003c/strong\u003e: $$A^{(k)} = f(W^{(k)} * X + b^{(k)})$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$A^{(k)}$: représente la carte des caractéristiques activées pour le k-ème filtre.\u003c/li\u003e\n\u003cli\u003e$f$: denotes the activation function, often ReLU, to introduce non-linearity.\u003c/li\u003e\n\u003cli\u003e$W^{(k)}$, $b^{(k)}$: weights and bias of the $k$-th 2D convolution filter.\u003c/li\u003e\n\u003cli\u003e$X$: the input MTS data.\u003c/li\u003e\n\u003cli\u003e$*$: the convolution operation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy extracting features in this manner, XCM is able to detect complex patterns in MTS that are crucial for precise series classification.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1D Convolution Formula for Temporal Information\u003c/strong\u003e: $$M^{(k)} = f(W^{(k)} \\circledast X + b^{(k)})$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$M^{(k)}$: the activated feature map resulting from 1D filters.\u003c/li\u003e\n\u003cli\u003e$\\circledast$: the 1D convolution operation focusing on the temporal dimension.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis dual convolution approach enables XCM to maintain high accuracy while offering a better understanding of the contributions of different variables and temporal dynamics to the final decision.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/Architecture_XCM.png\"\n  alt=\"alt text\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplainability\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOne of the hallmark features of the XCM architecture is its inherent capability to provide explainable predictions, leveraging the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. Grad-CAM produces heatmaps that highlight the regions of the input data that most significantly contribute to a specific class prediction. This feature is crucial for applications where understanding the model\u0026rsquo;s reasoning is as important as the prediction accuracy itself.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGrad-CAM Calculation\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGrad-CAM utilizes the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the input for predicting the concept. This method allows the visualization of which parts of the input are considered important by the CNN for classification tasks.\u003c/p\u003e\n\u003cp\u003eThe calculation involves the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFeature Map Extraction\u003c/strong\u003e: Firstly, the feature maps $A^{(k)}$ are extracted from the last convolutional layer. These feature maps are essentially the output of the convolution operations and contain the spatial information that the network has learned to identify.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGradient Calculation\u003c/strong\u003e: The gradients of the score for class $c$, denoted as $y^c$\n, with respect to the feature map activations $A^{(k)}$ of a convolutional layer, are computed. These gradients are pooled across the width and height dimensions (indexed by $i$ and $j$) to obtain the neuron importance weights $\\alpha_k^c$.\u003c/p\u003e\n\u003cp\u003eThe weights for the feature map activations are computed as follows:\u003c/p\u003e\n\u003cp\u003e$$\\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^{(k)}}$$ where $Z$ is the number of pixels in the feature map, and $y^c$ is the score for class $c$, before the softmax layer.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWeighted Combination of Feature Maps\u003c/strong\u003e: The weighted combination of feature maps, followed by a ReLU, gives the Grad-CAM heatmap $L_{\\text{Grad-CAM}}^c$ :\n$$L_{\\text{Grad-CAM}}^c = \\text{ReLU}\\left(\\sum_k \\alpha_k^c A^{(k)}\\right)$$\u003c/p\u003e\n\u003cp\u003eThis equation combines the feature maps of the last convolutional layer of the network with the neuron importance weights to produce a heatmap for each class. The ReLU function is applied to the linear combination of maps to only consider the features that have a positive influence on the class of interest, effectively highlighting the regions of the input that are important for predicting class $c$.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis process elucidates how certain input features contribute to the model\u0026rsquo;s predictions, offering a layer of transparency that can aid in the validation and trust-building of machine learning models in critical applications. The ability to generate such visual explanations not only helps in understanding the model\u0026rsquo;s behavior but also in identifying potential biases or errors in the learning process.\u003c/p\u003e\n\u003cp\u003eIn summary, the explainability aspect of XCM, powered by Grad-CAM, stands out as a significant advancement in making deep learning models more interpretable and trustworthy, especially in domains where decision-making processes need to be transparent and justifiable.\u003c/p\u003e\n\u003ch3 id=\"4-evaluation\"\u003e4. Evaluation\u003c/h3\u003e\n\u003cp\u003eThe evaluation of the XCM model focuses on its performance across various datasets from the UEA multivariate time series classification archive. The datasets are diverse, spanning different types such as motion, ECG, HAR (Human Activity Recognition), AS (Audio Spectra), and EEG/MEG (Electroencephalogram/Magnetoencephalogram), with varying lengths, dimensions, and number of classes. This diversity presents a rigorous challenge and a comprehensive platform to assess the capabilities of XCM.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s an exemple of datasets table used ine the paper:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTable: Datasets Overview from UEA Archive\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDatasets\u003c/th\u003e\n          \u003cth\u003eType\u003c/th\u003e\n          \u003cth\u003eTrain\u003c/th\u003e\n          \u003cth\u003eTest\u003c/th\u003e\n          \u003cth\u003eLength\u003c/th\u003e\n          \u003cth\u003eDimensions\u003c/th\u003e\n          \u003cth\u003eClasses\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eArticulary Word Recognition\u003c/td\u003e\n          \u003ctd\u003eMotion\u003c/td\u003e\n          \u003ctd\u003e275\u003c/td\u003e\n          \u003ctd\u003e300\u003c/td\u003e\n          \u003ctd\u003e144\u003c/td\u003e\n          \u003ctd\u003e9\u003c/td\u003e\n          \u003ctd\u003e25\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAtrial Fibrillation\u003c/td\u003e\n          \u003ctd\u003eECG\u003c/td\u003e\n          \u003ctd\u003e15\u003c/td\u003e\n          \u003ctd\u003e15\u003c/td\u003e\n          \u003ctd\u003e640\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasic Motions\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCharacter Trajectories\u003c/td\u003e\n          \u003ctd\u003eMotion\u003c/td\u003e\n          \u003ctd\u003e1422\u003c/td\u003e\n          \u003ctd\u003e1436\u003c/td\u003e\n          \u003ctd\u003e182\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCricket\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e108\u003c/td\u003e\n          \u003ctd\u003e72\u003c/td\u003e\n          \u003ctd\u003e1197\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003e12\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDuck Duck Geese\u003c/td\u003e\n          \u003ctd\u003eAS\u003c/td\u003e\n          \u003ctd\u003e60\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n          \u003ctd\u003e270\u003c/td\u003e\n          \u003ctd\u003e1345\u003c/td\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEigen Worms\u003c/td\u003e\n          \u003ctd\u003eMotion\u003c/td\u003e\n          \u003ctd\u003e128\u003c/td\u003e\n          \u003ctd\u003e131\u003c/td\u003e\n          \u003ctd\u003e17984\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEpilepsy\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e137\u003c/td\u003e\n          \u003ctd\u003e138\u003c/td\u003e\n          \u003ctd\u003e206\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEring\u003c/td\u003e\n          \u003ctd\u003eHAR\u003c/td\u003e\n          \u003ctd\u003e30\u003c/td\u003e\n          \u003ctd\u003e30\u003c/td\u003e\n          \u003ctd\u003e65\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEthanol Concentration\u003c/td\u003e\n          \u003ctd\u003eOther\u003c/td\u003e\n          \u003ctd\u003e261\u003c/td\u003e\n          \u003ctd\u003e263\u003c/td\u003e\n          \u003ctd\u003e1751\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFace Detection\u003c/td\u003e\n          \u003ctd\u003eEEG/MEG\u003c/td\u003e\n          \u003ctd\u003e5890\u003c/td\u003e\n          \u003ctd\u003e3524\u003c/td\u003e\n          \u003ctd\u003e62\u003c/td\u003e\n          \u003ctd\u003e144\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinger Movements\u003c/td\u003e\n          \u003ctd\u003eEEG/MEG\u003c/td\u003e\n          \u003ctd\u003e316\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n          \u003ctd\u003e50\u003c/td\u003e\n          \u003ctd\u003e28\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eInterpretation and Results:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eEach dataset presents unique challenges for MTS classification:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eArticulary Word Recognition\u003c/strong\u003e: With a substantial number of classes (25), the model must discern between intricate motion patterns. A high accuracy score here would indicate XCM\u0026rsquo;s ability to manage high-dimensional, complex pattern recognition tasks.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAtrial Fibrillation\u003c/strong\u003e: Given the high length of the time series (640) and fewer instances for training and testing, the model\u0026rsquo;s performance can signal its efficiency in overfitting prevention and extracting meaningful information from lengthy sequences with minimal data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBasic Motions\u003c/strong\u003e: A dataset like this with a shorter length and moderate dimensionality can showcase XCM\u0026rsquo;s quick learning capability for simple temporal patterns and basic human activities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCharacter Trajectories\u003c/strong\u003e: This dataset, with a large training set and many classes, is an excellent test of XCM\u0026rsquo;s scalability and classification robustness in handling motion data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCricket\u003c/strong\u003e: Long sequences (1197) and a fair number of classes (12) make this dataset suited for evaluating XCM\u0026rsquo;s temporal pattern learning and generalization over longer periods.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDuck Duck Geese\u003c/strong\u003e: An Audio Spectrum dataset with a high dimensionality challenges the model to process and classify complex audio patterns, testing XCM\u0026rsquo;s ability in handling non-motion data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEigen Worms\u003c/strong\u003e: With the longest sequences in the given datasets (17,984), XCM\u0026rsquo;s performance can be interpreted as its capability in modeling highly intricate temporal behaviors.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEpilepsy\u003c/strong\u003e: Human activity recognition data like this one requires the model to be sensitive to subtle variations, a good indicator of XCM\u0026rsquo;s precision in critical classification scenarios.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEring\u003c/strong\u003e: Small datasets with higher class counts test the model\u0026rsquo;s overfitting resilience and classification dexterity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEthanol Concentration\u003c/strong\u003e: An \u0026lsquo;Other\u0026rsquo; type dataset with long sequences will challenge any classifier\u0026rsquo;s ability to handle diverse, non-standard data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFace Detection\u003c/strong\u003e: This EEG/MEG dataset has a significant number of instances for both training and testing, focusing on XCM\u0026rsquo;s performance in biometric pattern recognition scenarios.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFinger Movements\u003c/strong\u003e: Another EEG/MEG dataset, but with shorter sequences and fewer dimensions, this can highlight how well XCM captures rapid, subtle changes in electrical activity related to movements.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eHyperparameters and Metrics\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn the evaluation of XCM, a systematic approach was taken to optimize hyperparameters for each dataset. A grid search was employed, where the hyperparameters were fine-tuned to achieve the best average accuracy. This process was underpinned by a stratified 5-fold cross-validation on the training set, ensuring a robust estimation of the model\u0026rsquo;s performance.\u003c/p\u003e\n\u003cp\u003eTo benchmark against other classifiers, the primary metric used was classification accuracy. This metric is standard for evaluating MTS classifiers on the public UEA datasets. Furthermore, classifiers were ranked based on their performance, with the number of wins or ties noted to establish a comparative landscape of classifier effectiveness.\u003c/p\u003e\n\u003cp\u003eBeyond accuracy, a critical difference diagram was used to provide a visual statistical comparison of multiple classifiers across multiple datasets. This method uses the nonparametric Friedman test to highlight performance disparities. For the implementation of this statistical test, the R package scmamp was utilized, which is a recognized tool for such analyses in the machine learning community.\u003c/p\u003e\n\u003cp\u003eThese rigorous evaluation methods ensure that the performance assessment of XCM is both comprehensive and reliable, offering clear insights into its classification capabilities and its standing relative to existing MTS classifiers.\u003c/p\u003e\n\u003cp\u003eFor our research paper based on the XCM method and its performance on various datasets, here’s how we could approach Section 5, which covers the analysis and interpretation of results:\u003c/p\u003e\n\u003ch3 id=\"5-results\"\u003e5. Results\u003c/h3\u003e\n\u003cp\u003eThe performance of the XCM method was rigorously evaluated across a comprehensive set of UEA datasets with a focus on multivariate time series classification. Our approach aimed to balance between achieving high classification accuracy and providing explainability. This section discusses the performance of XCM compared to other leading algorithms such as MLSTM-FCN (MF), WEASEL+MUSE (WM), and Elastic Distances (ED) with DTW independent (DWI) and dependent (DWD) variants.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTable: Performance Comparison on UEA Datasets\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDatasets\u003c/th\u003e\n          \u003cth\u003eXC\u003c/th\u003e\n          \u003cth\u003eXC Seq\u003c/th\u003e\n          \u003cth\u003eMC\u003c/th\u003e\n          \u003cth\u003eMF\u003c/th\u003e\n          \u003cth\u003eWM\u003c/th\u003e\n          \u003cth\u003eED (n)\u003c/th\u003e\n          \u003cth\u003eDWI\u003c/th\u003e\n          \u003cth\u003eDWD\u003c/th\u003e\n          \u003cth\u003e(XC Params) Batch\u003c/th\u003e\n          \u003cth\u003eWin %\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eArticulary Word Recognition\u003c/td\u003e\n          \u003ctd\u003e98.3\u003c/td\u003e\n          \u003ctd\u003e92.7\u003c/td\u003e\n          \u003ctd\u003e92.3\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e97.0\u003c/td\u003e\n          \u003ctd\u003e98.0\u003c/td\u003e\n          \u003ctd\u003e98.7\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAtrial Fibrillation\u003c/td\u003e\n          \u003ctd\u003e46.7\u003c/td\u003e\n          \u003ctd\u003e33.3\u003c/td\u003e\n          \u003ctd\u003e33.3\u003c/td\u003e\n          \u003ctd\u003e20.0\u003c/td\u003e\n          \u003ctd\u003e26.7\u003c/td\u003e\n          \u003ctd\u003e26.7\u003c/td\u003e\n          \u003ctd\u003e26.7\u003c/td\u003e\n          \u003ctd\u003e20.0\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e60\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasic Motions\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e67.6\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e97.5\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCharacter Trajectories\u003c/td\u003e\n          \u003ctd\u003e99.5\u003c/td\u003e\n          \u003ctd\u003e98.8\u003c/td\u003e\n          \u003ctd\u003e97.4\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e99.0\u003c/td\u003e\n          \u003ctd\u003e96.4\u003c/td\u003e\n          \u003ctd\u003e96.9\u003c/td\u003e\n          \u003ctd\u003e99.0\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCricket\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e93.1\u003c/td\u003e\n          \u003ctd\u003e90.3\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e98.6\u003c/td\u003e\n          \u003ctd\u003e100.0\u003c/td\u003e\n          \u003ctd\u003e94.4\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDuck Duck Geese\u003c/td\u003e\n          \u003ctd\u003e70.0\u003c/td\u003e\n          \u003ctd\u003e52.5\u003c/td\u003e\n          \u003ctd\u003e65.0\u003c/td\u003e\n          \u003ctd\u003e67.5\u003c/td\u003e\n          \u003ctd\u003e57.5\u003c/td\u003e\n          \u003ctd\u003e27.5\u003c/td\u003e\n          \u003ctd\u003e55.0\u003c/td\u003e\n          \u003ctd\u003e60.0\u003c/td\u003e\n          \u003ctd\u003e8\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEigen Worms\u003c/td\u003e\n          \u003ctd\u003e43.5\u003c/td\u003e\n          \u003ctd\u003e45.0\u003c/td\u003e\n          \u003ctd\u003e41.9\u003c/td\u003e\n          \u003ctd\u003e80.9\u003c/td\u003e\n          \u003ctd\u003e89.0\u003c/td\u003e\n          \u003ctd\u003e55.0\u003c/td\u003e\n          \u003ctd\u003e60.3\u003c/td\u003e\n          \u003ctd\u003e61.8\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEpilepsy\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e93.5\u003c/td\u003e\n          \u003ctd\u003e94.9\u003c/td\u003e\n          \u003ctd\u003e96.4\u003c/td\u003e\n          \u003ctd\u003e99.3\u003c/td\u003e\n          \u003ctd\u003e66.7\u003c/td\u003e\n          \u003ctd\u003e97.8\u003c/td\u003e\n          \u003ctd\u003e96.4\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEring\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e13.3\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEthanol Concentration\u003c/td\u003e\n          \u003ctd\u003e34.6\u003c/td\u003e\n          \u003ctd\u003e31.6\u003c/td\u003e\n          \u003ctd\u003e30.8\u003c/td\u003e\n          \u003ctd\u003e31.6\u003c/td\u003e\n          \u003ctd\u003e29.3\u003c/td\u003e\n          \u003ctd\u003e29.3\u003c/td\u003e\n          \u003ctd\u003e30.4\u003c/td\u003e\n          \u003ctd\u003e32.3\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e80\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFace Detection\u003c/td\u003e\n          \u003ctd\u003e63.9\u003c/td\u003e\n          \u003ctd\u003e63.8\u003c/td\u003e\n          \u003ctd\u003e50.0\u003c/td\u003e\n          \u003ctd\u003e57.4\u003c/td\u003e\n          \u003ctd\u003e54.5\u003c/td\u003e\n          \u003ctd\u003e51.9\u003c/td\u003e\n          \u003ctd\u003e51.3\u003c/td\u003e\n          \u003ctd\u003e52.9\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e60\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinger Movements\u003c/td\u003e\n          \u003ctd\u003e60.0\u003c/td\u003e\n          \u003ctd\u003e60.0\u003c/td\u003e\n          \u003ctd\u003e49.0\u003c/td\u003e\n          \u003ctd\u003e61.0\u003c/td\u003e\n          \u003ctd\u003e54.0\u003c/td\u003e\n          \u003ctd\u003e55.0\u003c/td\u003e\n          \u003ctd\u003e52.0\u003c/td\u003e\n          \u003ctd\u003e53.0\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e40\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e(Note: \u0026ldquo;XC\u0026rdquo; denotes the accuracy of XCM, \u0026ldquo;XC Seq\u0026rdquo; denotes the accuracy of XCM with sequential layers, \u0026ldquo;MC\u0026rdquo; represents MTEX-CNN, \u0026ldquo;MF\u0026rdquo; denotes MLSTM-FCN, \u0026ldquo;WM\u0026rdquo; stands for WEASEL+MUSE, \u0026ldquo;ED (n)\u0026rdquo; represents Elastic Distance (normalized), \u0026ldquo;DWI\u0026rdquo; and \u0026ldquo;DWD\u0026rdquo; refer to Dynamic Time Warping independent and dependent, respectively. \u0026ldquo;Win %\u0026rdquo; indicates the percentage of times XCM achieved the highest accuracy across all folds.)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInterpretation of Results\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eArticulary Word Recognition\u003c/strong\u003e: XCM achieved a high accuracy of 98.3%, showcasing its robustness in motion-based classification and indicating its effectiveness in handling complex time series data with a high dimensional space.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAtrial Fibrillation\u003c/strong\u003e: This dataset posed a challenge with lower accuracy across all methods. XCM\u0026rsquo;s performance at 46.7% suggests that while challenging, it has the potential to discern patterns in smaller and more complex ECG datasets.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBasic Motions\u003c/strong\u003e: XCM perfected the score, highlighting its proficiency in recognizing basic human activity patterns, a crucial capability for HAR applications.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCharacter Trajectories\u003c/strong\u003e: The high score of 99.5% reflects XCM\u0026rsquo;s strength in managing datasets with numerous classes, reinforcing its scalability for extensive data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCricket\u003c/strong\u003e: A perfect score of 100.0% emphasizes XCM\u0026rsquo;s ability to capture intricate temporal patterns, suggesting its suitability for complex HAR scenarios.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDuck Duck Geese\u003c/strong\u003e: XCM\u0026rsquo;s performance at 70.0% accuracy indicates a significant capability in audio spectrum data classification, a testament to its adaptability to different data types.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEigen Worms\u003c/strong\u003e: Despite the lower score, XCM\u0026rsquo;s handling of the longest sequences among the datasets indicates its potential to model complex temporal behaviors in motion data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEpilepsy\u003c/strong\u003e: An accuracy of 99.3% portrays XCM\u0026rsquo;s precision and reliability in critical classification scenarios, essential for medical applications.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEring\u003c/strong\u003e: The universally low scores across methods reflect the dataset\u0026rsquo;s complexity, underscoring a need for specialized approaches or additional features to aid classification.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEthanol Concentration\u003c/strong\u003e: Although challenging, XCM\u0026rsquo;s relatively higher score suggests its capacity to filter meaningful information from noisy data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFace Detection\u003c/strong\u003e: XCM\u0026rsquo;s ability to handle biometric patterns is evidenced by its performance, indicating its utility in EEG/MEG data interpretation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFinger Movements\u003c/strong\u003e: The moderate score reflects the complexity of the task but also suggests XCM\u0026rsquo;s capability to capture rapid changes in EEG/MEG datasets associated with movements.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u0026ldquo;Win %\u0026rdquo; column indicates the superiority of XCM in most datasets, which combined with its explainability features, positions it as a preferred choice for MTS classification in practical applications. This comprehensive analysis not only confirms the effectiveness of the XCM approach but also guides future advancements and potential improvements.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDiscussion\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe results underscore the effectiveness of XCM in multivariate time series classification across a variety of domains, highlighting its capability to maintain high accuracy even in datasets with challenging characteristics. Moreover, the high win percentage indicates XCM\u0026rsquo;s robustness as it frequently outperforms other methods. It is crucial to note that beyond accuracy, XCM\u0026rsquo;s design enables it to offer a layer of explainability which is not captured by accuracy metrics alone but is invaluable in practical applications.\u003c/p\u003e\n\u003ch3 id=\"6-implementation\"\u003e6. Implementation\u003c/h3\u003e\n\u003cp\u003eWe decided to implement ourselves the XCM model using \u003ca href=\"https://github.com/XAIseries/XCM\"\u003ethis GitHub Repository\u003c/a\u003e on a dataset used in the original paper : BasiMotions.\u003c/p\u003e\n\u003cp\u003eThe code of the XCM model is shown in the \u003ca href=\"#appendix\"\u003eAppendix\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere are the results we obtained for a 5 fold training :\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDataset\u003c/th\u003e\n          \u003cth\u003eModel_Name\u003c/th\u003e\n          \u003cth\u003eBatch_Size\u003c/th\u003e\n          \u003cth\u003eWindow_Size\u003c/th\u003e\n          \u003cth\u003eFold\u003c/th\u003e\n          \u003cth\u003eAccuracy_Train\u003c/th\u003e\n          \u003cth\u003eAccuracy_Validation\u003c/th\u003e\n          \u003cth\u003eAccuracy_Test\u003c/th\u003e\n          \u003cth\u003eAccuracy_Test_Full_Train\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e0.90625\u003c/td\u003e\n          \u003ctd\u003e0.75\u003c/td\u003e\n          \u003ctd\u003e0.825\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e0.925\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e0.925\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n          \u003ctd\u003e0.875\u003c/td\u003e\n          \u003ctd\u003e0.9\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBasicMotions\u003c/td\u003e\n          \u003ctd\u003eXCM\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n          \u003ctd\u003e20\u003c/td\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n          \u003ctd\u003e0.78125\u003c/td\u003e\n          \u003ctd\u003e0.875\u003c/td\u003e\n          \u003ctd\u003e0.825\u003c/td\u003e\n          \u003ctd\u003e1.0\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWe then analyzed with a graph the evolution of both accuaries with regard to the epochs. The model is thus perfoming really well as explained in the paper.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/Evolution_Accuracies.png\"\n  alt=\"Evolution of accuracies during traning\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eOne of the main improvment of XCM is his explainalibily of the features which can be explicitly shown with layer activations features map. Here is the one we extracted from the model we trained on BasicMotions dataset.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/test_MTS_0_layer_2D_Activation.png\"\n  alt=\"2D_activation_layer\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch3 id=\"7-conclusion\"\u003e7. Conclusion\u003c/h3\u003e\n\u003cp\u003eThe XCM approach signifies a substantial step forward in MTS classification, achieving high accuracy while providing explainability of features which is indispensable for applications demanding transparency in AI decision-making. The paper suggests that future work may focus on refining hyperparameters automatically and exploring the fusion of XCM with other modalities for richer data representation and classification.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"appendix\"\u003eAppendix\u003c/h3\u003e\n\u003cp\u003eImplementation of the XCM model with Keras\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Saint_Guerin/code_xcm.png\"\n  alt=\"XCM\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFauvel, K.; Lin, T.; Masson, V.; Fromont, É.; Termier, A. XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification. Mathematics 2021, 9, 3137. \u003ca href=\"http://dx.doi.org/10.3390/math9233137\"\u003eDOI: 10.3390/math9233137\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLi, J.; Rong, Y.; Meng, H.; Lu, Z.; Kwok, T.; Cheng, H. TATC: Predicting Alzheimer’s Disease with Actigraphy Data. In Proceedings\nof the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, London, UK, 19–23 August 2018.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJiang, R.; Song, X.; Huang, D.; Song, X.; Xia, T.; Cai, Z.; Wang, Z.; Kim, K.; Shibasaki, R. DeepUrbanEvent: A System for Predicting\nCitywide Crowd Dynamics at Big Events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, Anchorage, AK, USA, 4–8 August 2019.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFauvel, K.; Balouek-Thomert, D.; Melgar, D.; Silva, P.; Simonet, A.; Antoniu, G.; Costan, A.; Masson, V.; Parashar, M.; Rodero, I.;\net al. A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning. In Proceedings of the 34th AAAI\nConference on Artificial Intelligence, New York, NY, USA, 7–12 February 2020.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eKarim, F.; Majumdar, S.; Darabi, H.; Harford, S. Multivariate LSTM-FCNs for Time Series Classification. Neural Netw. 2019,\n116, 237–245. [CrossRef] [PubMed]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSchäfer, P.; Leser, U. Multivariate Time Series Classification with WEASEL+MUSE. arXiv 2017, arXiv:1711.11343.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBagnall, A.; Lines, J.; Keogh, E. The UEA Multivariate Time Series Classification Archive, 2018. arXiv 2018, arXiv:1811.00075.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/xcm/","date_published":"26036-26-09T355:2626:00+01:00","date_modified":"26036-26-09T355:2626:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"37b39cffb06a5944d96062052d7f779421cde6be","title":"RobustAI_RegMixup","summary":"","content_text":" \u003c!DOCTYPE html\u003e Styled Table RegMixup : Regularizer for robust AI Improve accuracy and Out-of-Distribution Robustness Authors: Marius Ortega, Ly An CHHAY Paper : RegMixup by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania Table of Contents Abstract Introduction Prerequisites Empirical Risk Minimization Vicinal Risk Minimization Mixup RegMixup in theory RegMixup in practice Conclusion Abstract In this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\nIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\nIntroduction Most real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\nThe question is how to improve the robustness of machine learning algorithms to OOD samples ? Many researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\nThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\nRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\n1. Prerequisites In order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\n1.1. Empirical Risk Minimization (ERM) Empirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\n$$ R_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1} $$\nwhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\n1.2. Vicinal Risk Minimization (VRM) Vicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\n$$ R_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2} $$\nConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\n1.3. Mixup Mixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\nIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\nFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\n$$ \\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm} \\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j $$\nWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\n$$ P_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3} $$\nMixup is an interesting method to consider but it possesses some limitations :\nSmall $\\alpha$ issues : With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the figure below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data. Model underconfidence : When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples. Mixup vs RegMixup, underconfidence and space exploration.\n2. RegMixup in theory Now that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\nWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\n$$ P(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4} $$\nHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\nFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\n$$ \\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5} $$\nWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\nSuch a model (equation 4) exhibits properties that lacked in Mixup :\nValues of $\\alpha$ and underconfidence : As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the figure). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate. Prediction entropy : Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a regularizer in essense. As a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\n3. RegMixup in practice (implementation) Now, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\nA baseline model trained with ERM A model trained with Mixup A model trained with RegMixup To do so, we have two possibilities :\nUse the official implementation of RegMixup available on Francesco Pinto's GitHub. Use the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on GitHub. In this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\n3.1. Installation First, we need to install the torch-uncertainty library. To do so, we can use pip :\npip install torch-uncertainty Note: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from PyTorch website) :\npip unistall torch torchvision pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118 To check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\nimport torch print(torch.cuda.is_available()) 3.2. Training the models with torch-uncertainty Now that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\nfrom torch_uncertainty import cli_main, init_args from torch_uncertainty.baselines.classification import ResNet from torch_uncertainty.optimization_procedures import optim_cifar10_resnet18 from torch_uncertainty.datamodules import CIFAR10DataModule from torchvision.datasets import CIFAR10 from torchvision import transforms from torch.nn import CrossEntropyLoss import torch import os from pathlib import Path from cli_test_helpers import ArgvContext Then, we can define the 3 models we discussed earlier :\nbaseline = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18).cuda() mixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, mixup=True, mixup_alpha=0.2).cuda() regmixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, reg_mixup=True, mixup_alpha=15).cuda() Before training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\nroot = Path(os.path.abspath(\u0026#34;\u0026#34;)) # We mock the arguments for the trainer with ArgvContext( \u0026#34;file.py\u0026#34;, \u0026#34;--max_epochs\u0026#34;, \u0026#34;20\u0026#34;, \u0026#34;--enable_progress_bar\u0026#34;, \u0026#34;False\u0026#34;, \u0026#34;--num_estimators\u0026#34;, \u0026#34;8\u0026#34; ): args = init_args(network=ResNet, datamodule=CIFAR10DataModule) net_name = \u0026#34;logs/reset18-cifar10\u0026#34; # datamodule args.root = str(root / \u0026#34;data\u0026#34;) dm = CIFAR10DataModule(**vars(args)) Finally, we can train the models using the cli_main function from torch-uncertainty :\nresults_baseline = cli_main(baseline, dm, root, net_name, args=args) results_mixup = cli_main(mixup, dm, root, net_name, args=args) results_regmixup = cli_main(regmixup, dm, root, net_name, args=args) Note: If you have a gpu, you can make a slight modification to the code to use it :\nClick on cli_main and press F12 to go to the function definition. Go to line 222 and replace the trainer definition by the following one : # trainer trainer = pl.Trainer.from_argparse_args( args, accelerator=\u0026#34;gpu\u0026#34;, devices=1, callbacks=callbacks, logger=tb_logger, deterministic=(args.seed is not None), inference_mode=not (args.opt_temp_scaling or args.val_temp_scaling), ) Save the file and you are all set. 3.3. Results So as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\nWith corruption severity factor of 5, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.656294 0.7480 0.349862 0.032466 0.729336 mixup 0.640811 0.7578 0.335403 0.024429 0.703844 regmixup 0.676174 0.7564 0.340233 0.023135 0.711405 First of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\nWith corruption severity factor of 15, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.615607 0.7402 0.358522 0.048414 0.750933 mixup 0.698558 0.7558 0.338540 0.014760 0.709190 regmixup 0.702599 0.7614 0.327945 0.008439 0.687550 Here the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\n4. Conclusion As a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\n","content_html":"\u003cstyle\nTYPE=\"text/css\"\u003e\n\ncode.has-jax {font:\ninherit;\nfont-size:\n100%; \nbackground: \ninherit; \nborder: \ninherit;}\n\n\u003c/style\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"UTF-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n\u003ctitle\u003eStyled Table\u003c/title\u003e\n\u003cstyle\u003e\n    table {\n        border-collapse: collapse;\n        width: 100%;\n    }\n    th, td {\n        padding: 8px;\n        text-align: center;\n        border-bottom: 1px solid #ddd;\n    }\n    th {\n        background-color: #f2f2f2;\n    }\n    tr:hover {\n        background-color: #f5f5f5;\n    }\n\u003c/style\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eRegMixup : Regularizer for robust AI\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eImprove accuracy and Out-of-Distribution Robustness\u003ch1\u003e\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: Marius Ortega, Ly An CHHAY \u003cbr /\u003e\nPaper : \u003ca href=\"https://arxiv.org/abs/2206.14502\"\u003eRegMixup\u003c/a\u003e  by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0.0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-0.1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003ePrerequisites\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1.1\"\u003eEmpirical Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.2\"\u003eVicinal Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.3\"\u003eMixup\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eRegMixup in theory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eRegMixup in practice \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0.0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eIn this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\u003c/p\u003e\n\u003cp\u003eIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\u003c/p\u003e\n\u003ch2 id=\"section-0.1\"\u003eIntroduction \u003c/h2\u003e\n\u003cp\u003eMost real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\u003c/p\u003e\n\u003cp\u003eThe question is how to improve the robustness of machine learning algorithms to OOD samples ?\nMany researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\u003c/p\u003e\n\u003cp\u003eThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\u003c/p\u003e\n\u003cp\u003eRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003e1. Prerequisites \u003c/h2\u003e\n\u003cp\u003eIn order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\u003c/p\u003e\n\u003ch3 id=\"section-1.1\"\u003e1.1. Empirical Risk Minimization (ERM)\u003c/h3\u003e\n\u003cp\u003eEmpirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\u003c/p\u003e\n\u003cp\u003e$$\nR_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\u003c/p\u003e\n\u003ch3 id=\"section-1.2\"\u003e1.2. Vicinal Risk Minimization (VRM)\u003c/h3\u003e\n\u003cp\u003eVicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\u003c/p\u003e\n\u003cp\u003e$$\nR_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2}\n$$\u003c/p\u003e\n\u003cp\u003eConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\u003c/p\u003e\n\u003ch3 id=\"section-1.3\"\u003e1.3. Mixup\u003c/h3\u003e\n\u003cp\u003eMixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\u003c/p\u003e\n\u003cp\u003eIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\u003c/p\u003e\n\u003cp\u003eFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\u003c/p\u003e\n\u003cp\u003e$$\n\\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm}\n\\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j\n$$\u003c/p\u003e\n\u003cp\u003eWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\u003c/p\u003e\n\u003cp\u003e$$\nP_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3}\n$$\u003c/p\u003e\n\u003cp\u003eMixup is an interesting method to consider but it possesses some limitations :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSmall $\\alpha$ issues :\u003c/strong\u003e With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel underconfidence :\u003c/strong\u003e When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure id=\"my-fig\" class=\"numbered\"\u003e\n    \u003cimg src=\"/images/regmixup/fig.png\" class=\"align-center\"\u003e\n    \u003cp style=\"text-align: center;\"\u003eMixup vs RegMixup, underconfidence and space exploration.\u003c/p\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"section-2\"\u003e2. RegMixup in theory\u003c/h2\u003e\n\u003cp\u003eNow that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\u003c/p\u003e\n\u003cp\u003eWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\u003c/p\u003e\n\u003cp\u003e$$\nP(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4}\n$$\u003c/p\u003e\n\u003cp\u003eHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\u003c/p\u003e\n\u003cp\u003eFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\u003c/p\u003e\n\u003cp\u003e$$\n\\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5}\n$$\u003c/p\u003e\n\u003cp\u003eWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\u003c/p\u003e\n\u003cp\u003eSuch a model (equation 4) exhibits properties that lacked in Mixup :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eValues of $\\alpha$ and underconfidence :\u003c/strong\u003e As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrediction entropy :\u003c/strong\u003e Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a \u003cstrong\u003eregularizer\u003c/strong\u003e in essense.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e3. RegMixup in practice (implementation)\u003c/h2\u003e\n\u003cp\u003eNow, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA baseline model trained with ERM\u003c/li\u003e\n\u003cli\u003eA model trained with Mixup\u003c/li\u003e\n\u003cli\u003eA model trained with RegMixup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo do so, we have two possibilities :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse the official implementation of RegMixup available on \u003ca href=\"https://github.com/FrancescoPinto/RegMixup\"\u003eFrancesco Pinto's GitHub\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eUse the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on \u003ca href=\"https://github.com/ENSTA-U2IS-AI/torch-uncertainty\"\u003eGitHub\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\u003c/p\u003e\n\u003ch3 id=\"31-installation\"\u003e3.1. Installation\u003c/h3\u003e\n\u003cp\u003eFirst, we need to install the torch-uncertainty library. To do so, we can use pip :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install torch-uncertainty\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from \u003ca href=\"https://pytorch.org/get-started/locally/\"\u003ePyTorch website\u003c/a\u003e) :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip unistall torch torchvision\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eis_available\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"32-training-the-models-with-torch-uncertainty\"\u003e3.2. Training the models with torch-uncertainty\u003c/h3\u003e\n\u003cp\u003eNow that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.baselines.classification\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.optimization_procedures\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.datamodules\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.datasets\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.nn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epathlib\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_test_helpers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen, we can define the 3 models we discussed earlier :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ereg_mixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBefore training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epath\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eabspath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# We mock the arguments for the trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003ewith\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;file.py\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--max_epochs\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;20\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--enable_progress_bar\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;False\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--num_estimators\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;8\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enetwork\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edatamodule\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;logs/reset18-cifar10\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# datamodule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;data\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003e\u003cspan style=\"color:#111\"\u003evars\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFinally, we can train the models using the \u003ccode\u003ecli_main\u003c/code\u003e function from torch-uncertainty :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_baseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_mixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_regmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you have a gpu, you can make a slight modification to the code to use it :\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClick on \u003ccode\u003ecli_main\u003c/code\u003e and press \u003ccode\u003eF12\u003c/code\u003e to go to the function definition.\u003c/li\u003e\n\u003cli\u003eGo to line 222 and replace the trainer definition by the following one :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003etrainer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epl\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTrainer\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efrom_argparse_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eaccelerator\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;gpu\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edevices\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003elogger\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etb_logger\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edeterministic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eseed\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eis\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eNone\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003einference_mode\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eopt_temp_scaling\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eval_temp_scaling\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eSave the file and you are all set.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"33-results\"\u003e3.3. Results\u003c/h3\u003e\n\u003cp\u003eSo as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 5, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth\u003eentropy\u003c/th\u003e\n          \u003cth\u003eaccuracy\u003c/th\u003e\n          \u003cth\u003ebrier\u003c/th\u003e\n          \u003cth\u003eece\u003c/th\u003e\n          \u003cth\u003enll\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ebaseline\u003c/td\u003e\n          \u003ctd\u003e0.656294\u003c/td\u003e\n          \u003ctd\u003e0.7480\u003c/td\u003e\n          \u003ctd\u003e0.349862\u003c/td\u003e\n          \u003ctd\u003e0.032466\u003c/td\u003e\n          \u003ctd\u003e0.729336\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003emixup\u003c/td\u003e\n          \u003ctd\u003e0.640811\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.7578\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e0.335403\u003c/td\u003e\n          \u003ctd\u003e0.024429\u003c/td\u003e\n          \u003ctd\u003e0.703844\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eregmixup\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.676174\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e0.7564\u003c/td\u003e\n          \u003ctd\u003e0.340233\u003c/td\u003e\n          \u003ctd\u003e0.023135\u003c/td\u003e\n          \u003ctd\u003e0.711405\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFirst of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 15, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth\u003eentropy\u003c/th\u003e\n          \u003cth\u003eaccuracy\u003c/th\u003e\n          \u003cth\u003ebrier\u003c/th\u003e\n          \u003cth\u003eece\u003c/th\u003e\n          \u003cth\u003enll\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ebaseline\u003c/td\u003e\n          \u003ctd\u003e0.615607\u003c/td\u003e\n          \u003ctd\u003e0.7402\u003c/td\u003e\n          \u003ctd\u003e0.358522\u003c/td\u003e\n          \u003ctd\u003e0.048414\u003c/td\u003e\n          \u003ctd\u003e0.750933\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003emixup\u003c/td\u003e\n          \u003ctd\u003e0.698558\u003c/td\u003e\n          \u003ctd\u003e0.7558\u003c/td\u003e\n          \u003ctd\u003e0.338540\u003c/td\u003e\n          \u003ctd\u003e0.014760\u003c/td\u003e\n          \u003ctd\u003e0.709190\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eregmixup\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.702599\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e0.7614\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e0.327945\u003c/td\u003e\n          \u003ctd\u003e0.008439\u003c/td\u003e\n          \u003ctd\u003e0.687550\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHere the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e4. Conclusion\u003c/h2\u003e\n\u003cp\u003eAs a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/robustai_regmixup/","date_published":"24036-24-09T338:2424:00+01:00","date_modified":"24036-24-09T338:2424:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"8b29896ef63d10ea5c0197faf234c9919f754125","title":"Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints","summary":"","content_text":"Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints Authors: Godefroy LAMBERT and Louise DAVY Table of Contents Introduction Definitions AUC-based fairness constraints ROC-based fairness constraints Results Reproducibility Conclusion This is a blog post about the paper Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints, published by R. Vogel et al. in 2021 and available here.\nIntroduction With recent advances in machine learning, applications are becoming increasingly numerous and the expectations are high. Those applications will only be able to be deployed if some important issues are addressed such as bias. There are famous datasets known for containing variables that induce a lot of bias such as Compas with racial bias and gender bias in the Adult dataset. To avoid those biases, new algorithms were created to provide more fairness in the prediction by using diverse methods.\nToday, we will be reviewing the methods presented in “Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints”. This paper uses basic metrics such as AUC constraint and ROC constraint and shows some limitations. Since this is bipartite ranking, we will only focus on binary prediction, such as will this person recid for the COMPAS dataset or will this person get his loan for the Adult dataset.\nDefinitions The goal of bipartite ranking is to acquire an ordering of X where positive instances are consistently ranked above negative ones with a high probability. This is done by learning an appropriate scoring function $s$. Such scoring functions are widely used in many critical domains such as loan granting, anomaly detection, or even in court decisions. A nice way to assess their performance is through the analysis of the Receiver Operating Characteristic (ROC) curve and the Area Under the ROC Curve (AUC).\nROC stands for Receiver Operating Characteristic curve and is a graph showing the performance of a classification model at all classification thresholds for a model. This curve plots two parameters:\nTrue Positive Rate False Positive Rate The formula for the True Positive Rate (TPR) is: $$TPR = \\frac{TP}{TP + FN}$$\nAnd the formula for the False Positive Rate (FPR) is: $$FPR = \\frac{FP}{FP + TN}$$\nWith ${FP}$ = False Positive, $FN$ = False Negative, $TP$ = True Positive, $TN$ = True Negative.\nBy varying the classifier, we can obtain different ROC curves that are represented in the following image. The curve that is closer to the upper-left corner is the best one, while the curve in diagonal represents a random classifier. AUC stands for Area Under the ROC Curve and is a widely used metric in machine learning, particularly in binary classification tasks. The AUC quantifies the overall performance of the model across all possible classification thresholds.\nThat is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). The AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0, one whose predictions are 100% correct has an AUC of 1.0.\nWhile fairness seems like a desirable goal for any ranking function, there are many different definitions of what fairness really is and thus, many different metrics to assess the fairness of an algorithm. In the case of loan grants for example, one could consider that fairness is achieved between men and women if we granted the same percentage of loans for both groups. Statistical parity, which compares the proportion of positive outcomes between different demographic groups, is a good metric in this case. However, this approach might overlook underlying disparities in socioeconomic status that affect loan approval rates. Another vision of fairness might ensure that individuals are all as likely to get a wrong decision, regardless of demographic factors such as gender or ethnicity. In this case, parity of mistreatment would be a good metric, as it ensures that the proportion of errors is the same for all demographic groups. However, this considers that all errors are the same, which means that one group could have a high false positive rate and another a high false negative rate. The authors thus decided to choose parity in false positive rates and/or parity in false negative rates.\nAUC-based fairness constraints This first approach is based on the AUC, it will help us to highlight the limitations of this metric which motivated the authors to introduce another approach based on ROC constraints.\nPrecise example of AUC based constraints presented in the paper are the intra-group pairwise AUC fairness (Beutel et al., 2019), Background Negative Subgroup Positive (BNSP) AUC fairness (Borkan et al., 2019), the inter-group pairwise AUC fairness (Kallus and Zhou, 2019). The first one require the ranking performance to be equal within groups, the second one enforces that positive instances from either group have the same probability of being ranked higher than a negative example and the last one imposes that the positives of a group can be distinguished from the negatives of the other group as effectively for both groups. Those 3 AUC based constraints are only a part of the many constraints that exist.\nThe paper introduces a new framework to generalize all relevant AUC-based constraint as a linear combination of 5 relevant elementary constraints noted $C_1$ to $C_5$.\nThe value of |$ C_ {1} $(s)| (resp. |$ C_ {2} $(s)|) quantifies the resemblance of the distribution of the negatives (resp. positives) between the two sensitive attributes.\n$ C_ {1} $(s) = $ AUC_ {{H_S^{(0)}} ,{H_S^{(1)}}} $ - $\\frac{1}{2}$\n$ C_ {2} $(s) = $\\frac{1}{2}$ - $ AUC_ {{G_S^{(0)}} ,{G_S^{(1)}}} $\nThe values of $ C_ {3} $(s), $ C_ {4} $(s) and $ C_ {5} $(s) measure the difference in ability of a score to discriminate between positives and negatives for any two pairs of sensitive attributes.\n$ C_ {3} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $\n$ C_ {4} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $\n$ C_ {5} $(s) = $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(1)}}} $\nThe family of fairness constraints considered is then the set of linear combinations of the $C_l(s)$ = 0:\n\\begin{align*} % $C_l(s)$ = 0 C_Γ(s): Γ^T C(s) = \\sum_{l=1}^{5} {Γ_l}{C_l}(s) = 0 \\end{align*}\nWhere $Γ$ = $(Γ_1, \u0026hellip; Γ_5)^T$.\nThe objective function is thus defined as follows :\n\\begin{align} \\label{eq:auc_general_problem} \\textstyle\\max_{s\\in\\mathcal{S}} \\quad AUC_{H_s,G_s} - \\lambda |\\Gamma^\\top C (s)|, \\end{align} where $\\lambda\\ge 0$ is a hyperparameter balancing ranking performance and fairness.\nThe paper focuses on a special case of fairness, the intra-group pairwise AUC fairness. This was to be more concise. In this example, the objective function becomes:\n$$ L_\\lambda(s) = AUC_{H_s,G_s} - \\lambda | AUC_{H_s^{(0)}, G_s^{(0)}} - AUC_{H_s^{(1)}, G_s^{(1)} } | $$\nIssues of AUC-Based constraint:\nFairness using AUC-based constraints defined by the equality between two AUC’s only quantify a stochastic order between distributions, not the equality between these distributions, and would lead to some unfair result, for a group or for the other group.\nThe authors conducted experiments with the credit-risk dataset and found that creditworthy individuals from both groups had equal chances of being ranked higher than a \u0026ldquo;bad borrower.\u0026rdquo; However, employing high thresholds (which represent low probabilities of default on approved loans) would result in unfair outcomes for one group.\nROC-based fairness constraints A richer approach is then to use pointwised ROC-based fairness constraints. Ideally, we would want to enforce the equality of all score distributions between both groups (i.e., identical ROC curves). This would satisfy all AUC-based fairness constraints previously mentioned. However, this condition is so restrictive that it will most likely lead to a significant drop in performances. As a result, the authors propose to satisfy this constraint on only a finite number of points. They were indeed able to prove that this was sufficient to ensure maximum fairness for a fixed false positive or false negative $\\alpha$.\nAs a result, the objective function becomes :\n\\begin{align*} % L_\\Lambda(s) = AUC_{H_s,G_s} \u0026amp;- \\sum_{k=1}^{m_H} \\lambda_H^{(k)} \\big| \\Delta_{H,\\alpha_H^{ (k)}}(s) \\big| - \\sum_{k=1}^{m_G} \\lambda_G^{(k)} \\big| \\Delta_{G,\\alpha_G^{(k)}}(s) \\big|, \\end{align*}\nWhere $\\Delta_{H,\\alpha_H^{(k)}}(s)$ and $\\Delta_{G,\\alpha_G^{(k)}}(s)$ represent the deviations between the positive (resp. negative) inter-group ROCs and the identity function:\n$$ \\Delta_{G, \\alpha}(s) = ROC_{G^{(0)}_s, G^{(1)}_s}(\\alpha) - \\alpha $$\n$$ \\Delta_{H, \\alpha}(s) = ROC_{H^{(0)}_s,H^{(1)}_s}(\\alpha) - \\alpha $$\nIn practice, the objective function is slightly modified to be able to maximise it. The authors applied a classic smooth surrogate relaxations of the AUCs or ROCs based on a logistic function. They also removed the absolute values and, instead, relied on some parameters to ensure positive values.\nResults The authors tested out their results on two datasets : Compas and Adult. Both are widely used when it comes to fairness. Indeed, they are known to be biased against race (for Compas) and gender (for both). Compas is a recidivism prediction dataset, whereas Adult predicts whether income exceeds $50K/yr based on census data. The results reported in the next figure show that the ROC-based method achieves its goal of mitigating the differences between favoured and unfavoured groups with limited drop in performances (the AUC went from 0.72 to 0.70 on the Compas dataset and from 0.91 to 0.87 on the Adult dataset). Indeed, the blue ROC curve, which is the ROC curve of the unfavoured group (Afro-American people for the Compas Dataset and women for the Adult Dataset), is brought closer to the green ROC curve (the ROC curve of the favoured group).\nReproducibility We were able to run the provided code without too much trouble on WSL2. The only modification we had to make was to change the calls for python in the sh files. We replace python with python3. However, as mentionned in the cide, the experiments were very long to run (several days) and we were not able to run the generate_all_figures.sh script fully as it made our computers crash. Still, we were able to get some of the figures found in the paper (see below) by launching some scripts separately.\nHere are two figure generated for the toy 1 dataset, one for the distribution of the scores and one for the ROC curve.\nConclusion The paper \u0026ldquo;Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints\u0026rdquo; underscores the growing importance of fairness in machine learning applications. It shows the limits of AUC-based fairness constraints for their inability to ensure equality between distributions, potentially leading to unfair outcomes. In contrast, ROC-based fairness constraints offer a richer approach by enforcing equality of score distributions between groups, albeit with some performance trade-offs. The paper tests the method on typical fairness datasets, but it is also possible to apply it to reel use cases. \u0026ldquo;A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization\u0026rdquo;, for example, explores the possibility to apply ROC-based methods for similarity learning, such as face recognition.\n","content_html":"\u003ch1 style=\"font-size: 24px;\"\u003eLearning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints\u003c/h1\u003e\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: Godefroy LAMBERT and Louise DAVY\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eDefinitions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eAUC-based fairness constraints\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eROC-based fairness constraints\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eResults\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReproducibility\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints, published by R. Vogel et al. in 2021 and available \u003ca href=\"http://proceedings.mlr.press/v130/vogel21a/vogel21a-supp.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"section-1\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eIntroduction\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eWith recent advances in machine learning, applications are becoming increasingly numerous and the expectations are high. Those applications will only be able to be deployed if some important issues are addressed such as bias. There are famous datasets known for containing variables that induce a lot of bias such as Compas with racial bias and gender bias in the Adult dataset. To avoid those biases, new algorithms were created to provide more fairness in the prediction by using diverse methods.\u003c/p\u003e\n\u003cp\u003eToday, we will be reviewing the methods presented in “Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints”. This paper uses basic metrics such as AUC constraint and ROC constraint and shows some limitations. Since this is bipartite ranking, we will only focus on binary prediction, such as will this person recid for the COMPAS dataset or will this person get his loan for the Adult dataset.\u003c/p\u003e\n\u003ch1 id=\"section-2\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eDefinitions\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe goal of \u003cstrong\u003ebipartite ranking\u003c/strong\u003e is to acquire an ordering of X where positive instances are consistently ranked above negative ones with a high probability. This is done by learning an appropriate scoring function $s$. Such scoring functions are widely used in many critical domains such as \u003cstrong\u003eloan granting\u003c/strong\u003e, \u003cstrong\u003eanomaly detection\u003c/strong\u003e, or even in \u003cstrong\u003ecourt decisions\u003c/strong\u003e. A nice way to assess their performance is through the analysis of the \u003cstrong\u003eReceiver Operating Characteristic\u003c/strong\u003e (ROC) curve and the \u003cstrong\u003eArea Under the ROC Curve\u003c/strong\u003e (AUC).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eROC\u003c/strong\u003e stands for \u003cstrong\u003eReceiver Operating Characteristic curve\u003c/strong\u003e and is a graph showing the performance of a classification model at all classification thresholds for a model. This curve plots two parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTrue Positive Rate\u003c/li\u003e\n\u003cli\u003eFalse Positive Rate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/roc_easy.png\"\n  alt=\"Roc_1\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe formula for the True Positive Rate (TPR) is:\n$$TPR = \\frac{TP}{TP + FN}$$\u003c/p\u003e\n\u003cp\u003eAnd the formula for the False Positive Rate (FPR) is:\n$$FPR = \\frac{FP}{FP + TN}$$\u003c/p\u003e\n\u003cp\u003eWith ${FP}$ = False Positive, $FN$ = False Negative, $TP$ = True Positive, $TN$ = True Negative.\u003c/p\u003e\n\u003cp\u003eBy varying the classifier, we can obtain different ROC curves that are represented in the following image. The curve that is closer to the upper-left corner is the best one, while the curve in diagonal represents a random classifier.\n\u003cimg\n  src=\"/images/lambert_davy/Roc_curve.svg.png\"\n  alt=\"Roc_full\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAUC\u003c/strong\u003e stands for \u003cstrong\u003eArea Under the ROC Curve\u003c/strong\u003e and is a widely used metric in machine learning, particularly in binary classification tasks. The AUC quantifies the \u003cstrong\u003eoverall performance of the model\u003c/strong\u003e across all possible classification thresholds.\u003c/p\u003e\n\u003cp\u003eThat is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). The AUC ranges in value from 0 to 1. A model whose predictions are \u003cstrong\u003e100% wrong has an AUC of 0.0\u003c/strong\u003e, one whose predictions are \u003cstrong\u003e100% correct has an AUC of 1.0\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/AUC.png\"\n  alt=\"AUC\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eWhile \u003cstrong\u003efairness\u003c/strong\u003e seems like a desirable goal for any ranking function, there are many different definitions of what fairness really is and thus, many different \u003cstrong\u003emetrics\u003c/strong\u003e to assess the fairness of an algorithm. In the case of loan grants for example, one could consider that fairness is achieved between men and women if we granted the same percentage of loans for both groups. \u003cstrong\u003eStatistical parity\u003c/strong\u003e, which  compares the proportion of positive outcomes between different demographic groups, is a good metric in this case.  However, this approach might overlook underlying disparities in socioeconomic status that affect loan approval rates. Another vision of fairness might ensure that individuals are all as likely to get a wrong decision, regardless of demographic factors such as gender or ethnicity. In this case, \u003cstrong\u003eparity of mistreatment\u003c/strong\u003e would be a good metric, as it ensures that the proportion of errors is the same for all demographic groups. However, this considers that all errors are the same, which means that one group could have a high false positive rate and another a high false negative rate. The authors thus decided to choose \u003cstrong\u003eparity in false positive rates\u003c/strong\u003e and/or \u003cstrong\u003eparity in false negative rates\u003c/strong\u003e.\u003c/p\u003e\n\u003ch1 id=\"section-3\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eAUC-based fairness constraints\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThis first approach is based on the AUC, it will help us to highlight the limitations of this metric which motivated the authors to introduce another approach based on ROC constraints.\u003c/p\u003e\n\u003cp\u003ePrecise example of AUC based constraints presented in the paper are the intra-group pairwise AUC fairness (Beutel et al., 2019), Background Negative Subgroup Positive (BNSP) AUC fairness (Borkan et al., 2019), the inter-group pairwise AUC fairness (Kallus and Zhou, 2019). The first one require the ranking performance to be equal within groups, the second one enforces that positive instances from either group have the same probability of being ranked higher than a negative example and the last one imposes that the positives of a group can be distinguished from the negatives of the other group as effectively for both groups. Those 3 AUC based constraints are only a part of the many constraints that exist.\u003c/p\u003e\n\u003cp\u003eThe paper introduces a new framework to generalize all relevant AUC-based constraint as a \u003cstrong\u003elinear combination of 5 relevant elementary constraints\u003c/strong\u003e noted $C_1$ to $C_5$.\u003c/p\u003e\n\u003cp\u003eThe value of |$ C_ {1} $(s)| (resp. |$ C_ {2} $(s)|) quantifies the \u003cstrong\u003eresemblance of the distribution\u003c/strong\u003e of the negatives (resp. positives) between the \u003cstrong\u003etwo sensitive attributes\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e$ C_ {1} $(s) = $ AUC_ {{H_S^{(0)}} ,{H_S^{(1)}}} $ - $\\frac{1}{2}$\u003c/p\u003e\n\u003cp\u003e$ C_ {2} $(s) = $\\frac{1}{2}$ - $ AUC_ {{G_S^{(0)}} ,{G_S^{(1)}}} $\u003c/p\u003e\n\u003cp\u003eThe values of $ C_ {3} $(s), $ C_ {4} $(s) and $ C_ {5} $(s) measure the \u003cstrong\u003edifference\nin ability of a score to discriminate\u003c/strong\u003e between positives and negatives for any two pairs of sensitive attributes.\u003c/p\u003e\n\u003cp\u003e$ C_ {3} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $\u003c/p\u003e\n\u003cp\u003e$ C_ {4} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $\u003c/p\u003e\n\u003cp\u003e$ C_ {5} $(s) = $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(1)}}} $\u003c/p\u003e\n\u003cp\u003eThe family of fairness constraints considered is then the set of linear combinations of the $C_l(s)$ = 0:\u003c/p\u003e\n\u003cp\u003e\\begin{align*}\n% $C_l(s)$ = 0\nC_Γ(s): Γ^T C(s) =\n\\sum_{l=1}^{5} {Γ_l}{C_l}(s) = 0\n\\end{align*}\u003c/p\u003e\n\u003cp\u003eWhere $Γ$ = $(Γ_1, \u0026hellip; Γ_5)^T$.\u003c/p\u003e\n\u003cp\u003eThe objective function is thus defined as follows :\u003c/p\u003e\n\u003cp\u003e\\begin{align}\n\\label{eq:auc_general_problem}\n\\textstyle\\max_{s\\in\\mathcal{S}} \\quad AUC_{H_s,G_s} - \\lambda\n|\\Gamma^\\top\nC\n(s)|,\n\\end{align}\nwhere $\\lambda\\ge 0$ is a hyperparameter balancing ranking performance\nand fairness.\u003c/p\u003e\n\u003cp\u003eThe paper focuses on a special case of fairness, the \u003cstrong\u003eintra-group pairwise AUC fairness\u003c/strong\u003e. This was to be more concise. In this example, the objective function becomes:\u003c/p\u003e\n\u003cp\u003e$$\nL_\\lambda(s) = AUC_{H_s,G_s} - \\lambda  | AUC_{H_s^{(0)}, G_s^{(0)}} -  AUC_{H_s^{(1)}, G_s^{(1)} } |\n$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cu\u003e Issues of AUC-Based constraint:\u003c/u\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFairness using AUC-based constraints defined by the equality between two AUC’s only quantify a stochastic order between distributions, not the equality between these distributions, and would lead to some unfair result, for a group or for the other group.\u003c/p\u003e\n\u003cp\u003eThe authors conducted experiments with the credit-risk dataset and found that creditworthy individuals from both groups had equal chances of being ranked higher than a \u0026ldquo;bad borrower.\u0026rdquo; However, employing high thresholds (which represent low probabilities of default on approved loans) would result in unfair outcomes for one group.\u003c/p\u003e\n\u003ch1 id=\"section-4\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eROC-based fairness constraints\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eA richer approach is then to use \u003cstrong\u003epointwised ROC-based fairness constraints\u003c/strong\u003e. Ideally, we would want to enforce the equality of all score distributions between both groups (i.e., identical ROC curves). This would satisfy all AUC-based fairness constraints previously mentioned. However, this condition is so restrictive that it will most likely lead to a significant drop in performances. As a result, the authors propose to satisfy this constraint on only a \u003cstrong\u003efinite number of points\u003c/strong\u003e. They were indeed able to prove that this was sufficient to ensure maximum fairness for a fixed false positive or false negative  $\\alpha$.\u003c/p\u003e\n\u003cp\u003eAs a result, the objective function becomes :\u003c/p\u003e\n\u003cp\u003e\\begin{align*}\n% L_\\Lambda(s) =\nAUC_{H_s,G_s} \u0026amp;-\n\\sum_{k=1}^{m_H} \\lambda_H^{(k)}  \\big| \\Delta_{H,\\alpha_H^{\n(k)}}(s) \\big|\n- \\sum_{k=1}^{m_G} \\lambda_G^{(k)} \\big| \\Delta_{G,\\alpha_G^{(k)}}(s) \\big|,\n\\end{align*}\u003c/p\u003e\n\u003cp\u003eWhere $\\Delta_{H,\\alpha_H^{(k)}}(s)$ and $\\Delta_{G,\\alpha_G^{(k)}}(s)$ represent the deviations between the positive (resp. negative) inter-group ROCs and the identity function:\u003c/p\u003e\n\u003cp\u003e$$\n\\Delta_{G, \\alpha}(s) = ROC_{G^{(0)}_s, G^{(1)}_s}(\\alpha) - \\alpha\n$$\u003c/p\u003e\n\u003cp\u003e$$\n\\Delta_{H, \\alpha}(s) = ROC_{H^{(0)}_s,H^{(1)}_s}(\\alpha) - \\alpha\n$$\u003c/p\u003e\n\u003cp\u003eIn practice, the objective function is slightly modified to be able to maximise it. The authors applied a classic smooth surrogate relaxations of the AUCs or ROCs based on a logistic function. They also removed the absolute values and, instead, relied on some parameters to ensure positive values.\u003c/p\u003e\n\u003ch1 id=\"section-5\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eResults\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe authors tested out their results on two datasets : \u003cstrong\u003eCompas\u003c/strong\u003e and \u003cstrong\u003eAdult\u003c/strong\u003e. Both are widely used when it comes to fairness. Indeed, they are known to be biased against race (for Compas) and gender (for both). Compas is a recidivism prediction dataset, whereas Adult predicts whether income exceeds $50K/yr based on census data. The results reported in the next figure show that the ROC-based method achieves its goal of mitigating the differences between favoured and unfavoured groups with limited drop in performances (the AUC went from 0.72 to 0.70 on the Compas dataset and from 0.91 to 0.87 on the Adult dataset). Indeed, the blue ROC curve, which is the ROC curve of the unfavoured group (Afro-American people for the Compas Dataset and women for the Adult Dataset), is brought closer to the green ROC curve (the ROC curve of the favoured group).\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/main_text_inkscape_all_rocs_no_train_new.svg\"\n  alt=\"AUC\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch1 id=\"section-6\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eReproducibility\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eWe were able to run the provided code without too much trouble on WSL2. The only modification we had to make was to change the calls for python in the sh files. We replace \u003ccode\u003epython\u003c/code\u003e with \u003ccode\u003epython3\u003c/code\u003e. However, as mentionned in the cide, the experiments were very long to run (several days) and we were not able to run the \u003ccode\u003egenerate_all_figures.sh\u003c/code\u003e script fully as it made our computers crash. Still, we were able to get some of the figures found in the paper (see below) by launching some scripts separately.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/dist.png\"\n  alt=\"dist\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/lambert_davy/roc.png\"\n  alt=\"roc_gen\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere are two figure generated for the toy 1 dataset, one for the distribution of the scores and one for the ROC curve.\u003c/p\u003e\n\u003ch1 id=\"section-7\"\u003e\u003ch1 style=\"font-size: 24px; text-decoration: underline;\"\u003eConclusion\u003c/h1\u003e\u003c/h1\u003e\n\u003cp\u003eThe paper \u0026ldquo;Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints\u0026rdquo; underscores the growing importance of fairness in machine learning applications. It shows the limits of AUC-based fairness constraints for their inability to ensure equality between distributions, potentially leading to unfair outcomes. In contrast, ROC-based fairness constraints offer a richer approach by enforcing equality of score distributions between groups, albeit with some performance trade-offs. The paper tests the method on typical fairness datasets, but it is also possible to apply it to reel use cases. \u0026ldquo;A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization\u0026rdquo;, for example, explores the possibility to apply ROC-based methods for similarity learning, such as face recognition.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e","url":"https://responsible-ai-datascience-ipParis.github.io/posts/lambert-davy/","date_published":"23036-23-09T339:2323:00+01:00","date_modified":"23036-23-09T339:2323:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"7caf312b2212cb7ecf9bc99e08261d1c412013a5","title":"Label-Free Explainability","summary":"","content_text":"Label-Free Explainability for Unsupervised Models Authors: Valentina Hu and Selma Zarga Table of Contents Incentives Introduction Feature Importance Example Importance Experiment Conclusion This is a blog post about the paper Label-Free Explainability for Unsupervised Models, published by J. Crabbé et al. in 2022 and available here.\nWhy do we need explainability ? Machine learning models are becoming increasingly capable of making advanced predictions. While models like linear regression are relatively easy to understand and explain, more complex models, often called \u0026ldquo;black boxes\u0026rdquo; due to their complexity, present challenges in explaining how they make predictions. These models can be problematic in highstakes applications such as healthcare, finance, and justice, where it\u0026rsquo;s crucial to justify decision-making. Additionally, in case of errors, it\u0026rsquo;s important to understand the origin in order to address and correct them.\n\u0026ldquo;Explainability is the cornerstone of trust in black box models; without it, they remain inscrutable and unreliable.\u0026rdquo; - Yoshua Bengio\nTo tackle this challenge, the field of Explainable Artificial Intelligence (XAI) has emerged, offering various methods to enhance model transparency. Post-Hoc explainability methods exist, which intervene after the model has generated its results, enabling users to comprehend the reasoning behind specific decisions or predictions. These methods supplement the predictions of black box models with diverse explanations of how they arrive at their predictions.\nI. Introduction The entire post focuses on the quest for explainability of unsupervised models. In these models, no labels are assigned to the data, making understanding the model even more complicated due to the absence of explicit guidance on what the model is learning. In the supervised setting, users know the meaning of the black-box output they are trying to interpret. However, this clarity is not always available in machine learning. Therefore, elucidating concepts such as feature importance and example importance provides insights into why the model makes certain decisions or identifies specific patterns in the data.\nA recent research conducted by Crabbé and van der Schaar in 2022 explores the explainability of unsupervised models. They have developed two new methods to explain these complex models without labels. The first method highlights important features in the data, while the second identifies training examples that have the biggest impact on the model\u0026rsquo;s construction of representations. In this post, we will attempt to explain these two methods.\nII. Feature Importance Feature importance aims to explain how the model arrives at its prediction for a given input by assigning an importance scores to each feature (or attribute) of the input. This helps understand which features have the most influence on the model\u0026rsquo;s predictions. This method is developed based on a linear reasoning that is extended to label-free settings.\nGiven a model $( f : \\mathcal{X} \\rightarrow \\mathcal{Y} )$, which maps an input space $( \\mathcal{X} \\subset \\mathbb{R}^{d_X} )$ to an output space $( \\mathcal{Y} \\subset \\mathbb{R}^{d_Y} )$. Where, $( d_X )$ and $( d_Y )$ is the dimensions of the input and output spaces.\nIn the traditionnal method, the process requires selecting one component $( f_j(x) )$ of the model\u0026rsquo;s output to compute the importance score for each feature $( i )$, denoted as $( a_i(f, x) )$. The selection is based on the ground-truth label, and $( j )$ corresponds to the class predicted with the highest probability.\nTo understand how the label-free feature importance method works, let\u0026rsquo;s start by looking at the labeled case:\n1. Labelled Feature Importance\nAuthors introduces an alternative approach to calculate feature importance scores. The method proposes to combine the importance scores of different components of the model\u0026rsquo;s output by weighting them with the associated class probabilities. For each component of the model\u0026rsquo;s output, we multiply the importance score of the corresponding feature by the probability of that component.\nThese weighted importance scores are then combined to obtain the final importance score of each feature.\nLet $a_i(f_j;x)$ be the importance score of feature $x_i$ calculated with respect to the component $f_j$ of the model\u0026rsquo;s output. The method proposes to calculate the importance score $b_i(f;x)$ for feature $x_i$ as follows:\n$b_i(f;x) = \\sum_{j=1}^{d_Y} f_j(x) \\times a_i(f_j,x)$\nHere, $f_j(x)$ represents the probability of class $j$, and $a_i(f_j;x)$ is the importance score of feature $x_i$ for class $j$.\nHovewer when the class probabilities are balanced, this method accounts for the contribution of each class to the feature importance score, rather than focusing only on the class with the highest probability, which is the usual practice.\nThis method proves to be efficient predominantly when the significance scores exhibit linearity in relation to the model. To facilitate a streamlined computation of weighted importance scores, another method is to introduce an auxiliary function, denoted as $(g_x)$ :\n$\\ g_x(z) = \\sum_{j=1}^{d_Y} f_j(x) \\cdot f_j(z) $\nWith the function $(g_x)$, it becomes feasible to calculate the weighted importance score, $(b_i(f, x))$, for each feature $(i)$, by merely employing $(g_x)$. This technique significantly simplifies the computational process, obviating the need to calculate $(d_Y \\times d_X)$ importance scores. Such a calculation becomes impractically cumbersome with the escalation of the number of classes, $(d_Y)$. With this trick, we can compute the weighted importance score by only calling the auxiliary function.\nWe can see that in the labeled case, the method is quite clear. A similar reasoning is used in the label-free setting. Now, let\u0026rsquo;s move on to the label-free setting.\n2. Label-Free Feature Importance\nIn the context of the unlabelled setting, we consider a latent space $H$ of dimension $d_H$ where a black-box model $f : X \\rightarrow H$ is given. The goal is to assign an importance score to each feature of the input $x$, even if the dimensions of the latent space have no clear relations with the labels.\nA similar weighting formula for importance scores is used, where the components $f_j(x)$ do not correspond to probabilities but to neuron activations. The weighted sum is considered as a inner product in the latent space.\nThe method is developed using linear feature importance functions, and it retains the completeness property, meaning that the sum of importance scores equals the black-box prediction up to a baseline constant.\nHere is how the method operates:\nPresentation of the Latent Space: We consider a latent space $H$ of dimension $d_H$ where each input $x$ is mapped by the black-box model $f$ to obtain a representation $h = f(x)$.\nAssignment of Importance Scores: The objective is to assign an importance score $b_i(f; x)$ to each feature $x_i$ of $x$. Unlike in the previous setting, where we had probabilities associated with each component, here, we do not have a clear method to choose a particular component $f_j$ in the latent space. Therefore, we use a similar approach to the one described previously.\nCalculation of Importance Scores: We use a weighting method where the importance score is given by $b_i(f; x) = a_i(\\sum_{j=1}^{d_H} f_j(x) \\cdot f_j(x))$. The individual components $f_j(x)$ do not correspond to probabilities in this case; they generally correspond to neuron activation functions. Inactive neurons will have a corresponding component that vanishes ($f_j(x) = 0$), meaning they will not contribute to the weighted sum, while more activated neurons will contribute more.\nCompleteness: An important property shared by many feature importance methods is completeness. This means that the sum of importance scores equals the black-box prediction up to a baseline constant. This establishes a connection between importance scores and black-box predictions.\nThis method proposes an extension of linear feature importance methods to the unlabelled setting by defining an auxiliary scalar function $g_x$ that encapsulates the black-box function $f$. This extension is achieved by using a function $g_x$ that computes the inner product between the representation $f(x)$ and the representation $f(\\tilde{x})$ for all $\\tilde{x}$ in the input space.\nIII. Example Importance In this section, we explain the approach to extending example importance methods to the label-free setting. Given that example importance methods vary significantly, they are separated into two families: loss-based and representation-based methods. The extension to the label-free setting differs for these two families, so we discuss them separately in distinct subsections.\n1. Loss-Based Example Importance\nSupervised Setting\nIn supervised learning, loss-based example importance methods determine how important each training example is by assessing the impact of its removal on the model\u0026rsquo;s performance on test data. This is measured by the change in the loss function, which quantifies how well the model\u0026rsquo;s predictions match the true data.\nMathematically, let $z$ represent the data of an example required to evaluate the loss, typically corresponding to a pair $(x, y)$ in supervised settings. The loss function $L(z; \\theta)$ is optimized over a parameter space $\\Theta$ to train the model. When an example $z_n$ is removed from the training set $D_{\\text{train}}$, it results in a parameter shift $\\theta_n - \\theta\u0026rsquo;_{-n}$, impacting the loss $L(z; \\theta\u0026rsquo;)$ on a test example $z$. This loss shift provides a meaningful measure of example importance.\nTo estimate the loss shift without retraining the model, methods like the influence function and checkpoint evaluation are employed. For example, Koh \u0026amp; Liang (2017) propose using the influence function:\nInfluence Function Formula\n\\begin{equation} \\langle \\delta_{\\theta}^{n} L(z; \\theta\u0026rsquo;) \\approx \\frac{1}{N} \\langle {\\nabla L(z; \\theta_{*})}, H^{-1} {\\nabla L(z_{n}; \\theta_{*}\u0026rsquo;)} \\rangle_{\\theta} \\end{equation}\nWhere:\n$( \\nabla_{\\theta} L(z, \\theta^*) )$ is the gradient of the loss with respect to the parameters for the test example. $( H_{\\theta^*} )$ is the Hessian matrix. $( \\nabla_{\\theta} L(z^n, \\theta^*) )$ is the gradient of the loss for the removed training example. $( \\langle \\cdot, \\cdot \\rangle_{\\theta} )$ denotes the inner product in the parameter space. $( N )$ is the number of training examples. Label-Free Setting\nIn a label-free setting, the models are trained without explicit labels. Instead, they use a label-free loss function, which typically tries to capture the structure of the data itself rather than fitting to specific target labels.\nIn the context of autoencoders, determining the importance of a training example can be tricky due to the loss function used during training (uses the encoder and decoder). When we are only interested in the encoder part and it is not sufficient to only use the model\u0026rsquo;s loss function as this also include the influence of the decoder.\nTo address this, we decompose the parameter space into relevant and irrelevant components. The proposed method computes the example importance scores by considering only the relevant parameters. The model to interpret, denoted as $f_r$, is parametrized only by the relevant parameters $\\theta_r$.\nThis motivates the definition of Label-Free Loss-Based Example Importance:\n\\begin{equation} c_n(f_r; x) = \\theta_n L(x; \\theta\u0026rsquo;) \\end{equation}\nLabel-Free Loss-Based Example Importance score $( c_n(f_{\\theta_r}, x) )$ measures the impact of removing a training example $( x_n )$ from the training set on the learned latent representation $( f_{\\theta_r}(x) )$ of a test example $( x )$. It uses $( \\delta_{\\theta_r} L )$ to denote the part of the loss shift that is only due to changes in the relevant parameters $(( \\theta_r ))$.\nThis definition extends any loss-based example importance method to the label-free setting, where the unsupervised loss $L$ is used to fit the model, and the gradients with respect to the parameters of the encoder are computed.\n2. Representation-Based Example Importance\nRepresentation-based example importance methods analyze the latent representations of examples to assign importance scores.\nSupervised Setting\nThese methods quantify the affinity between a test example and the training set examples based on their latent representations. For instance, in a model $f_l \\circ f_e: X \\rightarrow Y$, where $f_e: X \\rightarrow H$ maps inputs to latent representations and $f_l: H \\rightarrow Y$ maps representations to labels, representation-based methods reconstruct the test example\u0026rsquo;s latent representation using training representations. The reconstruction involves assigning weights to training representations, typically based on nearest neighbors or learned weights. For example, using a kernel function $\\mathcal{K}$:\n\\begin{equation} w_n(x) = \\frac{1}{|KNN(x)|} \\sum_{n\u0026rsquo; \\in KNN(x)} \\mathcal{K}(\\text{fe}(x_n), \\text{fe}(x)) \\end{equation}\nLabel-Free Setting\nRrepresentation-based methods remain valid by replacing supervised representation maps with unsupervised ones. Hence, no additional modifications are needed.\nIV. Experiments: Evaluation and Results Consistency Checks Now, we are verifying the consistency of results obtained from different methods of assessing feature and example importance using the MNIST dataset.\nIn MNIST, important features are the pixels of the images, and various methods can be employed to evaluate their importance. To assess feature importance, we can measure the impact of selectively removing the most important pixels on the latent representation constructed by the encoder, as described in the previous example. By comparing the results of different methods of importance assessment, such as perturbing the most important pixels according to various importance measures, we can check if the same pixels are identified as important and if their removal consistently affects the latent representation.\nWe rerun the tests provided in the GitHub repository:\nOn the MNIST dataset, we perturb the most important pixels and observe how this perturbation affects the quality or relevance of the latent representation generated by the encoder. Here we can see the result of the experiment :\nThe results obtained from the representation shift curves as a function of the percentage of perturbed pixels demonstrate the effectiveness of Feature Importance methods on the MNIST dataset.\nWe observe that Feature Importance methods such as Gradient Shap and Integrated Gradients show a significant increase in representation shift when the most important pixels are perturbed. This indicates that these methods successfully identify the most relevant pixels for constructing the latent representation. However, after perturbing approximately 20% of the most important pixels, we notice a stabilization of the representation shift, suggesting that adding additional perturbations does not necessarily lead to a significant increase in impact on the latent representation.\nOn the other hand, the Saliency method appears to be less effective, with an almost linear representation shift curve, suggesting that it fails to selectively identify the most important pixels for the latent representation.\nOverall, this confirms the effectiveness of Feature Importance methods, particularly Integrated Gradients.\nSimilarly, to evaluate the importance of examples in MNIST, we select training examples that have a significant influence on predicting the latent representation of test examples. By comparing the results obtained with different methods of assessing example importance, we can verify if the same examples are identified as important and if their relevance is consistent with the model\u0026rsquo;s predictions.\nFor all example importance methods, we observe a decrease in similarity rates, with a consistent trend across all curves.\nThis observation highlights that the similarity rate is significantly higher among the most similar examples compared to the least similar examples, confirming the effectiveness of label-free importance scores cn(fe; x) in identifying training examples related to the test example we wish to explain.\nIn summary, these results affirm the capability of label-free importance scores in effectively selecting relevant training examples and distinguishing between similar and dissimilar examples.\nV. Conclusion In this post you learned about label-free explainability a new framework developped by Crabbé and van der Schaar in 2022, wich extend linear feature importance and example importance methods to the unsupervised setting with a focus on the MNIST dataset.\nReferences Crabbé, J. \u0026amp; van der Schaar, M.. (2022). Label-Free Explainability for Unsupervised Models. Proceedings of the 39th International Conference on Machine Learning, in Proceedings of Machine Learning Research 162:4391-4420 Available from https://proceedings.mlr.press/v162/crabbe22a.html. ","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eLabel-Free Explainability for Unsupervised Models\u003c/h1\u003e\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: \u003ca href=\"https://github.com/Valentinahxu\"\u003eValentina Hu \u003c/a\u003e and  \u003ca href=\"https://github.com/selmazrg\"\u003e Selma Zarga\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIncentives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eFeature Importance \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eExample Importance\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Label-Free Explainability for Unsupervised Models, published by J. Crabbé et al. in 2022 and available \u003ca href=\"https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eWhy do we need explainability ?\u003c/h2\u003e\n\u003cp\u003eMachine learning models are becoming increasingly capable of making advanced predictions. While models like linear regression are relatively easy to understand and explain, more complex models, often called \u003cstrong\u003e\u0026ldquo;black boxes\u0026rdquo;\u003c/strong\u003e due to their complexity, present challenges in explaining how they make predictions. These models can be problematic in highstakes applications such as healthcare, finance, and justice, where it\u0026rsquo;s crucial to justify decision-making. Additionally, in case of errors, it\u0026rsquo;s important to understand the origin in order to address and correct them.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cp\u003e\u0026ldquo;\u003cstrong\u003eExplainability is the cornerstone of trust in black box models; without it, they remain inscrutable and unreliable.\u003c/strong\u003e\u0026rdquo; - \u003cem\u003eYoshua Bengio\u003c/em\u003e\u003c/p\u003e\n\u003c/center\u003e\n\u003cp\u003eTo tackle this challenge, the field of Explainable Artificial Intelligence (XAI) has emerged, offering various methods to enhance \u003cstrong\u003emodel transparency\u003c/strong\u003e. \u003cstrong\u003ePost-Hoc explainability\u003c/strong\u003e methods exist, which intervene after the model has generated its results, enabling users to comprehend the reasoning behind specific decisions or predictions. These methods supplement the predictions of black box models with diverse explanations of how they arrive at their predictions.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/explainability/Black-Box.png\"\n  alt=\"XAI explainability\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eI. Introduction\u003c/h2\u003e\n\u003cp\u003eThe entire post focuses on the quest for explainability of unsupervised models. In these models, no labels are assigned to the data, making understanding the model even more complicated due to the absence of explicit guidance on what the model is learning. In the supervised setting, users know the meaning of the black-box output they are trying to interpret. However, this clarity is not always available in machine learning. Therefore, elucidating concepts such as feature importance and example importance provides insights into why the model makes certain decisions or identifies specific patterns in the data.\u003c/p\u003e\n\u003cp\u003eA recent research conducted by Crabbé and van der Schaar in 2022 explores the explainability of unsupervised models. They have developed two new methods to explain these complex models without labels. The first method highlights important features in the data, while the second identifies training examples that have the biggest impact on the model\u0026rsquo;s construction of representations. In this post, we will attempt to explain these two methods.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eII. Feature Importance\u003c/h2\u003e\n\u003cp\u003eFeature importance aims to explain how the model arrives at its prediction for a given input by assigning an  importance scores to each feature (or attribute) of the input. This helps understand which features have the most influence on the model\u0026rsquo;s predictions. This method is developed based on a linear reasoning that is extended to label-free settings.\u003c/p\u003e\n\u003cp\u003eGiven a model $( f : \\mathcal{X} \\rightarrow \\mathcal{Y} )$, which maps an input space $( \\mathcal{X} \\subset \\mathbb{R}^{d_X} )$ to an output space $( \\mathcal{Y} \\subset \\mathbb{R}^{d_Y} )$. Where, $( d_X )$ and $( d_Y )$ is the dimensions of the input and output spaces.\u003c/p\u003e\n\u003cp\u003eIn the traditionnal method, the process requires selecting one component $( f_j(x) )$ of the model\u0026rsquo;s output to compute the importance score for each feature $( i )$, denoted as $( a_i(f, x) )$. The selection is based on the ground-truth label, and $( j )$ corresponds to the class predicted with the highest probability.\u003c/p\u003e\n\u003cp\u003eTo understand how the label-free feature importance method works, let\u0026rsquo;s start by looking at the labeled case:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Labelled Feature Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAuthors introduces an alternative approach to calculate feature importance scores. The method proposes to combine the importance scores of different components of the model\u0026rsquo;s output by weighting them with the associated class probabilities. For each component of the model\u0026rsquo;s output, we multiply the importance score of the corresponding feature by the probability of that component.\u003c/p\u003e\n\u003cp\u003eThese weighted importance scores are then combined to obtain the final importance score of each feature.\u003c/p\u003e\n\u003cp\u003eLet $a_i(f_j;x)$ be the importance score of feature $x_i$ calculated with respect to the component $f_j$ of the model\u0026rsquo;s output. The method proposes to calculate the importance score $b_i(f;x)$ for feature $x_i$ as follows:\u003c/p\u003e\n\u003cp\u003e$b_i(f;x) = \\sum_{j=1}^{d_Y} f_j(x) \\times a_i(f_j,x)$\u003c/p\u003e\n\u003cp\u003eHere, $f_j(x)$ represents the probability of class $j$, and $a_i(f_j;x)$ is the importance score of feature $x_i$ for class $j$.\u003c/p\u003e\n\u003cp\u003eHovewer when the class probabilities are balanced, this method accounts for the contribution of each class to the feature importance score, rather than focusing only on the class with the highest probability, which is the usual practice.\u003c/p\u003e\n\u003cp\u003eThis method proves to be efficient predominantly when the significance scores exhibit linearity in relation to the model. To facilitate a streamlined computation of weighted importance scores, another method is to introduce an auxiliary function, denoted as $(g_x)$ :\u003c/p\u003e\n\u003cp\u003e$\\ g_x(z) = \\sum_{j=1}^{d_Y} f_j(x) \\cdot f_j(z) $\u003c/p\u003e\n\u003cp\u003eWith the function $(g_x)$, it becomes feasible to calculate the weighted importance score, $(b_i(f, x))$, for each feature $(i)$, by merely employing $(g_x)$. This technique significantly simplifies the computational process, obviating the need to calculate $(d_Y \\times d_X)$ importance scores. Such a calculation becomes impractically cumbersome with the escalation of the number of classes, $(d_Y)$. With this trick, we can compute the weighted importance score by only calling the auxiliary function.\u003c/p\u003e\n\u003cp\u003eWe can see that in the labeled case, the method is quite clear. A similar reasoning is used in the label-free setting. Now, let\u0026rsquo;s move on to the label-free setting.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Label-Free Feature Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn the context of the unlabelled setting, we consider a latent space $H$ of dimension $d_H$ where a black-box model $f : X \\rightarrow H$ is given. The goal is to assign an importance score to each feature of the input $x$, even if the dimensions of the latent space have no clear relations with the labels.\u003c/p\u003e\n\u003cp\u003eA similar weighting formula for importance scores is used, where the components $f_j(x)$ do not correspond to probabilities but to neuron activations. The weighted sum is considered as a inner product in the latent space.\u003c/p\u003e\n\u003cp\u003eThe method is developed using linear feature importance functions, and it retains the completeness property, meaning that the sum of importance scores equals the black-box prediction up to a baseline constant.\u003c/p\u003e\n\u003cp\u003eHere is how the method operates:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePresentation of the Latent Space:\u003c/strong\u003e We consider a latent space $H$ of dimension $d_H$ where each input $x$ is mapped by the black-box model $f$ to obtain a representation $h = f(x)$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAssignment of Importance Scores:\u003c/strong\u003e The objective is to assign an importance score $b_i(f; x)$ to each feature $x_i$ of $x$. Unlike in the previous setting, where we had probabilities associated with each component, here, we do not have a clear method to choose a particular component $f_j$ in the latent space. Therefore, we use a similar approach to the one described previously.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCalculation of Importance Scores:\u003c/strong\u003e We use a weighting method where the importance score is given by $b_i(f; x) = a_i(\\sum_{j=1}^{d_H} f_j(x) \\cdot f_j(x))$. The individual components $f_j(x)$ do not correspond to probabilities in this case; they generally correspond to neuron activation functions. Inactive neurons will have a corresponding component that vanishes ($f_j(x) = 0$), meaning they will not contribute to the weighted sum, while more activated neurons will contribute more.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCompleteness:\u003c/strong\u003e An important property shared by many feature importance methods is completeness. This means that the sum of importance scores equals the black-box prediction up to a baseline constant. This establishes a connection between importance scores and black-box predictions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis method proposes an extension of linear feature importance methods to the unlabelled setting by defining an auxiliary scalar function $g_x$ that encapsulates the black-box function $f$. This extension is achieved by using a function $g_x$ that computes the inner product between the representation $f(x)$ and the representation $f(\\tilde{x})$ for all $\\tilde{x}$ in the input space.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eIII. Example Importance\u003c/h2\u003e\n\u003cp\u003eIn this section, we explain the approach to extending example importance methods to the label-free setting. Given that example importance methods vary significantly, they are separated into two families: loss-based and representation-based methods. The extension to the label-free setting differs for these two families, so we discuss them separately in distinct subsections.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Loss-Based Example Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSupervised Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn supervised learning, loss-based example importance methods determine how important each training example is by assessing the impact of its removal on the model\u0026rsquo;s performance on test data. This is measured by the change in the loss function, which quantifies how well the model\u0026rsquo;s predictions match the true data.\u003c/p\u003e\n\u003cp\u003eMathematically, let $z$ represent the data of an example required to evaluate the loss, typically corresponding to a pair $(x, y)$ in supervised settings. The loss function $L(z; \\theta)$ is optimized over a parameter space $\\Theta$ to train the model. When an example $z_n$ is removed from the training set $D_{\\text{train}}$, it results in a parameter shift $\\theta_n - \\theta\u0026rsquo;_{-n}$, impacting the loss $L(z; \\theta\u0026rsquo;)$ on a test example $z$. This loss shift provides a meaningful measure of example importance.\u003c/p\u003e\n\u003cp\u003eTo estimate the loss shift without retraining the model, methods like the influence function and checkpoint evaluation are employed. For example, Koh \u0026amp; Liang (2017) propose using the influence function:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eInfluence Function Formula\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\\begin{equation} \\langle\n\\delta_{\\theta}^{n} L(z; \\theta\u0026rsquo;) \\approx \\frac{1}{N} \\langle {\\nabla L(z; \\theta_{*})}, H^{-1} {\\nabla L(z_{n}; \\theta_{*}\u0026rsquo;)} \\rangle_{\\theta} \u003cbr\u003e\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$( \\nabla_{\\theta} L(z, \\theta^*) )$ is the gradient of the loss with respect to the parameters for the test example.\u003c/li\u003e\n\u003cli\u003e$( H_{\\theta^*} )$ is the Hessian matrix.\u003c/li\u003e\n\u003cli\u003e$( \\nabla_{\\theta} L(z^n, \\theta^*) )$ is the gradient of the loss for the removed training example.\u003c/li\u003e\n\u003cli\u003e$( \\langle \\cdot, \\cdot \\rangle_{\\theta} )$ denotes the inner product in the parameter space.\u003c/li\u003e\n\u003cli\u003e$( N )$ is the number of training examples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLabel-Free Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn a label-free setting, the models are trained without explicit labels. Instead, they use a label-free loss function, which typically tries to capture the structure of the data itself rather than fitting to specific target labels.\u003c/p\u003e\n\u003cp\u003eIn the context of autoencoders, determining the importance of a training example can be tricky due to the loss function used during training (uses the encoder and decoder). When we are only interested in the encoder part and it is not sufficient to only use the model\u0026rsquo;s loss function as this also include the influence of the decoder.\u003c/p\u003e\n\u003cp\u003eTo address this, we decompose the parameter space into relevant and irrelevant components. The proposed method computes the example importance scores by considering only the relevant parameters. The model to interpret, denoted as $f_r$, is parametrized only by the relevant parameters $\\theta_r$.\u003c/p\u003e\n\u003cp\u003eThis motivates the definition of Label-Free Loss-Based Example Importance:\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\nc_n(f_r; x) = \\theta_n L(x; \\theta\u0026rsquo;)\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eLabel-Free Loss-Based Example Importance score $( c_n(f_{\\theta_r}, x) )$ measures the impact of removing a training example $( x_n )$ from the training set on the learned latent representation $( f_{\\theta_r}(x) )$ of a test example $( x )$. It uses $( \\delta_{\\theta_r} L )$ to denote the part of the loss shift that is only due to changes in the relevant parameters $(( \\theta_r ))$.\u003c/p\u003e\n\u003cp\u003eThis definition extends any loss-based example importance method to the label-free setting, where the unsupervised loss $L$ is used to fit the model, and the gradients with respect to the parameters of the encoder are computed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Representation-Based Example Importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRepresentation-based example importance methods analyze the latent representations of examples to assign importance scores.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSupervised Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThese methods quantify the affinity between a test example and the training set examples based on their latent representations. For instance, in a model $f_l \\circ f_e: X \\rightarrow Y$, where $f_e: X \\rightarrow H$ maps inputs to latent representations and $f_l: H \\rightarrow Y$ maps representations to labels, representation-based methods reconstruct the test example\u0026rsquo;s latent representation using training representations. The reconstruction involves assigning weights to training representations, typically based on nearest neighbors or learned weights. For example, using a kernel function $\\mathcal{K}$:\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\nw_n(x) = \\frac{1}{|KNN(x)|} \\sum_{n\u0026rsquo; \\in KNN(x)} \\mathcal{K}(\\text{fe}(x_n), \\text{fe}(x))\n\\end{equation}\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLabel-Free Setting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRrepresentation-based methods remain valid by replacing supervised representation maps with unsupervised ones. Hence, no additional modifications are needed.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eIV. Experiments: Evaluation and Results\u003c/h2\u003e\n\u003ch3 id=\"section-111\"\u003eConsistency Checks\u003c/h3\u003e\n\u003cp\u003eNow, we are verifying the consistency of results obtained from different methods of assessing feature and example importance using the MNIST dataset.\u003c/p\u003e\n\u003cp\u003eIn MNIST, important features are the pixels of the images, and various methods can be employed to evaluate their importance. To assess feature importance, we can measure the impact of selectively removing the most important pixels on the latent representation constructed by the encoder, as described in the previous example. By comparing the results of different methods of importance assessment, such as perturbing the most important pixels according to various importance measures, we can check if the same pixels are identified as important and if their removal consistently affects the latent representation.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eWe rerun the tests provided in the \u003ca href=\"https://github.com/JonathanCrabbe/Label-Free-XAI\"\u003eGitHub repository\u003c/a\u003e:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eOn the MNIST dataset, we perturb the most important pixels and observe how this perturbation affects the quality or relevance of the latent representation generated by the encoder.\nHere we can see the result of the experiment :\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/explainability/mnist_consistency_features.png\"\n  alt=\"XAI explainability\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe results obtained from the representation shift curves as a function of the percentage of perturbed pixels demonstrate the effectiveness of Feature Importance methods on the MNIST dataset.\u003c/p\u003e\n\u003cp\u003eWe observe that Feature Importance methods such as Gradient Shap and Integrated Gradients show a significant increase in representation shift when the most important pixels are perturbed. This indicates that these methods successfully identify the most relevant pixels for constructing the latent representation. However, after perturbing approximately 20% of the most important pixels, we notice a stabilization of the representation shift, suggesting that adding additional perturbations does not necessarily lead to a significant increase in impact on the latent representation.\u003c/p\u003e\n\u003cp\u003eOn the other hand, the Saliency method appears to be less effective, with an almost linear representation shift curve, suggesting that it fails to selectively identify the most important pixels for the latent representation.\u003c/p\u003e\n\u003cp\u003eOverall, this confirms the effectiveness of Feature Importance methods, particularly Integrated Gradients.\u003c/p\u003e\n\u003cp\u003eSimilarly, to evaluate the importance of examples in MNIST, we select training examples that have a significant influence on predicting the latent representation of test examples. By comparing the results obtained with different methods of assessing example importance, we can verify if the same examples are identified as important and if their relevance is consistent with the model\u0026rsquo;s predictions.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/explainability/example.png\"\n  alt=\"XAI explainability\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eFor all example importance methods, we observe a decrease in similarity rates, with a consistent trend across all curves.\u003c/p\u003e\n\u003cp\u003eThis observation highlights that the similarity rate is significantly higher among the most similar examples compared to the least similar examples, confirming the effectiveness of label-free importance scores cn(fe; x) in identifying training examples related to the test example we wish to explain.\u003c/p\u003e\n\u003cp\u003eIn summary, these results affirm the capability of label-free importance scores in effectively selecting relevant training examples and distinguishing between similar and dissimilar examples.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eV. Conclusion\u003c/h2\u003e\n\u003cp\u003eIn this post you learned about label-free explainability a new framework developped by Crabbé and van der Schaar in 2022, wich extend linear feature importance and example importance\nmethods to the unsupervised setting with a focus on the MNIST dataset.\u003c/p\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eCrabbé, J. \u0026amp; van der Schaar, M.. (2022). Label-Free Explainability for Unsupervised Models. \u003ci\u003eProceedings of the 39th International Conference on Machine Learning\u003c/i\u003e, in \u003ci\u003eProceedings of Machine Learning Research\u003c/i\u003e 162:4391-4420 Available from \u003ca href=\"https://proceedings.mlr.press/v162/crabbe22a.html\"\u003ehttps://proceedings.mlr.press/v162/crabbe22a.html\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cstyle\u003e\n.highlight {\n  background-color: red;\n}\n.highlight-on-hover:hover {\n        background-color: yellow;\n    }\n/* Quiz form styles */\n.quiz-form {\n        max-width: 500px;\n        margin: auto;\n        padding: 20px;\n        border: 1px solid #ccc;\n        border-radius: 5px;\n        background-color: #f9f9f9;\n}\n\n.quiz-question {\n        margin-bottom: 20px;\n}\n\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n}\n\n.quiz-submit {\n        background-color: #4caf50;\n        color: white;\n        padding: 10px 20px;\n        border: none;\n        border-radius: 5px;\n        cursor: pointer;\n}\n\n.quiz-submit:hover {\n        background-color: #45a049;\n}\n\n/* Quiz results styles */\n.quiz-results {\n        margin-top: 20px;\n        font-weight: bold;\n}\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n    }\n.quiz-options label.correct {\n        color: green;\n}\n.quiz-options label.incorrect {\n        color: red;\n}\na[name]:hover {\n        background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */\n        text-decoration: none; /* Optionally remove underline on hover */\n}\n\u003c/style\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/label-free-explainability/","date_published":"17036-17-09T331:1717:00+01:00","date_modified":"17036-17-09T331:1717:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"9bec8a0b2d12c702729b80a3b910f16d3d73250f","title":"Adversarially Reweighted Learning","summary":"","content_text":"Fairness without Demographics through Adversarially Reweighted Learning Authors: Pierre Fihey \u0026 Guerlain Messin Table of Contents Fairness issues in ML and AI The privacy of demographic’s data The Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels Computational identifiability of protected groups The Rawlsian Max-Min Fairness principle The ARL objective The Model Architecture Results analysis Conclusion This is a blog post about the paper Fairness without Demographics through Adversarially Reweighted Learning, published by P. Lahoti et al. in 2020 and available here.\nFairness issues in ML and AI As Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\nSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments. Machine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\nThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\nThe privacy of demographic’s data Strict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\nIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\nThe Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels While access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\nThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\nThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\nComputational identifiability of protected groups Computational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\nFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\nThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\nThe Rawlsian Max-Min Fairness principle In philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\nDefinition (Rawslan Max-Min Fairness): Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility. $$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\nThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged. This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\nThe ARL objective To adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally. The aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\n$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$ $$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\nWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\nThe Model Architecture As previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\nThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\nThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\nThe learner then adjusts itself to minimize the adversarial loss: $$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\nTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\n$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\nThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\nResults analysis This section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\nReproducibility We first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\nReplicability We replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\nSignificance Evaluation We conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\nConclusion In this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\nAnnexes References [1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\n[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\n[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\n[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\n[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\n[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\n[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\n[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\n[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\n","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eFairness without Demographics through Adversarially Reweighted Learning\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Pierre Fihey \u0026 Guerlain Messin\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eFairness issues in ML and AI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eThe privacy of demographic’s data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eComputational identifiability of protected groups\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eThe ARL objective\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eThe Model Architecture\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eResults analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-9\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Fairness without Demographics through Adversarially Reweighted Learning, published by P. Lahoti et al. in 2020 and available \u003ca href=\"https://dl.acm.org/doi/abs/10.5555/3495724.3495786\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eFairness issues in ML and AI\u003c/h2\u003e\n\u003cp\u003eAs Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\u003c/p\u003e\n\u003cp\u003eSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments.\nMachine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\u003c/p\u003e\n\u003cp\u003eThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eThe privacy of demographic’s data\u003c/h2\u003e\n\u003cp\u003eStrict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\u003c/p\u003e\n\u003cp\u003eIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/h2\u003e\n\u003ch3 id=\"section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/h3\u003e\n\u003cp\u003eWhile access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\u003c/p\u003e\n\u003cp\u003eThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Fihey_Messin/Identifying_Groups.png\"\n  alt=\"Identifying Groups\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\u003c/p\u003e\n\u003ch3 id=\"section-4\"\u003eComputational identifiability of protected groups\u003c/h3\u003e\n\u003cp\u003eComputational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\u003c/p\u003e\n\u003cp\u003eFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\u003c/p\u003e\n\u003cp\u003eThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/h3\u003e\n\u003cp\u003eIn philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition (Rawslan Max-Min Fairness):\u003c/strong\u003e Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility.\n$$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\u003c/p\u003e\n\u003cp\u003eThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged.  This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\u003c/p\u003e\n\u003ch3 id=\"section-6\"\u003eThe ARL objective\u003c/h3\u003e\n\u003cp\u003eTo adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally.\nThe aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\u003c/p\u003e\n\u003cp\u003e$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$\n$$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eThe Model Architecture\u003c/h3\u003e\n\u003cp\u003eAs previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\u003c/p\u003e\n\u003cp\u003eThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\u003c/p\u003e\n\u003cp\u003eThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\u003c/p\u003e\n\u003cp\u003eThe learner then adjusts itself to minimize the adversarial loss:\n$$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\u003c/p\u003e\n\u003cp\u003e$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\u003c/p\u003e\n\u003cp\u003eThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Fihey_Messin/ARL_Computational_Graph.png\"\n  alt=\"ARL Computational Graph\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch2 id=\"section-8\"\u003eResults analysis\u003c/h2\u003e\n\u003cp\u003eThis section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\u003c/p\u003e\n\u003ch3 id=\"reproducibility\"\u003eReproducibility\u003c/h3\u003e\n\u003cp\u003eWe first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\u003c/p\u003e\n\u003ch3 id=\"replicability\"\u003eReplicability\u003c/h3\u003e\n\u003cp\u003eWe replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\u003c/p\u003e\n\u003ch3 id=\"significance-evaluation\"\u003eSignificance Evaluation\u003c/h3\u003e\n\u003cp\u003eWe conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\u003c/p\u003e\n\u003ch2 id=\"section-9\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\u003c/p\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003cp\u003e[1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\u003c/p\u003e\n\u003cp\u003e[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\u003c/p\u003e\n\u003cp\u003e[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\u003c/p\u003e\n\u003cp\u003e[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\u003c/p\u003e\n\u003cp\u003e[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\u003c/p\u003e\n\u003cp\u003e[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\u003c/p\u003e\n\u003cp\u003e[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\u003c/p\u003e\n\u003cp\u003e[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\u003c/p\u003e\n\u003cp\u003e[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/adversarially_reweighted_learning/","date_published":"4036-04-09T335:44:00+01:00","date_modified":"4036-04-09T335:44:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"d7ac2f8435ec113c47081837ac2b76e60dcb8690","title":"Packed Ensembles","summary":"","content_text":" This is a blog post about the paper Packed-Ensembles for Efficient Uncertainty Estimation, published by O. Laurent et al. in 2023 and available [here](https://openreview.net/pdf?id=XXTyv1zD9zD). Authors: Cynthia Obeid and Elie Nakad Introduction The document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems. Presentation of the model Packed-Ensembles\nThe base network and Packed-Ensembles\nPacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\nBenefits of Packed-Ensembles\nPacked-Ensembles offer several benefits over traditional ensemble methods, including:\nEfficiency: Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\nAccuracy: Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\nCalibration: Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\nOut-of-distribution (OOD) detection: Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\nComparison to other ensemble methods\nThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\nPacked-Ensembles: A Technique for Efficient Neural Network Ensembles Packed-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\nUnderstanding Convolutional Layers and Grouped Convolutions:\nConvolutional Layers: These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows: $z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\nwhere:\n$c$ represents the channel index\n$h^j$ denotes the input feature map\n$ω^j$ represents the weight tensor (kernel)\n$⋆$ denotes the 2D cross-correlation operator\nGrouped Convolutions: This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating independent subnetworks. The mathematical formulation for grouped convolutions is given by:\n$$ z^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right) $$\nwhere:\n$γ$ represents the number of groups $C_{j+1}$ and $C_j$ denote the number of output and input channels, respectively. The formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask $\\text{mask}_{m}^j$ $\\in \\{{ 0, 1 \\}}^{C_{j+1} \\times C_j \\times s_j^2}$ with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\nThe condition $\\text{mask}_{m}^j(k, l, :, :) = 1$ happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\nComplete Mask and Convolution: $\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$. $z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here: $z^{j+1}$: Output feature map of the layer. $h^j$: Input feature map. $ω^j$: Convolution weights for layer j. $\\otimes$: Denotes convolution operation. $◦$: Denotes Hadamard product (element-wise multiplication). In simpler terms:\nGrouped convolution divides the input channels and weights into groups. A separate mask is created for each group, ensuring elements within a group are aligned. These masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group. The final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask. Background on Deep Ensembles This section delves into Deep Ensembles (DE), a technique for image classification tasks.\nDeep Ensembles\nSetting the Scene\nWe have a dataset $D$ containing pairs of images and their corresponding labels:\n$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width). $y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes). The dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\nA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\n$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$. Traditional Approach:\nThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\nIntroducing Deep Ensembles\nDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\nThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\n$$ P(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m) $$\nThis essentially combines the outputs of multiple networks to create a more robust prediction.\nIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\nBuilding Packed-Ensembles:\nPacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\nSubnetworks: The ensemble is formed by creating $M$ smaller subnetworks within the main network architecture. These subnetworks share the same structure but have independent parameters due to the use of grouped convolutions. Hyperparameters: Packed-Ensembles are defined by three hyperparameters: $α$ (alpha): expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters). $M$: number of subnetworks in the ensemble (represents the ensemble size). $γ$ (gamma): number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity). Mathematical Implementation:\nThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\n$$ \\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j $$\nwhere:\n$\\hat{y}$ represents the ensemble\u0026rsquo;s predicted label $P(y|θ_a^m, x)$ denotes the probability of class $y$ given the input $x$ and the parameters $θ_a^m$ of the $m-th$ subnetwork $\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$ represents the parameters of the $m-th$ subnetwork, obtained by applying element-wise multiplication ($∘$) between the expanded weights ($\\omega_j^{\\alpha}$) and the group mask ($\\text{mask}_{m}$) for each layer $j$ Implementation\nEquivalent architectures for Packed-Ensembles\nThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\nDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\nExperiments The experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\nDatasets: CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels. Architectures: PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. Metrics: Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used. Implementation Details: Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift. Code: PyTorch-Lightning framework is used for implementation. Results The experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\nCIFAR-10/100: PE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet). Smaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100. ImageNet: PE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models. PE achieves better accuracy with a reasonable increase in training and inference cost. These results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\nPacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\nEthics This section emphasizes the ethical considerations of the research. Here are the key points:\nGoal: This research proposes a method to improve uncertainty estimation in deep learning models. Limitations: The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications. Concerns: The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like: Unknown situations Corner cases (uncommon but important situations) Adversarial attacks (attempts to intentionally mislead the model) Potential biases in the model Overall: The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems. Reproducibility: Packed-Ensemble on CIFAR-10 We attempted to reproduce the experiment outlined in the tutorial available at https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\nData Loading and Preprocessing: Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images. Packed-Ensemble Definition: Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture. Loss Function and Optimizer: Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training. Training: Training the Packed-Ensemble model on the CIFAR-10 training data. Testing and Evaluation: Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance. Experimental Runs and Observations:\nTest 1:\nGroundTruth: cat ship ship plane\nThe predicted labels are: cat ship ship ship\nTest 2:\nGroundTruth: dog bird horse bird\nThe predicted labels are: dog frog car dog\nTest 3:\nGroundTruth: dog truck plane car The predicted labels are: dog horse ship truck\nChallenges and Limitations:\nA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\n","content_html":"\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\nThis is a blog post about the paper Packed-Ensembles for Efficient Uncertainty Estimation, published by O. Laurent et al. in 2023 and available [here](https://openreview.net/pdf?id=XXTyv1zD9zD).\n\u003ch3 id=\"authors-cynthia-obeid-and-elie-nakad\"\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e: Cynthia Obeid and Elie Nakad\u003c/h3\u003e\n\u003ch1\u003eIntroduction\u003c/h1\u003e\n\u003c/div\u003e\nThe document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems.\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003ePresentation of the model\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003ePacked-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig1.jpg\" alt=\"The base network and Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eThe base network and Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBenefits of Packed-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles offer several benefits over traditional ensemble methods, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEfficiency:\u003c/strong\u003e Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccuracy:\u003c/strong\u003e Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCalibration:\u003c/strong\u003e Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOut-of-distribution (OOD) detection:\u003c/strong\u003e Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eComparison to other ensemble methods\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003ePacked-Ensembles: A Technique for Efficient Neural Network Ensembles\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003ePacked-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUnderstanding Convolutional Layers and Grouped Convolutions:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConvolutional Layers:\u003c/strong\u003e These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$c$\u003c/strong\u003e represents the channel index\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$h^j$\u003c/strong\u003e denotes the input feature map\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$ω^j$\u003c/strong\u003e represents the weight tensor (kernel)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$⋆$\u003c/strong\u003e denotes the 2D cross-correlation operator\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGrouped Convolutions:\u003c/strong\u003e This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating \u003cstrong\u003eindependent subnetworks\u003c/strong\u003e. The mathematical formulation for grouped convolutions is given by:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nz^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right)\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$\u003c/strong\u003e represents the number of groups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$C_{j+1}$\u003c/strong\u003e and \u003cstrong\u003e$C_j$\u003c/strong\u003e denote the number of output and input channels, respectively.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask \u003cstrong\u003e$\\text{mask}_{m}^j$\u003c/strong\u003e\n\u003cstrong\u003e$\\in \\{{ 0, 1 \\}}^{C_{j+1} \\times C_j \\times s_j^2}$\u003c/strong\u003e with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\u003c/p\u003e\n\u003cp\u003eThe condition \u003cstrong\u003e$\\text{mask}_{m}^j(k, l, :, :) = 1$\u003c/strong\u003e happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComplete Mask and Convolution:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e$\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$.\u003c/li\u003e\n\u003cli\u003e$z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here:\n\u003cul\u003e\n\u003cli\u003e$z^{j+1}$: Output feature map of the layer.\u003c/li\u003e\n\u003cli\u003e$h^j$: Input feature map.\u003c/li\u003e\n\u003cli\u003e$ω^j$: Convolution weights for layer \u003ccode\u003ej\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e$\\otimes$: Denotes convolution operation.\u003c/li\u003e\n\u003cli\u003e$◦$: Denotes Hadamard product (element-wise multiplication).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eIn simpler terms:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGrouped convolution divides the input channels and weights into groups.\u003c/li\u003e\n\u003cli\u003eA separate mask is created for each group, ensuring elements within a group are aligned.\u003c/li\u003e\n\u003cli\u003eThese masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group.\u003c/li\u003e\n\u003cli\u003eThe final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eBackground on Deep Ensembles\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThis section delves into Deep Ensembles (DE), a technique for image classification tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig2.png\" alt=\"Deep Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDeep Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSetting the Scene\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe have a dataset $D$ containing pairs of images and their corresponding labels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width).\u003c/li\u003e\n\u003cli\u003e$y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\u003c/p\u003e\n\u003cp\u003eA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTraditional Approach:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroducing Deep Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\u003c/p\u003e\n\u003cp\u003eThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\u003c/p\u003e\n\u003cp\u003e$$\nP(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m)\n$$\u003c/p\u003e\n\u003cp\u003eThis essentially combines the outputs of multiple networks to create a more robust prediction.\u003c/p\u003e\n\u003cp\u003eIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBuilding Packed-Ensembles:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSubnetworks:\u003c/strong\u003e The ensemble is formed by creating \u003cstrong\u003e$M$\u003c/strong\u003e smaller subnetworks within the main network architecture. These subnetworks share the same structure but have \u003cstrong\u003eindependent parameters\u003c/strong\u003e due to the use of grouped convolutions.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHyperparameters:\u003c/strong\u003e Packed-Ensembles are defined by three hyperparameters:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$α$ (alpha):\u003c/strong\u003e expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$M$:\u003c/strong\u003e number of subnetworks in the ensemble (represents the ensemble size).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$ (gamma):\u003c/strong\u003e number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical Implementation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$\\hat{y}$\u003c/strong\u003e represents the ensemble\u0026rsquo;s predicted label\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$P(y|θ_a^m, x)$\u003c/strong\u003e denotes the probability of class \u003cstrong\u003e$y$\u003c/strong\u003e given the input \u003cstrong\u003e$x$\u003c/strong\u003e and the parameters \u003cstrong\u003e$θ_a^m$\u003c/strong\u003e of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$\u003c/strong\u003e represents the parameters of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork, obtained by applying element-wise multiplication (\u003cstrong\u003e$∘$\u003c/strong\u003e) between the expanded weights (\u003cstrong\u003e$\\omega_j^{\\alpha}$\u003c/strong\u003e) and the group mask (\u003cstrong\u003e$\\text{mask}_{m}$\u003c/strong\u003e) for each layer \u003cstrong\u003e$j$\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig4.png\" alt=\"Equivalent architectures for Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eEquivalent architectures for Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig5.png\" alt=\"subnetwork mask\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\u003c/i\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eExperiments\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThe experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDatasets:\u003c/strong\u003e CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchitectures:\u003c/strong\u003e PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetrics:\u003c/strong\u003e Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation Details:\u003c/strong\u003e Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCode:\u003c/strong\u003e PyTorch-Lightning framework is used for implementation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eResults\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThe experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCIFAR-10/100:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet).\u003c/li\u003e\n\u003cli\u003eSmaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImageNet:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models.\u003c/li\u003e\n\u003cli\u003ePE achieves better accuracy with a reasonable increase in training and inference cost.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig3.png\" alt=\"ResNet50 performance\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003ePacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\u003c/i\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eEthics\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThis section emphasizes the ethical considerations of the research. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e This research proposes a method to improve uncertainty estimation in deep learning models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimitations:\u003c/strong\u003e The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConcerns:\u003c/strong\u003e The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like:\n\u003cul\u003e\n\u003cli\u003eUnknown situations\u003c/li\u003e\n\u003cli\u003eCorner cases (uncommon but important situations)\u003c/li\u003e\n\u003cli\u003eAdversarial attacks (attempts to intentionally mislead the model)\u003c/li\u003e\n\u003cli\u003ePotential biases in the model\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOverall:\u003c/strong\u003e The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eReproducibility: Packed-Ensemble on CIFAR-10\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eWe attempted to reproduce the experiment outlined in the tutorial available at \u003ca href=\"https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html\"\u003ehttps://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html\u003c/a\u003e which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Loading and Preprocessing:\u003c/strong\u003e Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePacked-Ensemble Definition:\u003c/strong\u003e Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLoss Function and Optimizer:\u003c/strong\u003e Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining:\u003c/strong\u003e Training the Packed-Ensemble model on the CIFAR-10 training data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTesting and Evaluation:\u003c/strong\u003e Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eExperimental Runs and Observations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTest 1:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result1.png\" alt=\"First result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  cat   ship  ship  plane\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: cat   ship  ship  ship\u003c/p\u003e\n\u003cp\u003eTest 2:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result2.png\" alt=\"Second result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth: dog bird horse bird\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: dog  frog  car  dog\u003c/p\u003e\n\u003cp\u003eTest 3:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result3.png\" alt=\"Third result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  dog truck plane car \u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: dog  horse ship  truck\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChallenges and Limitations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/packed-ensembles/","date_published":"27026-27-09T25:2727:00+01:00","date_modified":"27026-27-09T25:2727:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"fc0f8a815b8318f3a6944187159cad98a8481ca8","title":"A Framework to Learn with Interpretation","summary":"","content_text":" A Framework to Learn with Interpretation Authors: Maroun ABOU BOUTROS, Mohamad EL OSMAN\nArticle: A Framework to Learn with Interpretation by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc\nTable of Contents Introduction Learning a classifier and an interpreter Design of FLINT Interpretation in FLINT Learning by imposing interpretability properties Understanding encoded concepts in FLINT Reproducing the experiments Global interpretation Local interpretation Subjective evaluation Specialization of FLINT to post-hoc interpretability Conclusion 1 Introduction In this blog post, we’ll explore FLINT, a framework introduced in the paper titled “A Framework to Learn with Interpretation” by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc, available on the following link, addressing the crucial need for interpretability in machine learning as complex predictive models become more prevalent in fields like law, healthcare, and defense. Interpretability, synonymous with explainability, provides insights into a model’s decision-making process. Two main approaches, post-hoc methods and “interpretable by design” methods, tackle the challenge of interpreting models, each with its pros and cons. A new approach, Supervised Learning with Interpretation (SLI), jointly learns a predictive model and an interpreter model. FLINT, specifically designed for deep neural network classifiers, introduces a novel interpreter network architecture promoting local and global interpretability. It also proposes a criterion for concise and diverse attribute functions, enhancing interpretability. We’ll delve into the architecture of FLINT and how it works to give explainable predictions, and we will reproduce some experiments done in the experimental section of the article and evaluate their outputs to study FLINT\u0026rsquo;s performance. And finally, we will present a specialization of FLINT for post-hoc interpretability.\n2 Learning a classifier and its interpreter with FLINT The paper introduces Supervised Learning with Interpretation (SLI), a new task aimed at incorporating interpretability alongside prediction in machine learning models. In SLI, a separate model, called an interpreter, is employed to interpret the predictions made by the primary predictive model. The task involves minimizing a combined loss function consisting of prediction error and interpretability objectives. The paper focuses on addressing SLI within the context of deep neural networks for multi-class classification tasks. It proposes a framework called Framework to Learn with INTerpretation (FLINT), which utilizes a specialized architecture for the interpreter model, distinguishes between local and global interpretations, and introduces corresponding penalties in the loss function to achieve the desired interpretability.\nSo for a dataset $S$ and a given model $f \\in F$ where $F$ is a class of classifiers (here neural networks) and an interpreter model $g \\in G_f$ where $G_f$ is a family of models, the SLI problem is presented by: $$ \\arg{\\min_{f \\in F, g \\in G_f}{L_{pred}(f, S) + L_{int}(f, g, S)}} $$ Where $L_{pred}(f, S)$ denotes a loss term related to prediction error and $L_{int}(f, g, S)$ measures the ability of $g$ to provide interpretations of predictions by $f$.\n2.1 Design of FLINT In FLINT, depicted in the image above, both a prediction model ($f$) and an interpreter model ($g$) are used. The input to FLINT is a vector $x \\in X$, where $X = \\mathbb{R}^d$, and the output is a vector $y \\in Y$, where $Y$ is defined as the set of one-hot encoding vectors with binary components of size $C$ (the number of classes to predict). The prediction model $f$ is structured as a deep neural network with $l$ hidden layers, represented as $f = f_{l+1} \\circ f_l \\circ \\ldots \\circ f_1$. Each $f_k$ represents a hidden layer mapping from $R^{d_{k-1}}$ to $R^{d_k}$. To interpret the outputs of $f$, we randomly select a subset of $T$ hidden layers, indexed by $I=\\{i_1, i_2, \\ldots, i_T\\}$, and concatenate their outputs to form a new vector $f_I(x) \\in \\mathbb{R}^D$, where $D = \\sum_{t=1}^T d_{i_t}$. This vector is then fed into a neural network $\\Psi$ to produce an output vector $\\Phi(x) = \\Psi(f_I(x)) = (\\phi_1(x), \u0026hellip;, \\phi_J (x)) \\in \\mathbb{R}^J$, representing an attribute dictionary comprising functions $\\phi_j: X \\rightarrow \\mathbb{R}^+$, where $\\phi_j(x)$ captures the activation of a high-level attribute or a \u0026ldquo;concept\u0026rdquo; over $X$. Finally, $g$ computes the composition of the attribute dictionnary with an interpretable function $h: R^J \\rightarrow Y$. $$ \\forall x \\in X, g(x) = h(\\Phi(x)) $$ For now we take $h(x) = softmax(W^T \\Phi(x))$ but $h$ can be any interpretable function (like a decision tree for example).\nNote that $d$ in the image is a decoder network that takes $\\Phi(x)$ and reconstructs the input $x$. This decoder is used for training and its purpose will be detailed later on in section 2.3.\n2.2 Interpretation in FLINT With the interpreter defined, let\u0026rsquo;s clarify its role and interpretability objectives within FLINT. Interpretation serves as an additional task alongside prediction. We\u0026rsquo;re interested in two types: global interpretation, which aids in understanding which attribute functions contribute to predicting a class, and local interpretation, which pinpoints the attribute functions involved in predicting a specific sample.\nTo interpret a local prediction $f(x)$, it\u0026rsquo;s crucial that the interpreter\u0026rsquo;s output $g(x)$ aligns with $f(x)$. Any discrepancy prompts analysis of conflicting data, potentially raising concerns about the prediction\u0026rsquo;s confidence.\nTo establish local and global interpretation, we rely on attribute relevance. Given an interpreter with parameters $\\Theta_g = (\\theta_\\Psi, \\theta_h)$ and an input $x$, an attribute $\\phi_j$\u0026rsquo;s relevance is defined concerning the prediction $g(x) = f(x) = \\hat{y}$. The attribute\u0026rsquo;s contribution to the unnormalized score of class $\\hat{y}$ is $\\alpha_{j, \\hat{y}, x} = \\phi_j(x) \\cdot w_{j, \\hat{y}}$, where $w_{j, \\hat{y}}$ is the coefficient associated with this class. Relevance score $r_{j, x}$ is computed by normalizing $\\alpha$ as $r_{j, x} = \\frac{\\alpha_{j, \\hat{y}, x}}{\\max_i |\\alpha_{i, \\hat{y}, x}|}$. An attribute $\\phi_j$ is considered relevant for a local prediction if it\u0026rsquo;s both activated and effectively used in the linear model.\nAttribute relevance extends to its overall importance in predicting any class $c$. This is achieved by averaging relevance scores from local interpretations over a random subset or the entirety of the training set $S$ where the predicted class is $c$. Thus, $r_{j, c} = \\frac{1}{|S_c|} \\sum_{x \\in S_c} r_{j, x}$, where $S_c = \\{x \\in S \\mid \\hat{y} = c\\}$.\nNow, let\u0026rsquo;s introduce the local and global interpretations the interpreter will provide:\nGlobal interpretation ($G(g, f)$) identifies class-attribute pairs $(c, \\phi_j)$ where the global relevance $r_{j, c}$ exceeds a threshold $\\frac{1}{\\tau}$.\nLocal interpretation ($L(x, g, f)$) for a sample $x$ includes attribute functions $\\phi_j$ with local relevance $r_{j, x}$ surpassing $\\frac{1}{\\tau}$. These definitions don\u0026rsquo;t assess interpretation quality directly.\n2.3 Learning by imposing interpretability properties For learning, the paper defines certain penalties to minimize, where each one aims to enforce a certain desirable property:\nFidelity to output: The output of $g(x)=h(\\Psi(f_I(x)))$ should be close to $f(x)$ for any x. This can be imposed through a cross-entropy loss: $$ L_{of}(f, g, S) = - \\sum_{x \\in S} h(\\Psi(f_I(x)))^T \\log(f(x)) $$\nConciseness and Diversity of Interpretations: We aim for concise local interpretations, containing only essential attributes per sample, promoting clearer understanding and capturing high-level concepts. Simultaneously, we seek diverse interpretations across samples to prevent attribute functions from being class-exclusive. To achieve this, the paper proposes that we leverage entropy (defined for a vector as $\\mathcal{E}(v) = - \\sum_i p_i \\log(p_i)$), which quantifies uncertainty in real vectors. Conciseness is fostered by minimizing the entropy of the interpreter\u0026rsquo;s output, $\\Phi(x) = \\Psi(f_I(x))$, while diversity is encouraged by maximizing the entropy of the average $\\Psi(f_I(x))$ over a mini-batch. This approach promotes sparse and varied coding of $f_I(x)$, enhancing interpretability. However, as entropy-based losses lack attribute activation constraints, leading to suboptimal optimization, we also minimize the $l_1$ norm of $\\Psi(f_I(x))$ with hyperparameter $\\eta$. Although $l_1$-regularization commonly encourages sparsity, the experiments done in the paper show that entropy-based methods are more effective. $$ L_{cd}(f, g, S) = -\\mathcal{E}(\\frac{1}{\\lvert S \\lvert} \\sum_{x \\in S} \\Psi(f_I(x))) + \\sum_{x \\in S} \\mathcal{E}(\\Psi(f_I(x))) + \\sum_{x \\in S} \\eta \\lVert \\Psi(f_I(x)) \\lVert_1 $$\nFidelity to input: In order to promote the representation of intricate patterns associated with the input within $\\Phi(x)$, a decoder network $d : \\mathbb{R}^J \\rightarrow X$ is employed. This network is designed to take the attribute dictionary $\\Phi(x)=\\Psi(f_I(x))$ as input and reconstruct the original input $x$. $$ L_{if}(f, g, d, S) = \\sum_{x \\in S} (d(\\Psi(f_I(x))) - x)^2 $$\nGiven the proposed loss terms, the loss for the interpretability model writes as follows: $$ L_{int}(f, g, d, S) = \\beta L_{of}(f, g, S) + \\gamma L_{if}(f, g, d, S) + \\delta L_{cd}(f, g, S) $$ Where $\\beta, \\gamma, \\delta$ are non-negative hyperparameters. the total loss to be minimized $L = L_{pred} + L_{int}$, where the prediction loss, $L_{pred}$, is the well-know cross entropy loss (since this a classification problem).\n3 Understanding encoded concepts in FLINT Once the predictor and interpreter networks are jointly learned, interpretation can be conducted at both global and local levels . A critical aspect highlighted by the authors is understanding the concepts encoded by each individual attribute function ​$\\phi_j$ . Focusing on image classification, the authors propose representing an encoded concept as a collection of visual patterns in the input space that strongly activate $\\phi_j$ . They present a pipeline for generating visualizations for both global and local interpretation, adapting various existing tools .\nFor global interpretation visualization, the authors propose starting by selecting a small subset of training samples from a given class c that maximally activate ​$\\phi_j$ . This subset, referred to as Maximum Activating Samples (MAS), is denoted as $MAS(c , ​\\phi_j , l)$ where $l$ is the subset size (set as 3 in their experiments). However, while MAS provides some insight into the encoded concept, further analysis is required to understand the specific aspects of these samples that cause ​$\\phi_j$ activation. To achieve this, the authors propose utilizing a modified version of activation maximization called Activation Maximization with Partial Initialization (AM+PI). This technique aims to synthesize input that maximally activates ​$\\phi_j$ by optimizing a common activation maximization objective, initialized with a low-intensity version of the sample from MAS.\nFor local analysis, given any test sample $x_{0}$ , its local interpretation $L(x_{0},f,g)$ can be determined, representing the relevant attribute functions . To visualize a relevant attribute ​$\\phi_j$, the authors suggest repeating the AM+PI procedure with initialization using a low-intensity version of $x_{0}$ to enhance the concept detected by ​$\\phi_j$ in $x_{0}$ .\n4 Reproducing the experiments In the experimental section of the article, several experiments were conducted to do a quantitative evaluation of FLINT\u0026rsquo;s performance compared to other state-of-the-art models designed for interpretability, such as SENN and PrototypeDNN. Additionally, FLINT was compared to LIME and VIBI to evaluate the fidelity of its interpretations, measuring the proportion of samples where the predictions of a model and its interpreter agree. Across these tests, FLINT consistently outperformed the other models, demonstrating its reliability and effectiveness.\nHowever, in this blog post we will specifically focus on reproducing the experiments in the article related to FLINT\u0026rsquo;s explainability, that aim to do a qualitative analysis of it. To achieve a thorough understanding of the model and its operational dynamics across prevalent datasets, we replicated the study by cloning the project from the GitHub repository referenced in the article (repo link). Our experimentation involved the CIFAR10 and QuickDRAW datasets, employing a ResNet18-based network for both. For the QuickDRAW dataset, we utilized J=24 attributes, while for the CIFAR10 dataset, we used J=36 attributes.\nThe instructions provided in the GitHub repository for executing the model are clear, and the model runs flawlessly. We have the option to either train the model ourselves or download the pre-trained models. Furthermore, there is a well-detailed Python notebook named \u0026ldquo;FLINT demo.ipynb\u0026rdquo;, which contains code for visualizing data, such as attribute relevance scores for each class and local interpretations for data samples. We will execute FLINT on test images and take a look at how interpretability is done with FLINT in this section.\n4.1 Global interpretation In the article, the authors explore global interpretation using a figure similar to the one provided below which was reproduced from the notebook, and which illustrates the generated global relevances $r_{j,c}$ for all class-attribute pairs in the QuickDraw dataset.\nAdditionally, by running the model on the CIFAR10 and QuickDRAW dataset we got visual outputs representative of class-attribute pair analyses for both datasets. These outputs served as pivotal tools in elucidating interrelations and facilitating comparative assessments between attributes and classes. We present below two figures derived from the resultant class-attribute pair analyses for each of the 2 datasets. The class-attribute pairs shown are different from the examples shown in the paper.\nCaption: Class-attribute pair analysis on dataset CIFAR10\nCaption: Class-attribute pair analysis on dataset QuickDraw\nWe focus on class-attribute pairs with high relevance, showcasing examples in the provided figure above . For each pair, we examine Maximum Activating Samples (MAS) alongside their corresponding Activation Maximization with Partial Initialization (AM+PI) outputs.\nMAS analysis alone provides valuable insights into the encoded concept. For instance, on QuickDRAW dataset, attribute $\\phi_{16}$ relevant for class \u0026lsquo;Banana\u0026rsquo; activates the curve shape of the banana. However, AM+PI outputs offer deeper insights by elucidating which parts of the input activate an attribute function more clearly. And on CIFAR10 dataset , attribute $\\phi_{12}$ activates for \u0026lsquo;Deer\u0026rsquo; class , but the specific focus of the attribute remains ambiguous. The outputs of the AM+PI method indicate that attribute $\\phi_{12}$ predominantly highlights the area encompassing the legs and the horns of the deer, characterized as the most prominently enhanced regions.\n4.2 Local interpretation Similarly to the article, we explored local interpretation through the figure provided below which was generated in the notebook, which showcases visualizations for 4 test samples of the QuickDRAW dataset. Both predictor $f$ and interpreter $g$ accurately predict the true class in all cases, for the first 2 it\u0026rsquo;s \u0026ldquo;Cat\u0026rdquo; and the last 2 it\u0026rsquo;s \u0026ldquo;Banana\u0026rdquo;. For each case, they highlighted the top 3 relevant attributes to the prediction along with their relevances and corresponding AM+PI outputs.\nAnalysis of the AM+PI outputs reveals that attribute functions generally activate for patterns corresponding to the same concept inferred during global analysis. This consistency is evident for attribute functions present in the previous figures. Additionaly, by looking at the figure showing the relevance of class-attribute pairs in section 4.1 for the QuickDRAW dataset we observe that the 3 most important features for each class in the local interpretations are also those having the highest relevence for these classes. For example for the \u0026ldquo;Banana\u0026rdquo; class, $\\phi_{16}$, which activates the curve shape, is by far the most important feature for identifying this class by looking at both the local interpretations and the class-attribute relevences. While for the \u0026ldquo;Cat\u0026rdquo; class, it seems that the most important features are in order $\\phi_{23}$, $\\phi_1$ and $\\phi_{19}$ when looking at both the local interpretations and the class-attribute relevences.\n5 Subjective evaluation In the article, a subjective evaluation survey with 20 respondents using the QuickDraw dataset to assess FLINT\u0026rsquo;s interpretability is conducted. The authors selected 10 attributes covering 17 class-attribute pairs and presented visualizations (3 MAS and AM+PI outputs) along with textual descriptions for each attribute to the respondents. They were asked to indicate their level of agreement with the association between the descriptions and the patterns in the visualizations using predefined choices.\nDescriptions were manually generated, including 40% incorrect ones to ensure informed responses. Results showed that for correct descriptions, 77.5% of respondents agreed, 10.0% were unsure, and 12.5% disagreed. For incorrect descriptions, 83.7% disagreed, 7.5% were unsure, and 8.8% agreed. These results affirm that the concepts encoded in FLINT\u0026rsquo;s learned attributes are understandable to humans.\n6 Specialization of FLINT to post-hoc interpretability FLINT primarily aims for interpretability by design, but the authors of the article propose that it can also be adapted to provide post-hoc interpretations when a classifier $\\hat{f}$ is already available. Post-hoc interpretation learning, a special case of SLI, involves building an interpreter for $\\hat{f}$ by minimizing a certain objective function. Specifically, Given a classifier $\\hat{f} \\in F$ and a training set $S$, the goal is to build an interpreter of $\\hat{f}$ by solving: $$ \\text{arg} \\min_{g \\in G_{f}} L_{int}(\\hat{f}, g, S) $$ Where $g(x)=h(\\Phi(\\hat{f_I} (x)))$ for a given set of $I$ hidden layers and an attribute dictionnary of size $J$. The learning is performed the same as before but we only keep the parameters $\\theta_\\Psi$, $\\theta_h$ and $\\theta_d$. We fix $\\theta_\\hat{f}$ and remove $L_{pred}$ from the training loss $L$.\nThere are experimental results in the article and in the supplements that are not mentionned here that demonstrate the effectiveness of post-hoc interpretation within FLINT, showing that even without fine-tuning the internal layers of the classifier, meaningful interpretations can be generated with high fidelity.\n7 Conclusion In conclusion, FLINT offers a robust framework for enhancing the interpretability of machine learning models, particularly deep neural networks, in critical domains like healthcare, law, and defense. By jointly learning predictor and interpreter models, FLINT addresses the challenge of providing both global and local interpretations of model predictions. Through carefully designed loss functions, FLINT ensures fidelity to input and output, promotes concise and diverse interpretations, and facilitates the representation of intricate patterns associated with input data. Reproducing experiments on datasets such as CIFAR10 and QuickDRAW showcases FLINT\u0026rsquo;s effectiveness in providing interpretable insights into model predictions. Subjective evaluations affirm the understandability of FLINT\u0026rsquo;s learned attributes, reinforcing its potential for real-world applications. Moreover, FLINT\u0026rsquo;s adaptability for post-hoc interpretability underscores its versatility, enabling meaningful interpretations without extensive modification of the underlying classifier. Overall, FLINT emerges as a valuable tool for fostering transparency and trust in complex machine learning models, contributing to the development of interpretable AI systems across various domains.\n","content_html":"\u003chr\u003e\u003c/hr\u003e\n\u003cstyle\nTYPE=\"text/css\"\u003e\n\u003cp\u003ecode.has-jax {font:\ninherit;\nfont-size:\n100%;\nbackground:\ninherit;\nborder:\ninherit;}\u003c/p\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eA Framework to Learn with Interpretation\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors: Maroun ABOU BOUTROS, Mohamad EL OSMAN\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArticle: \u003ca href=\"https://arxiv.org/abs/2010.09345\"\u003eA Framework to Learn with Interpretation\u003c/a\u003e by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eLearning a classifier and an interpreter\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-2.1\"\u003eDesign of FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.2\"\u003eInterpretation in FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.3\"\u003eLearning by imposing interpretability properties\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eUnderstanding encoded concepts in FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eReproducing the experiments\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-4.1\"\u003eGlobal interpretation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4.2\"\u003eLocal interpretation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eSubjective evaluation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eSpecialization of FLINT to post-hoc interpretability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"section-1\"\u003e1 Introduction\u003c/h1\u003e\n\u003cp\u003eIn this blog post, we’ll explore FLINT, a framework introduced in the paper titled “A Framework to Learn with Interpretation” by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc, available on the following \u003ca href=\"https://arxiv.org/abs/2010.09345\"\u003elink\u003c/a\u003e, addressing the crucial need for interpretability in machine learning as complex predictive models become more prevalent in fields like law, healthcare, and defense. Interpretability, synonymous with explainability, provides insights into a model’s decision-making process. Two main approaches, post-hoc methods and “interpretable by design” methods, tackle the challenge of interpreting models, each with its pros and cons. A new approach, Supervised Learning with Interpretation (SLI), jointly learns a predictive model and an interpreter model. FLINT, specifically designed for deep neural network classifiers, introduces a novel interpreter network architecture promoting local and global interpretability. It also proposes a criterion for concise and diverse attribute functions, enhancing interpretability. We’ll delve into the architecture of FLINT and how it works to give explainable predictions, and we will reproduce some experiments done in the experimental section of the article and evaluate their outputs to study FLINT\u0026rsquo;s performance. And finally, we will present a specialization of FLINT for post-hoc interpretability.\u003c/p\u003e\n\u003ch1 id=\"section-2\"\u003e2 Learning a classifier and its interpreter with FLINT\u003c/h1\u003e\n\u003cp\u003eThe paper introduces Supervised Learning with Interpretation (SLI), a new task aimed at incorporating interpretability alongside prediction in machine learning models. In SLI, a separate model, called an interpreter, is employed to interpret the predictions made by the primary predictive model. The task involves minimizing a combined loss function consisting of prediction error and interpretability objectives. The paper focuses on addressing SLI within the context of deep neural networks for multi-class classification tasks. It proposes a framework called Framework to Learn with INTerpretation (FLINT), which utilizes a specialized architecture for the interpreter model, distinguishes between local and global interpretations, and introduces corresponding penalties in the loss function to achieve the desired interpretability.\u003cbr\u003e\nSo for a dataset $S$ and a given model $f \\in F$ where $F$ is a class of classifiers (here neural networks) and an interpreter model $g \\in G_f$ where $G_f$ is a family of models, the SLI problem is presented by:\n$$\n\\arg{\\min_{f \\in F, g \\in G_f}{L_{pred}(f, S) + L_{int}(f, g, S)}}\n$$\nWhere $L_{pred}(f, S)$ denotes a loss term related to prediction error and $L_{int}(f, g, S)$ measures the ability of $g$ to provide interpretations of predictions by $f$.\u003c/p\u003e\n\u003ch2 id=\"section-2.1\"\u003e2.1 Design of FLINT\u003c/h2\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/FLINT_design.png\"\n  alt=\"design of FLINT\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn FLINT, depicted in the image above, both a prediction model ($f$) and an interpreter model ($g$) are used. The input to FLINT is a vector $x \\in X$, where $X = \\mathbb{R}^d$, and the output is a vector $y \\in Y$, where $Y$ is defined as the set of one-hot encoding vectors with binary components of size $C$ (the number of classes to predict). The prediction model $f$ is structured as a deep neural network with $l$ hidden layers, represented as $f = f_{l+1} \\circ f_l \\circ \\ldots \\circ f_1$. Each $f_k$ represents a hidden layer mapping from $R^{d_{k-1}}$ to $R^{d_k}$. To interpret the outputs of $f$, we randomly select a subset of $T$ hidden layers, indexed by $I=\\{i_1, i_2, \\ldots, i_T\\}$, and concatenate their outputs to form a new vector $f_I(x) \\in \\mathbb{R}^D$, where $D = \\sum_{t=1}^T d_{i_t}$. This vector is then fed into a neural network $\\Psi$ to produce an output vector $\\Phi(x) = \\Psi(f_I(x)) = (\\phi_1(x), \u0026hellip;, \\phi_J (x)) \\in \\mathbb{R}^J$, representing an attribute dictionary comprising functions $\\phi_j: X \\rightarrow \\mathbb{R}^+$, where $\\phi_j(x)$ captures the activation of a high-level attribute or a \u0026ldquo;concept\u0026rdquo; over $X$. Finally, $g$ computes the composition of the attribute dictionnary with an interpretable function $h: R^J \\rightarrow Y$.\n$$\n\\forall x \\in X, g(x) = h(\\Phi(x))\n$$\nFor now we take $h(x) = softmax(W^T \\Phi(x))$ but $h$ can be any interpretable function (like a decision tree for example).\u003c/p\u003e\n\u003cp\u003eNote that $d$ in the image is a decoder network that takes $\\Phi(x)$ and reconstructs the input $x$. This decoder is used for training and its purpose will be detailed later on in section 2.3.\u003c/p\u003e\n\u003ch2 id=\"section-2.2\"\u003e2.2 Interpretation in FLINT\u003c/h2\u003e\n\u003cp\u003eWith the interpreter defined, let\u0026rsquo;s clarify its role and interpretability objectives within FLINT. Interpretation serves as an additional task alongside prediction. We\u0026rsquo;re interested in two types: global interpretation, which aids in understanding which attribute functions contribute to predicting a class, and local interpretation, which pinpoints the attribute functions involved in predicting a specific sample.\u003c/p\u003e\n\u003cp\u003eTo interpret a local prediction $f(x)$, it\u0026rsquo;s crucial that the interpreter\u0026rsquo;s output $g(x)$ aligns with $f(x)$. Any discrepancy prompts analysis of conflicting data, potentially raising concerns about the prediction\u0026rsquo;s confidence.\u003c/p\u003e\n\u003cp\u003eTo establish local and global interpretation, we rely on attribute relevance. Given an interpreter with parameters $\\Theta_g = (\\theta_\\Psi, \\theta_h)$ and an input $x$, an attribute $\\phi_j$\u0026rsquo;s relevance is defined concerning the prediction $g(x) = f(x) = \\hat{y}$. The attribute\u0026rsquo;s contribution to the unnormalized score of class $\\hat{y}$ is $\\alpha_{j, \\hat{y}, x} = \\phi_j(x) \\cdot w_{j, \\hat{y}}$, where $w_{j, \\hat{y}}$ is the coefficient associated with this class. Relevance score $r_{j, x}$ is computed by normalizing $\\alpha$ as $r_{j, x} = \\frac{\\alpha_{j, \\hat{y}, x}}{\\max_i |\\alpha_{i, \\hat{y}, x}|}$. An attribute $\\phi_j$ is considered relevant for a local prediction if it\u0026rsquo;s both activated and effectively used in the linear model.\u003c/p\u003e\n\u003cp\u003eAttribute relevance extends to its overall importance in predicting any class $c$. This is achieved by averaging relevance scores from local interpretations over a random subset or the entirety of the training set $S$ where the predicted class is $c$. Thus, $r_{j, c} = \\frac{1}{|S_c|} \\sum_{x \\in S_c} r_{j, x}$, where $S_c = \\{x \\in S \\mid \\hat{y} = c\\}$.\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s introduce the local and global interpretations the interpreter will provide:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGlobal interpretation ($G(g, f)$) identifies class-attribute pairs $(c, \\phi_j)$ where the global relevance $r_{j, c}$ exceeds a threshold $\\frac{1}{\\tau}$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLocal interpretation ($L(x, g, f)$) for a sample $x$ includes attribute functions $\\phi_j$ with local relevance $r_{j, x}$ surpassing $\\frac{1}{\\tau}$. These definitions don\u0026rsquo;t assess interpretation quality directly.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2.3\"\u003e2.3 Learning by imposing interpretability properties\u003c/h2\u003e\n\u003cp\u003eFor learning, the paper defines certain penalties to minimize, where each one aims to enforce a certain desirable property:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eFidelity to output:\u003c/strong\u003e\u003c/em\u003e The output of $g(x)=h(\\Psi(f_I(x)))$ should be close to $f(x)$ for any x. This can be imposed through a cross-entropy loss:\n$$\nL_{of}(f, g, S) = - \\sum_{x \\in S} h(\\Psi(f_I(x)))^T \\log(f(x))\n$$\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eConciseness and Diversity of Interpretations:\u003c/strong\u003e\u003c/em\u003e We aim for concise local interpretations, containing only essential attributes per sample, promoting clearer understanding and capturing high-level concepts. Simultaneously, we seek diverse interpretations across samples to prevent attribute functions from being class-exclusive. To achieve this, the paper proposes that we leverage entropy (defined for a vector as $\\mathcal{E}(v) = - \\sum_i p_i \\log(p_i)$), which quantifies uncertainty in real vectors. Conciseness is fostered by minimizing the entropy of the interpreter\u0026rsquo;s output, $\\Phi(x) = \\Psi(f_I(x))$, while diversity is encouraged by maximizing the entropy of the average $\\Psi(f_I(x))$ over a mini-batch. This approach promotes sparse and varied coding of $f_I(x)$, enhancing interpretability. However, as entropy-based losses lack attribute activation constraints, leading to suboptimal optimization, we also minimize the $l_1$ norm of $\\Psi(f_I(x))$ with hyperparameter $\\eta$. Although $l_1$-regularization commonly encourages sparsity, the experiments done in the paper show that entropy-based methods are more effective.\n$$\nL_{cd}(f, g, S) = -\\mathcal{E}(\\frac{1}{\\lvert S \\lvert} \\sum_{x \\in S} \\Psi(f_I(x))) + \\sum_{x \\in S} \\mathcal{E}(\\Psi(f_I(x))) + \\sum_{x \\in S} \\eta \\lVert \\Psi(f_I(x)) \\lVert_1\n$$\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eFidelity to input:\u003c/strong\u003e\u003c/em\u003e In order to promote the representation of intricate patterns associated with the input within $\\Phi(x)$, a decoder network $d : \\mathbb{R}^J \\rightarrow X$ is employed. This network is designed to take the attribute dictionary $\\Phi(x)=\\Psi(f_I(x))$ as input and reconstruct the original input $x$.\n$$\nL_{if}(f, g, d, S) = \\sum_{x \\in S} (d(\\Psi(f_I(x))) - x)^2\n$$\u003c/p\u003e\n\u003cp\u003eGiven the proposed loss terms, the loss for the interpretability model writes as follows:\n$$\nL_{int}(f, g, d, S) = \\beta L_{of}(f, g, S) + \\gamma L_{if}(f, g, d, S) + \\delta L_{cd}(f, g, S)\n$$\nWhere $\\beta, \\gamma, \\delta$ are non-negative hyperparameters. the total loss to be minimized $L = L_{pred} + L_{int}$, where the prediction loss, $L_{pred}$, is the well-know cross entropy loss (since this a classification problem).\u003c/p\u003e\n\u003ch1 id=\"section-3\"\u003e3 Understanding encoded concepts in FLINT\u003c/h1\u003e\n\u003cp\u003eOnce the predictor and interpreter networks are jointly learned, interpretation can be conducted at both global and local levels . A critical aspect highlighted by the authors is understanding the concepts encoded by each individual attribute function ​$\\phi_j$ . Focusing on image classification, the authors propose representing an encoded concept as a collection of visual patterns in the input space that strongly activate $\\phi_j$ . They present a pipeline for generating visualizations for both global and local interpretation, adapting various existing tools .\u003c/p\u003e\n\u003cp\u003eFor global interpretation visualization, the authors propose starting by selecting a small subset of training samples from a given class c that maximally activate ​$\\phi_j$ . This subset, referred to as Maximum Activating Samples (MAS), is denoted as $MAS(c , ​\\phi_j , l)$ where $l$ is the subset size (set as 3 in their experiments). However, while MAS provides some insight into the encoded concept, further analysis is required to understand the specific aspects of these samples that cause ​$\\phi_j$ activation. To achieve this, the authors propose utilizing a modified version of activation maximization called Activation Maximization with Partial Initialization (AM+PI). This technique aims to synthesize input that maximally activates ​$\\phi_j$ by optimizing a common activation maximization objective, initialized with a low-intensity version of the sample from MAS.\u003c/p\u003e\n\u003cp\u003eFor local analysis, given any test sample $x_{0}$ , its local interpretation $L(x_{0},f,g)$ can be determined, representing the relevant attribute functions . To visualize a relevant attribute ​$\\phi_j$, the authors suggest repeating the AM+PI procedure with initialization using a low-intensity version of $x_{0}$ to enhance the concept detected by ​$\\phi_j$ in $x_{0}$ .\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e4 Reproducing the experiments\u003c/h2\u003e\n\u003cp\u003eIn the experimental section of the article, several experiments were conducted to do a quantitative evaluation of FLINT\u0026rsquo;s performance compared to other state-of-the-art models designed for interpretability, such as SENN and PrototypeDNN. Additionally, FLINT was compared to LIME and VIBI to evaluate the fidelity of its interpretations, measuring the proportion of samples where the predictions of a model and its interpreter agree. Across these tests, FLINT consistently outperformed the other models, demonstrating its reliability and effectiveness.\u003c/p\u003e\n\u003cp\u003eHowever, in this blog post we will specifically focus on reproducing the experiments in the article related to FLINT\u0026rsquo;s explainability, that aim to do a qualitative analysis of it. To achieve a thorough understanding of the model and its operational dynamics across prevalent datasets, we replicated the study by cloning the project from the GitHub repository referenced in the article (\u003ca href=\"https://github.com/jayneelparekh/FLINT\"\u003erepo link\u003c/a\u003e). Our experimentation involved the CIFAR10 and QuickDRAW datasets, employing a ResNet18-based network for both. For the QuickDRAW dataset, we utilized J=24 attributes, while for the CIFAR10 dataset, we used J=36 attributes.\u003c/p\u003e\n\u003cp\u003eThe instructions provided in the GitHub repository for executing the model are clear, and the model runs flawlessly. We have the option to either train the model ourselves or download the pre-trained models. Furthermore, there is a well-detailed Python notebook named \u0026ldquo;FLINT demo.ipynb\u0026rdquo;, which contains code for visualizing data, such as attribute relevance scores for each class and local interpretations for data samples. We will execute FLINT on test images and take a look at how interpretability is done with FLINT in this section.\u003c/p\u003e\n\u003ch3 id=\"section-4.1\"\u003e4.1 Global interpretation\u003c/h3\u003e\n\u003cp\u003eIn the article, the authors explore global interpretation using a figure similar to the one provided below which was reproduced from the notebook, and which illustrates the generated global relevances $r_{j,c}$ for all class-attribute pairs in the QuickDraw dataset.\u003c/p\u003e\n\u003c!-- ![Global class-attribute relevances](/images/FLINT/Global_class_attribute_QuickDRAW.png) --\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/FLINT/Global_class_attribute_QuickDRAW.png\" alt=\"Image\" width=\"300\" height=\"200\"\u003e\n\u003c/div\u003e\n\u003cp\u003eAdditionally, by running the model on the CIFAR10 and QuickDRAW dataset we got visual outputs representative of class-attribute pair analyses for both datasets. These outputs served as pivotal tools in elucidating interrelations and facilitating comparative assessments between attributes and classes. We present below two figures derived from the resultant class-attribute pair analyses for each of the 2 datasets. The class-attribute pairs shown are different from the examples shown in the paper.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Class_attribute_pair_CIFAR10.png\"\n  alt=\"Class-attribute pair analysis on dataset CIFAR10\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Class-attribute pair analysis on dataset CIFAR10\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Class_attribute_pair_QuickDraw.png\"\n  alt=\"Class-attribute pair analysis on dataset QuickDraw\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Class-attribute pair analysis on dataset QuickDraw\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe focus on class-attribute pairs with high relevance, showcasing examples in the provided figure above . For each pair, we examine Maximum Activating Samples (MAS) alongside their corresponding Activation Maximization with Partial Initialization (AM+PI) outputs.\u003c/p\u003e\n\u003cp\u003eMAS analysis alone provides valuable insights into the encoded concept. For instance, on QuickDRAW dataset, attribute $\\phi_{16}$  relevant for class \u0026lsquo;Banana\u0026rsquo; activates the curve shape of the banana. However, AM+PI outputs offer deeper insights by elucidating which parts of the input activate an attribute function more clearly. And on CIFAR10 dataset , attribute $\\phi_{12}$ activates for \u0026lsquo;Deer\u0026rsquo; class , but the specific focus of the attribute remains ambiguous. The outputs of the AM+PI method indicate that attribute $\\phi_{12}$ predominantly highlights the area encompassing the legs and the horns of the deer, characterized as the most prominently enhanced regions.\u003c/p\u003e\n\u003ch3 id=\"section-4.2\"\u003e4.2 Local interpretation\u003c/h3\u003e\n\u003cp\u003eSimilarly to the article, we explored local interpretation through the figure provided below which was generated in the notebook, which showcases visualizations for 4 test samples of the QuickDRAW dataset. Both predictor $f$ and interpreter $g$ accurately predict the true class in all cases, for the first 2 it\u0026rsquo;s \u0026ldquo;Cat\u0026rdquo; and the last 2 it\u0026rsquo;s \u0026ldquo;Banana\u0026rdquo;. For each case, they highlighted the top 3 relevant attributes to the prediction along with their relevances and corresponding AM+PI outputs.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Local_interpretations.jpg\"\n  alt=\"Local interpretations for test samples\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAnalysis of the AM+PI outputs reveals that attribute functions generally activate for patterns corresponding to the same concept inferred during global analysis. This consistency is evident for attribute functions present in the previous figures. Additionaly, by looking at the figure showing the relevance of class-attribute pairs in section 4.1 for the QuickDRAW dataset we observe that the 3 most important features for each class in the local interpretations are also those having the highest relevence for these classes. For example for the \u0026ldquo;Banana\u0026rdquo; class, $\\phi_{16}$, which activates the curve shape, is by far the most important feature for identifying this class by looking at both the local interpretations and the class-attribute relevences. While for the \u0026ldquo;Cat\u0026rdquo; class, it seems that the most important features are in order $\\phi_{23}$, $\\phi_1$ and $\\phi_{19}$ when looking at both the local interpretations and the class-attribute relevences.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003e5 Subjective evaluation\u003c/h2\u003e\n\u003cp\u003eIn the article,  a subjective evaluation survey with 20 respondents using the QuickDraw dataset to assess FLINT\u0026rsquo;s interpretability is conducted. The authors selected 10 attributes covering 17 class-attribute pairs and presented visualizations (3 MAS and AM+PI outputs) along with textual descriptions for each attribute to the respondents. They were asked to indicate their level of agreement with the association between the descriptions and the patterns in the visualizations using predefined choices.\u003c/p\u003e\n\u003cp\u003eDescriptions were manually generated, including 40% incorrect ones to ensure informed responses. Results showed that for correct descriptions, 77.5% of respondents agreed, 10.0% were unsure, and 12.5% disagreed. For incorrect descriptions, 83.7% disagreed, 7.5% were unsure, and 8.8% agreed. These results affirm that the concepts encoded in FLINT\u0026rsquo;s learned attributes are understandable to humans.\u003c/p\u003e\n\u003ch1 id=\"section-6\"\u003e6 Specialization of FLINT to post-hoc interpretability\u003c/h1\u003e\n\u003cp\u003eFLINT primarily aims for interpretability by design, but the authors of the article propose that it can also be adapted to provide post-hoc interpretations when a classifier $\\hat{f}$ is already available. Post-hoc interpretation learning, a special case of SLI, involves building an interpreter for $\\hat{f}$ by minimizing a certain objective function. Specifically, Given a classifier\n$\\hat{f} \\in F$ and a training set $S$, the goal is to build an interpreter of $\\hat{f}$ by solving:\n$$\n\\text{arg} \\min_{g \\in G_{f}} L_{int}(\\hat{f}, g, S)\n$$\nWhere $g(x)=h(\\Phi(\\hat{f_I} (x)))$ for a given set of $I$ hidden layers and an attribute dictionnary of size $J$. The learning is performed the same as before but we only keep the parameters $\\theta_\\Psi$, $\\theta_h$ and $\\theta_d$. We fix $\\theta_\\hat{f}$ and remove $L_{pred}$ from the training loss $L$.\u003c/p\u003e\n\u003cp\u003eThere are experimental results in the article and in the supplements that are not mentionned here that demonstrate the effectiveness of post-hoc interpretation within FLINT, showing that even without fine-tuning the internal layers of the classifier, meaningful interpretations can be generated with high fidelity.\u003c/p\u003e\n\u003ch1 id=\"section-7\"\u003e7 Conclusion\u003c/h1\u003e\n\u003cp\u003eIn conclusion, FLINT offers a robust framework for enhancing the interpretability of machine learning models, particularly deep neural networks, in critical domains like healthcare, law, and defense. By jointly learning predictor and interpreter models, FLINT addresses the challenge of providing both global and local interpretations of model predictions. Through carefully designed loss functions, FLINT ensures fidelity to input and output, promotes concise and diverse interpretations, and facilitates the representation of intricate patterns associated with input data. Reproducing experiments on datasets such as CIFAR10 and QuickDRAW showcases FLINT\u0026rsquo;s effectiveness in providing interpretable insights into model predictions. Subjective evaluations affirm the understandability of FLINT\u0026rsquo;s learned attributes, reinforcing its potential for real-world applications. Moreover, FLINT\u0026rsquo;s adaptability for post-hoc interpretability underscores its versatility, enabling meaningful interpretations without extensive modification of the underlying classifier. Overall, FLINT emerges as a valuable tool for fostering transparency and trust in complex machine learning models, contributing to the development of interpretable AI systems across various domains.\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/a-framework-to-learn-with-interpretation/","date_published":"13026-13-09T256:1313:00+01:00","date_modified":"13026-13-09T256:1313:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"2b742e35078ad5eed5470d116570f0fc58f294aa","title":"NTK-SAP: IMPROVING NEURAL NETWORK PRUNING BY ALIGNING TRAINING DYNAMICS","summary":"","content_text":"This is a blog post about the paper NTK-SAP: Improving neural network pruning by aligning training dynamics, published by Y. Wang et al. in 2023 and available here.\nIntroduction:\nIn a world increasingly driven by demand for data and computational resources, the narrative of artificial intelligence has been one of abundance: more data, more power, more precision. Yet, nestled within this grand tale, lies a quieter narrative - one that champions the concept of achieving more with less—Frugal AI.\nImagine a craftsman from a bygone era, working in a workshop filled with natural light. Instead of an overwhelming array of tools, he possesses only a few, each worn and refined by years of careful use. With these simple instruments, he creates works of unexpected beauty, demonstrating that the value lies not in the abundance of resources, but in the skill and wisdom with which they are used. Frugal AI embodies this craftsman’s spirit in the digital age. It does not revel in the excesses of computational power or data. Instead, it thrives in constraint, finding clever pathways through the limitations, optimizing algorithms not just for performance, but for efficiency and accessibility.\nIn the quest for efficiency, neural network pruning has emerged as a foundation of Frugal AI principles. Just as craftsmen meticulously select and refine their tools, neural network pruning systematically removes redundant, non-critical components from a network, optimizing its performance without compromising its functionality.\nNeural network pruning\nNeural network pruning stems from the recognition that many models, especially deep learning networks, are often over-parameterized. This means they contain more parameters than are necessary for effective learning or inference. In the context of Frugal AI, this over-parameterization is analogous to an artist\u0026rsquo;s studio cluttered with unused tools and materials, which, rather than aiding, only serve to overwhelm and complicate. The act of pruning, therefore, can be seen as an effort to streamline and refine. It\u0026rsquo;s about identifying and removing the \u0026rsquo;excess\u0026rsquo; in the network—those weights and connections that contribute little to the output. This not only reduces the computational load, making the network faster and more energy-efficient, but also often improves its generalization ability, making the model less prone to overfitting and more adaptable to different tasks or datasets.\nPruning Methods:\nPruning methodologies come in various forms, each tailored to specific needs and objectives. These methodologies can be categorized into three main types: post-hoc pruning, pruning during training, and foresight pruning.\nPost-hoc Pruning: This technique trims neural networks after training, typically requiring multiple train-prune-retrain cycles. It utilizes various metrics, like magnitude and Hessian values, to determine which weights to eliminate, primarily aiming to reduce inference time.\nPruning During Training: This approach involves gradually removing connections within a neural network as it trains, employing regularization or trainable masks. It aims to save training time but doesn\u0026rsquo;t necessarily reduce memory costs.\nForesight Pruning: This strategy prunes networks before training begins to prevent unnecessary computational waste. It seeks to address issues like layer collapse collapse at high sparsity levels. Recent advancements aim to overcome the limitations of early pruning methods by incorporating more informed strategies, such as meta-gradients.\nForesight pruning methods - saliency score:\nForesight pruning methods optimize neural network structures by identifying and removing less important connections, reducing computational complexity while maintaining performance. At the heart of these methods lies the loss function, which serves as the guiding metric for evaluating the network\u0026rsquo;s performance on a given dataset and determining which connections to prune. Given the complexity of directly solving the loss function, an indirect method is employed. Each potential connection within the network is assigned a \u0026ldquo;saliency score,\u0026rdquo; reflecting its influence on the loss function. This score is computed by assessing how changes in the connection impact the loss function, scaled by the initial weight value. Essentially, connections with higher saliency scores, indicating greater impact on the loss function, are retained, while those with lower scores are pruned. This systematic approach ensures that the network remains efficient while preserving its effectiveness in solving tasks.\nKey pruning methods such as SNIP, Iterative SNIP, GraSP, and Synflow, introduce specific saliency measures to assess the importance of connections:\n1. SNIP calculates saliency as $S_{\\text{SNIP}}(m\u0026rsquo;) = \\left|\\frac{\\partial L}{\\partial \\theta}\\odot \\theta\\right|$, focusing on the impact of each connection on the loss. SNIP\u0026rsquo;s saliency score is the difference in the loss function before and after pruning a connection.\n2. Iterative SNIP repeats the process of SNIP multiple times for a refined pruning.\n3. GraSP employs the Hessian-gradient product to identify connections important for preserving gradient flow, with saliency defined as $S_{\\text{GraSP}}(m\u0026rsquo;) = -\\left[H(\\theta \\odot m\u0026rsquo;; D)\\frac{\\partial L}{\\partial \\theta}\\right] \\odot \\theta$.\n4. Synflow uses $S_{\\text{Synflow}}(m\u0026rsquo;) = \\left|\\theta\\right| \\odot \\left|\\frac{\\partial L}{\\partial \\theta}\\right|$ as a data-agnostic measure, emphasizing connections\u0026rsquo; overall contribution to the network\u0026rsquo;s output irrespective of the dataset.\nEach method\u0026rsquo;s saliency score guides the pruning process by ranking the connections based on their calculated importance to only keep the top-ranked connections - the most salient ones. Therefore, the overall idea is to start with a complex network, score each connection by importance, and keep only the most important connections. This results in a simpler network that is cheaper to train and run but still capable of learning effectively from the data.\nNeural Tangent Kernel (NTK):\nIn recent studies, there has been significant exploration into optimizing neural networks on a global scale. One notable area of focus involves leveraging the neural tangent kernel (NTK) to gain deeper insights into how gradient descent functions within extensive deep neural networks. The NTK spectrum provides valuable information about convergence patterns. Remarkably, researchers have observed that the NTK remains consistent throughout training in sufficiently large DNNs. This suggests that the NTK spectrum could serve as a comprehensive measure for understanding training dynamics.\nNeural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP):\nConsequently, a novel pruning approach has emerged: selectively removing connections that exert minimal influence on the NTK spectrum.\nIn order to implement this conceptual pruning methods, there are a few considerations:\n1. Metric Selection: Due to the complexity and time required to calculate the full range of eigenvalues (the eigenspectrum) of the Neural Tangent Kernel, the nuclear norm—essentially the sum of these eigenvalues—is used instead as a scalar to summarize the characteristics of the eigenspectrum.\n2. Choosing the Right NTK Matrix:\nWe can distinguish between wo types of NTK matrices:\nFixed-Weight NTK: Related to the network\u0026rsquo;s initial setup. Analytic NTK: A theoretical model assuming a network of infinite size However, since calculating the Analytic NTK is highly resource-intensive, the researchers use a practical workaround. They approximate the Analytic NTK by averaging multiple Fixed-Weight NTKs from various initial setups, balancing computational efficiency with accuracy.\n3. Computational Efficiency: To manage computation costs, there is a technique known as the \u0026ldquo;new-input-new-weight\u0026rdquo; (NINW) method. This approach involves changing the network\u0026rsquo;s weights for each new set of input data. By doing this, they can efficiently evaluate the properties of the Neural Tangent Kernel (NTK) across different scenarios without significantly adding to the computational load.\nBased on these considerations, Wang and colleagues have developed an innovative approach called Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP).\nNTK-SAP leverages the NTK spectrum for efficient foresight pruning by using multi-sampling to predict pruning outcomes and ensure accuracy. It also incorporates the Novel Iterative Network Weighting (NINW) technique to reduce computation costs. This method streamlines neural networks by preemptively removing less impactful parts, optimizing both the pruning process and the network\u0026rsquo;s performance with minimal resource expenditure.\nNTK-SAP follows the following implementation:\nCalculation of NTK-SAP Saliency Score:\n1. Finite Approximation Approach\nThe NTK-SAP method introduces a finite approximation expression to calculate a saliency score S-NTK-SA, which leverages the pruning dataset to approximate the entire training set. This foresight pruning approach identifies and prunes weights with the lowest saliency scores.\nSaliency score based on a fixed-weight Neural Tangent Kernel:\n$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\mathbb{E}_{\\Delta\\theta \\sim \\mathcal{N}(0, \\epsilon I)} \\left[ \\left| f(\\mathbf{X}_D; \\theta_0 \\odot m) - f(\\mathbf{X}_D; (\\theta_0 + \\Delta\\theta) \\odot m) \\right|_2^2 \\right] \\right|$$\n2. Multi-Sampling Approach:\nWhile a single fixed-weight-NTK provides an approximation of the analytic NTK, averaging over multiple fixed-weight-NTKs offers a closer approximation to the expected behavior of the analytic NTK. This method entails sampling several independent weight configurations and averaging their fixed-weight-NTKs to better understand the parameter space and the anticipated performance of pruned networks.\nA stabilized version of the saliency score, S-NTK-SAP(mj) is introduced and incorporates the average of fixed-weight-NTKs computed across multiple random weight configurations, to assess the impact of pruning. Unlike most existing foresight pruning scores, which are dependent on specific weight configurations, this proposed saliency score is weight-agnostic; it primarily reflects the structure of the mask applied for pruning rather than the weights themselves. This distinction highlights the score\u0026rsquo;s focus on the inherent characteristics of the pruning method over the variability of weight initializations.\n3. New-input-new-weight (NINW) trick:\nTo reconcile the theoretical aspirations with practical viability, NTK-SAP leverage the \u0026rsquo;new-input-new-weight\u0026rsquo; (NINW) trick. This technique estimates the expected behavior of pruned networks by utilizing a new set of weights for each mini-batch of input data. This approach ensures that the pruning algorithm remains computationally feasible, allowing for the real-world application without prohibitive resource demands.\n4. Random Input Trick:\nNTK-SAP relies on another trick that consists in replacing the pruning set with random inputs. This allows to approximate the network\u0026rsquo;s behavior without depending on real data, thus highlighting NTK-SAP\u0026rsquo;s ability to adapt to any dataset without requiring specific adjustments or optimization.\n$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\left[ \\left| f\\left(Z_i; \\theta_{0,i} \\odot m\\right) - f\\left(Z_i; \\left(\\theta_{0,i} + \\Delta\\theta_i\\right) \\odot m\\right) \\right|_2^2 \\right] \\right|$$\nExperimental validation:\nExperiments were performed on CIFAR-10, CIFAR-100, and Tiny-ImageNet data sets to validate NTK-SAP\u0026rsquo;s superiority across various sparsity levels. Particularly noteworthy is its robust performance at extreme sparsity ratios, where traditional methods falter. These results underscore the efficacy of our multi-sampling strategy and the practical utility of the NINW trick.\nExtending the analysis to the more challenging ImageNet dataset, NTK-SAP consistently outperforms baseline pruning methods, including SNIP and GraSP, especially at high sparsity levels. This success highlights NTK-SAP\u0026rsquo;s scalability and its potential to facilitate efficient neural network training on large-scale datasets.\nReproductive experiments:\nTo ensure reproducibility, begin by installing the required packages:\npip install -r requirements.txt Next, to run NTK-SAP with the default dataset and parameters using the following command:\npython main.py The default parameters are as follows:\n--dataset: Mnist --model-class: default --model: fc --pruner: rand --prune-batch-size: 256 --compression: 0.0 --prune-train-mode: False --prune-epochs: 1 --ntksap_R: 1 --ntk_epsilon: 0.01 For experimenting with different parameters, proceed with the desired adjustments.\n1. Experiment NTK-SAP with Cifar100 dataset, a 0.01 perturbation hyper-parameter\npython main.py --dataset cifar100 --ntksap_epsilon 0.01 Train results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.607083 1.00 4.96 Pre-Prune 0 NaN 4.607083 1.00 4.96 Post-Prune 0 NaN 4.607083 1.00 4.96 Final 10 3.337817 3.421804 17.91 45.41 2. Experiment NTK-SAP with Cifar100 dataset and a 0.02 perturbation hyper-parameter\npython main.py --dataset cifar100 --ntksap_epsilon 0.02 Train results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.607163 1.02 4.72 Pre-Prune 0 NaN 4.607163 1.02 4.72 Post-Prune 0 NaN 4.607163 1.02 4.72 Final 10 3.341863 3.460254 17.74 43.78 3. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 3\npython main.py --dataset cifar100 --prune-epochs 3 Train results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.606948 0.96 5.02 Pre-Prune 0 NaN 4.606948 0.96 5.02 Post-Prune 0 NaN 4.606948 0.96 5.02 Final 10 3.337061 3.448972 18.09 43.97 4. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 7\nTrain results:\ntrain_loss test_loss top1_accuracy top5_accuracy Init. 0 NaN 4.606786 1.01 4.95 Pre-Prune 0 NaN 4.606786 1.01 4.95 Post-Prune 0 NaN 4.606786 1.01 4.95 Final 10 3.335409 3.397401 18.93 44.89 Analysis from experiments:\n1. Dataset Adaptability:\nThe study demonstrated NTK-SAP as being data-free. This quality allows pruned networks developed via these methods to be seamlessly adapted to various datasets without requiring additional data, highlighting their versatility and efficiency.\n2. Robustness across hyper-parameter variations:\nThe robustness of NTK-SAPP is evident through its consistent performance across varying perturbation hyper-parameters (ϵ) in experiments conducted on the Cifar100 dataset. When the perturbation hyper-parameter is set to 0.01, the model exhibits stable behavior throughout training and pruning phases, yielding a final top-1 accuracy of 17.91% and a top-5 accuracy of 45.41%. Similarly, when the perturbation hyper-parameter is increased to 0.02, the model maintains its stability, with minimal fluctuations observed in performance metrics compared to the unperturbed model. Both pre-prune and post-prune stages demonstrate resilience to perturbations, showcasing nearly identical results to the unperturbed model. This consistency across different perturbation levels underscores the robustness of NTK-SAPP, making it a reliable choice for tasks where stability under varying conditions is crucial.\n3. Fewer iterations for small datasets:\nAn exploration into how the number of iterations (T) affects performance across datasets reveals that for smaller datasets, reducing T slightly impacts outcomes, suggesting that computational efficiency can be achieved without significantly compromising results.\nConclusion:\nIn conclusion, NTK-SAP stands as a pivotal advancement in the realm of neural network pruning, showcasing its efficacy across diverse datasets and network architectures. By pruning at initialization, it eliminates the necessity for post-training methods and mask training. Moreover, by leveraging NTK theory, it addresses the oversight of training dynamics post-pruning, enabling iterative pruning without data dependency. NTK-SAP effectively bridges the theoretical underpinnings of optimization with practical neural network training, thus pushing the boundaries of frugal neural networks.\nWhile NTK-SAP represents a significant leap forward, it also unveils several avenues for future exploration. Subsequent research could delve into alternative spectral measures or extend the methodology to other forms of network optimization.\nIn essence, NTK-SAP not only signifies a crucial stride towards more efficient and theoretically grounded neural network pruning but also sets the stage for future innovations in enhancing network frugality. By Elia Lejzerowicz and Adrien Oleksiak.\n","content_html":"\u003cp\u003eThis is a blog post about the paper NTK-SAP: Improving neural network pruning by aligning training dynamics, published by Y. Wang et al. in 2023 and available \u003ca href=\"https://openreview.net/pdf?id=-5EWhW_4qWP\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroduction:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn a world increasingly driven by demand for data and computational resources, the narrative of artificial intelligence has been one of abundance: more data, more power, more precision. Yet, nestled within this grand tale, lies a quieter narrative - one that champions the concept of achieving more with less—Frugal AI.\u003c/p\u003e\n\u003cp\u003eImagine a craftsman from a bygone era, working in a workshop filled with natural light. Instead of an overwhelming array of tools, he possesses only a few, each worn and refined by years of careful use. With these simple instruments, he creates works of unexpected beauty, demonstrating that the value lies not in the abundance of resources, but in the skill and wisdom with which they are used.\nFrugal AI embodies this craftsman’s spirit in the digital age. It does not revel in the excesses of computational power or data. Instead, it thrives in constraint, finding clever pathways through the limitations, optimizing algorithms not just for performance, but for efficiency and accessibility.\u003c/p\u003e\n\u003cp\u003eIn the quest for efficiency, neural network pruning has emerged as a foundation of Frugal AI principles. Just as craftsmen meticulously select and refine their tools, neural network pruning systematically removes redundant, non-critical components from a network, optimizing its performance without compromising its functionality.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNeural network pruning\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNeural network pruning stems from the recognition that many models, especially deep learning networks, are often over-parameterized. This means they contain more parameters than are necessary for effective learning or inference. In the context of Frugal AI, this over-parameterization is analogous to an artist\u0026rsquo;s studio cluttered with unused tools and materials, which, rather than aiding, only serve to overwhelm and complicate. The act of pruning, therefore, can be seen as an effort to streamline and refine. It\u0026rsquo;s about identifying and removing the \u0026rsquo;excess\u0026rsquo; in the network—those weights and connections that contribute little to the output. This not only reduces the computational load, making the network faster and more energy-efficient, but also often improves its generalization ability, making the model less prone to overfitting and more adaptable to different tasks or datasets.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePruning Methods:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePruning methodologies come in various forms, each tailored to specific needs and objectives. These methodologies can be categorized into three main types: \u003cstrong\u003epost-hoc pruning\u003c/strong\u003e, \u003cstrong\u003epruning during training\u003c/strong\u003e, and \u003cstrong\u003eforesight pruning\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePost-hoc Pruning:\u003c/strong\u003e This technique trims neural networks after training, typically requiring multiple train-prune-retrain cycles. It utilizes various metrics, like magnitude and Hessian values, to determine which weights to eliminate, primarily aiming to reduce inference time.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePruning During Training:\u003c/strong\u003e This approach involves gradually removing connections within a neural network as it trains, employing regularization or trainable masks. It aims to save training time but doesn\u0026rsquo;t necessarily reduce memory costs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eForesight Pruning:\u003c/strong\u003e This strategy prunes networks before training begins to prevent unnecessary computational waste. It seeks to address issues like layer collapse collapse at high sparsity levels. Recent advancements aim to overcome the limitations of early pruning methods by incorporating more informed strategies, such as meta-gradients.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eForesight pruning methods - saliency score:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eForesight pruning methods optimize neural network structures by identifying and removing less important connections, reducing computational complexity while maintaining performance. At the heart of these methods lies the loss function, which serves as the guiding metric for evaluating the network\u0026rsquo;s performance on a given dataset and determining which connections to prune. Given the complexity of directly solving the loss function, an indirect method is employed. Each potential connection within the network is assigned a \u0026ldquo;saliency score,\u0026rdquo; reflecting its influence on the loss function. This score is computed by assessing how changes in the connection impact the loss function, scaled by the initial weight value. Essentially, connections with higher saliency scores, indicating greater impact on the loss function, are retained, while those with lower scores are pruned. This systematic approach ensures that the network remains efficient while preserving its effectiveness in solving tasks.\u003c/p\u003e\n\u003cp\u003eKey pruning methods such as \u003cstrong\u003eSNIP\u003c/strong\u003e, \u003cstrong\u003eIterative SNIP\u003c/strong\u003e, \u003cstrong\u003eGraSP\u003c/strong\u003e, and \u003cstrong\u003eSynflow\u003c/strong\u003e, introduce specific saliency measures to assess the importance of connections:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. SNIP\u003c/strong\u003e calculates saliency as $S_{\\text{SNIP}}(m\u0026rsquo;) = \\left|\\frac{\\partial L}{\\partial \\theta}\\odot \\theta\\right|$, focusing on the impact of each connection on the loss.  SNIP\u0026rsquo;s saliency score is the difference in the loss function before and after pruning a connection.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Iterative SNIP\u003c/strong\u003e  repeats the process of SNIP multiple times for a refined pruning.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. GraSP\u003c/strong\u003e employs the Hessian-gradient product to identify connections important for preserving gradient flow, with saliency defined as $S_{\\text{GraSP}}(m\u0026rsquo;) = -\\left[H(\\theta \\odot m\u0026rsquo;; D)\\frac{\\partial L}{\\partial \\theta}\\right] \\odot \\theta$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Synflow\u003c/strong\u003e  uses $S_{\\text{Synflow}}(m\u0026rsquo;) = \\left|\\theta\\right| \\odot \\left|\\frac{\\partial L}{\\partial \\theta}\\right|$ as a data-agnostic measure, emphasizing connections\u0026rsquo; overall contribution to the network\u0026rsquo;s output irrespective of the dataset.\u003c/p\u003e\n\u003cp\u003eEach method\u0026rsquo;s saliency score guides the pruning process by ranking the connections based on their calculated importance to only keep the top-ranked connections - the most salient ones. Therefore, the overall idea is to start with a complex network, score each connection by importance, and keep only the most important connections. This results in a simpler network that is cheaper to train and run but still capable of learning effectively from the data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNeural Tangent Kernel (NTK):\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn recent studies, there has been significant exploration into optimizing neural networks on a global scale. One notable area of focus involves leveraging the neural tangent kernel (NTK) to gain deeper insights into how gradient descent functions within extensive deep neural networks. The NTK spectrum provides valuable information about convergence patterns. Remarkably, researchers have observed that the NTK remains consistent throughout training in sufficiently large DNNs. This suggests that the NTK spectrum could serve as a comprehensive measure for understanding training dynamics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNeural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP):\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eConsequently, a novel pruning approach has emerged: selectively removing connections that exert minimal influence on the NTK spectrum.\u003c/p\u003e\n\u003cp\u003eIn order to implement this conceptual pruning methods, there are a few considerations:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Metric Selection:\u003c/strong\u003e  Due to the complexity and time required to calculate the full range of eigenvalues (the eigenspectrum) of the Neural Tangent Kernel, the nuclear norm—essentially the sum of these eigenvalues—is used instead as a scalar to summarize the characteristics of the eigenspectrum.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Choosing the Right NTK Matrix:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe can distinguish between wo types of NTK matrices:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFixed-Weight NTK: Related to the network\u0026rsquo;s initial setup.\u003c/li\u003e\n\u003cli\u003eAnalytic NTK: A theoretical model assuming a network of infinite size\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, since calculating the Analytic NTK is highly resource-intensive, the researchers use a practical workaround. They approximate the Analytic NTK by averaging multiple Fixed-Weight NTKs from various initial setups, balancing computational efficiency with accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Computational Efficiency:\u003c/strong\u003e To manage computation costs, there is a technique known as the \u0026ldquo;new-input-new-weight\u0026rdquo; (NINW) method. This approach involves changing the network\u0026rsquo;s weights for each new set of input data. By doing this, they can efficiently evaluate the properties of the Neural Tangent Kernel (NTK) across different scenarios without significantly adding to the computational load.\u003c/p\u003e\n\u003cp\u003eBased on these considerations, Wang and colleagues have developed an innovative approach called \u003cstrong\u003eNeural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eNTK-SAP leverages the NTK spectrum for efficient foresight pruning by using multi-sampling to predict pruning outcomes and ensure accuracy. It also incorporates the Novel Iterative Network Weighting (NINW) technique to reduce computation costs. This method streamlines neural networks by preemptively removing less impactful parts, optimizing both the pruning process and the network\u0026rsquo;s performance with minimal resource expenditure.\u003c/p\u003e\n\u003cp\u003eNTK-SAP follows the following implementation:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Adrien_Elia/algo.png\"\n  alt=\"algorithm\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCalculation of NTK-SAP Saliency Score:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Finite Approximation Approach\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe NTK-SAP method introduces a finite approximation expression to calculate a saliency score S-NTK-SA, which leverages the pruning dataset to approximate the entire training set. This foresight pruning approach identifies and prunes weights with the lowest saliency scores.\u003c/p\u003e\n\u003cp\u003eSaliency score based on a fixed-weight Neural Tangent Kernel:\u003c/p\u003e\n\u003cp\u003e$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\mathbb{E}_{\\Delta\\theta \\sim \\mathcal{N}(0, \\epsilon I)} \\left[ \\left| f(\\mathbf{X}_D; \\theta_0 \\odot m) - f(\\mathbf{X}_D; (\\theta_0 + \\Delta\\theta) \\odot m) \\right|_2^2 \\right] \\right|$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Multi-Sampling Approach:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhile a single fixed-weight-NTK provides an approximation of the analytic NTK, averaging over multiple fixed-weight-NTKs offers a closer approximation to the expected behavior of the analytic NTK. This method entails sampling several independent weight configurations and averaging their fixed-weight-NTKs to better understand the parameter space and the anticipated performance of pruned networks.\u003c/p\u003e\n\u003cp\u003eA stabilized version of the saliency score, S-NTK-SAP(mj) is introduced and incorporates the average of fixed-weight-NTKs computed across multiple random weight configurations, to assess the impact of pruning. Unlike most existing foresight pruning scores, which are dependent on specific weight configurations, this proposed saliency score is weight-agnostic; it primarily reflects the structure of the mask applied for pruning rather than the weights themselves. This distinction highlights the score\u0026rsquo;s focus on the inherent characteristics of the pruning method over the variability of weight initializations.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. New-input-new-weight (NINW) trick:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo reconcile the theoretical aspirations with practical viability, NTK-SAP leverage the \u0026rsquo;new-input-new-weight\u0026rsquo; (NINW) trick. This technique estimates the expected behavior of pruned networks by utilizing a new set of weights for each mini-batch of input data. This approach ensures that the pruning algorithm remains computationally feasible, allowing for the real-world application without prohibitive resource demands.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Random Input Trick:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNTK-SAP relies on another trick that consists in replacing the pruning set with random inputs. This allows to approximate the network\u0026rsquo;s behavior without depending on real data, thus highlighting NTK-SAP\u0026rsquo;s ability to adapt to any dataset without requiring specific adjustments or optimization.\u003c/p\u003e\n\u003cp\u003e$$S_{\\text{NTK-SAP}}(m^j) = \\left| \\frac{\\partial}{\\partial m_j} \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\left[ \\left| f\\left(Z_i; \\theta_{0,i} \\odot m\\right) - f\\left(Z_i; \\left(\\theta_{0,i} + \\Delta\\theta_i\\right) \\odot m\\right) \\right|_2^2 \\right] \\right|$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExperimental validation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eExperiments were performed on CIFAR-10, CIFAR-100, and Tiny-ImageNet data sets to validate NTK-SAP\u0026rsquo;s superiority across various sparsity levels. Particularly noteworthy is its robust performance at extreme sparsity ratios, where traditional methods falter. These results underscore the efficacy of our multi-sampling strategy and the practical utility of the NINW trick.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Adrien_Elia/performance_curves.png\"\n  alt=\"performance_curves\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eExtending the analysis to the more challenging ImageNet dataset, NTK-SAP consistently outperforms baseline pruning methods, including SNIP and GraSP, especially at high sparsity levels. This success highlights NTK-SAP\u0026rsquo;s scalability and its potential to facilitate efficient neural network training on large-scale datasets.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Adrien_Elia/performance_table.png\"\n  alt=\"performance_table\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReproductive experiments:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo ensure reproducibility, begin by installing the required packages:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install -r requirements.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNext, to run NTK-SAP with the default dataset and parameters using the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe default parameters are as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e--dataset\u003c/code\u003e: Mnist\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--model-class\u003c/code\u003e: default\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--model\u003c/code\u003e:  fc\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--pruner\u003c/code\u003e: rand\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--prune-batch-size\u003c/code\u003e: 256\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--compression\u003c/code\u003e: 0.0\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--prune-train-mode\u003c/code\u003e: False\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--prune-epochs\u003c/code\u003e: 1\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--ntksap_R\u003c/code\u003e:  1\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e--ntk_epsilon\u003c/code\u003e: 0.01\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor experimenting with different parameters, proceed with the desired adjustments.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Experiment NTK-SAP with Cifar100 dataset, a 0.01 perturbation hyper-parameter\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py --dataset cifar100 --ntksap_epsilon 0.01\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607083\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.00\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.96\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607083\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.00\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.96\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607083\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.00\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.96\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.337817\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.421804\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e17.91\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e45.41\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e2. Experiment NTK-SAP with Cifar100 dataset and a 0.02 perturbation hyper-parameter\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py --dataset cifar100 --ntksap_epsilon  0.02\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607163\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.02\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.72\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607163\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.02\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.72\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.607163\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.02\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.72\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.341863\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.460254\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e17.74\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e43.78\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e3. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 3\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython main.py --dataset cifar100 --prune-epochs \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606948\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0.96\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e5.02\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606948\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0.96\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e5.02\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606948\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0.96\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e5.02\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.337061\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.448972\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e18.09\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e43.97\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e4. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 7\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTrain results:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etrain_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etest_loss\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop1_accuracy\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e   \u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003etop5_accuracy\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eInit.\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606786\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.01\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.95\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePre-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606786\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.01\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.95\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePost-Prune\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e0\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003eNaN\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.606786\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e1.01\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e4.95\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFinal\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.335409\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e3.397401\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e18.93\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e   \u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e44.89\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eAnalysis from experiments:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Dataset Adaptability:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe study demonstrated NTK-SAP as being data-free. This quality allows pruned networks developed via these methods to be seamlessly adapted to various datasets without requiring additional data, highlighting their versatility and efficiency.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Robustness across hyper-parameter variations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe robustness of NTK-SAPP is evident through its consistent performance across varying perturbation hyper-parameters (ϵ) in experiments conducted on the Cifar100 dataset. When the perturbation hyper-parameter is set to 0.01, the model exhibits stable behavior throughout training and pruning phases, yielding a final top-1 accuracy of 17.91% and a top-5 accuracy of 45.41%. Similarly, when the perturbation hyper-parameter is increased to 0.02, the model maintains its stability, with minimal fluctuations observed in performance metrics compared to the unperturbed model. Both pre-prune and post-prune stages demonstrate resilience to perturbations, showcasing nearly identical results to the unperturbed model. This consistency across different perturbation levels underscores the robustness of NTK-SAPP, making it a reliable choice for tasks where stability under varying conditions is crucial.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Fewer iterations for small datasets:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAn exploration into how the number of iterations (T) affects performance across datasets reveals that for smaller datasets, reducing T slightly impacts outcomes, suggesting that computational efficiency can be achieved without significantly compromising results.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConclusion:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn conclusion, NTK-SAP stands as a pivotal advancement in the realm of neural network pruning, showcasing its efficacy across diverse datasets and network architectures. By pruning at initialization, it eliminates the necessity for post-training methods and mask training. Moreover, by leveraging NTK theory, it addresses the oversight of training dynamics post-pruning, enabling iterative pruning without data dependency. NTK-SAP effectively bridges the theoretical underpinnings of optimization with practical neural network training, thus pushing the boundaries of frugal neural networks.\u003c/p\u003e\n\u003cp\u003eWhile NTK-SAP represents a significant leap forward, it also unveils several avenues for future exploration. Subsequent research could delve into alternative spectral measures or extend the methodology to other forms of network optimization.\u003c/p\u003e\n\u003cp\u003eIn essence, NTK-SAP not only signifies a crucial stride towards more efficient and theoretically grounded neural network pruning but also sets the stage for future innovations in enhancing network frugality.\n\u003cbr\u003e\u003cbr\u003e\u003cbr\u003e\nBy Elia Lejzerowicz and Adrien Oleksiak.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/ntk-sap/","date_published":"7026-07-09T27:77:00+01:00","date_modified":"7026-07-09T27:77:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"de45490fdb0386b037083151783da0d850a9940c","title":"Do Perceptually Aligned Gradients imply Robustness?","summary":"","content_text":" Robustness and Perceptually Aligned Gradients : does the converse stand ? Author: Yohann Zerbib Table of Contents Introduction Adversarial Attacks Perceptually Aligned Gradients Experiment To go further Conclusion References This is a blog post about the paper Do Perceptually Aligned Gradients Imply Robustness?, published by R. Ganz et al. in 2023 and available here.\nIntroduction In the context of image recognition in Machine Learning, one could quickly realize that building robust models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against adversarials attacks, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily interpretable by humans, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\nAdversarial attacks But before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\nAdversarial attacks refer to a class of techniques in machine learning where intentionally crafted input data is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be imperceptible to humans. They are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\nConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a stop sign as speed limit sign.\n(Eykholt et al. [1])\nNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen. Several points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the loss is steep. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a small change of the input can cause abrupt shifts in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\nThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\na model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$ the input to pertub : $x \\in \\mathcal{X}$ a potential target label : $t \\in \\mathcal{Y}$ a small perturbation : $\\eta$ Then, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\nNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\nFast Gradient Sign Method (FGSM) : This method can be targeted or untargeted. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]: One compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$. The perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\nBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\n$L_{2 }$ norm : This norm captures the global quantity of changes. It is the euclidean distance.\n$L_{\\infty }$ : This norm captures the maximum change in the vector.\nSo, we have several ways to have a level of control over the changed features.\nNow that the first intuition for attack is understood, one should take a rapid look at PGD (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\nThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the opposite direction, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target. After taking a step, the perturbation is projected back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range. This process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\nHowever, our role here is not to learn how to create the best attacks, but more to learn how to defend them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks. Then, it all comes down to this optimization problem :\n$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\nA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\nThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the best trade-off on a model.\nPerceptually Aligned gradients Finally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have perceptually aligned gradients. Here, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are perceptually relevant. In other words, the gradients make sense from a human perspective.\nHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a ghost information.\nNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\nThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\nThen, it is shown that models with aligneds gradients can be considered as robust.\nFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\n1. Algorithm of the Model\nTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\nthe classical cross-entropy loss from the usual categorization problem framework,\nan auxiliary loss on the input-gradients, differentiable.\nThen, our global loss function would look like this :\n$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\nIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\nThis does not use robust model of any sort, on the hypothesis that we have ground-true PAG in the input. This is a strong hypothesis, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\nAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\n2. Creation of Perceptually Aligned Gradients\nAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are approximated. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\nWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\nTo implement this heuristic, three setups are provided.\n$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\n$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\n$\\textbf{Nearest Neighbor (NN):}$ For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define $r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l} \\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x \\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\nwhere $ D_{y_{t}}$ is the set of sample images with class $y_t$.\nNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used Denoising Diffusion Probabilistic Models (DDPMs), to generate approximations of PAG.\nLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\n$(p_t({x_{t})})_{t=1}^{T}$.\nAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\n\\begin{equation} \\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t), \\end{equation}\nwhich results in\n\\begin{equation} \\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t). \\end{equation}\nThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\nTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\nAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\nExperiment Now, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains three mods (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\nThe code is available at this link.\nTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\nAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\nLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600. How can this be explained ? One should observe the decision boundaries.\nThis is what is obtained for the regular neural network with cross-entropy Loss.\nHere is the result obtained for the particular neural network with the new loss.\nWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really stick to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\nHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a much greater margin of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\nAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\nTo go further What\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\nPAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\nBut, the question is not yet answered : Do Perceptually Aligned Gradients imply Robustness?\nAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\nAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the ghost features of the target class are visible (even if it not always comprehensible), the more the model is robust.\nSo, it seems that yes, models with PAG would be more robust.\nConclusion To draw a conclusion, this paper has empirically shown that PAG lead to more robustness in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it ouperforms Adversarially Training and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\nReferences EYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\nGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\nGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\n","content_html":"\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eRobustness and Perceptually Aligned Gradients : does the converse stand ?\u003c/h1\u003e\n\u003ch3 style=\"font-size: 24px;\"\u003eAuthor: Yohann Zerbib\u003c/h3\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eAdversarial Attacks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003ePerceptually Aligned Gradients\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eTo go further\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper Do Perceptually Aligned Gradients Imply Robustness?, published by R. Ganz et al. in 2023 and available \u003ca href=\"https://openreview.net/pdf?id=W6topEXC2-v\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the context of image recognition in Machine Learning, one could quickly realize that building \u003cem\u003erobust\u003c/em\u003e models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against \u003cem\u003e\u003cstrong\u003eadversarials attacks\u003c/strong\u003e\u003c/em\u003e, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily \u003cstrong\u003einterpretable by humans\u003c/strong\u003e, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eAdversarial attacks\u003c/h2\u003e\n\u003cp\u003eBut before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\u003c/p\u003e\n\u003cp\u003eAdversarial attacks refer to a class of techniques in machine learning where \u003cstrong\u003eintentionally crafted input data\u003c/strong\u003e is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be \u003cstrong\u003eimperceptible to humans\u003c/strong\u003e.\nThey are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\u003c/p\u003e\n\u003cp\u003eConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a \u003cem\u003e\u003cstrong\u003estop sign as speed limit sign\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/stop.png\"\n  alt=\"stop\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e(Eykholt et al. [1])\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen.\nSeveral points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the \u003cstrong\u003eloss is steep\u003c/strong\u003e. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a \u003cstrong\u003esmall change\u003c/strong\u003e of the input can cause \u003cstrong\u003eabrupt shifts\u003c/strong\u003e in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\u003c/p\u003e\n\u003cp\u003eThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ethe input to pertub : $x \\in \\mathcal{X}$\u003c/li\u003e\n\u003cli\u003ea potential target label : $t \\in  \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ea small perturbation : $\\eta$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\u003c/p\u003e\n\u003cp\u003eNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\u003c/p\u003e\n\u003ch3 id=\"fast-gradient-sign-method-fgsm-\"\u003eFast Gradient Sign Method (FGSM) :\u003c/h3\u003e\n\u003cp\u003eThis method can be \u003cem\u003e\u003cstrong\u003etargeted\u003c/strong\u003e\u003c/em\u003e or \u003cem\u003e\u003cstrong\u003euntargeted\u003c/strong\u003e\u003c/em\u003e. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]:\nOne compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$.\nThe perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\u003c/p\u003e\n\u003cp\u003eBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{2 }$ norm : This norm captures the \u003cstrong\u003eglobal quantity of changes\u003c/strong\u003e. It is the euclidean distance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{\\infty }$ : This norm captures the \u003cstrong\u003emaximum change\u003c/strong\u003e in the vector.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, we have several ways to have a level of control over the changed features.\u003c/p\u003e\n\u003cp\u003eNow that the first intuition for attack is understood, one should take a rapid look at \u003cstrong\u003ePGD\u003c/strong\u003e (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\u003c/p\u003e\n\u003cp\u003eThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the \u003cstrong\u003eopposite direction\u003c/strong\u003e, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target.\nAfter taking a step, the perturbation is \u003cem\u003eprojected\u003c/em\u003e back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range.\nThis process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\u003c/p\u003e\n\u003cp\u003eHowever, our role here is not to learn how to create the best attacks, but more to learn how to \u003cem\u003e\u003cstrong\u003edefend\u003c/strong\u003e\u003c/em\u003e them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks.\nThen, it all comes down to this optimization problem :\u003c/p\u003e\n\u003cp\u003e$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\u003c/p\u003e\n\u003cp\u003eA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\u003c/p\u003e\n\u003cp\u003eThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the \u003cstrong\u003ebest trade-off\u003c/strong\u003e on a model.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003ePerceptually Aligned gradients\u003c/h2\u003e\n\u003cp\u003eFinally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have \u003cem\u003eperceptually aligned gradients\u003c/em\u003e.\nHere, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are \u003cstrong\u003eperceptually relevant\u003c/strong\u003e. In other words, the gradients \u003cem\u003emake sense\u003c/em\u003e from a human perspective.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/pagdemo.png\"\n  alt=\"demopag\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a \u003cem\u003eghost\u003c/em\u003e information.\u003c/p\u003e\n\u003cp\u003eNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\u003c/p\u003e\n\u003cp\u003eThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\u003c/p\u003e\n\u003cp\u003eThen, it is shown that models with aligneds gradients can be considered as robust.\u003c/p\u003e\n\u003cp\u003eFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Algorithm of the Model\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ethe classical cross-entropy loss from the usual categorization problem framework,\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ean auxiliary loss on the input-gradients, differentiable.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, our global loss function would look like this :\u003c/p\u003e\n\u003cp\u003e$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\u003c/p\u003e\n\u003cp\u003eIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\u003c/p\u003e\n\u003cp\u003eThis does not use robust model of any sort, on the hypothesis that we have \u003cstrong\u003eground-true PAG\u003c/strong\u003e in the input. This is a \u003cstrong\u003estrong hypothesis\u003c/strong\u003e, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\u003c/p\u003e\n\u003cp\u003eAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Creation of Perceptually Aligned Gradients\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are \u003cstrong\u003eapproximated\u003c/strong\u003e. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\u003c/p\u003e\n\u003cp\u003eWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/target.png\"\n  alt=\"target\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eTo implement this heuristic, three setups are provided.\u003c/p\u003e\n\u003cp\u003e$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Nearest Neighbor (NN):}$  For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define\n$r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l}\n\\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x\n\\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\u003c/p\u003e\n\u003cp\u003ewhere $ D_{y_{t}}$\nis the set of sample images with class $y_t$.\u003c/p\u003e\n\u003cp\u003eNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used \u003cstrong\u003eDenoising Diffusion Probabilistic Models\u003c/strong\u003e (DDPMs), to generate approximations of PAG.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\u003c/p\u003e\n\u003cp\u003e$(p_t({x_{t})})_{t=1}^{T}$.\u003c/p\u003e\n\u003cp\u003eAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t),\n\\end{equation}\u003c/p\u003e\n\u003cp\u003ewhich results in\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t).\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\u003c/p\u003e\n\u003cp\u003eTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\u003c/p\u003e\n\u003cp\u003eAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eExperiment\u003c/h2\u003e\n\u003cp\u003eNow, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains \u003cstrong\u003ethree mods\u003c/strong\u003e (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\u003c/p\u003e\n\u003cp\u003eThe code is available at this \u003ca href=\"https://github.com/YohannZe/responsible-ai-datascience-ipParis.github.io.git\"\u003elink\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\u003c/p\u003e\n\u003cp\u003eAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600.\nHow can this be explained ? One should observe the decision boundaries.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/vanilla_l2_toy.png\"\n  alt=\"vanillal2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis is what is obtained for the regular neural network with cross-entropy Loss.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/nn_l2_toy.png\"\n  alt=\"nnl2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere is the result obtained for the particular neural network with the new loss.\u003c/p\u003e\n\u003cp\u003eWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really \u003cem\u003e\u003cstrong\u003estick\u003c/strong\u003e\u003c/em\u003e to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\u003c/p\u003e\n\u003cp\u003eHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a \u003cstrong\u003emuch greater margin\u003c/strong\u003e of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\u003c/p\u003e\n\u003cp\u003eAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003eTo go further\u003c/h2\u003e\n\u003cp\u003eWhat\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\u003c/p\u003e\n\u003cp\u003ePAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\u003c/p\u003e\n\u003cp\u003eBut, the question is not yet answered : \u003cem\u003e\u003cstrong\u003eDo Perceptually Aligned Gradients imply Robustness?\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/regu.png\"\n  alt=\"regu\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the \u003cem\u003eghost\u003c/em\u003e features of the target class are visible (even if it not always comprehensible), the more the model is robust.\u003c/p\u003e\n\u003cp\u003eSo, it seems that yes, models with \u003cstrong\u003ePAG would be more robust\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eTo draw a conclusion, this paper has empirically shown that \u003cstrong\u003ePAG lead to more robustness\u003c/strong\u003e in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it \u003cstrong\u003eouperforms Adversarially Training\u003c/strong\u003e and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eReferences\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eEYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/robustness-and-pag-the-converse/","date_published":"7026-07-09T26:77:00+01:00","date_modified":"7026-07-09T26:77:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"546ff9c63aab59b8a7ddb24d9f5939b60b89ee70","title":"To update or not to update? Neurons at equilibrium in deep models","summary":"","content_text":"To update or not to update? Neurons at equilibrium in deep models Author: Alexis WINTER Augustin CREUSILLET Table of content Introduction NEq Experiments Results Reproducibility Conclusion References This is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).\nIntroduction Background Recent advances in deep learning have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for computational power has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both affordability and environmental sustainability, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.\nThis paper tries to focus on the overall behavior of neurons, leveraging the notion of neuronal equilibrium (NEq). When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.\nRelated works Pruning strategies Pruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to improve efficiency and reduce computational complexity. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.\nDespite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically do not alleviate the computational complexity associated with training neural networks. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.\nLottery ticket hypothesis The lottery ticket hypothesis is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or \u0026ldquo;winning tickets,\u0026rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even surpass the performance of the original dense network.\nThe hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.\nThe significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these winning tickets and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.\nNEq Neuronal equilibrium The concept of neuronal equilibrium aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.\nTo assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\\Xi_{val}$ as follows:\nThe neuron $i$-th reaches the equilibrium when $(\\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium we need to detect when :\n$$\\lim_{t\\rightarrow \\infty} \\phi_{i}^t = k,$$\nSince it is not trivial to assess this statment we prefer to work with variations of $(\\phi_{i})_t$ that can be defined as :\n\\begin{equation} v_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1}, \\end{equation}\nWith $\\mu_{eq}$ the momentum coefficient.\nThis only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\\Delta \\phi_i^t \\rightarrow 0$$\nSince we want to track the evolution of $\\Delta \\phi_i^t$ over time we introduce the velocity of the variations: $$ v_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1}, $$\nWith $\\mu_{eq}$ the momentum coefficient.\nRewrited:\nWe need to have $$\\mu_{eq} \\in [0; 0.5]$$ to prevent the velocity from exploding.\nFinally we can set the condition for the neuron to be at the equilibrium as: \\begin{equation} \\left| v_{\\Delta \\phi}^t \\right | \u0026lt; \\varepsilon,~~~~~\\varepsilon \\geq 0. \\end{equation}\nIt is important to know that this relation might not hold for all $t$ since there could be an instant $t\u0026rsquo; \u0026lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.\nTraining scheme The training scheme can be presented according to this scheme:\nAt the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:\nAn epoch of training is made for all trainable neurons on the training set. The training either stops due to the end of training criterion being met or continues to the next step. The velocity of the similarities is evaluated for every neuron. The set of trainable neurons is determined for the next step according to the equilibrium criterion. Comparing with regular training, we can see two more hyper-parameters:\n$\\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities. $\\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities. Experiments SGD vs Adam The authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.\nIn the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.\nAs training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that Adam brings the network towards this equilibrium faster than SGD, but also note that in this specific task, SGD achieves a slightly higher final accuracy than Adam. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD\u0026rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.\nThe experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.\nDistribution of $\\phi$ \u0026amp; choice of $µ_{eq}$ The paper also discusses the distribution of $\\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.\nThe parameter $\\phi$ measures the cosine similarity between the outputs of a particular neuron at two consecutive training epochs, over the validation set. It is used to determine if a neuron\u0026rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\\phi$ equals 1, it indicates that the neuron\u0026rsquo;s output is stable across the epochs, signifying it has reached equilibrium.\nThe paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\\phi$ values across epochs, and $v_{∆\\phi}$, which measures the velocity of this change considering a momentum coefficient $µ_{eq}$. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.\nBy examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper\u0026rsquo;s Figure 5, which shows the distribution of $\\phi$, $∆\\phi$, and $v_{∆\\phi}$ for a ResNet-32 model trained on CIFAR-10.\nIn summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally \u0026ldquo;unfreeze\u0026rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.\nImpact of the validation set size and ε The authors found that the size of the validation set does not significantly impact the performance of the model. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.\nWhen examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, striking a balance between model performance and computational efficiency.\nResults Reproducibility Using the author\u0026rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from limited computational resources. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.\nExperiment This experiment aims to replicate the section 4.1.1 \u0026ldquo;SGD vs Adam\u0026rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:\npython3 train_classification.py --amp=1 --arch=resnet32-cifar --batch-size=100 --dataset=cifar10 --device=cuda --epochs=250 --eps=0.001 --lr=0.1 --momentum=0.9 --optim=sgd --val-size=0.01 --velocity-mu=0.5 --weight-decay=0.0005 The code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named Weights \u0026amp; Biases (wandb) to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.\nAs expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.\nConclusion From the initial problem of computational resources saving, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, NEq produces a new knowledge that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and might lead to new training strategies.\nOne possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.\nReferences Bragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022. Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017. J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019. ","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eTo update or not to update? Neurons at equilibrium in deep models\n\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Alexis WINTER Augustin CREUSILLET\u003c/h1\u003e\n\u003ch1 id=\"table-of-content\"\u003eTable of content\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eNEq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eExperiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eResults\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eReproducibility\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).\u003c/p\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e\n\u003cp\u003eRecent advances in \u003cstrong\u003edeep learning\u003c/strong\u003e have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for \u003cstrong\u003ecomputational power\u003c/strong\u003e has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both \u003cstrong\u003eaffordability and environmental sustainability\u003c/strong\u003e, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.\u003c/p\u003e\n\u003cp\u003eThis paper tries to focus on the overall behavior of neurons, leveraging the notion of \u003cstrong\u003eneuronal equilibrium (NEq)\u003c/strong\u003e. When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.\u003c/p\u003e\n\u003ch3 id=\"related-works\"\u003eRelated works\u003c/h3\u003e\n\u003ch4 id=\"pruning-strategies\"\u003ePruning strategies\u003c/h4\u003e\n\u003cp\u003ePruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to \u003cstrong\u003eimprove efficiency and reduce computational complexity\u003c/strong\u003e. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.\u003c/p\u003e\n\u003cp\u003eDespite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically \u003cstrong\u003edo not alleviate the computational complexity associated with training neural networks\u003c/strong\u003e. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.\u003c/p\u003e\n\u003ch4 id=\"lottery-ticket-hypothesis\"\u003eLottery ticket hypothesis\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003elottery ticket hypothesis\u003c/strong\u003e is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or \u0026ldquo;winning tickets,\u0026rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even \u003cstrong\u003esurpass the performance of the original dense network\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.\u003c/p\u003e\n\u003cp\u003eThe significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these \u003cstrong\u003ewinning tickets\u003c/strong\u003e and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eNEq\u003c/h2\u003e\n\u003ch3 id=\"neuronal-equilibrium\"\u003eNeuronal equilibrium\u003c/h3\u003e\n\u003cp\u003eThe concept of \u003cstrong\u003eneuronal equilibrium\u003c/strong\u003e aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.\u003c/p\u003e\n\u003cp\u003eTo assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\\Xi_{val}$ as follows:\u003c/p\u003e\n\u003cimg src=\"/images/images_Winter_Creusillet/neq_formula.png\" width=\"300\"/\u003e\n\u003cp\u003eThe neuron $i$-th reaches the equilibrium when $(\\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium  we need to detect when :\u003c/p\u003e\n\u003cp\u003e$$\\lim_{t\\rightarrow \\infty} \\phi_{i}^t = k,$$\u003c/p\u003e\n\u003cp\u003eSince it is not trivial to assess this statment we prefer to work with variations of $(\\phi_{i})_t$ that can be defined as :\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\nv_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1},\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eWith $\\mu_{eq}$ the momentum coefficient.\u003c/p\u003e\n\u003cp\u003eThis only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\\Delta \\phi_i^t \\rightarrow 0$$\u003c/p\u003e\n\u003cp\u003eSince we want to track the evolution of $\\Delta \\phi_i^t$ over time we introduce the \u003cstrong\u003evelocity of the variations\u003c/strong\u003e:\n$$\nv_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1},\n$$\u003c/p\u003e\n\u003cp\u003eWith $\\mu_{eq}$ the momentum coefficient.\u003c/p\u003e\n\u003cp\u003eRewrited:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/momentum_coef.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eWe need to have $$\\mu_{eq} \\in [0; 0.5]$$ to prevent the velocity from exploding.\u003c/p\u003e\n\u003cp\u003eFinally we can set the condition for the neuron to be at the equilibrium as:\n\\begin{equation}\n\\left| v_{\\Delta \\phi}^t \\right | \u0026lt; \\varepsilon,~~~~~\\varepsilon \\geq 0.\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eIt is important to know that this relation might not hold for all $t$ since there could be an instant $t\u0026rsquo; \u0026lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.\u003c/p\u003e\n\u003ch3 id=\"training-scheme\"\u003eTraining scheme\u003c/h3\u003e\n\u003cp\u003eThe training scheme can be presented according to this scheme:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/prunedbackprop-scheme_full-1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAt the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn epoch of training is made for all trainable neurons on the training set.\u003c/li\u003e\n\u003cli\u003eThe training either stops due to the end of training criterion being met or continues to the next step.\u003c/li\u003e\n\u003cli\u003eThe velocity of the similarities is evaluated for every neuron.\u003c/li\u003e\n\u003cli\u003eThe set of trainable neurons is determined for the next step according to the equilibrium criterion.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eComparing with regular training, we can see two more hyper-parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities.\u003c/li\u003e\n\u003cli\u003e$\\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2\"\u003eExperiments\u003c/h2\u003e\n\u003ch3 id=\"sgd-vs-adam\"\u003eSGD vs Adam\u003c/h3\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/sgd_vs_adam.png\"\n  alt=\"adam/sgd\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.\u003c/p\u003e\n\u003cp\u003eIn the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.\u003c/p\u003e\n\u003cp\u003eAs training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that \u003cstrong\u003eAdam brings the network towards this equilibrium faster than SGD\u003c/strong\u003e, but also note that in this specific task, \u003cstrong\u003eSGD achieves a slightly higher final accuracy than Adam\u003c/strong\u003e. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD\u0026rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.\u003c/p\u003e\n\u003cp\u003eThe experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.\u003c/p\u003e\n\u003ch3 id=\"distribution-of-phi--choice-of-µ_eq\"\u003eDistribution of $\\phi$ \u0026amp; choice of $µ_{eq}$\u003c/h3\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/mu-line-1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe paper also discusses the distribution of $\\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.\u003c/p\u003e\n\u003cp\u003eThe parameter $\\phi$ measures the \u003cstrong\u003ecosine similarity between the outputs of a particular neuron at two consecutive training epochs\u003c/strong\u003e, over the validation set. It is used to determine if a neuron\u0026rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\\phi$ equals 1, it indicates that the neuron\u0026rsquo;s output is stable across the epochs, signifying it has reached equilibrium.\u003c/p\u003e\n\u003cp\u003eThe paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\\phi$ values across epochs, and $v_{∆\\phi}$, which measures the velocity of this change considering a \u003cstrong\u003emomentum coefficient $µ_{eq}$\u003c/strong\u003e. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.\u003c/p\u003e\n\u003cp\u003eBy examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper\u0026rsquo;s Figure 5, which shows the distribution of $\\phi$, $∆\\phi$, and $v_{∆\\phi}$ for a ResNet-32 model trained on CIFAR-10.\u003c/p\u003e\n\u003cp\u003eIn summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally \u0026ldquo;unfreeze\u0026rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.\u003c/p\u003e\n\u003ch3 id=\"impact-of-the-validation-set-size-and-ε\"\u003eImpact of the validation set size and ε\u003c/h3\u003e\n\u003cp\u003eThe authors found that the size of the validation set \u003cstrong\u003edoes not significantly impact the performance of the model\u003c/strong\u003e. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.\u003c/p\u003e\n\u003cp\u003eWhen examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, \u003cstrong\u003estriking a balance between model performance and computational efficiency\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eResults\u003c/h2\u003e\n\u003ch2 id=\"section-4\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003eUsing the author\u0026rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from \u003cstrong\u003elimited computational resources\u003c/strong\u003e. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.\u003c/p\u003e\n\u003ch3 id=\"experiment\"\u003eExperiment\u003c/h3\u003e\n\u003cp\u003eThis experiment aims to replicate the section 4.1.1 \u0026ldquo;SGD vs Adam\u0026rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython3 train_classification.py --amp\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e --arch\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eresnet32-cifar --batch-size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e --dataset\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ecifar10 --device\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ecuda --epochs\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e250\u003c/span\u003e --eps\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.001 --lr\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.1 --momentum\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.9 --optim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003esgd --val-size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.01 --velocity-mu\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.5 --weight-decay\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.0005\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named \u003cstrong\u003eWeights \u0026amp; Biases (wandb)\u003c/strong\u003e to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/frozen_sgd_vs_adam1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cimg\n  src=\"/images/images_Winter_Creusillet/accuracy_sgd_vs_adam1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eFrom the initial problem of \u003cstrong\u003ecomputational resources saving\u003c/strong\u003e, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, \u003cstrong\u003eNEq produces a new knowledge\u003c/strong\u003e that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and \u003cstrong\u003emight lead to new training strategies\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eOne possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eReferences\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eBragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022.\u003c/li\u003e\n\u003cli\u003eDmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017.\u003c/li\u003e\n\u003cli\u003eJ. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/neq/","date_published":"7026-07-09T255:77:00+01:00","date_modified":"7026-07-09T255:77:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"0af752892a7ab6b8e738b7ff6f9468359b3cb382","title":"Optimal Transport Based Adversarial Patch Attacks","summary":"","content_text":" Authors: Mohammed Jawhar Aymane Rahmoune Paper : Optimal Transport Based Adversarial Based Patch To Leverage Large Scale Attack Transferability Table of contents : Introduction Understanding Adversarial Patch Attacks Decision boundary based Feature point based Distribution based Transferability Optimal Transport Experiments Experimental Setup Results and Findings Digital Experiments Hybrid Experiments Physical Experiments Reproducibility Conclusion Introduction Imagine you\u0026rsquo;re showing a picture to a friend, asking them to guess who\u0026rsquo;s in it, then sticking a tiny, almost invisible sticker on that photo. For some reason, this sticker makes your friend completely unable to recognize who\u0026rsquo;s in the picture. This might sound like magic, but something similar can happen with Computer Vision models designed to capture an image content, either through a classification, a segmentation or even a generation task. These AI programs can be vulnerable to such tricks, that we call technically, Adversarial Patch Attacks.\nAs AI becomes increasingly integrated into various aspects of our lives, including critical applications like passport security systems, autonomous vehicles, traffic sign detection, and surgical assistance; the reliability, trustworthiness, and performance of these systems under all conditions became of prime importance. This has led to a growing interest in the area of Robust AI, which focuses on enhancing the safety and security of AI technologies by improving their resilience to adverse conditions and digital threats. Within this domain, the study of Attacks and Defense ways plays a pivotal role.\nWhile these attacks might not seem like a big deal, nor dangerous in this context, the consequences can be severe in critical scenarios - take for example an autonomous vehicle failing to recognize a stop sign, hurting potentially a pedestrian. In this blog we will explore a new approach used for developping such adversarial patch attacks, based on Optimal Transport, as outlined in the paper Optimal Transport Based Adversarial Patch To Leverage Large Scale Attack Transferability. We will try to follow the same structure as in the paper to make the reading easier for you, but with much more simplicity.\nUnderstanding Adversarial Attacks First thing first, let us redefine some previously mentionned concepts, while making them into context.\nAs deep neural networks keep getting better, developers are working hard to make sure they are trustworthy and reliable. This means constantly testing them to see how well they can handle different challenges, quantifying their robustness, and developping some robustification methods. In the context of image classification for instance, one way to do this is by designing adversarial attacks, which consists of a perturbation or noise, sometimes invisible patterns added to the input images in order to confuse the model and make it misclassify them, causing a huge drop in the accuracy.\nAdversarial Patch Attacks are a specific type that consists of altering only a small part(patch) of the input, either physically or digitally by inserting a crafted \u0026ldquo;sticker\u0026rdquo;. These attacks happen to be more threatful as they can be easily applied in real life, they do not require modification of the entire image, and they can fool multiple, vastly different models with the same crafted patch. This last property is called transferability and aims to test these engineered adversarial patches on various target models, beyond the original one used for learning, even if the two models(source and target) have been trained on different data or use different architectures, to evaluate the attack\u0026rsquo;s efficacy, and measure the models robustness.\nDespite the fact that crafting adversarial patch attacks is mainly based around maximizing the classification error through a gradient ascent, we can differenciate between three distinct approaches:\nDecision boundaries based : Which is the most applied approach in previous works and litterature. It focuses on pushing the image\u0026rsquo;s representation in the neural network\u0026rsquo;s decision space, across the decision boundary, making the network perceive it as belonging to a different, probability maximized class.\nTo simplify this approach, imagine a group of fans attempting to sneak into a VIP section at a concert by dressing in a fancy way, like known VIP guests(targeted class). The idea is to blend in so well that they are indistinguishable from actual VIPs to the security guards (the ML model). Despite the simplicity and goodness of this strategy, it has some drawbacks :\nIt is highly dependant on the model on which the attack is based, which makes it not really transferable: The success of this method hinges on the security\u0026rsquo;s lack of detail. If they are controlled by another security gard who is very familiar with the actual VIPs, the disguises will fail.\nThe patch may push the corrupted image representations into unknown regions of the representation space: In their attempt to mimic the VIPs, there\u0026rsquo;s a risk that their disguises might be so overdone that they don\u0026rsquo;t resemble any actual VIPs, pushing them to have a weird unique look. Hence, they end up in a no-man\u0026rsquo;s-land, not fitting in with either the regular attendees or the VIPs.\nFeature point based : Instead of crossing a decision boundary, this strategy aims to modify the input so its representation in the feature space matches the one of a target point belonging to a different class. This is like fine-tuning the attack to match a specific \u0026ldquo;signature\u0026rdquo; that the model associates to a specific point.\nRevisiting our concert analogy, consider the fans now opting to mimic a specific celebrity known to be attending the concert, assuming that matching this one high-profile individual\u0026rsquo;s appearance will guarantee them entry. Although it seems more precise and effective than the first approach, this strategy has a significan drawback :\nIt depends heavily on the targeted point selection, this later may be not representative of all instances in the target class : For instance, if the celebrity is known for a distinctive but uncommon style or if it\u0026rsquo;s unusual for such celebrities to attend such events, their attempt to copy him might not match what the security team usually expects from VIP guests. Distribution based : This new approach implemented in the paper we are analyzing , is based on Optimal Transport theory, and aims to alter the overall feature distribution ofa set of input images belonging to a specific class, to resemble another class\u0026rsquo;s distribution, reducing the gap between them in the feature space. It is more sophisticated than the previous ones as it exploits the fundamental way neural networks process and classify images based on learned distributions.\nThis time, the group studies a wide variety of guests behaviors and appearances to craft a new, ambiguous look that doesn\u0026rsquo;t specifically mimic any single guest type, nor disguise blindly in a \u0026ldquo;VIP\u0026rdquo; style, but instead blends into the overall crowd, avoiding easy detection.\nThe main advantage of this approach is that it allows a better transferability between models, enhancing the performance in the blackbox configuration, as it is independant of the classifier\u0026rsquo;s decision boundary , and the choice of a specific target point. Furthermore it captures the useful characteristics (features) from an input in a more universal way. Why do we need transferability ? You surely noticed that we mentionned the transferability term many times in the last section, showing that is an essential property for designing such attacks, but why do we focus so much to make our patch transferable through many models? Well, it is like discovering a master key for many locks : It enables bad actors to compromise and confuse an AI system using a crafted patch they made without knowing anything about that system(architecture, training,\u0026hellip;).\nThis ability to create a \u0026lsquo;one-size-fits-all\u0026rsquo; adversarial patch allows to challenge many models, making it more difficult to develop defense mechanisms, and fostering the development of more robust AI systems. Unfortunately, this important property, which confronts the real-world variability of target systems, whose specific architectures or training details are often unknown, was not achieved strongly by previously developped Adversarial attacks; it was studied only by some specialized Adversarial Patch Attacks models(GAP, LaVan, PS-GAN) and gave very modest rsults, being evaluated on dated, non state of the art models Other models (TTP, M3D, Inkawhich et al.) conducted some experiments to measure the transferability of ivisible adversarial attacks and gave promizing results, but they didn\u0026rsquo;t focus i their work on patch attacks transferability.\nDiving into Optimal Transport theory The method introduced in this paper represents a remarkable success, as it bridges the gap between transferability studies of invisible adversarial examples and adversarial patch attacks, and provides a trade-off between an efficient non complex patch designing approach, and an exceptional transferability among many advanced state-of-the-art models. The key reason for this success lies in the inherent capabilities of optimal transport to measure the distance between two distributions. Particularly, the loss optimized in this method is relevant, as it can be used when the distributions do not overlap, and the theory behind it is intuitive. It is based mainly on the Wasserstain distance defined as :\n$$W_{p}^p(\\mu,\\nu) = \\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} ||x - y||^p d\\pi(x, y)$$\n\u003c!DOCTYPE html\u003e MathJax Visualization Example or its more computationnaly efficient Sliced version, which compares the two distributions by computing the expected Wasserstein distance between their one-dimensional linear projections : $$SW_{p}^p(\\mu,\\nu) = \\int_{S^{d-1}} W_{p}^p(\\theta_{\\#}^{*}\\mu, \\theta_{\\#}^{*}\\nu) d\\sigma(\\theta)$$\nWhere $\\mu$ and $\\nu$ are two propbability distributions on $\\mathbb{R}^d$, $||.||$ the euclidean norm, $\\pi$ is a transport plan between $\\mu$ and $\\nu$, and $ \\theta_{\\#}^{*} \\mu $ and $ \\theta_{\\#}^{*} \\nu $ the push-forward by $\\theta^{*}(x)=\u003c\\theta, x\u003e$ of $\\mu$ and $\\nu$ respectively. This image is taken and adapted from the Sliced-Wasserstein distance for large-scale machine learning: theory, methodology and extensions paper. To delve more into the mathematical details, let us explore how Optimal Transport, specifically the Wasserstein distance, is employed to craft effective adversarial examples: In the context of image classification, we consider the standard notation where a set of image-label pairs $(x_i, y_i)$ is drawn from a joint distribution of random variables $X$ and $Y$. The images $X$ are typically multi-dimensional arrays representing the height, width, and color channels of an image (e.g., a colored $256 \\times 256$ pixel image would have $h = 256$, $w = 256$, and $c = 3$). Meanwhile, $Y$ is a set of discrete labels that classify these images (e.g., \u0026lsquo;cat\u0026rsquo;, \u0026lsquo;dog\u0026rsquo;, etc.). Within a given encoder-decoder neural network $F$, designed to predict these labels, the encoder function $f$ compresses the raw image data $X$ throughout each pooling layer into a feature space $S^{(l)}$, capturing essential patterns.\nThe Wasserstein distance $W_p$, calculated between the distributions of these feature spaces, reflects how much \u0026ldquo;effort\u0026rdquo; it would take to transform the distribution of features from one class into another. In the case of the proposed method, crafting the patch consits of minimizing the transformation cost (distance)of the features distribution from a corrupted \u0026ldquo;true\u0026rdquo; class into a \u0026ldquo;target\u0026rdquo; adversarial class across multiple layers. This can be formulated as follows:\n\u003c!DOCTYPE html\u003e MathJax Visualization Example $$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_X \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{X_{\\delta}}^{(l)}, \\nu_y^{(l)}) \\right]$$\nWhere $OT$ is the optimal transport distance (Wasserstein or Sliced Wasserstein), $\\mu_{X_{\\delta}}^{(l)}$ is the feature distribution of images with the patch and $\\nu_y^{(l)}$ is the target feature distribution for the incorrect class.\nThis can be further enhanced by adding a regularization term to ensure that the patches are effective under various conditions, and can be physically realisable. The problem becomes as follows :\n$$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_{X, t\\sim \\tau, e\\sim E} \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{A(\\delta, X, e, t)}^{(l)}, \\nu_y^{(l)}) + TV(\\delta)\\right]$$ where TV is the total variation loss discouraging high-frequency patterns.\nExperiments Experimental setup To confirm the theoretical results and assumptions, several experiments were conducted under different conditions and settings. For the sake of simplicity, we will not delve into the exhaustive details of the experimental setup, procedures, and results. In summary:\nThe experiments aimed to evaluate the impact and transferability of the proposed adversarial patch - referred to as $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ - across a range of models.\n$(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ performance was benchmarked against other adversarial patch attack (APA) methods such as GAP, LaVAN, TNT, TTP, M3D, and others.\nThe source and target models chosen for this analysis were regrouped into six categories based on their architecture: CNNs-V1, CNNs-V2, ENet, CNext(ConvNext), DeiT, and Swin.\nTested patches were randomly placed to the side of images, in order to avoid occluding the object of interest and replicate more closely the real world conditions.\nTargeted success rate (tSuc) metric was used for evaluating transferability. It consists of the percentage of instances where the network, when presented with an image containing the adversarial patch, incorrectly classifies the image as the attacker\u0026rsquo;s intended target class, out of the total number of attempts.\nResults and Findings : The experiments are structured into three main categories:\nDigital experiments : Simple configuration : In this configuration, the patches efficacy was tested in a purely digital environment, using images from the ImageNet-1K dataset, which was used also for training. Patches were first designed to attack one of the source models, then tested on other target models to measure the attacking transferability. The table below summarizes for each APA method, the best transferring attack performance achieved :\nAs expected through the novelty of $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, it shows the highest transferability capacity(mean, min and max) and outperforms all the other methods. Additionaly, we can make the following observations:\nNetworks trained with older training recipes (CNNs-v1) seem more vulnerable to attacks, unlike tansformers and models trained with new training recipes (scheduler, augmenting training data like RandAug and Mixup, \u0026hellip;) which appear to be more robust.\nFor all APA methods, patches learned using Swin or CNext are more universal as they can transfer uniformly to multiple models.\nIn general, baseline methods tend to overfit and fail to generate patches that effectively transfer to complex architectures like CNext and Swin models, even if these patches are developed using the same category of models.\nMethods based on feature space optimization, including L2 and the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, demonstrate improved transferability and are less likely to overfit.\nRobustified configuration : In the second configuration of the digital experiments, the same procedures were reapplied. However this time, the methods learn on Swin, and transfer to a robustified version, by Local Gradients Smoothing (LGS) - a defense mechanism smoothing salient regions in images before passing them to the network - , of the six model categories.\nSimilarly, $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ outperforms significantly all other methods as we can see in the following table :\nHybrid experiments: In order to simulate real-world applications more closely, the hybrid experiments conducted within this section involved printing adversarial patches trained with Swin, placing them in physical environments, capturing the images, and then digitally analyzing the results, for simple, and robustified models.\nThe table below shows the criticality of the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ giving very large tSuc in comparison with the other methods, for all settings:\nPhysical experiments: In this last experiments category, we get closer to the real world situations, by recording a video of some ImageNet-1K objects (banana, cup , keyboard) while moving a designed patch in the set. This aims to quantify the severity of each attack, for realistic scenarios (as the example provided above about the autonomous vehicule not detecting the stop sign while driving due to an adversarial patch designed without knowing the AI system at all).\nAll APA methods failed to transfer properly on all architectures except for L2 with a modest tSuc(9.3%) and $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ that gave much better results (23.4% and 29.3%)\nReproducibility In this section, we wanted to reproduce some of the experiments conducted in the paper to validate the results and the findings. However, by exploring the code provided with the paper, and analyzing the python files, we found that it is not well documented, and the structure is not very clear, which makes it difficult to understand and reproduce the complex experiments involving transferability evaluation. Furthermore, given that the paper is based on the ImageNet dataset, which is very large and requires a lot of computational resources, we were not able to run the experiments on our local machines, as we do not have access to a powerful GPU cluster. Consequently, we opted for the CIFAR-10 dataset, which is smaller and more manageable. Despite this adjustment, we still faced some issues Specifically, the model is built from scratch without an available pre-trained, and there are missing components, notably the function required to extract feature vectors from each layer of the target models. To address these challenges and make the reproduction process easier, we decided to develop the missing feature extraction function as an enhancement, and save the obtained results into files(in the same way it was done in the code), to be able to apply the optimal transport method and craft the adversarial patches later as perspectives\nHere are the code that we developed :\nimport torch import torchvision.models as models import torchvision.transforms as transforms from torch.utils.data import DataLoader import torchvision.datasets as datasets def get_loader(dataset: str, split: str, batch_size: int) -\u0026gt; torch.utils.data.DataLoader: \u0026#34;\u0026#34;\u0026#34;Return a DataLoader object for a given dataset and split.\u0026#34;\u0026#34;\u0026#34; return torch.utils.data.DataLoader(get_dataset(dataset, split), batch_size=batch_size, shuffle=True) def extract_features(data_loader, list_models): \u0026#34;\u0026#34;\u0026#34; Extracts features from each layer of the pre-trained models provided in the list_models by applying average pooling, and saves the extracted features into files. \u0026#34;\u0026#34;\u0026#34; for model_name in list_models: model = models.__dict__[model_name](pretrained=True) model.eval() for layer_name, layer in model.named_children(): if isinstance(layer, torch.nn.Sequential): layer_features = [] for i, (input, target) in enumerate(data_loader): with torch.no_grad(): output = layer(input) output = torch.nn.functional.adaptive_avg_pool2d(output, (1, 1)) output = output.view(output.size(0), -1) layer_features.append(output) layer_features = torch.cat(layer_features) torch.save(layer_features, f\u0026#34;./data/CIFAR/all_images_feature/{layer_name}/{model_name}.pt\u0026#34;) # Apply the function extract_features to some targeted models list_models = [\u0026#34;resnet18\u0026#34;, \u0026#34;vgg19\u0026#34;, \u0026#34;convnext_tiny\u0026#34;, \u0026#34;swin_t\u0026#34;] data_loader = get_loader(\u0026#34;CIFAR10\u0026#34;, \u0026#34;train\u0026#34;, batch_size=64) extract_features(data_loader, list_models) Conclusion In conclusion, our exploration of the paper OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY, revealed an innovative and promizing technique that uses Optimal Transport to make adversarial patches more effectively fool different models. This method, focusing on altering image feature distributions to match a target distribution from another class, has proven to be both theoretically sound and practically successful. It significantly outperforms current state of the art methods in creating patches that can be highly transferable between models and potentially very harmful, showing great promise for both advancements in the field and potential challenges in security applications.\nReferences: OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY\nSliced-Wasserstein distance for large-scale machine learning : theory, methodology and extensions\nComputational Optimal Transport\nFeature Space Perturbations Yield More Transferable Adversarial Examples\n","content_html":"\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 id=\"authors\"\u003eAuthors:\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eMohammed Jawhar\u003c/li\u003e\n\u003cli\u003eAymane Rahmoune\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"paper--optimal-transport-based-adversarial-based-patch-to-leverage-large-scale-attack-transferability\"\u003ePaper : \u003ca href=\"https://openreview.net/forum?id=nZP10evtkV\"\u003eOptimal Transport Based Adversarial Based Patch To Leverage Large Scale Attack Transferability\u003c/a\u003e\u003c/h3\u003e\n\u003ch1 id=\"table-of-contents-\"\u003eTable of contents :\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eUnderstanding Adversarial Patch Attacks\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subsection-11\"\u003eDecision boundary based\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-12\"\u003eFeature point based\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-13\"\u003eDistribution based\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eTransferability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eOptimal Transport\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eExperiments\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subsection-41\"\u003eExperimental Setup\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-42\"\u003eResults and Findings\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subsection-421\"\u003eDigital Experiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-422\"\u003eHybrid Experiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subsection-423\"\u003ePhysical Experiments\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eReproducibility\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eImagine you\u0026rsquo;re showing a picture to a friend, asking them to guess who\u0026rsquo;s in it, then sticking a tiny, almost invisible sticker on that photo. For some reason, this sticker makes your friend completely unable to recognize who\u0026rsquo;s in the picture. This might sound like magic, but something similar can happen with Computer Vision models designed to capture an image content, either through a classification, a segmentation or even a generation task. These AI programs can be vulnerable to such tricks, that we call technically, Adversarial Patch Attacks.\u003c/p\u003e\n\u003cp\u003eAs AI becomes increasingly integrated into various aspects of our lives, including critical applications like passport security systems, autonomous vehicles, traffic sign detection, and surgical assistance; the reliability, trustworthiness, and performance of these systems under all conditions became of prime importance. This has led to a growing interest in the area of Robust AI, which focuses on enhancing the safety and security of AI technologies by improving their resilience to adverse conditions and digital threats. Within this domain, the study of Attacks and Defense ways plays a pivotal role.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/road_scene.png\" alt=\"Road_scene\"\u003e\n\u003c/p\u003e\n\u003cp\u003eWhile these attacks might not seem like a big deal, nor dangerous in this context, the consequences can be severe in critical scenarios - take for example an autonomous vehicle failing to recognize a stop sign, hurting potentially a pedestrian. In this blog we will explore a new approach used for developping such adversarial patch attacks, based on Optimal Transport, as outlined in the paper \u003cem\u003e\u003cstrong\u003eOptimal Transport Based Adversarial Patch To Leverage Large Scale Attack Transferability\u003c/strong\u003e\u003c/em\u003e. We will try to follow the same structure as in the paper to make the reading easier for you, but with much more simplicity.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eUnderstanding Adversarial Attacks\u003c/h2\u003e\n\u003cp\u003eFirst thing first, let us redefine some previously mentionned concepts, while making them into context.\u003c/p\u003e\n\u003cp\u003eAs deep neural networks keep getting better, developers are working hard to make sure they are trustworthy and reliable. This means constantly testing them to see how well they can handle different challenges, quantifying their robustness, and developping some robustification methods. In the context of image classification for instance, one way to do this is by designing adversarial attacks, which consists of a perturbation or noise, sometimes invisible patterns added to the input images in order to confuse the model and make it misclassify them, causing a huge drop in the accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAdversarial Patch Attacks\u003c/strong\u003e are a specific type that consists of altering only a small part(patch) of the input, either physically or digitally by inserting a crafted \u0026ldquo;sticker\u0026rdquo;. These attacks happen to be more threatful as they can be easily applied in real life, they do not require modification of the entire image, and they can fool multiple, vastly different models with the same crafted patch. This last property is called \u003cstrong\u003etransferability\u003c/strong\u003e and aims to test these engineered adversarial patches on various target models, beyond the original one used for learning, even if the two models(source and target) have been trained on different data or use different architectures, to evaluate the attack\u0026rsquo;s efficacy, and measure the models robustness.\u003c/p\u003e\n\u003cp\u003eDespite the fact that crafting adversarial patch attacks is mainly based around maximizing the classification error through a gradient ascent, we can differenciate between three distinct approaches:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/APA_strategies.png\" alt=\"APA_strategies\"\u003e\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDecision boundaries based :\u003c/strong\u003e \u003ca name=\"subsection-11\"\u003e\u003c/a\u003e Which is the most applied approach in previous works and litterature. It focuses on pushing the image\u0026rsquo;s representation in the neural network\u0026rsquo;s \u003cstrong\u003edecision\u003c/strong\u003e space, across the decision boundary, making the network perceive it as belonging to a different, probability maximized class.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTo simplify this approach, imagine a group of fans attempting to sneak into a VIP section at a concert by dressing in a fancy way, like known VIP guests(targeted class). The idea is to blend in so well that they are indistinguishable from actual VIPs to the security guards (the ML model). Despite the simplicity and goodness of this strategy, it has some drawbacks :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIt is highly dependant on the model on which the attack is based, which makes it not really transferable: The success of this method hinges on the security\u0026rsquo;s lack of detail. If they are controlled by another security gard who is very familiar with the actual VIPs, the disguises will fail.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe patch may push the corrupted image representations into unknown regions of the representation space: In their attempt to mimic the VIPs, there\u0026rsquo;s a risk that their disguises might be so overdone that they don\u0026rsquo;t resemble any actual VIPs, pushing them to have a weird unique look. Hence, they end up in a no-man\u0026rsquo;s-land, not fitting in with either the regular attendees or the VIPs.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFeature point based :\u003c/strong\u003e \u003ca name=\"subsection-12\"\u003e\u003c/a\u003eInstead of crossing a decision boundary, this strategy aims to modify the input so its representation in the \u003cstrong\u003efeature space\u003c/strong\u003e matches the one of a target point belonging to a different class. This is like fine-tuning the attack to match a specific \u0026ldquo;signature\u0026rdquo; that the model associates to a specific point.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRevisiting our concert analogy, consider the fans now opting to mimic a specific celebrity known to be attending the concert, assuming that matching this one high-profile individual\u0026rsquo;s appearance will guarantee them entry. Although it seems more precise and effective than the first approach, this strategy has a significan drawback :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt depends heavily on the targeted point selection, this later may be not representative of all instances in the target class :  For instance, if the celebrity is known for a distinctive but uncommon style or if it\u0026rsquo;s unusual for such celebrities to attend such events, their attempt to copy him might not match what the security team usually expects from VIP guests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDistribution based :\u003c/strong\u003e \u003ca name=\"subsection-13\"\u003e\u003c/a\u003eThis new approach implemented in the paper we are analyzing , is based on Optimal Transport theory, and aims to alter the overall feature distribution ofa set of input images belonging to a specific class, to resemble another class\u0026rsquo;s distribution, reducing the gap between them in the \u003cstrong\u003efeature space\u003c/strong\u003e. It is more sophisticated than the previous ones as it exploits the fundamental way neural networks process and classify images based on learned distributions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThis time, the group studies a wide variety of guests behaviors and appearances to craft a new, ambiguous look that doesn\u0026rsquo;t specifically mimic any single guest type, nor disguise blindly in a \u0026ldquo;VIP\u0026rdquo; style, but instead blends into the overall crowd, avoiding easy detection.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe main advantage of this approach is that it allows a better transferability between models, enhancing the performance in the blackbox configuration, as it is independant of the classifier\u0026rsquo;s decision boundary , and the choice of a specific target point. Furthermore it captures the useful characteristics (features) from an input in a more universal way.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2\"\u003eWhy do we need transferability ?\u003c/h2\u003e\n\u003cp\u003eYou surely noticed that we mentionned the transferability term many times in the last section, showing that is an essential property for designing such attacks, but why do we focus so much to make our patch transferable through many models? Well, it is like discovering a master key for many locks : It enables bad actors to compromise and confuse an AI system using a crafted patch they made without knowing anything about that system(architecture, training,\u0026hellip;).\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/transferability_diagram.png\" alt=\"transferability_diagram\"\u003e\n\u003c/p\u003e\n\u003cp\u003eThis ability to create a \u0026lsquo;one-size-fits-all\u0026rsquo; adversarial patch allows to challenge many models, making it more difficult to develop defense mechanisms, and fostering the development of more robust AI systems. Unfortunately, this important property, which confronts the real-world variability of target systems, whose specific architectures or training details are often unknown, was not achieved strongly by previously developped Adversarial attacks; it was studied only by some specialized Adversarial Patch Attacks models(GAP, LaVan, PS-GAN) and gave very modest rsults, being evaluated on dated, non state of the art models Other models (TTP, M3D, Inkawhich et al.) conducted some experiments to measure the transferability of ivisible adversarial attacks and gave promizing results, but they didn\u0026rsquo;t focus i their work on patch attacks transferability.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eDiving into Optimal Transport theory\u003c/h2\u003e\n\u003cp\u003eThe method introduced in this paper represents a remarkable success, as it bridges the gap between transferability studies of invisible adversarial examples and adversarial patch attacks, and provides a trade-off between an efficient non complex patch designing approach, and an exceptional transferability among many advanced state-of-the-art models. The key reason for this success lies in the inherent capabilities of \u003cstrong\u003eoptimal transport\u003c/strong\u003e to measure the distance between two distributions. Particularly, the loss optimized in this method is relevant, as it can be used when the distributions do not overlap, and the theory behind it is intuitive. It is based mainly on the \u003cstrong\u003eWasserstain distance\u003c/strong\u003e defined as :\u003c/p\u003e\n\u003cp\u003e$$W_{p}^p(\\mu,\\nu) = \\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} ||x - y||^p d\\pi(x, y)$$\u003c/p\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003ctitle\u003eMathJax Visualization Example\u003c/title\u003e\n    \u003cscript type=\"text/x-mathjax-config\"\u003e\n    MathJax.Hub.Config({\n        tex2jax: {\n            inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n            displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']\n        }\n    });\n    \u003c/script\u003e\n    \u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eor its more computationnaly efficient Sliced version, which compares the two distributions by computing the expected Wasserstein distance between their one-dimensional linear projections :\n$$SW_{p}^p(\\mu,\\nu) = \\int_{S^{d-1}} W_{p}^p(\\theta_{\\#}^{*}\\mu, \\theta_{\\#}^{*}\\nu) d\\sigma(\\theta)$$\u003c/p\u003e\n\u003cp\u003e\nWhere $\\mu$ and $\\nu$ are two propbability distributions on $\\mathbb{R}^d$, $||.||$ the euclidean norm, $\\pi$ is a transport plan between $\\mu$ and $\\nu$, and $ \\theta_{\\#}^{*} \\mu $ and $ \\theta_{\\#}^{*} \\nu $ the push-forward by $\\theta^{*}(x)=\u003c\\theta, x\u003e$ of $\\mu$ and $\\nu$ respectively.\n\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Sliced_wasserstain.png\" alt=\"Sliced Wasserstain\"\u003e\n  \u003cbr\u003e\n  \u003cem\u003eThis image is taken and adapted from the \u003ca href=\"https://theses.hal.science/tel-03533097/document\"\u003eSliced-Wasserstein distance for large-scale machine learning: theory, methodology and extensions\u003c/a\u003e paper.\u003c/em\u003e\n\u003c/p\u003e\n\u003c!-- $$SW_{p}(\\mu,\\nu) = \\int_{S^{d-1}} W_{p}(\\theta_{\\#}\\mu, \\theta_{\\#}\\nu) d\\sigma(\\theta)$$--\u003e\n\u003cp\u003eTo delve more into the mathematical details, let us explore how Optimal Transport, specifically the Wasserstein distance, is employed to craft effective adversarial examples:\nIn the context of image classification, we consider the standard notation where a set of image-label pairs $(x_i, y_i)$ is drawn from a joint distribution of random variables $X$ and $Y$. The images $X$ are typically multi-dimensional arrays representing the height, width, and color channels of an image (e.g., a colored $256 \\times 256$ pixel image would have $h = 256$, $w = 256$, and $c = 3$). Meanwhile, $Y$ is a set of discrete labels that classify these images (e.g., \u0026lsquo;cat\u0026rsquo;, \u0026lsquo;dog\u0026rsquo;, etc.). Within a given encoder-decoder neural network $F$, designed to predict these labels, the encoder function $f$ compresses the raw image data $X$ throughout each pooling layer into a feature space $S^{(l)}$, capturing essential patterns.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Optimal transport.png\" alt=\"Optimal Transport\"\u003e\n\u003c/p\u003e\n\u003cp\u003eThe Wasserstein distance $W_p$, calculated between the distributions of these feature spaces, reflects how much \u0026ldquo;effort\u0026rdquo; it would take to transform the distribution of features from one class into another. In the case of the proposed method, crafting the patch consits of minimizing the transformation cost (distance)of the features distribution from a corrupted \u0026ldquo;true\u0026rdquo; class into a \u0026ldquo;target\u0026rdquo; adversarial class across multiple layers. This can be formulated as follows:\u003c/p\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003ctitle\u003eMathJax Visualization Example\u003c/title\u003e\n    \u003cscript type=\"text/x-mathjax-config\"\u003e\n    MathJax.Hub.Config({\n        tex2jax: {\n            inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n            displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']\n        }\n    });\n    \u003c/script\u003e\n    \u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\n$$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_X \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{X_{\\delta}}^{(l)}, \\nu_y^{(l)}) \\right]$$\u003c/p\u003e\n\u003cp\u003e\nWhere $OT$ is the optimal transport distance (Wasserstein or Sliced Wasserstein), $\\mu_{X_{\\delta}}^{(l)}$ is the feature distribution of images with the patch and $\\nu_y^{(l)}$ is the target feature distribution for the incorrect class.\u003c/p\u003e\n\u003cp\u003eThis can be further enhanced by adding a regularization term to ensure that the patches are effective under various conditions, and can be physically realisable. The problem becomes as follows :\u003c/p\u003e\n\u003cp\u003e\n$$\\delta^* = \\arg \\min_{\\delta} \\mathbb{E}_{X, t\\sim \\tau, e\\sim E} \\left[ \\sum_{l \\in \\mathcal{L}} OT(\\mu_{A(\\delta, X, e, t)}^{(l)}, \\nu_y^{(l)}) + TV(\\delta)\\right]$$\nwhere TV is the total variation loss discouraging high-frequency patterns.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\u003ch2 id=\"section-4\"\u003eExperiments\u003c/h2\u003e\n\u003ch3 id=\"experimental-setup\"\u003eExperimental setup \u003ca name=\"subsection-41\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eTo confirm the theoretical results and assumptions, several experiments were conducted under different conditions and settings. For the sake of simplicity, we will not delve into the exhaustive details of the experimental setup, procedures, and results. In summary:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe experiments aimed to evaluate the impact and transferability of the proposed adversarial patch - referred to as $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ - across a range of models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ performance was benchmarked against other adversarial patch attack (APA) methods such as GAP, LaVAN, TNT, TTP, M3D, and others.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe source and target models chosen for this analysis were regrouped into six categories based on their architecture: CNNs-V1, CNNs-V2, ENet, CNext(ConvNext), DeiT, and Swin.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTested patches were randomly placed to the side of images, in order to avoid occluding the object of interest and replicate more closely the real world conditions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTargeted success rate (tSuc)\u003c/strong\u003e metric was used for evaluating transferability. It consists of the percentage of instances where the network, when presented with an image containing the adversarial patch, incorrectly classifies the image as the attacker\u0026rsquo;s intended target class, out of the total number of attempts.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"results-and-findings-\"\u003eResults and Findings : \u003ca name=\"subsection-42\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe experiments are structured into three main categories:\u003c/p\u003e\n\u003ch4 id=\"digital-experiments-\"\u003eDigital experiments : \u003ca name=\"subsection-421\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003ch5 id=\"simple-configuration-\"\u003eSimple configuration :\u003c/h5\u003e\n\u003cp\u003eIn this configuration, the patches efficacy was tested in a purely digital environment, using images from the ImageNet-1K dataset, which was used also for training. Patches were first designed to attack one of the source models, then tested on other target models to measure the attacking transferability. The table below summarizes for each APA method, the best transferring attack performance achieved :\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Digital_transferability.png\" alt=\"Digital Transferability\"\u003e\n\u003c/p\u003e\n\u003cp\u003eAs expected through the novelty of $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, it shows the highest transferability capacity(mean, min and max) and outperforms all the other methods. Additionaly, we can make the following observations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNetworks trained with older training recipes (CNNs-v1) seem more vulnerable to attacks, unlike tansformers and models trained with new training recipes (scheduler, augmenting training data like RandAug and Mixup, \u0026hellip;) which appear to be more robust.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor all APA methods, patches learned using Swin or CNext are more universal as they can transfer uniformly to multiple models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn general, baseline methods tend to overfit and fail to generate patches that effectively transfer to complex architectures like CNext and Swin models, even if these patches are developed using the same category of models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMethods based on feature space optimization, including L2 and the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, demonstrate improved transferability and are less likely to overfit.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"robustified-configuration-\"\u003eRobustified configuration :\u003c/h5\u003e\n\u003cp\u003eIn the second configuration of the digital experiments, the same procedures were reapplied. However this time, the methods learn on Swin, and transfer to a robustified version, by Local Gradients Smoothing (LGS) - a defense mechanism smoothing salient regions in images before passing them to the network - , of the six model categories.\u003c/p\u003e\n\u003cp\u003eSimilarly, $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ outperforms significantly all other methods as we can see in the following table :\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Digital_robustified.png\" alt=\"Digital robustified\"\u003e\n\u003c/p\u003e\n\u003ch4 id=\"hybrid-experiments\"\u003eHybrid experiments: \u003ca name=\"subsection-422\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eIn order to simulate real-world applications more closely, the hybrid experiments conducted within this section involved printing adversarial patches trained with Swin, placing them in physical environments, capturing the images, and then digitally analyzing the results, for simple, and robustified models.\u003c/p\u003e\n\u003cp\u003eThe table below shows the criticality of the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ giving very large tSuc in comparison with the other methods, for all settings:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/image_optimal_transport_patch/Digital_robustified.png\" alt=\"Digital robustified\"\u003e\n\u003c/p\u003e\n\u003ch4 id=\"physical-experiments\"\u003ePhysical experiments: \u003ca name=\"subsection-423\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eIn this last experiments category, we get closer to the real world situations, by recording a video of some ImageNet-1K objects (banana, cup , keyboard) while moving a designed patch in the set. This aims to quantify the severity of each attack, for realistic scenarios (as the example provided above about the autonomous vehicule not detecting the stop sign while driving due to an adversarial patch designed without knowing the AI system at all).\u003c/p\u003e\n\u003cp\u003eAll APA methods failed to transfer properly on all architectures except for L2 with a modest tSuc(9.3%) and $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ that gave much better results (23.4% and 29.3%)\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003eIn this section, we wanted to reproduce some of the experiments conducted in the paper to validate the results and the findings. However, by exploring the code provided with the paper, and analyzing the python files, we found that it is not well documented, and the structure is not very clear, which makes it difficult to understand and reproduce the complex experiments involving transferability evaluation. Furthermore, given that the paper is based on the ImageNet dataset, which is very large and requires a lot of computational resources, we were not able to run the experiments on our local machines, as we do not have access to a powerful GPU cluster. Consequently, we opted for the CIFAR-10 dataset, which is smaller and more manageable. Despite this adjustment, we still faced some issues Specifically, the model is built from scratch without an available pre-trained, and there are missing components, notably the function required to extract feature vectors from each layer of the target models. To address these challenges and make the reproduction process easier, we decided to develop the missing feature extraction function as an enhancement, and save the obtained results into files(in the same way it was done in the code), to be able to apply the optimal transport method and craft the adversarial patches later as perspectives\u003c/p\u003e\n\u003cp\u003eHere are the code that we developed :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.models\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodels\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.transforms\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.utils.data\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.datasets\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eas\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edatasets\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eget_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esplit\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eutils\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u0026#34;Return a DataLoader object for a given dataset and split.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eutils\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eDataLoader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eget_dataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edataset\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003esplit\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eshuffle\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#75af00\"\u003eextract_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    Extracts features from each layer of the pre-trained models provided in the list_models\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    by applying average pooling, and saves the extracted features into files.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#d88200\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodel_name\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e__dict__\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emodel_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e](\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epretrained\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eeval\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003emodel\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enamed_children\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#00a8c8\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eisinstance\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eSequential\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#00a8c8\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ei\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003einput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etarget\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eenumerate\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#00a8c8\"\u003ewith\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eno_grad\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elayer\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003einput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enn\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efunctional\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eadaptive_avg_pool2d\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eview\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esize\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        \u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eappend\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoutput\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecat\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003esave\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;./data/CIFAR/all_images_feature/\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003elayer_name\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e{\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emodel_name\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e.pt\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Apply the function extract_features to some targeted models\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;resnet18\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;vgg19\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;convnext_tiny\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;swin_t\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eget_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;CIFAR10\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;train\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ebatch_size\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e64\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eextract_features\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003edata_loader\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003elist_models\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"section-6\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn conclusion, our exploration of the paper \u003cem\u003e\u003cstrong\u003eOPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY\u003c/strong\u003e\u003c/em\u003e, revealed an innovative and promizing technique that uses Optimal Transport to make adversarial patches more effectively fool different models. This method, focusing on altering image feature distributions to match a target distribution from another class, has proven to be both theoretically sound and practically successful. It significantly outperforms current state of the art methods in creating patches that can be highly transferable between models and potentially very harmful, showing great promise for both advancements in the field and potential challenges in security applications.\u003c/p\u003e\n\u003ch2 id=\"references\"\u003eReferences:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://openreview.net/forum?id=nZP10evtkV\"\u003eOPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://theses.hal.science/tel-03533097/document\"\u003eSliced-Wasserstein distance for large-scale machine learning : theory, methodology and extensions\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1803.00567.pdf\"\u003eComputational Optimal Transport\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf\"\u003eFeature Space Perturbations Yield More Transferable Adversarial Examples\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/optimal_transport_based_adversarial_patch/","date_published":"3026-03-09T222:33:00+01:00","date_modified":"3026-03-09T222:33:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"00988a4ecebb51af6cf28a7b318568929bf7a01d","title":"Statistical Minimax Rates Under Privacy","summary":"","content_text":"Estimating Privacy in Data Science: A Comprehensive Guide Author: Antoine Klein Github Link Table of Contents Incentives Introduction Definition Theory The case of multinomial estimation The case of density estimation Experiment Conclusion Quizz Why do we care about privacy ? Imagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you struggle. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an ethical dilemma: transparency towards the state versus protection of personal data.\n$$\\text{In short, transparency goes against your privacy. }$$\nThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you underestimate your answers. On a wider scale, this leads to a suffrage bias and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\n\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\nThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is not to have protected yourself against a malicious agent.\nAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better certify usage by means of cyber protection labels and leads to such a norm to achieve trust: In this blog, we propose to tackle this problem from a completely different angle: how to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data. We\u0026rsquo;ll also use minimax bounds to answer the question: for a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation? (fundamental trade-offs between privacy and convergence rate)\nScientific introduction Our blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that response randomization achieves optimal convergence in the case of multinomial estimation, and then that this process can be generalized to any nonparametric distribution estimation. To this end, we will introduce the notion of local differential privacy as well as the minimax theory for obtaining optimal limits. All this will shed light on the trade-off between privacy and estimation rates. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\nSome key definitions Let assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a markov kernel that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\nThe privacy mechanism is to be said non interactive if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is memory less. If not, the mechnism is said to be interactive.\nIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\n$Z_i$ is said to be α-local-differentially private for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\nAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more difficult it is to distinguish the distribution of Z conditional on two different X data.\nTheoretical results The case of multinomial estimation In this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$. This problem is a special case of multinomial estimation, where θ is now a multidimensional parameter that is amenable to simplex probability. $∆d := (θ ∈ ℝ+ |∑θ_j = 1)$.\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and $$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\nRecall from standard statistics: For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that: $$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\nIn others term, providing α-local-differentially privacy causes a reduction in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the asymptotically rate of convergences remains unchanged which is a really good news !\nPractical strategies The paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\nRandomized responses Laplace Noise (beyond paper) Randomized responses The intuition of this section is the following : to not allow the statistician to retrieve your personnal data in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, he can\u0026rsquo;t distinguish if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\nFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\n$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$ $$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\nSuch a mechanism achieves α-local-differentially privacy because one can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\nWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\n$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\nThis leads to the natural moment-estimator :\n$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\nOne can also show that it verifies :\n$$E[ ||θ_{hat}- θ||_2] ≤ \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\nLaplace Noise (beyond paper) Instead of saying the truth with some probability, one may think of adding noise to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is not covered in the paper.\nDefinition: A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\nA visualisation for differents parameters is given below. We can see that Laplace distribution is a shaper verson of the gaussian distribution : The trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\nOne can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\nThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, it verifies α-local-differentially privacy. The proposed estimator is the following :\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\nOne can show that it is an unbiaised estimator that achieves the optimal rates:\n$$E[\\hat{Z}] = E[X]$$\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$ $$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\nThis is exactly the optimal rates, quite outstanding !\nThe case of density estimation One accurate question that can raise is : what about others distribution ? Is privacy more costly in general cases ? What is the trade-off ?\nTo answer this question, let\u0026rsquo;s precise the problem.\nWe want to estimate in a non-parametric way a 1D-density function f belonging to one of theses classes :\n-Hölder Class (β, L): $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\n-Sobolev Class: $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\nIn a intuitition way, those two classes express that f is smooth enough to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\nTheorem Without privacy One can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\nIn the case of d-multidimensionnal density f, the optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\nThis illustrates once again the curse of dimensionnality.\nWith privacy Let assume that f bellongs to one of the two classes with β as smoothness parameter.\nThen, the optimal α-local-differentially private optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\nOne may observe two pessimistic news:\n-The rate is affected by a factor of $\\alpha^2$ as for the multinomial estimation\n-More damageable: the rate is slower in term of n unlike the previous problem which make privacy in this case more costly.\nPractical strategies Eventhough this rate is pessimistic and proves that privacy comes at a cost, it remains to illustrates how can we achieves this best but not great rate. For this end, once again, two strategies are possible.\nRandomized responses Laplace Noise (beyond paper) Randomized responses This is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of α. As it is not the most comprehensive and straightforward method, we prefer to dive in depth into the second one; uncovered in the paper.\nLaplace Noise (beyond paper) Let assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\nWe consider the histogramm estimator: $$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\nWe now construct the private mechanism as follow:\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\nIn an intuitive way, we add a Laplace noise realisation for each bin.\nThis guarantees α-local-differentially privacy as : $$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\nThis leads to the α-local-differentially private estimator :\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\nThe biais is the same as the unprivate case as :\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\nOne may prove that if f bellongs to the β-Hölder Class:\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\nMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total :\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$ Minimizing over K (hyperparameters) leads to : $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\nExperiment: Illustration of the Minimax privacy rate Overview The aim of this section is to provide illustrations of the theoretical results set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\nFor the sake of reproducibility and transparency, the source code can be found in the notebook at this: Github link.\nMethodology Data Preparation: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects. More precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\nPrivacy Metric Calculation: We will look at the use case of estimating the mean of a distribution.\nEvaluation: The results will be compared in terms of Mean Square Error (MSE).\nResults In terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\nAs expected, the greater the desired privacy (low $\\alpha$), the more spread out the distribution of observed data.\nWhen it comes to estimating the true average from private data, we obtain the following figure:\nThis figure illustrates two major points:\n-The first is that whatever the level of privacy, we have an unbiased estimator of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\n-The second is that, unfortunately, the greater the privacy (low alpha), the greater the variance of this estimator.\nWe recall our main theorem demonstrated above Previous theorem :\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\nWe now want to compare the theoretical optimal rate with empirical results. To do this, we distinguish two situations:\n-The first is with fixed alpha, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\n-The second has a fixed n and determines the MSE as a function of alpha. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\nConclusion From a problem rooted in an ethical dilemma (privacy versus completeness and transparency), we have looked at the cost of guaranteeing one at the expense of the other, to better sketch out desirable situations.\nThis has enabled us to develop theoretical results in terms of minimax rates. There is indeed a trade-off between these criteria, which is even more costly in the case of non-parametric density estimation.\nFinally, we have compared these theoretical limits with empirical results, which confirm the conformity of the statements.\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following quiz to ensure his or her understanding.\nQuizz To test yourself abour privacy:\nWhat is privacy?\nAvoid asking questions that can raise private information A mechanism that prevents other agent to retrieve personnal information in your answer An ethical-washing trend Which situation is α-local-differentially privacy?\nsup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α) You tell the truth half the time, you lie otherwise. Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1) What is the privacy cost in term of optimal rate ?\nMultinomial estimation: A factor α^2/d Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2)) We loose nothing, that's the surprising finding of the paper Submit Annexes References Warner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830. John C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013) Dwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407. Narayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE. ","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eEstimating Privacy in Data Science: A Comprehensive Guide\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Antoine Klein \u003ca href=\"https://github.com/AntoineTSP\"\u003eGithub Link\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIncentives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eDefinition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eTheory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eThe case of multinomial estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe case of density estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eQuizz\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eWhy do we care about privacy ?\u003c/h2\u003e\n\u003cp\u003eImagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you \u003cem\u003estruggle\u003c/em\u003e. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an \u003cstrong\u003eethical dilemma\u003c/strong\u003e: transparency towards the state versus protection of personal data.\u003cbr\u003e\n$$\\text{In short, transparency goes against your privacy. }$$\u003c/p\u003e\n\u003cp\u003eThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you \u003cstrong\u003eunderestimate\u003c/strong\u003e your answers. On a wider scale, this leads to a \u003cstrong\u003esuffrage bias\u003c/strong\u003e and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\u003cbr\u003e\n\u003cstrong\u003e\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is \u003cstrong\u003enot to have protected yourself against a malicious agent\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better \u003cstrong\u003ecertify usage\u003c/strong\u003e by means of cyber protection labels and leads to such a norm to achieve trust:\n\u003cimg\n  src=\"/images/Antoine_Klein/Umbrella.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn this blog, we propose to tackle this problem from a completely different angle: \u003cstrong\u003ehow to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data\u003c/strong\u003e. We\u0026rsquo;ll also use minimax bounds to answer the question: \u003cstrong\u003efor a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation?\u003c/strong\u003e (fundamental trade-offs between privacy and convergence rate)\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eScientific introduction\u003c/h2\u003e\n\u003cp\u003eOur blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that \u003cstrong\u003eresponse randomization achieves optimal convergence\u003c/strong\u003e in the case of multinomial estimation, and then that this process can be generalized to any \u003cem\u003enonparametric distribution estimation\u003c/em\u003e. To this end, we will introduce the notion of \u003cstrong\u003elocal differential privacy\u003c/strong\u003e as well as the \u003cstrong\u003eminimax theory\u003c/strong\u003e for obtaining optimal limits. All this will shed light on the \u003cstrong\u003etrade-off between privacy and estimation rates\u003c/strong\u003e. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eSome key definitions\u003c/h2\u003e\n\u003cp\u003eLet assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a \u003cstrong\u003emarkov kernel\u003c/strong\u003e that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\u003c/p\u003e\n\u003cp\u003eThe privacy mechanism is to be said \u003cstrong\u003enon interactive\u003c/strong\u003e if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is \u003cstrong\u003ememory less\u003c/strong\u003e. If not, the mechnism is said to be interactive.\u003c/p\u003e\n\u003cp\u003eIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\u003c/p\u003e\n\u003cp\u003e$Z_i$ is said to be \u003cstrong\u003eα-local-differentially private\u003c/strong\u003e for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\u003c/p\u003e\n\u003cp\u003eAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more \u003cstrong\u003edifficult it is to distinguish\u003c/strong\u003e the distribution of Z conditional on two different X data.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eTheoretical results\u003c/h2\u003e\n\u003ch3 id=\"section-4\"\u003eThe case of multinomial estimation\u003c/h3\u003e\n\u003cp\u003eIn this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$.\nThis problem is a special case of multinomial estimation, where \u003ccode\u003eθ\u003c/code\u003e is now a multidimensional parameter that is amenable to simplex probability. $∆\u003cem\u003ed := (θ ∈ ℝ\u003c/em\u003e+ |∑θ_j = 1)$.\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"Recall\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem :\u003c/strong\u003e Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and\n$$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRecall from standard statistics:\u003c/strong\u003e For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that:\n$$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\u003c/p\u003e\n\u003cp\u003eIn others term, providing α-local-differentially privacy \u003cstrong\u003ecauses a reduction\u003c/strong\u003e in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the \u003cstrong\u003easymptotically rate of convergences remains unchanged\u003c/strong\u003e which is a really good news !\u003c/p\u003e\n\u003ch4 id=\"practical-strategies\"\u003ePractical strategies\u003c/h4\u003e\n\u003cp\u003eThe paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-10\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-11\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-10\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThe \u003cem\u003eintuition\u003c/em\u003e of this section is the following : \u003cstrong\u003eto not allow the statistician to retrieve your personnal data\u003c/strong\u003e in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, \u003cstrong\u003ehe can\u0026rsquo;t distinguish\u003c/strong\u003e if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\u003c/p\u003e\n\u003cp\u003eFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\u003c/p\u003e\n\u003cp\u003e$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$\n$$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\u003c/p\u003e\n\u003cp\u003eSuch a mechanism achieves \u003cem\u003eα-local-differentially privacy\u003c/em\u003e because one can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\u003c/p\u003e\n\u003cp\u003eWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\u003c/p\u003e\n\u003cp\u003e$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\u003c/p\u003e\n\u003cp\u003eThis leads to the natural moment-estimator :\u003c/p\u003e\n\u003cp\u003e$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\u003c/p\u003e\n\u003cp\u003eOne can also show that it verifies :\u003c/p\u003e\n\u003cp\u003e$$E[ ||θ_{hat}- θ||_2] ≤  \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\u003c/p\u003e\n\u003ch5 id=\"section-11\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eInstead of saying the truth with some probability, one may think of \u003cstrong\u003eadding noise\u003c/strong\u003e to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is \u003cstrong\u003enot covered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition:\u003c/strong\u003e A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\u003cbr\u003e\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\u003c/p\u003e\n\u003cp\u003eA visualisation for differents parameters is given below. We can see that Laplace distribution is a \u003cstrong\u003eshaper verson of the gaussian distribution\u003c/strong\u003e :\n\u003cimg\n  src=\"/images/Antoine_Klein/Laplace.png\"\n  alt=\"Laplace\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\u003c/p\u003e\n\u003cp\u003eOne can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\u003c/p\u003e\n\u003cp\u003eThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, \u003cstrong\u003eit verifies α-local-differentially privacy\u003c/strong\u003e. The proposed estimator is the following :\u003cbr\u003e\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\u003c/p\u003e\n\u003cp\u003eOne can show that it is an unbiaised estimator that achieves the optimal rates:\u003cbr\u003e\n$$E[\\hat{Z}] = E[X]$$\u003cbr\u003e\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$\n$$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\u003c/p\u003e\n\u003cp\u003eThis is \u003cstrong\u003eexactly the optimal rates\u003c/strong\u003e, quite outstanding !\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe case of density estimation\u003c/h3\u003e\n\u003cp\u003eOne accurate question that can raise is : \u003cstrong\u003ewhat about others distribution ?\u003c/strong\u003e Is privacy more costly in general cases ? What is the trade-off ?\u003c/p\u003e\n\u003cp\u003eTo answer this question, let\u0026rsquo;s precise the problem.\u003c/p\u003e\n\u003cp\u003eWe want to estimate in a non-parametric way a 1D-density function \u003ccode\u003ef\u003c/code\u003e belonging to one of theses classes :\u003cbr\u003e\n-\u003cstrong\u003eHölder Class (β, L):\u003c/strong\u003e $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\u003cbr\u003e\n-\u003cstrong\u003eSobolev Class:\u003c/strong\u003e $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\u003c/p\u003e\n\u003cp\u003eIn a intuitition way, those two classes express that \u003ccode\u003ef\u003c/code\u003e is \u003cstrong\u003esmooth enough\u003c/strong\u003e to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\u003c/p\u003e\n\u003ch4 id=\"theorem\"\u003eTheorem\u003c/h4\u003e\n\u003ch5 id=\"without-privacy\"\u003eWithout privacy\u003c/h5\u003e\n\u003cp\u003eOne can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\u003c/p\u003e\n\u003cp\u003eIn the case of d-multidimensionnal density \u003ccode\u003ef\u003c/code\u003e, the optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\u003c/p\u003e\n\u003cp\u003eThis illustrates once again the \u003cstrong\u003ecurse of dimensionnality\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"with-privacy\"\u003eWith privacy\u003c/h5\u003e\n\u003cp\u003eLet assume that \u003ccode\u003ef\u003c/code\u003e bellongs to one of the two classes with  \u003ccode\u003eβ\u003c/code\u003e as smoothness parameter.\u003cbr\u003e\nThen, the optimal α-local-differentially private optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\u003c/p\u003e\n\u003cp\u003eOne may observe \u003cstrong\u003etwo pessimistic news\u003c/strong\u003e:\u003cbr\u003e\n-The rate is \u003cstrong\u003eaffected by a factor\u003c/strong\u003e of $\\alpha^2$ as for the multinomial estimation\u003cbr\u003e\n-More damageable: the \u003cstrong\u003erate is slower\u003c/strong\u003e in term of \u003ccode\u003en\u003c/code\u003e unlike the previous problem which make privacy in this case \u003cstrong\u003emore costly\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"practical-strategies-1\"\u003ePractical strategies\u003c/h5\u003e\n\u003cp\u003eEventhough this rate is pessimistic and proves that \u003cstrong\u003eprivacy comes at a cost\u003c/strong\u003e, it remains to illustrates how can we achieves this best but not great rate.\nFor this end, once again, two strategies are possible.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-12\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-13\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-12\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThis is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of \u003ccode\u003eα\u003c/code\u003e.\nAs it is not the most comprehensive and straightforward method, \u003cstrong\u003ewe prefer to dive in depth into the second one; uncovered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"section-13\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eLet assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\u003c/p\u003e\n\u003cp\u003eWe consider the histogramm estimator:\n$$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\u003c/p\u003e\n\u003cp\u003eWe now construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\u003c/p\u003e\n\u003cp\u003eIn an intuitive way, we add a Laplace noise realisation for each bin.\u003c/p\u003e\n\u003cp\u003eThis guarantees α-local-differentially privacy as :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\u003c/p\u003e\n\u003cp\u003eThis leads to the α-local-differentially private estimator :\u003cbr\u003e\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\u003c/p\u003e\n\u003cp\u003eThe biais is the same as the unprivate case as :\u003cbr\u003e\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\u003c/p\u003e\n\u003cp\u003eOne may prove that if f bellongs to the β-Hölder Class:\u003cbr\u003e\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\u003c/p\u003e\n\u003cp\u003eMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total  :\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$\nMinimizing over K (hyperparameters) leads to :  $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"section-6\"\u003eExperiment: Illustration of the Minimax privacy rate\u003c/h2\u003e\n\u003ch3 id=\"section-111\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe aim of this section is to \u003cstrong\u003eprovide illustrations of the theoretical results\u003c/strong\u003e set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\u003c/p\u003e\n\u003cp\u003eFor the sake of \u003cstrong\u003ereproducibility and transparency\u003c/strong\u003e, the source code can be found in the notebook at this: \u003ca href=\"https://github.com/AntoineTSP/responsible-ai-datascience-ipParis.github.io.git\"\u003eGithub link\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"methodology\"\u003eMethodology\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Preparation\u003c/strong\u003e: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMore precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\u003cbr\u003e\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrivacy Metric Calculation\u003c/strong\u003e: We will look at the use case of estimating the mean of a distribution.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: The results will be compared in terms of Mean Square Error (MSE).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003cp\u003eIn terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Private_distribution.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs expected, the greater the desired privacy (low $\\alpha$), \u003cstrong\u003ethe more spread out\u003c/strong\u003e the distribution of observed data.\u003c/p\u003e\n\u003cp\u003eWhen it comes to estimating the true average from private data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Estimated_mean.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis figure illustrates two major points:\u003cbr\u003e\n-The first is that whatever the level of privacy, we have an \u003cstrong\u003eunbiased estimator\u003c/strong\u003e of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\u003cbr\u003e\n-The second is that, unfortunately, the greater the privacy (low alpha), \u003cstrong\u003ethe greater the variance\u003c/strong\u003e of this estimator.\u003c/p\u003e\n\u003cp\u003eWe recall our main theorem demonstrated above \u003ca href=\"#Recall\" style=\"background-color: yellow; padding: 2px 5px; border-radius: 3px;\"\u003ePrevious theorem\u003c/a\u003e :\u003cbr\u003e\n\u003cstrong\u003eTheorem\u003c/strong\u003e : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\u003c/p\u003e\n\u003cp\u003eWe now want to \u003cstrong\u003ecompare the theoretical optimal rate with empirical results\u003c/strong\u003e. To do this, we distinguish two situations:\u003cbr\u003e\n-The first is with \u003cstrong\u003efixed alpha\u003c/strong\u003e, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Minimax_rate_n.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\u003c/p\u003e\n\u003cp\u003e-The second has a \u003cstrong\u003efixed n\u003c/strong\u003e and determines the MSE as a function of alpha. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Minimax_rate_alpha.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eFrom a problem rooted in an \u003cstrong\u003eethical dilemma\u003c/strong\u003e (privacy versus completeness and transparency), we have looked at the \u003cstrong\u003ecost of guaranteeing\u003c/strong\u003e one at the expense of the other, to better sketch out desirable situations.\u003cbr\u003e\nThis has enabled us to develop theoretical results in terms of \u003cstrong\u003eminimax rates\u003c/strong\u003e. There is indeed a \u003cstrong\u003etrade-off\u003c/strong\u003e between these criteria, which is even more costly in the case of non-parametric density estimation.\u003cbr\u003e\nFinally, we have compared these theoretical limits with empirical results, which \u003cstrong\u003econfirm the conformity of the statements\u003c/strong\u003e.\u003cbr\u003e\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following \u003cstrong\u003equiz\u003c/strong\u003e to ensure his or her understanding.\u003c/p\u003e\n\u003ch1 id=\"section-8\"\u003eQuizz\u003c/h1\u003e\n\u003cp\u003eTo test yourself abour privacy:\u003c/p\u003e\n\u003cform id=\"quiz-form\" class=\"quiz-form\"\u003e\n    \u003cdiv class=\"quiz-question\"\u003e\n        \u003cp\u003eWhat is privacy?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"1\"\u003e\n                Avoid asking questions that can raise private information\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"2\"\u003e\n                A mechanism that prevents other agent to retrieve personnal information in your answer\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"3\"\u003e\n                An ethical-washing trend\n            \u003c/label\u003e\n        \u003c/div\u003e\n        \u003cp\u003eWhich situation is α-local-differentially privacy?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"1\"\u003e\n                sup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α)\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"2\"\u003e\n                You tell the truth half the time, you lie otherwise.\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"3\"\u003e\n                Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1)\n            \u003c/label\u003e\n        \u003c/div\u003e\n        \u003cp\u003eWhat is the privacy cost in term of optimal rate ?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"1\"\u003e\n                Multinomial estimation: A factor α^2/d\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"2\"\u003e\n                Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2))\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"3\"\u003e\n                We loose nothing, that's the surprising finding of the paper\n            \u003c/label\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c!-- Add more quiz questions as needed --\u003e\n    \u003cbutton type=\"submit\" class=\"quiz-submit\"\u003eSubmit\u003c/button\u003e\n\u003c/form\u003e\n\u003cdiv id=\"quiz-results\" class=\"quiz-results\"\u003e\u003c/div\u003e\n\u003cscript\u003e\n    // Define quiz questions and correct answers\n    const quizQuestions = [\n        {\n            question: \"What is privacy?\",\n            answer: \"2\"\n        },\n        //Add more quiz questions as needed\n        {\n            question: \"Which situation is α-local-differentially privacy?\",\n            answer: \"3\"\n        },\n        //Add more quiz questions as needed\n        {\n            question: \"What is the privacy cost in term of optimal rate ?\",\n            answer: \"1\"\n        }\n    ];\n\n    // Handle form submission\n    document.getElementById('quiz-form').addEventListener('submit', function(event) {\n        event.preventDefault();\n\n        // Calculate quiz score\n        let score = 0;\n        quizQuestions.forEach(question =\u003e {\n            const selectedAnswer = document.querySelector(`input[name=\"question${quizQuestions.indexOf(question) + 1}\"]:checked`);\n            if (selectedAnswer) {\n                if (selectedAnswer.value.toLowerCase() === question.answer) {\n                    score++;\n                    selectedAnswer.parentElement.classList.add('correct');\n                } else {\n                    selectedAnswer.parentElement.classList.add('incorrect');\n                }\n            }\n        });\n\n        // Display quiz results\n        const quizResults = document.getElementById('quiz-results');\n        quizResults.innerHTML = `\u003cp\u003eYou scored ${score} out of ${quizQuestions.length}.\u003c/p\u003e`;\n    });\n\u003c/script\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eWarner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830.\u003c/li\u003e\n\u003cli\u003eJohn C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013)\u003c/li\u003e\n\u003cli\u003eDwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407.\u003c/li\u003e\n\u003cli\u003eNarayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cscript\u003e\nfunction highlight(text) {\n  var inputText = document.getElementById(\"markdown-content\");\n  var innerHTML = inputText.innerHTML;\n  var index = innerHTML.indexOf(text);\n  if (index \u003e= 0) {\n    innerHTML = innerHTML.substring(0,index) + \"\u003cspan class='highlight'\u003e\" + innerHTML.substring(index,index+text.length) + \"\u003c/span\u003e\" + innerHTML.substring(index + text.length);\n    inputText.innerHTML = innerHTML;\n  }\n}\nhighlight(\"Estimating Privacy in Data Science\");\n\n\u003c/script\u003e\n\u003chr\u003e\n\u003cscript\u003e\n    function displayInput() {\n        var inputValue = document.getElementById(\"inputField\").value;\n        document.getElementById(\"output\").innerText = \"You typed: \" + inputValue;\n    }\n\u003c/script\u003e\n\u003cstyle\u003e\n.highlight {\n  background-color: red;\n}\n.highlight-on-hover:hover {\n        background-color: yellow;\n    }\n/* Quiz form styles */\n.quiz-form {\n        max-width: 500px;\n        margin: auto;\n        padding: 20px;\n        border: 1px solid #ccc;\n        border-radius: 5px;\n        background-color: #f9f9f9;\n}\n\n.quiz-question {\n        margin-bottom: 20px;\n}\n\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n}\n\n.quiz-submit {\n        background-color: #4caf50;\n        color: white;\n        padding: 10px 20px;\n        border: none;\n        border-radius: 5px;\n        cursor: pointer;\n}\n\n.quiz-submit:hover {\n        background-color: #45a049;\n}\n\n/* Quiz results styles */\n.quiz-results {\n        margin-top: 20px;\n        font-weight: bold;\n}\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n    }\n.quiz-options label.correct {\n        color: green;\n}\n.quiz-options label.incorrect {\n        color: red;\n}\na[name]:hover {\n        background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */\n        text-decoration: none; /* Optionally remove underline on hover */\n}\n\u003c/style\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/statistical_minimax_rates_under_privacy/","date_published":"31016-31-09T122:3131:00+01:00","date_modified":"31016-31-09T122:3131:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"64ca00e483d62f1f64025e3844d00befbed90629","title":"Measuring the Transferability of Pre-trained Models: a link with Neural Collapse Distances on Target Datasets","summary":"","content_text":" Authors : Marion Chadal and Julie Massé\nThis blog post discusses the paper \u0026ldquo;How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability\u0026rdquo; [1]. It provides an explanation of it so that you can understand the usefulness of measuring transferability, and a reproduction of the authors\u0026rsquo; experiment so that you can better visualize their methodology.\nPre-trained models and fine-tuning Pre-trained models are currently one of the most active fields in Machine Learning. They can be found in a wide range of applications, from image recognition and natural language processing to autonomous driving and medical diagnosis. These models are \u0026ldquo;pre-trained\u0026rdquo; on massive datasets, most of the time encompassing millions of examples across diverse domains. The training process leverages Deep Learning algorithms and can take weeks or even months, utilizing powerful computing resources to iteratively adjust the model\u0026rsquo;s parameters until it achieves high accuracy on the training data.\nThe first purpose of pre-training is to enable the model to learn a broad understanding of the world, capturing intricate patterns, relationships, and features that are not easily discernible. This extensive learning phase allows the model to develop a deep amount of knowledge, which it can then apply to more specific tasks through a process known as fine-tuning.\nWhat is fine-tuning? It consists in adapting a general-purpose model to perform well on a specific task. This adaptation allows the model to fine-tune its learned features to better align with the nuances of the new task, enhancing its accuracy and performance. Whether it\u0026rsquo;s identifying specific types of objects in images, understanding the subtleties of natural language in a particular context, or diagnosing medical conditions from scans, fine-tuning enables pre-trained models to become specialized tools capable of tackling a wide range of applications.\nFine-tuning begins with a pre-trained model—a model that has already learned a vast array of features and patterns from a comprehensive dataset, often spanning millions of examples. This model, equipped with a deep understanding of various data representations, serves as a robust starting point. The fine-tuning process then adapts this model to a specific task by continuing the training process on a smaller, task-specific dataset. This additional training phase is typically shorter and requires significantly fewer data and computational resources than training a model from scratch, as the model already possesses a foundational knowledge base.\nOne of the key aspects of fine-tuning is its efficiency in data utilization. Since the model has already learned general features and patterns, the fine-tuning process can achieve high performance with relatively small datasets. This characteristic is particularly valuable in domains where collecting large amounts of labeled data is challenging or expensive.\nTraining from scratch is the complete opposite of fine-tuned pre-trained models, as it involves starting with randomly initialized parameters and requires a substantial dataset specific to the task at hand, along with considerable computational resources and time to achieve comparable performance to a fine-tuned pre-trained model. While training from scratch can be beneficial in certain scenarios where highly specialized knowledge is required or when a suitable pre-trained model is not available, the efficiency and effectiveness of leveraging pre-trained models are nowadays undeniable.\nTransferability Transferability caracterizes the ability of pre-trained models to run on downstream tasks without performing fine-tuning, but achieving comparable results. Models that exhibit high transferability are those that have learned generalizable features during pre-training—features that are not overly specific to the training data but that capture universal patterns or structures present across different datasets and domains.\nBeside, transferability arises as an attempt of improvement in scalable AI, as it enables researchers and practitioners to build upon existing knowledge without reinventing the wheel for every new task. This characteristic is especially crucial in our current case where data is abundant, but labeled data is scarce or expensive to obtain. Transferable models can leverage unlabeled data from similar domains, or even entirely different domains, to achieve impressive results with minimal effort.\nMoreover, the pursuit of enhancing transferability has led to innovations in model architecture, training strategies, and domain adaptation techniques. Few-shot learning for instance, where models learn from a very small amount of labeled data, and zero-shot learning, where models apply their knowledge to tasks they have not explicitly been trained on.\nThe concept of transferability also intersects with ethical AI development, as it encourages the use of more generalizable models that can perform equitably across diverse datasets and demographics, reducing the risk of biased or unfair outcomes.\nWhy measuring transferability? Fine-tuning pre-trained models works as follows. First, you pick a downstream task, for which you have at your disposal several pre-trained models candidates. You want to compare their performances to pick the best one on test set, with the optimal fine-tuning configuration. Then, you have to fine-tune each of them. Even if the dataset to train on is smaller, thanks to fine-tuning, you have to repeat it for all your models candidates, and one does not want that, as it can quickly become computationnally expensive.\nTransferability estimation arises as a solution to anticipate and avoid unnecessary fine-tuning, by ranking the performances of pre-trained models on a downstream task without any fine-tuning. Having a benchmark on the pre-trained models\u0026rsquo; transferability would allow you to pick the relevant ones for your own downstream task.\nThis measure is also in line with frugality in AI, which means using limited resources at every step of the Machine Learning lifecycle, while maintaining an acceptable accuracy. This frugality is especially relevant for small and medium-sized enterprises (SMEs) or startups, which may not have the vast computational resources that larger corporations possess. Transferable models democratize access to advanced AI capabilities, enabling these smaller entities to innovate and compete effectively. Frugality in AI also speaks to the broader goal of creating models that are not only powerful but also lean and efficient. Models with high transferability can achieve excellent performance across multiple tasks using significantly less data and fewer computational resources. This efficiency reduces the carbon footprint of training models and makes AI more accessible to a wider range of users and applications.\nNeural Collapse Neural Collapse happens when training beyond 0 training error, i.e training error is at 0 while pushing training loss approaching 0 even further down. Imagine training a deep neural network on a dataset for a classification task. As the training process nears its end—particularly when the model is trained to a point of perfect or near-perfect classification accuracy on the training data. Intuitively, one would expect a highly overfitted and noisy model. Instead, a remarkable simplification occurs in the way the model represents the data, as it was shown in [2]. This training approach offers better generalization performance, better robustness, and better interpretability.\nNeural Collapse is characterized by three distinct proxies:\nWithin-Class Variability Collapse: for any given class, the feature vectors of all samples converge to a singular point or a tightly compact cluster in the high-dimensional feature space. This collapsing effect reduces the within-class variance to near zero, meaning that all samples of a class are represented almost identically from the model\u0026rsquo;s perspective ; Simplex Encoded Label Interpolation (SELI) geometry: measures the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. The higher the rank, the smaller the difference, the closer to Neural Collapse ; Nearest Center Classifier: ensures that the means of the collapsed points for different classes are maximally separated in the feature space. Let\u0026rsquo;s look at this visual example of neural collapse :\nWhere :\nThe Green Balls represent the coordinates of a simplex equiangular tight frame (ETF). The Red Lines represent the Final Layer Classifier. The direction of the sticks indicates the orientation of its decision boundaries, while the ball-end represents the centroid in the feature space used for classification. The Blue Lines represent the class means of the activations in the last hidden layer. The sticks show the variance around these means. The Small Blue Balls represent the last hidden layer activations. It shows how data points from each class are distributed around the class means, forming tight clusters. Initially these elements are all scattered, but as training progresses and neuronal collapse occurs, at each epoch, they move and converged gradually as shown in the GIF.\nWhy choosing Neural Collapse proxies? Let\u0026rsquo;s go back to imagining you have to perform a downstream task, and to do so you have to measure transferability between pre-trained models candidates. The three Neural Collapse proxies were previously defined, but we did not mention yet the three model\u0026rsquo;s aspects that are crucial to evaluate when choosing one:\nGeneralization: through Within-Class Variability Collapse, we gain insight into a model\u0026rsquo;s ability to generalize ; Interpretability: the convergence toward SELI geometry not only enhances the model\u0026rsquo;s interpretability but also its alignment with optimal data representation structures. This alignment signifies a model\u0026rsquo;s capacity to distill and encode information in a way that mirrors the inherent structure of the data itself ; Robustness: the Nearest Center Classifier proxy underscores a model\u0026rsquo;s robustness. By ensuring that class means are well-separated, the model demonstrates resilience against noise and variability in data. Authors in [3] demonstrate both theoretically and empirically that Neural Collapse not only generalizes to new samples from the same classes seen during training but also, and more crucially, to entirely new classes. Also, a more recent research [4] proposes a fine-tuning method based on Neural Collapse that achieves even better performance while reducing fine-tuning parameters by at least 70% !\nThe NCTI Given these promising results, the authors developed a transferability estimation metric : the Neural Collapse Transferability Index (NCTI). This metric measures the proximity between the current state of a pre-trained model and its final fine-tuning stage on target, using the three neural collapse proxies defined above : Within-Class Variability Collapse, SELI geometry and Nearest Center Classifier. For each of them, a score is established : $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$. These three scores are then grouped together using normalization to prevent one score from dominating due to different scales. The final transferability estimation metric is obtained by adding the normalized scores:\n$$ S^m_{total} = S^m_{vc}(H^m) + S^m_{seli}(H^m) + S^{m}_{ncc}(H^m) $$\nWhere $H_m$ is the feature extracted by the $m$-th pre-trained model (after ranking a set of $M$ pre-trained models).\nThe higher the score $S^m_{total}$, the better the transferability of the model for target dataset.\nLet\u0026rsquo;s detail the scores $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$:\nWithin-Class Variability Collapse The authors noticed that larger singular values indicate higher within-class variability because the features within the class exhibit significant variation from the mean, which is desirable for effective feature representation. But since singular value decomposition (SVD) is computationally expensive for large matrices, the nuclear norm which calculates the sum of singular values in a less expensive way was used. Additionally, as feature spaces are high dimensionnal, noise may appear and affect the calculation of variability. Therefore, instead of using the feature matrix $H^m_c$ directly, the classwise logits $Z^m_c$ are substituted to calculate the feature variability.\nThus, the score $S_{vc}$ is calculated as follow :\n$$ S^m_{vc}(H^m) = - \\sum_{c=1}^{C} ||Z^m_c||_* $$\nWhere $Z^m_c$ denotes the logits of the $c$-th class extracted by the $m$-th model.\nThe higher the score $S_{vc}$, the higher the within-class variability, which means that the pre-trained model is closer to the final fine-tuning stage.\nSELI geometry SELI geometry is a concept proposed in [6] as a generalized geometric structure version of the simplex equiangular tight frame (ETF). ETF is defined in the context of the phenomenon of neuronal collapse, but it is limited to balanced datasets. In contrast, SELI extends this concept to both balanced and unbalanced datasets. Difference between the two geometries is shown in the figure below :\nEmbeddings $H$ (in blue) and classifiers $W$ (in red) follow the SELI geometry if :\n$$ W^T W \\alpha V \\Lambda V^T, H^T H \\alpha U \\Lambda U^T \\text{and} W^T H \\alpha \\hat{Z} $$\nWhere $\\hat{Z} = V \\Lambda U^T$ is the SEL matrix [6]. $U$ and $V$ denote the left and right singular vector matrix of $\\hat{Z}$. $\\Lambda$ represents the diagonal singular value matrix.\nA method to assess the SELI geometry structure involves computing the difference between the logits $Z^m$ extracted from the pre-trained model and the optimal logits $\\hat{Z}$. However, obtaining $Z^m$ directly without fine-tuning on the target dataset is time-consuming. Therefore, features $H^m$ of the model are extracted and their difference is measured to form the SELI structure. The complexity of achieving the optimal logits $\\hat{Z}$ through features $H_m$ is approximated via the nuclear norm.\nThus, the score $S^m_{seli}$ is calculated as :\n$$S^m_{seli}(H^m) = ||H^m||_*$$\nThe higher the score $S^m_{seli}$ the higher the rank of the feature matrix $H_m$, making $Z$ closer to a full rank matrix.\nNearest Center Classifier First, the posterior probability $P(y = c|h)$ for each class $c$ is calculated using Bayes\u0026rsquo; Rule:\n$$ \\log P(y = c|h) = \\frac{1}{2}(h_i - \\mu_c)^T \\Sigma (h_j - \\mu_c) + \\log P(y = c) $$\nWhere:\n$\\mu_c$ is the mean vector for class $c$. $\\Sigma$ is the covariance matrix. $P(y = c)$ is the prior probability of class $c$. $h$ is the feature vector extracted by the pre-trained model. Next, the softmax function is applied to obtain the normalized posterior probability $z^m_{i,c}$ for each class $c$ of the $i$-th sample:\n$$ z^m_{i,c} = \\frac{\\exp(\\log P(y = c|h^m_i))}{\\Sigma ^C_{k=1} \\exp(\\log P(y = k|h^m_i))} $$\nWhere:\n$C$ is the number of classes. $h^m_i$ is the feature vector of the $i$-th sample extracted by the m-th pre-trained model. Finally, the score $S^m_{ncc}$ is computed as the average of the dot product of the normalized posterior probabilities $z^m_i$ and the ground truth labels $y_i$ for all samples:\n$$ S^m_{ncc}(H^m) = \\frac{1}{N} \\Sigma ^N_{i=1} z^m_i \\cdot y_i $$\nWhere:\n$N$ is the number of samples. $y_i$ is the ground truth label of the $i$-th sample (in one-hot encoding). The higher the score $S^{m}_{ncc}(H^m)$, the smaller the deviation to the nearest optimal centroid classifier and therefore the greater the transferability to the target dataset.\nNumerical Experiment To reproduce their experiment, the authors\u0026rsquo; code available on a Github repository was used. A first encountered issue was the required torch and torchvision versions, which are quite old, and thus not always available to install, which was the case here. Fortunately, the most recent versions were compatible with the code. A requirements.txt file would have been welcome.\nA second issue is that there are remaining personal paths in some scripts, which should be replaced by downloading paths to PyTorch source models. As a consequence, the loading method from torch should also be replaced.\nOther issues considering the datasets loading remained unsolved.\nAfter these modifications, it is possible to run the authors\u0026rsquo; experiments on the CIFAR10 dataset for the group of supervised pre-trained models. Consisting of 60 000 32x32 colour images in 10 classes, this dataset is broadly used in benchmarks for image classification. 12 pre-trained models were ran on CIFAR10 to establish a ranking based on their performances in terms of NCTI available below.\nModel NCTI Score ResNet152 2.0 ResNet101 1.799 DenseNet201 1.434 DenseNet169 1.146 ResNet34 0.757 ResNet50 0.709 DenseNet121 0.655 MnasNet1_0 0.031 GoogleNet -0.251 MobileNetV2 -0.444 InceptionV3 -0.732 Results show that the deepest architectures offer the best NCTI scores. The depth of a network is closely related to its ability to learn and represent complex features and patterns from the training data, which contributes to a model\u0026rsquo;s superior transferability. The different performances between ResNet and DenseNet could be attributed to the way DenseNet connects each layer to every other layer in a feed-forward fashion, which, while efficient in parameter use and reducing overfitting, may not capture as complex a feature hierarchy as ResNet. Models like MnasNet, MobileNetV2, and InceptionV3, designed for efficiency and speed with a compromise on depth, understandably score lower in transferability, as reflected by their NCTI scores.\nThen, we evaluated the transferability of the supervised pre-trained models, in terms of weighted Kendall\u0026rsquo; τ, and obtained the exact same result as the one presented in the paper: 0.843.\nIt was not possible for us to run the experiment on the group of self-supervised pre-trained models as the authors\u0026rsquo; code included personal paths, and we were not able to find them online.\nA Github repository with all the necessary modifications from the original code is at your disposal here.\nWhat about source features? Through extensive testing, authors have identified that two specific attributes related to neural collapse, observed in the source features, consistently predicted the model\u0026rsquo;s performance on new tasks. These attributes were the diversity within data categories and the compactness of category representations. Remarkably, models showing higher within-category diversity and more compact category representations in their source features tended to adapt better to new tasks. On the other hand, SELI did not consistently correlate with transferability.\nChallenges Authors did experiments on the effectiveness of each individual component in NCTI. They used the three terms individually and removed them one at a time from the full system, and it turned out that for supervised learning, the NCTI without NCC achieved the best weighted Kendall\u0026rsquo; τ. Instead of having normalized the three NCTI components equally, it could have been interesting to tune hyperparameters. Moreover, the current implementation and validation of NCTI are confined to image classification tasks, suggesting its applicability may be limited to similar types of problems. Future work could extend the method\u0026rsquo;s applicability to a broader range of tasks beyond classification, such as detection or segmentation​​. Pre-trained language models could also be considered to measure their transferability based on Neural Collapse. For example, the Fair Collapse (FaCe) method [7] considers both Computer Vision and Natural Language Processing tasks, using different proxies of Neural Collapse than NCTI, and producing a slightly less good τ on the CIFAR-10 dataset (0.81).\nTakeaways Key points to remember are :\nCalculating model transferability and choosing the optimal pre-trained model is important for reasons of computational cost, environmental impact, and overall performance.\nThe authors have developed a new metric, the Neural Collapse informed Transferability Index (NCTI), which is based on the concept of neural collapse and measures the gap between the current feature geometry and the geometry at the terminal stage after hypothetical fine-tuning on the downstream task.\nThe NCTI metric integrates three aspects equally: SELI geometry, within-class variability, and nearest center classifier.\nThis method is light to compute, enabling rapid evaluation of model transferability.\nEmpirical results demonstrate that the ranking of model transferability has a very strong correlation with the ground truth ranking and compares with state-of-the-art methods, highlighting its effectiveness in selecting pre-trained models for specific tasks.\nIn summary, the development of metrics such as NCTI is crucial for optimizing the use of pre-trained models, considering both performance and associated costs in real-world applications.\nReferences 1. Z. Wang Y.Luo, L.Zheng, Z.Huang, M.Baktashmotlagh (2023), How far pre-trained models are from neural collapse on the target dataset informs their transferabilityWang, ICCV.\n2. V. Papyan,1 , X. Y. Hanb,1 , and D.L. Donoho (2020), Prevalence of neural collapse during the terminal phase of deep learning training, National Academy of Sciences.\n3. Galanti, T., György, A., \u0026amp; Hutter, M. (2021). On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121.\n4. Li, X., Liu, S., Zhou, J., Lu, X., Fernandez-Granda, C., Zhu, Z., \u0026amp; Qu, Q. (2022). Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206.\n5. Vignesh Kothapalli, (2023). Neural Collapse: A Review on Modelling Principles and Generalization. arXiv preprint arXiv:2206.04041.\n6. Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. (2022). Imbalance trouble: Revisiting neural-collapse geometry. arXiv preprint arXiv:2208.05512.\n7. Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang. (2023). Unleashing the power of neural collapse for transferability estimation. arXiv preprint arXiv:2310.05754v1.\nStart writing here !\n","content_html":"\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : Marion Chadal and Julie Massé\u003c/p\u003e\n\u003cp\u003eThis blog post discusses the paper \u0026ldquo;How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability\u0026rdquo; \u003ca href=\"#ref1\"\u003e[1]\u003c/a\u003e. It provides an explanation of it so that you can understand the usefulness of measuring transferability, and a reproduction of the authors\u0026rsquo; experiment so that you can better visualize their methodology.\u003c/p\u003e\n\u003ch1 id=\"pre-trained-models-and-fine-tuning\"\u003ePre-trained models and fine-tuning\u003c/h1\u003e\n\u003cp\u003ePre-trained models are currently one of the most active fields in Machine Learning. They can be found in a wide range of applications, from image recognition and natural language processing to autonomous driving and medical diagnosis. These models are \u0026ldquo;pre-trained\u0026rdquo; on massive datasets, most of the time encompassing millions of examples across diverse domains. The training process leverages Deep Learning algorithms and can take weeks or even months, utilizing powerful computing resources to iteratively adjust the model\u0026rsquo;s parameters until it achieves high accuracy on the training data.\u003c/p\u003e\n\u003cp\u003eThe first purpose of pre-training is to enable the model to learn a broad understanding of the world, capturing intricate patterns, relationships, and features that are not easily discernible. This extensive learning phase allows the model to develop a deep amount of knowledge, which it can then apply to more specific tasks through a process known as fine-tuning.\u003c/p\u003e\n\u003cp\u003eWhat is \u003cstrong\u003efine-tuning\u003c/strong\u003e? It consists in adapting a general-purpose model to perform well on a specific task. This adaptation allows the model to fine-tune its learned features to better align with the nuances of the new task, enhancing its accuracy and performance. Whether it\u0026rsquo;s identifying specific types of objects in images, understanding the subtleties of natural language in a particular context, or diagnosing medical conditions from scans, fine-tuning enables pre-trained models to become specialized tools capable of tackling a wide range of applications.\u003c/p\u003e\n\u003cp\u003eFine-tuning begins with a pre-trained model—a model that has already learned a vast array of features and patterns from a comprehensive dataset, often spanning millions of examples. This model, equipped with a deep understanding of various data representations, serves as a robust starting point. The fine-tuning process then adapts this model to a specific task by continuing the training process on a smaller, task-specific dataset. This additional training phase is typically shorter and requires significantly fewer data and computational resources than training a model from scratch, as the model already possesses a foundational knowledge base.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"/images/ChadalMasse/schema.png\" width=\"600\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eOne of the key aspects of fine-tuning is its efficiency in data utilization. Since the model has already learned general features and patterns, the fine-tuning process can achieve high performance with relatively small datasets. This characteristic is particularly valuable in domains where collecting large amounts of labeled data is challenging or expensive.\u003c/p\u003e\n\u003cp\u003eTraining from scratch is the complete opposite of fine-tuned pre-trained models, as it involves starting with randomly initialized parameters and requires a substantial dataset specific to the task at hand, along with considerable computational resources and time to achieve comparable performance to a fine-tuned pre-trained model. While training from scratch can be beneficial in certain scenarios where highly specialized knowledge is required or when a suitable pre-trained model is not available, the efficiency and effectiveness of leveraging pre-trained models are nowadays undeniable.\u003c/p\u003e\n\u003ch1 id=\"transferability\"\u003eTransferability\u003c/h1\u003e\n\u003cp\u003eTransferability caracterizes the \u003cem\u003eability of pre-trained models to run on downstream tasks without performing fine-tuning, but achieving comparable results\u003c/em\u003e. Models that exhibit \u003cstrong\u003ehigh transferability\u003c/strong\u003e are those that have learned \u003cstrong\u003egeneralizable features\u003c/strong\u003e during pre-training—features that are not overly specific to the training data but that capture universal patterns or structures present across different datasets and domains.\u003c/p\u003e\n\u003cp\u003eBeside, transferability arises as an attempt of improvement in \u003cstrong\u003escalable AI\u003c/strong\u003e, as it enables researchers and practitioners to build upon existing knowledge without reinventing the wheel for every new task. This characteristic is especially crucial in our current case where data is abundant, but labeled data is scarce or expensive to obtain. Transferable models can leverage unlabeled data from similar domains, or even entirely different domains, to achieve impressive results with minimal effort.\u003c/p\u003e\n\u003cp\u003eMoreover, the pursuit of enhancing transferability has led to innovations in model architecture, training strategies, and domain adaptation techniques. \u003cstrong\u003eFew-shot learning\u003c/strong\u003e for instance, where models learn from a very small amount of labeled data, and zero-shot learning, where models apply their knowledge to tasks they have not explicitly been trained on.\u003c/p\u003e\n\u003cp\u003eThe concept of transferability also intersects with \u003cstrong\u003eethical AI\u003c/strong\u003e development, as it encourages the use of more generalizable models that can perform equitably across diverse datasets and demographics, reducing the risk of biased or unfair outcomes.\u003c/p\u003e\n\u003ch1 id=\"why-measuring-transferability\"\u003eWhy measuring transferability?\u003c/h1\u003e\n\u003cp\u003eFine-tuning pre-trained models works as follows. First, you \u003cstrong\u003epick a downstream task\u003c/strong\u003e, for which you have at your disposal several pre-trained models candidates. You want to compare their performances to pick the best one on test set, with the \u003cstrong\u003eoptimal fine-tuning configuration\u003c/strong\u003e. Then, you have to fine-tune each of them. Even if the dataset to train on is smaller, thanks to fine-tuning, you have to repeat it for all your models candidates, and one does not want that, as it can quickly become \u003cstrong\u003ecomputationnally expensive\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eTransferability estimation arises as a solution to anticipate and avoid unnecessary fine-tuning, by \u003cstrong\u003eranking the performances of pre-trained models\u003c/strong\u003e on a downstream task without any fine-tuning. Having a \u003cstrong\u003ebenchmark on the pre-trained models\u0026rsquo; transferability\u003c/strong\u003e would allow you to pick the relevant ones for your own downstream task.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"/images/ChadalMasse/machine-learning-life-cycle.png\" width=\"250\" height=\"250\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eThis measure is also in line with \u003cstrong\u003efrugality in AI\u003c/strong\u003e, which means using limited resources at every step of the Machine Learning lifecycle, while maintaining an acceptable accuracy. This frugality is especially relevant for small and medium-sized enterprises (SMEs) or startups, which may not have the vast computational resources that larger corporations possess. Transferable models democratize access to advanced AI capabilities, enabling these smaller entities to innovate and compete effectively. Frugality in AI also speaks to the broader goal of creating models that are not only powerful but also lean and efficient. Models with high transferability can achieve excellent performance across multiple tasks using significantly less data and fewer computational resources. This efficiency reduces the carbon footprint of training models and makes AI more accessible to a wider range of users and applications.\u003c/p\u003e\n\u003ch1 id=\"neural-collapse\"\u003eNeural Collapse\u003c/h1\u003e\n\u003cp\u003eNeural Collapse happens when training beyond 0 training error, i.e training error is at 0 while pushing training loss approaching 0 even further down. Imagine training a deep neural network on a dataset for a classification task. As the training process nears its end—particularly when the model is trained to a point of perfect or near-perfect classification accuracy on the training data. Intuitively, one would expect a highly overfitted and noisy model. Instead, a remarkable simplification occurs in the way the model represents the data, as it was shown in \u003ca href=\"#ref2\"\u003e[2]\u003c/a\u003e. This training approach offers better \u003cstrong\u003egeneralization\u003c/strong\u003e performance, better \u003cstrong\u003erobustness\u003c/strong\u003e, and better \u003cstrong\u003einterpretability\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eNeural Collapse is characterized by three distinct proxies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWithin-Class Variability Collapse:\u003c/strong\u003e for any given class, the feature vectors of all samples converge to a singular point or a tightly compact cluster in the high-dimensional feature space. This collapsing effect reduces the within-class variance to near zero,  meaning that all samples of a class are represented almost identically from the model\u0026rsquo;s perspective ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSimplex Encoded Label Interpolation (SELI) geometry:\u003c/strong\u003e measures the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. The higher the rank, the smaller the difference, the closer to Neural Collapse ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNearest Center Classifier:\u003c/strong\u003e ensures that the means of the collapsed points for different classes are maximally separated in the feature space.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet\u0026rsquo;s look at this visual example of neural collapse :\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"/images/ChadalMasse/neural_collapse.gif\" width=\"250\" height=\"250\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eWhere :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003eGreen Balls\u003c/strong\u003e  represent the coordinates of a simplex equiangular tight frame (ETF).\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eRed Lines\u003c/strong\u003e represent the Final Layer Classifier. The direction of the sticks indicates the orientation of its decision boundaries, while the ball-end represents the centroid in the feature space used for classification.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eBlue Lines\u003c/strong\u003e represent the class means of the activations in the last hidden layer. The sticks show the variance around these means.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eSmall Blue Balls\u003c/strong\u003e represent the last hidden layer activations. It shows how data points from each class are distributed around the class means, forming tight clusters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInitially these elements are all scattered, but as training progresses and neuronal collapse occurs, at each epoch, they move and converged gradually as shown in the GIF.\u003c/p\u003e\n\u003ch1 id=\"why-choosing-neural-collapse-proxies\"\u003eWhy choosing Neural Collapse proxies?\u003c/h1\u003e\n\u003cp\u003eLet\u0026rsquo;s go back to imagining you have to perform a downstream task, and to do so you have to measure transferability between pre-trained models candidates. The three Neural Collapse proxies were previously defined, but we did not mention yet the three model\u0026rsquo;s aspects that are crucial to evaluate when choosing one:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization:\u003c/strong\u003e through Within-Class Variability Collapse, we gain insight into a model\u0026rsquo;s ability to generalize ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInterpretability:\u003c/strong\u003e the convergence toward SELI geometry not only enhances the model\u0026rsquo;s interpretability but also its alignment with optimal data representation structures. This alignment signifies a model\u0026rsquo;s capacity to distill and encode information in a way that mirrors the inherent structure of the data itself ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobustness:\u003c/strong\u003e the Nearest Center Classifier proxy underscores a model\u0026rsquo;s robustness. By ensuring that class means are well-separated, the model demonstrates resilience against noise and variability in data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAuthors in \u003ca href=\"#ref3\"\u003e[3]\u003c/a\u003e demonstrate \u003cstrong\u003eboth theoretically and empirically\u003c/strong\u003e that Neural Collapse not only generalizes to new samples from the same classes seen during training but also, and more crucially, to entirely new classes. Also, a more recent research \u003ca href=\"#ref4\"\u003e[4]\u003c/a\u003e proposes a fine-tuning method based on Neural Collapse that achieves even better performance while reducing fine-tuning parameters by at least \u003cstrong\u003e70%\u003c/strong\u003e !\u003c/p\u003e\n\u003ch1 id=\"the-ncti\"\u003eThe NCTI\u003c/h1\u003e\n\u003cp\u003eGiven these promising results, the authors developed a transferability estimation metric : the Neural Collapse Transferability Index (NCTI). This metric measures the proximity between the current state of a pre-trained model and its final fine-tuning stage on target, using the three neural collapse proxies defined above : Within-Class Variability Collapse, SELI geometry and Nearest Center Classifier. For each of them, a score is established :  $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$. These three scores are then grouped together using normalization to prevent one score from dominating due to different scales. The final transferability estimation metric is obtained by adding the normalized scores:\u003c/p\u003e\n\u003cp\u003e$$ S^m_{total} = S^m_{vc}(H^m) + S^m_{seli}(H^m) + S^{m}_{ncc}(H^m) $$\u003c/p\u003e\n\u003cp\u003eWhere $H_m$ is the feature extracted by the $m$-th pre-trained model (after ranking a set of $M$ pre-trained models).\u003c/p\u003e\n\u003cp\u003eThe higher the score $S^m_{total}$, the better the transferability of the model for target dataset.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s detail the scores $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$:\u003c/p\u003e\n\u003ch3 id=\"within-class-variability-collapse\"\u003eWithin-Class Variability Collapse\u003c/h3\u003e\n\u003cp\u003eThe authors noticed that larger singular values indicate higher within-class variability because the features within the class exhibit significant variation from the mean, which is desirable for effective feature representation. But since singular value decomposition (SVD) is computationally expensive for large matrices, the nuclear norm which calculates the sum of singular values in a less expensive way was used. Additionally, as feature spaces are high dimensionnal, noise may appear and affect the calculation of variability. Therefore, instead of using the feature matrix $H^m_c$ directly, the classwise logits $Z^m_c$ are substituted to calculate the feature variability.\u003c/p\u003e\n\u003cp\u003eThus, the score $S_{vc}$ is calculated as follow :\u003c/p\u003e\n\u003cp\u003e$$ S^m_{vc}(H^m) = - \\sum_{c=1}^{C} ||Z^m_c||_* $$\u003c/p\u003e\n\u003cp\u003eWhere $Z^m_c$ denotes the logits of the $c$-th class extracted by the $m$-th model.\u003c/p\u003e\n\u003cp\u003eThe higher the score $S_{vc}$, the higher the within-class variability, which means that the pre-trained model is closer to the final fine-tuning stage.\u003c/p\u003e\n\u003ch3 id=\"seli-geometry\"\u003eSELI geometry\u003c/h3\u003e\n\u003cp\u003eSELI geometry is a concept proposed in \u003ca href=\"#ref6\"\u003e[6]\u003c/a\u003e as a generalized geometric structure version of the simplex equiangular tight frame (ETF). ETF is defined in the context of the phenomenon of neuronal collapse, but it is limited to balanced datasets. In contrast, SELI extends this concept to both balanced and unbalanced datasets. Difference between the two geometries is shown in the figure below :\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center; align-items: center;\"\u003e\n    \u003cimg src=\"/images/ChadalMasse/geometry.png\" alt=\"Image 1\" style=\"width: 49%; max-width: 100%; height: auto;\"\u003e\n    \u003cimg src=\"/images/ChadalMasse/neural_network.png\" alt=\"Image 2\" style=\"width: 49%; max-width: 100%; height: auto;\"\u003e\n\u003c/div\u003e\n\u003cp\u003eEmbeddings $H$ (in blue) and classifiers $W$ (in red) follow the SELI geometry if :\u003c/p\u003e\n\u003cp\u003e$$ W^T W \\alpha V \\Lambda V^T,  H^T H \\alpha U \\Lambda U^T \\text{and} W^T H \\alpha \\hat{Z} $$\u003c/p\u003e\n\u003cp\u003eWhere $\\hat{Z} = V \\Lambda U^T$ is the SEL matrix \u003ca href=\"#ref6\"\u003e[6]\u003c/a\u003e. $U$ and $V$ denote the left and right singular vector matrix of $\\hat{Z}$. $\\Lambda$ represents the diagonal singular value matrix.\u003c/p\u003e\n\u003cp\u003eA method to assess the SELI geometry structure involves computing the difference between the logits $Z^m$ extracted from the pre-trained model and the optimal logits $\\hat{Z}$. However, obtaining $Z^m$ directly without fine-tuning on the target dataset is time-consuming. Therefore, features $H^m$ of the model are extracted and their difference is measured to form the SELI structure. The complexity of achieving the optimal logits $\\hat{Z}$ through features $H_m$ is approximated via the nuclear norm.\u003c/p\u003e\n\u003cp\u003eThus, the score $S^m_{seli}$ is calculated as :\u003c/p\u003e\n\u003cp\u003e$$S^m_{seli}(H^m) = ||H^m||_*$$\u003c/p\u003e\n\u003cp\u003eThe higher the score $S^m_{seli}$ the higher the rank of the feature matrix $H_m$, making $Z$ closer to a full rank matrix.\u003c/p\u003e\n\u003ch3 id=\"nearest-center-classifier\"\u003eNearest Center Classifier\u003c/h3\u003e\n\u003cp\u003eFirst, the posterior probability $P(y = c|h)$ for each class $c$ is calculated using Bayes\u0026rsquo; Rule:\u003c/p\u003e\n\u003cp\u003e$$ \\log P(y = c|h) = \\frac{1}{2}(h_i - \\mu_c)^T \\Sigma (h_j - \\mu_c) + \\log P(y = c) $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\mu_c$ is the mean vector for class $c$.\u003c/li\u003e\n\u003cli\u003e$\\Sigma$ is the covariance matrix.\u003c/li\u003e\n\u003cli\u003e$P(y = c)$ is the prior probability of class $c$.\u003c/li\u003e\n\u003cli\u003e$h$ is the feature vector extracted by the pre-trained model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNext, the softmax function is applied to obtain the normalized posterior probability $z^m_{i,c}$ for each class $c$ of the $i$-th sample:\u003c/p\u003e\n\u003cp\u003e$$ z^m_{i,c} = \\frac{\\exp(\\log P(y = c|h^m_i))}{\\Sigma ^C_{k=1} \\exp(\\log P(y = k|h^m_i))} $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$C$ is the number of classes.\u003c/li\u003e\n\u003cli\u003e$h^m_i$ is the feature vector of the $i$-th sample extracted by the m-th pre-trained model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinally, the score $S^m_{ncc}$ is computed as the average of the dot product of the normalized posterior probabilities $z^m_i$ and the ground truth labels $y_i$ for all samples:\u003c/p\u003e\n\u003cp\u003e$$ S^m_{ncc}(H^m) = \\frac{1}{N} \\Sigma ^N_{i=1} z^m_i \\cdot y_i $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$N$ is the number of samples.\u003c/li\u003e\n\u003cli\u003e$y_i$ is the ground truth label of the $i$-th sample (in one-hot encoding).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe higher the score $S^{m}_{ncc}(H^m)$, the smaller the deviation to the nearest optimal centroid classifier and therefore the greater the transferability to the target dataset.\u003c/p\u003e\n\u003ch1 id=\"numerical-experiment\"\u003eNumerical Experiment\u003c/h1\u003e\n\u003cp\u003eTo reproduce their experiment, the authors\u0026rsquo; code available on a \u003ca href=\"https://github.com/BUserName/NCTI/tree/main\"\u003eGithub\u003c/a\u003e repository was used. A first encountered issue was the required \u003ccode\u003etorch\u003c/code\u003e and \u003ccode\u003etorchvision\u003c/code\u003e versions, which are quite old, and thus not always available to install, which was the case here. Fortunately, the  most recent versions were compatible with the code. A \u003ccode\u003erequirements.txt\u003c/code\u003e file would have been welcome.\u003c/p\u003e\n\u003cp\u003eA second issue is that there are remaining personal paths in some scripts, which should be replaced by downloading paths to PyTorch source models. As a consequence, the loading method from \u003ccode\u003etorch\u003c/code\u003e should also be replaced.\u003c/p\u003e\n\u003cp\u003eOther issues considering the datasets loading remained unsolved.\u003c/p\u003e\n\u003cp\u003eAfter these modifications, it is possible to run the authors\u0026rsquo; experiments on the CIFAR10 dataset for the group of supervised pre-trained models. Consisting of 60 000 32x32 colour images in 10 classes, this dataset is broadly used in benchmarks for image classification. 12 pre-trained models were ran on CIFAR10 to establish a ranking based on their performances in terms of NCTI available below.\u003c/p\u003e\n\u003ctable style=\"width:100%; border-collapse: collapse;\" border=\"1\"\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align:left; padding: 8px;\"\u003eModel\u003c/th\u003e\n      \u003cth style=\"text-align:left; padding: 8px;\"\u003eNCTI Score\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet152\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e2.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet101\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.799\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet201\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.434\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet169\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.146\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet34\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.757\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet50\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.709\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet121\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.655\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eMnasNet1_0\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.031\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eGoogleNet\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.251\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eMobileNetV2\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.444\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eInceptionV3\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.732\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eResults show that the deepest architectures offer the best NCTI scores.  The depth of a network is closely related to its ability to learn and represent complex features and patterns from the training data, which contributes to a model\u0026rsquo;s superior transferability. The different performances between ResNet and DenseNet could be attributed to the way DenseNet connects each layer to every other layer in a feed-forward fashion, which, while efficient in parameter use and reducing overfitting, may not capture as complex a feature hierarchy as ResNet. Models like MnasNet, MobileNetV2, and InceptionV3, designed for efficiency and speed with a compromise on depth, understandably score lower in transferability, as reflected by their NCTI scores.\u003c/p\u003e\n\u003cp\u003eThen, we evaluated the transferability of the supervised pre-trained models, in terms of weighted Kendall\u0026rsquo; τ, and obtained the exact same result as the one presented in the paper: \u003cstrong\u003e0.843\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIt was not possible for us to run the experiment on the group of self-supervised pre-trained models as the authors\u0026rsquo; code included personal paths, and we were not able to find them online.\u003c/p\u003e\n\u003cp\u003eA Github repository with all the necessary modifications from the original code is at your disposal \u003ca href=\"https://github.com/marionchadal/NCTI\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"what-about-source-features\"\u003eWhat about source features?\u003c/h1\u003e\n\u003cp\u003eThrough extensive testing, authors have identified that two specific attributes related to neural collapse, observed in the source features, consistently predicted the model\u0026rsquo;s performance on new tasks. These attributes were the diversity within data categories and the compactness of category representations. Remarkably, models showing higher within-category diversity and more compact category representations in their source features tended to adapt better to new tasks. On the other hand, SELI did not consistently correlate with transferability.\u003c/p\u003e\n\u003ch1 id=\"challenges\"\u003eChallenges\u003c/h1\u003e\n\u003cp\u003eAuthors did experiments on the effectiveness of each individual component in NCTI. They used the three terms individually and removed them one at a time from the full system, and it turned out that for supervised learning, the NCTI without NCC achieved the best weighted Kendall\u0026rsquo; τ. Instead of having normalized the three NCTI components equally, it could have been interesting to tune hyperparameters.\nMoreover, the current implementation and validation of NCTI are confined to image classification tasks, suggesting its applicability may be limited to similar types of problems. Future work could extend the method\u0026rsquo;s applicability to a broader range of tasks beyond classification, such as detection or segmentation​​. Pre-trained language models could also be considered to measure their transferability based on Neural Collapse. For example, the Fair Collapse (FaCe) method \u003ca href=\"#ref7\"\u003e[7]\u003c/a\u003e considers both Computer Vision and Natural Language Processing tasks, using different proxies of Neural Collapse than NCTI, and producing a slightly less good τ on the CIFAR-10 dataset (0.81).\u003c/p\u003e\n\u003ch1 id=\"takeaways\"\u003eTakeaways\u003c/h1\u003e\n\u003cp\u003eKey points to remember are :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCalculating model transferability and choosing the optimal pre-trained model is important for reasons of computational cost, environmental impact, and overall performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe authors have developed a new metric, the \u003cstrong\u003eNeural Collapse informed Transferability Index (NCTI)\u003c/strong\u003e, which is based on the concept of \u003cstrong\u003eneural collapse\u003c/strong\u003e and measures \u003cem\u003ethe gap between the current feature geometry and the geometry at the terminal stage after hypothetical fine-tuning on the downstream task.\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe NCTI metric integrates three aspects equally: \u003cstrong\u003eSELI geometry\u003c/strong\u003e, \u003cstrong\u003ewithin-class variability\u003c/strong\u003e, and \u003cstrong\u003enearest center classifier\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis method is \u003cem\u003elight to compute\u003c/em\u003e, enabling rapid evaluation of model transferability.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEmpirical results demonstrate that \u003cem\u003ethe ranking of model transferability has a very strong correlation with the ground truth ranking\u003c/em\u003e and \u003cstrong\u003ecompares with state-of-the-art methods\u003c/strong\u003e, highlighting its effectiveness in selecting pre-trained models for specific tasks.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn summary, the development of metrics such as NCTI is crucial for optimizing the use of pre-trained models, considering both performance and associated costs in real-world applications.\u003c/p\u003e\n\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003cp\u003e\u003ca id=\"ref1\"\u003e\u003c/a\u003e1. Z. Wang Y.Luo, L.Zheng, Z.Huang, M.Baktashmotlagh (2023), How far pre-trained models are from neural collapse on the target dataset informs their transferabilityWang, ICCV.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref2\"\u003e\u003c/a\u003e2. V. Papyan,1 , X. Y. Hanb,1 , and D.L. Donoho (2020), Prevalence of neural collapse during the terminal phase of deep learning training, National Academy of Sciences.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref3\"\u003e\u003c/a\u003e3. Galanti, T., György, A., \u0026amp; Hutter, M. (2021). On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref4\"\u003e\u003c/a\u003e4. Li, X., Liu, S., Zhou, J., Lu, X., Fernandez-Granda, C., Zhu, Z., \u0026amp; Qu, Q. (2022). Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref5\"\u003e\u003c/a\u003e5. Vignesh Kothapalli, (2023). Neural Collapse: A Review on Modelling Principles and Generalization. arXiv preprint arXiv:2206.04041.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref6\"\u003e\u003c/a\u003e6. Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. (2022). Imbalance trouble: Revisiting neural-collapse\ngeometry. arXiv preprint arXiv:2208.05512.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref7\"\u003e\u003c/a\u003e7. Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang. (2023). Unleashing the power of neural collapse for transferability estimation. arXiv preprint arXiv:2310.05754v1.\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\n\u003cp\u003eStart writing here !\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/transferability/","date_published":"8016-08-09T126:88:00+01:00","date_modified":"8016-08-09T126:88:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}}]}