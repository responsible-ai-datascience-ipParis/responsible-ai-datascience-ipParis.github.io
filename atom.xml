<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bloggin on Responsible AI</title><description>Bloggin on Responsible AI</description><link>https://responsible-ai-datascience-ipParis.github.io/</link><language>en</language><copyright>Copyright 2025, Calvin Tran</copyright><lastBuildDate>Thu, 27 Mar 2025 18:53:17 +0100</lastBuildDate><generator>Hugo - gohugo.io</generator><docs>http://cyber.harvard.edu/rss/rss.html</docs><atom:link href="https://responsible-ai-datascience-ipParis.github.io//atom.xml" rel="self" type="application/atom+xml"/><item><title>Robust Classifiers Energy Based Models</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/robust-classifiers-energy-based-models/</link><description>&lt;h1 style="font-size: 36px;">Unlocking the Secrets of Robust AI: How Energy-Based Models Are
Revolutionizing Adversarial Training&lt;/h1>
&lt;h1 style="font-size: 20px;">Authors: Lucas ALJANCIC, Solal DANAN, Maxime APPERT&lt;/h1>
&lt;h1 style="font-size: 24px;">Introduction: The Hidden World of Adversarial Attacks &lt;/h1>
=====================================================
&lt;p>Imagine you&amp;rsquo;ve built a state-of-the-art AI model that can classify
images with near-perfect accuracy. But then, someone adds a tiny, almost
invisible perturbation to an image, and suddenly your model confidently
misclassifies it. This is the world of &lt;strong>adversarial attacks&lt;/strong>, where
small, carefully crafted changes can fool some advanced AI systems.&lt;/p>
&lt;p>In this post, we&amp;rsquo;ll dive into a groundbreaking research paper that
rethinks how we train AI models to resist these attacks. By leveraging
&lt;strong>Energy-Based Models (EBMs)&lt;/strong>, the authors propose a novel approach
called &lt;strong>Weighted Energy Adversarial Training (WEAT)&lt;/strong> that not only
makes models more robust but also unlocks surprising generative
capabilities. Let&amp;rsquo;s break it down step by step, from the basics to the
big picture.&lt;/p>
&lt;h1 style="font-size: 24px;">Understanding Adversarial Attacks and Robust AI &lt;/h1>
===============================================
&lt;h2 id="what-are-adversarial-attacks">What Are Adversarial Attacks?&lt;/h2>
&lt;p>Adversarial attacks can be thought of as carefully crafted optical
illusions for AI. By making tiny changes to an input, attackers can
manipulate the model&amp;rsquo;s output, causing incorrect predictions. These
perturbations are often imperceptible to the human eye but can
significantly impact AI decision-making.&lt;/p>
&lt;h4 id="technical-explanation">Technical Explanation:&lt;/h4>
&lt;p>Mathematically, an adversarial example $x&amp;rsquo;$ is generated by adding a
small perturbation $\delta$ to an original input $x$, such that:
$$x&amp;rsquo; = x + \delta,$$ where $\delta$ is chosen to maximize the model&amp;rsquo;s
prediction error, often by solving:
$$\arg\max_{\delta} ; L\bigl(f(x+\delta), y\bigr) \quad \text{subject to} \quad |\delta| \le \epsilon,$$
ensuring that the perturbation remains small. Here, $f(x)$ represents
the model&amp;rsquo;s prediction, $y$ is the true label, and $L$ is the loss
function.&lt;/p>
&lt;h4 id="schematic-explanation">Schematic Explanation:&lt;/h4>
&lt;p>A simple visualization consists of two decision boundaries: one for
clean samples and another distorted by adversarial perturbations. The
adversarial example, though close to the clean sample in the input
space, crosses the decision boundary, leading to a misclassification.
This is illustrated in the following image, which showcases the
adversarial perturbation technique. For example, a small amount of
noise&amp;mdash;imperceptible to humans&amp;mdash;can be added to an image of a panda,
changing its classification to a different label (e.g., from &amp;quot;panda&amp;quot;
to &amp;quot;gibbon&amp;quot;) with high confidence, even though the image still looks
exactly like a panda to the human eye. This occurs because the noise is
specifically designed to fool the model by maximizing the prediction
error.&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/AA.png"
alt="Adversarial Example: A small perturbation (middle) is added to a
correctly classified image (left), causing a deep neural network to
misclassify it with high confidence (right)."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> A small perturbation (middle) is added to a
correctly classified image (left), causing a deep neural network to
misclassify it with high confidence (right).&lt;/p>
&lt;p>These adversarial attacks can also be used maliciously in what is known
as a backdoor attack, where mislabeling during training leads to
misclassification at test time. For instance, a stop sign with a slight
alteration may be classified as a speed limit sign due to adversarial
manipulation of the training data.&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/stop.png"
alt="Backdoor Attack: Poisoning the training data by mislabeling
specific inputs (top right) forces the model to associate a manipulated
stop sign with a speed limit sign (bottom), leading to targeted
misclassification at test time."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Poisoning the training data by mislabeling
specific inputs (top right) forces the model to associate a manipulated
stop sign with a speed limit sign (bottom), leading to targeted
misclassification at test time.&lt;/p>
&lt;p>We can easily imagine scenarios in which an adversarial attack can be
critical. For example, a self-driving car misinterpreting a slightly
modified stop sign as a yield sign due to adversarial perturbations
could lead to catastrophic consequences.&lt;/p>
&lt;p>To counteract these risks, engineers strive to build robust models where
adversarial attacks are ineffective. Techniques such as feature
squeezing or input transformations can sometimes remove adversarial
perturbations, reducing their effectiveness.&lt;/p>
&lt;h2 id="the-quest-for-robust-ai-adversarial-training">The Quest for Robust AI: Adversarial Training&lt;/h2>
&lt;p>To combat adversarial attacks, researchers have developed Adversarial
Training. The idea is simple: train the model on both normal data and
adversarial examples (data modified to fool the model). This way, the
model learns to recognize and resist these attacks.&lt;/p>
&lt;p>During training, adversarial examples are generated using algorithms
like Projected Gradient Descent (PGD) or the Fast Gradient Sign Method
(FGSM). The model is then trained to correctly classify both clean and
adversarial examples, thereby improving its resilience.&lt;/p>
&lt;p>Mathematically, adversarial training involves solving the following
optimization problem:
$$\underbrace{\min_{\theta} E_{(x,y) \sim \mathcal{D}} \left[ \overbrace{\max_{\delta: |\delta| \le \epsilon} L\bigl(f(x+\delta;\theta), y\bigr)}^{\text{Adversarial Attack}} \right]}_{\text{Adversarial Training}}.$$
Here, the inner maximization aims to generate the worst-case
perturbation $\delta$, while the outer minimization updates the model
parameters $\theta$ to minimize the loss on these adversarial examples.
This iterative process enhances the model&amp;rsquo;s robustness against
adversarial attacks.&lt;/p>
&lt;p>A schematic representation helps visualize this concept. The image below
shows how a labeled or unlabeled image is used to generate an
adversarial example, which is then processed by a model. The predictions
of both the original and adversarial images are compared using KL
divergence, with a weighted sum incorporating cross-entropy loss (for
labeled images) to compute the final loss.&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/schema.png"
alt="Adversarial Training with Consistency Loss: A model processes both
clean and adversarial examples, minimizing cross-entropy loss for
labeled images and KL divergence between predictions to improve
robustness. The final loss is a weighted sum of both
terms."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Representation of Adversarial Training, a model processes both
clean and adversarial examples, minimizing cross-entropy loss for
labeled images and KL divergence between predictions.&lt;/p>
&lt;p>A useful analogy is training a security system against burglary
attempts. Imagine a house protected against intruders. A basic security
system might work well against naive burglars but fail against
sophisticated thieves who exploit specific vulnerabilities. To
strengthen the system, the homeowner continuously updates it by testing
against simulated break-in attempts&amp;mdash;much like adversarial training
tests the model against simulated attacks. However, if the security
system memorizes these specific attack patterns, it may remain
vulnerable to novel intrusion techniques. This weakness highlights what
is known as robust overfitting: the model learns to defend against the
training attacks but generalizes poorly to new, unseen attacks.&lt;/p>
&lt;p>To truly address robust overfitting, we must understand why it occurs.
One way to do this is to analyze adversarial training through an
energy-based perspective. In the next section, we explore how
energy-based models offer critical insights that can help address this
problem.&lt;/p>
&lt;h1 style="font-size: 24px;">Energy-Based Models (EBMs) -- A New Perspective &lt;/h1>
===============================================
&lt;p>Inspired by previous works linking adversarial training (AT) and
energy-based models&amp;mdash;which reveal a shared contrastive approach&amp;mdash;the
authors reinterpret robust discriminative classifiers as EBMs. This new
perspective provides fresh insights into the dynamics of AT.&lt;/p>
&lt;h2 id="what-are-energy-based-models">What Are Energy-Based Models?&lt;/h2>
&lt;p>In traditional classification, a neural network is trained to output the
most likely label for an input. However, when recast as an EBM, the
classifier assigns an &lt;em>energy&lt;/em> to each input-label pair,
$E_\theta(x,y)$, which reflects how &amp;ldquo;plausible&amp;rdquo; that combination is. In
this framework, lower energy indicates that the model is more confident
the input belongs to that class. EBMs rely on the assumption that any
probability density function $p(x)$ can be represented via a Boltzmann
distribution:
$$p_\theta(x) = \frac{\exp\bigl(-E_\theta(x)\bigr)}{Z(\theta)}$$,&lt;/p>
&lt;p>where $E_\theta(x)$ is the energy function mapping each input $x$ to a
scalar value, and $Z(\theta) = \int \exp\bigl(-E_\theta(x)\bigr) , dx,$
is the normalizing constant ensuring that $p_\theta(x)$ is a valid
probability distribution.&lt;/p>
&lt;p>Similarly, the joint probability of an input and a label can be defined
as:
$$p_\theta(x,y) = \frac{\exp\bigl(-E_\theta(x,y)\bigr)}{Z&amp;rsquo;(\theta)}$$,&lt;/p>
&lt;p>which leads to the formulation of a discriminative classifier:
$$p_\theta(y \mid x) = \frac{\exp\bigl(-E_\theta(x,y)\bigr)}{\sum_{k=1}^{K} \exp\bigl(-E_\theta(x,k)\bigr)}.$$&lt;/p>
&lt;p>In this context, &lt;strong>energy serves as a measure of confidence&lt;/strong> for both
the input-label pair and, when marginalized over labels, the input
itself:&lt;/p>
&lt;h2 id="adversarial-attacks-through-the-energy-lens">Adversarial Attacks Through the Energy Lens&lt;/h2>
&lt;p>An intriguing insight from the paper is that adversarial attacks can be
interpreted by examining how they alter the energy landscape. Recall
that the cross-entropy loss can be written in terms of energy as:
$$L_{CE}(x,y;\theta) = -\log p_\theta(y\mid x)
;=; E_\theta(x,y) - E_\theta(x),$$ implying that an attack impact the
loss&amp;mdash;by modifying both $E_\theta(x,y)$ and $E_\theta(x)$.&lt;/p>
&lt;h4 id="untargeted-attacks">Untargeted Attacks.&lt;/h4>
&lt;p>These attacks aim to make the classifier output &lt;em>any&lt;/em> incorrect label.
They raise the joint energy $E_\theta(x,y)$ (making the model to
&amp;quot;dislike&amp;quot; the correct label) while lowering the marginal energy
$E_\theta(x)$. Consequently, the adversarial example looks very
&amp;ldquo;natural&amp;rdquo; (low $E_\theta(x)$) yet becomes unrecognizable for the true
label (high energy).&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/pgd1.png"
alt="PGD Untargeted attacks: (Left) Distributions of (E_\theta(x)); (Right) Distributions of (E_\theta(x, y)). Blue indicates natural data, red indicates adversarial data."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/pgd2.png"
alt="PGD Untargeted attacks: (Left) Distributions of (E_\theta(x)); (Right) Distributions of (E_\theta(x, y)). Blue indicates natural data, red indicates adversarial data."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> PGD Untargeted attacks: (Above) Distributions of $(E_\theta(x))$; (Below) Distributions of $(E_\theta(x, y))$. Blue indicates natural data, red indicates adversarial data.&lt;/p>
&lt;h4 id="targeted-attacks">Targeted Attacks.&lt;/h4>
&lt;p>These attacks force the classifier to predict a specific (incorrect)
label $t$. The adversary perturbs $x$ to &lt;em>minimize&lt;/em> the energy for the
target label $E_\theta(x,t)$, thereby steering the classifier&amp;rsquo;s decision
toward $t$ but raise $E_\theta(x)$ overall, making the input look
out-of-distribution despite being confidently misclassified as $t$.&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/apgd1.png"
alt="APGD Targeted attacks: (Left) Distributions of $E_\theta(x)$;
(Right) Distributions of $E_\theta(x,y)$. Blue indicates natural data,
red indicates adversarial data."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;img
src="./images/Robust_Classifiers_EBM/apgd2.png"
alt="APGD Targeted attacks: (Left) Distributions of $E_\theta(x)$; (Right)
Distributions of $E_\theta(x,y)$. Blue indicates natural data, red
indicates adversarial data."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> APGD Targeted attacks: (Above) Distributions of $(E_\theta(x))$; (Below) Distributions of $(E_\theta(x, y))$. Blue indicates natural data, red indicates adversarial data.&lt;/p>
&lt;p>By revealing how each attack reshapes the energies of clean vs.
adversarial examples, the authors highlight distinct patterns for
untargeted and targeted strategies&amp;mdash;informing more effective defenses.&lt;/p>
&lt;h2 id="energy-based-insights-on-robust-overfitting">Energy-Based Insights on Robust Overfitting&lt;/h2>
&lt;p>Overfitting means a model excels on training data but falters on unseen
examples. In the robust setting, this manifests when a network memorizes
the specific adversarial perturbations seen during training, yet
struggles with new adversarial attacks. The authors reveal that robust
overfitting is closely linked to a widening &lt;em>energy gap&lt;/em> between clean
and adversarial samples. As adversarial training progresses, the energy
$E_\theta(x)$ of natural (clean) examples and the energy $E_\theta(x^\ast)$
associated with their adversarially perturbed versions diverge
significantly. Surprisingly, even if a sample is &amp;ldquo;easy&amp;rdquo; (i.e., it has a
low loss and the model is highly confident), only minimal perturbations
are needed to flip its label due to the model&amp;rsquo;s over-confidence. These
weaker perturbations, paradoxically, cause a larger distortion in the
energy landscape, so that a slight adversarial change on a low-loss
sample can result in a substantial discrepancy between the natural and
adversarial energies.&lt;/p>
&lt;h2 id="trades-aligning-clean-and-adversarial-energies">TRADES: Aligning Clean and Adversarial Energies&lt;/h2>
&lt;p>TRADES (&lt;em>TRadeoff-inspired Adversarial DEfense via Surrogate-loss
minimization&lt;/em>) is a refined adversarial training approach that adds a
term to the standard loss, ensuring that a model&amp;rsquo;s predictions on a
clean input $x$ remain close to those on its adversarially perturbed
version $x+\delta$. When expressed in energy terms, this extra term
aligns the energies of natural and adversarial samples, thereby reducing
robust overfitting. In essence, TRADES enforces similar energy values
for $x$ and $x+\delta$, creating a smoother energy landscape and helping
the model generalize more effectively against unseen attacks.&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/trades.png"
alt="Energy alignment in TRADES vs. SAT (another AT approach). While
SAT (line curve) experiences a steep divergence between clean and
adversarial energies ($\Delta E_\theta(x)$ = $E_\theta(x)$ -
$E_\theta(x^\ast)$ ) in the 3rd phase (leading to robust overfitting),
TRADES (dashed curve) maintains a relatively constant energy gap. This
smoother alignment mitigates overfitting and improves
robustness."
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> While
SAT (line curve) experiences a steep divergence between clean and
adversarial energies ($\Delta E_\theta(x)$ = $E_\theta(x)$ -
$E_\theta(x^\ast)$ ) in the 3rd phase (leading to robust overfitting),
TRADES (dashed curve) maintains a relatively constant energy gap. This
smoother alignment mitigates overfitting and improves
robustness.&lt;/p>
&lt;p>One of the key empirical observations is that top-performing robust
classifiers tend to exhibit a smooth energy landscape around natural
data points. Concretely, the energies $E_\theta(x)$ and
$E_\theta(x+\delta)$ remain closer in these models and this alignment
strongly correlates with improved adversarial defense.&lt;br>
&lt;br>
To sum up, these insights clarify that robust overfitting is not merely
about memorizing specific adversarial examples, but rather about how the
model&amp;rsquo;s internal energy representation becomes distorted. When the gap
between $E_\theta(x)$ and $E_\theta(x+\delta)$ grows, the model&amp;rsquo;s
ability to generalize its robustness to new attacks is compromised.
These observations deepen our understanding of adversarial dynamics and
informed the strategy for effective robust training methods WEAT, as
described in the following section.&lt;/p>
&lt;h1 style="font-size: 24px;">Weighted Energy Adversarial Training (WEAT) &lt;/h1>
===============================================
&lt;h2 id="main-principle-of-weat">Main Principle of WEAT&lt;/h2>
&lt;p>The authors propose &lt;strong>WEAT&lt;/strong>, a novel method that weights training
samples based on their energy. WEAT relies on &lt;strong>Energy-Based Models
(EBMs)&lt;/strong> to measure the model&amp;rsquo;s &amp;quot;confidence&amp;quot;:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Marginal Energy&lt;/strong> $E_\theta(x) = -\log \sum_k \exp(\theta(x)[k])$:
The lower it is, the more &amp;quot;probable&amp;quot; the input $x$ is according to
the model.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Joint Energy&lt;/strong> $E_\theta(x, y) = -\log \exp(\theta(x)[y])$:
Measures the confidence for a specific class $y$.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The article classifies samples according to their energy into three
categories:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>High-energy samples&lt;/strong> ($E_\theta (x) &amp;gt; -3.87$): These are
difficult examples, close to decision boundaries. WEAT gives them
more weight because they help the model learn better.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Low-energy samples&lt;/strong> ($E_\theta (x) \leq -11.47$): These are easy
examples. WEAT gives them less weight to prevent overfitting.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Intermediate samples&lt;/strong> (between these two thresholds)&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Key Idea:&lt;/strong> Samples with &lt;strong>high energy&lt;/strong> (hard to classify) are
crucial for robustness, while those with &lt;strong>low energy&lt;/strong> (easy) risk
causing overfitting. Therefore, WEAT weights them differently:&lt;/p>
&lt;p>$$\text{weight}(x) = \frac{1}{\log(1 + \exp(|E_\theta(x)|))}$$&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/weighting.png"
alt="Weighting Visualization"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Weighting Visualization.&lt;/p>
&lt;h2 id="weat-in-details">WEAT in details&lt;/h2>
&lt;p>The core formula of WEAT is an improvement of TRADES by introducing a
&lt;strong>dynamic weighting weight$(x)$&lt;/strong> based on the energy $E_\theta(x)$.
WEAT combines:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Cross-entropy loss (CE)&lt;/strong>: Standard classification performance
measure. It contains
$E_{\boldsymbol{\theta}}(\mathbf{x}, y) - E_{\boldsymbol{\theta}}(\mathbf{x})$.
When this term is minimized, it allows to &amp;quot;dig&amp;quot; the valleys
representing the good predictions (low energy = high confidence).
For the correct class y, $E_{\boldsymbol{\theta}}(\mathbf{x}, y)$
becomes &amp;quot;lower&amp;quot; than $E_{\boldsymbol{\theta}}(\mathbf{x})$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>KL divergence&lt;/strong>: Controls the gap between predictions on natural
data $p(y|x)$ and adversarial data $p(y|x^\ast)$ with the marginal term
$E_{\boldsymbol{\theta}}(\mathbf{x}) - E_{\boldsymbol{\theta}}(\mathbf{x}^\ast)$.
By minimizing this gap, the model smooths the energy landscape and
flattens out disturbed areas.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>WEAT formula&lt;/strong>:&lt;/p>
&lt;p>$$L_{\text{WEAT}} = \underset{\text{Weighting}}{\boxed{\text{weight}(x)}} \cdot \left[
\underset{\text{Standard Loss}}{\boxed{L_{\text{CE}}(x, y)}} +
\beta \cdot \underset{\text{Robust Regularization}}{\boxed{\text{KL}(p(y|x) || p(y|x^\ast))}}
\right]
$$&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/valley1.png"
alt="Diagram of WEAT dynamics"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/valley2.png"
alt="Diagram of WEAT dynamics"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;img
src="./images/Robust_Classifiers_EBM/valley3.png"
alt="Diagram of WEAT dynamics"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Figure 8&lt;/strong>: Diagram of WEAT dynamics&lt;/p>
&lt;p>The figure above highlights the trade-off with the $\beta$ coefficient
for regularization to maintain a smooth energy landscape despite
adversarial attacks.&lt;/p>
&lt;p>While not the main focus of this article (to avoid technical overload),
WEAT also demonstrates remarkable performance in image generation
through its integration with Stochastic Gradient Langevin Dynamics
(SGLD). On standard benchmarks like CIFAR-10, WEAT matches the
performance of hybrid models such as JEM while offering superior
robustness (see Table 2c in the paper).&lt;/p>
&lt;h1 style="font-size: 24px;">Conclusion: A New Era for Robust AI &lt;/h1>
===============================================
&lt;h2 id="why-this-matters">Why This Matters&lt;/h2>
&lt;p>This research isn&amp;rsquo;t just about making AI models more robust. It&amp;rsquo;s about
fundamentally understanding how these models work and how we can improve
them. By rethinking adversarial training through the lens of EBMs, the
authors have opened up new possibilities for both robustness and
generative modeling. As AI continues to evolve, approaches like WEAT
will be crucial for building models that are both accurate and secure.
Surely, this type of model will play a key role in improving trust in
critical technologies such as autonomous vehicles.&lt;/p>
&lt;h2 id="potential-societal-impact">Potential Societal Impact&lt;/h2>
&lt;p>Although robust models are often considered safe from adversarial
attacks, their susceptibility to inversion poses a privacy risk. Because
robust classifiers can be interpreted as energy-based models, they
capture substantial information about their training data&amp;mdash;including
its distribution and structure. This makes it possible, using inversion
techniques, to reconstruct or approximate the original training data. If
sensitive information (e.g., personal data, proprietary content, or
other confidential details) is exposed, it could lead to significant
privacy breaches with broader societal implications.&lt;/p>
&lt;h1 id="references">References&lt;/h1>
&lt;p>Sik-Ho Tsang. &lt;em>Review: Virtual Adversarial Training (VAT)&lt;/em>. Apr 21,
2022. Available at:
&lt;a href="https://sh-tsang.medium.com/review-virtual-adversarial-training-vat-4b3d8b7b2e92">https://sh-tsang.medium.com/review-virtual-adversarial-training-vat-4b3d8b7b2e92&lt;/a>.&lt;/p>
&lt;p>Gaudenz Boesch. &lt;em>Attack Methods: What Is Adversarial Machine Learning?&lt;/em>.
December 2, 2023. Available at:
&lt;a href="https://viso.ai/deep-learning/adversarial-machine-learning/">https://viso.ai/deep-learning/adversarial-machine-learning/&lt;/a>.&lt;/p>
&lt;p>Mujtaba Hussain Mirza1, Maria Rosaria Briglia, Senad Beadini, and Iacopo
Masi. &lt;em>Shedding More Light on Robust Classifiers under the lens of
Energy-based Models&lt;/em>. 2025. Available at:
&lt;a href="https://arxiv.org/abs/2407.06315">https://arxiv.org/abs/2407.06315&lt;/a>.&lt;/p>
&lt;p>&lt;/style>&lt;/p>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/robust-classifiers-energy-based-models/</guid><pubDate>Thu, 27 Mar 2025 18:53:17 +0100</pubDate></item><item><title>Fairness in Social Influence Maximization via Optimal Transport</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/fairness-in-social-influence-maximization-via-optimal-transport/</link><description>&lt;h3 id="authors-guillaume-marin-bertin--jaishan-burton-elmo">Authors: Guillaume MARIN-BERTIN &amp;amp; Jaishan BURTON ELMO&lt;/h3>
&lt;p>&lt;strong>Table of Contents&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mutual-fairness">2. Mutual Fairness: A New Metric&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#why-make-a-new-metric">2.1 Why Make a New Metric?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#proposed-fairness-metric">2.2 Proposed Fairness Metric&lt;/a>&lt;/li>
&lt;li>&lt;a href="#short-example">2.3 Short Example&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#fairness-evaluation">3. Metric in Practice&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#mutual-fairness-practice">3.1 Mutual Fairness in Practice&lt;/a>&lt;/li>
&lt;li>&lt;a href="#impact-of-beta">3.2 Impact of β&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#s3d-algorithm">4. Improving Fairness with S3D&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#stochastic-seed-selection-descent">4.1 Stochastic Seed Selection Descent (S3D)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#experimentation">4.2 Experimentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#impact-of-s3d">4.3 Impact of S3D&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a blog post about the article “Fairness in Social Influence Maximization via Optimal Transport” published by Shubham Chowdhary et al. in 2024 and available &lt;a href="https://neurips.cc/virtual/2024/poster/94521">&lt;strong>here&lt;/strong>&lt;/a>.&lt;/p>
&lt;h3 id="1-introduction">&lt;a href="#introduction">1. Introduction&lt;/a>&lt;/h3>
&lt;p>In today’s digital society, social networks play a major role in how information spreads. Whether it is a public health campaign, a political message, or viral marketing, the ability to maximize influence is crucial. Companies, governments, and organizations leverage Influence Maximization (IM) algorithms to strategically select key individuals—called seeds—who will initiate a diffusion process, ensuring that information reaches the largest possible audience.&lt;/p>
&lt;p>Imagine a scenario where a job-matching platform uses an Influence Maximization (IM) strategy to promote career opportunities to young professionals. The goal is to spread job postings efficiently across different professional communities. However, due to the structure of the social network, the algorithm selects key individuals (seeds) in such a way that:&lt;/p>
&lt;ul>
&lt;li>In 50% of cases, all the job opportunities are shared within a network of university graduates, while vocational school graduates receive none.&lt;/li>
&lt;li>In the other 50% of cases, the opposite happens.&lt;/li>
&lt;/ul>
&lt;p>At first glance, this might seem fair: both groups have an equal expected chance of seeing the job offers. However, in practice, one group is always entirely excluded in each scenario, creating a systemic disadvantage for those who miss out on critical career opportunities.&lt;/p>
&lt;p>To fix this, researchers have tried adding fairness constraints to IM algorithms. The main ideas include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Equity-based fairness&lt;/strong> (Stoica et al., 2020): Ensures that each group has the same expected proportion of influenced users.&lt;/li>
&lt;li>&lt;strong>Max-min fairness&lt;/strong> (Fish et al., 2019; Zhu et al., 2019): Maximizes the minimum probability that any group receives information.&lt;/li>
&lt;li>&lt;strong>Diversity-aware methods&lt;/strong> (Tsang et al., 2019): Ensure that no single group dominates the influence process.&lt;/li>
&lt;/ul>
&lt;p>Despite these efforts, a fundamental issue remains: existing metrics treat groups independently and do not capture the joint probability of outreach. This means they can create the illusion of fairness while still allowing systematic exclusion of certain communities.&lt;/p>
&lt;p>To address this, we propose a new fairness-aware framework that better distributes influence across communities:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Mutual Fairness&lt;/strong>: A better way to measure fairness, inspired by Optimal Transport. Instead of just looking at how much information each group gets in total, it ensures they receive the message at the same time.&lt;/li>
&lt;li>&lt;strong>S3D Algorithm&lt;/strong>: A smarter way to choose influencers, balancing fairness and efficiency. It adjusts seed selection dynamically to improve fairness while still reaching as many people as possible.&lt;/li>
&lt;/ol>
&lt;h3 id="2-mutual-fairness-a-new-metric">&lt;a href="#mutual-fairness">2. Mutual Fairness: A New Metric&lt;/a>&lt;/h3>
&lt;h5 id="why-make-a-new-metric">&lt;strong>Why make a new metric?&lt;/strong>&lt;/h5>
&lt;p>To understand why a new metric is necessary, let’s examine two situations where information propagation appears fair,we take the example of the introduction.&lt;/p>
&lt;p>Consider two groups, C1 and C2, each with an outreach probability distribution:&lt;/p>
&lt;p align="center">
&lt;a name="fig1">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure1.png" alt="figure1">
&lt;br>
&lt;i>Figure 1 : Outreach Probability Distribution for Groups C1 and C2&lt;/i>
&lt;/p>
&lt;ul>
&lt;li>δ&lt;sub>0&lt;/sub> represents the case where the information is not transmitted.&lt;/li>
&lt;li>δ&lt;sub>1&lt;/sub> represents the case where the information is successfully received by the group.&lt;/li>
&lt;/ul>
&lt;p>On average, both groups have a 50% chance of receiving the information, which might suggest a fair situation. However, this average hides significant differences in the actual distribution of information, as illustrated in the example below:&lt;/p>
&lt;p align="center">
&lt;a name="fig2">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure2.png" alt="figure2">
&lt;br>
&lt;i>Figure 2 : Comparison of Two Joint Probability Distributions: γa and γb&lt;/i>
&lt;/p>
&lt;p>First case (γa) :&lt;/p>
&lt;ul>
&lt;li>δ&lt;sub>(0,0)&lt;/sub>: Neither group receives the information (50%).&lt;/li>
&lt;li>δ&lt;sub>(1,1)&lt;/sub>: Both groups receive the information simultaneously (50%).&lt;/li>
&lt;/ul>
&lt;p>This configuration is fair because if one group does not receive the information, the other does not either.&lt;/p>
&lt;p>Second case (γb) :&lt;/p>
&lt;ul>
&lt;li>δ&lt;sub>(0,0)&lt;/sub>: Neither group receives the information (25%).&lt;/li>
&lt;li>δ&lt;sub>(1,1)&lt;/sub>: Both groups receive the information simultaneously (25%).&lt;/li>
&lt;li>δ&lt;sub>(0,1)&lt;/sub>: Group 1 does not receive the information, but Group 2 does (25%).&lt;/li>
&lt;li>δ&lt;sub>(1,0)&lt;/sub>: Group 1 receives the information, but Group 2 does not (25%).&lt;/li>
&lt;/ul>
&lt;p>In this case, there is a 50% chance that only one group will receive the information, creating an inequality. However, the marginal averages remain the same (µ&lt;sub>1&lt;/sub>=µ&lt;sub>2&lt;/sub>=50%).&lt;/p>
&lt;p>The distributions γ&lt;sub>a&lt;/sub> and γ&lt;sub>b&lt;/sub> are very different, yet the marginal averages do not capture these nuances. This highlights the need for a new metric to evaluate fairness more precisely, beyond simple averages.&lt;/p>
&lt;h4 id="proposed-fairness-metric">&lt;strong>Proposed fairness metric&lt;/strong>&lt;/h4>
&lt;p>The idea is to start from a joint probability measure γ rather than the marginal averages μ&lt;sub>i&lt;/sub>. From γ, we aim to measure the distance between it and an ideal distribution γ*, where each group receives the same amount of information in all cases.&lt;/p>
&lt;p>To illustrate this on a figure:&lt;/p>
&lt;ul>
&lt;li>The x-axis represents the amount of information received by group 1.&lt;/li>
&lt;li>The y-axis represents the amount of information received by group 2.&lt;/li>
&lt;li>The diagonal signifies that both groups receive exactly the same amount of information, which corresponds to the ideal sought.&lt;/li>
&lt;/ul>
&lt;p>A movement perpendicular to the diagonal should be penalized, as it indicates an imbalance in the distribution of information between the groups. Similarly, we must penalize movement along the diagonal, as it affects efficiency. These costs are measured by Euclidean distance.
In the &lt;a href="#fig3">Figure 3&lt;/a> below, we observe the transition from a distribution γa to a distribution γb, broken down into two components. One is dashed, representing the efficiency, and the other is solid, representing the fairness.&lt;/p>
&lt;p align="center">
&lt;a name="fig3">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure3.png" alt="figure3">
&lt;br>
&lt;i>Figure 3 : Representation of the transport cost between two points (x1,x2) and(y1,y2) with an intermediate point z(x1,x2,y1,y2)&lt;/i>
&lt;/p>
&lt;p>To define this cost, we use the following formula:&lt;/p>
&lt;p align="center">
&lt;a name="fig4">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure4.png" alt="figure4">
&lt;br>
&lt;i>Figure 4 : Optimal Transport Cost Wcβ(γa,γb) Between Distributions γa and γb&lt;/i>
&lt;/p>
&lt;p>We can draw an analogy with a move, where the goal is to transport items from House A to House B at the lowest possible cost, while ensuring that each item reaches its final destination according to W&lt;sub>cβ&lt;/sub>(γ&lt;sub>a&lt;/sub>, γ&lt;sub>b&lt;/sub>).&lt;/p>
&lt;ul>
&lt;li>γ&lt;sub>a&lt;/sub>: represents the list of items present in House A, specifying where they are located and in what quantity.&lt;/li>
&lt;li>γ&lt;sub>b&lt;/sub>: represents the list of items expected in House B, indicating where they should arrive and in what quantity.&lt;/li>
&lt;li>c: This is the function that specifies the cost of moving an item from one location to another. In &lt;a href="#fig5">figure 5&lt;/a>, we evaluate the transport cost between the point (x1, x2) and (y1, y2). We take z(x1, x2, y1, y2) as an intermediate point in the movement of an item to simplify the calculations. The transport cost is then defined as follows:&lt;/li>
&lt;/ul>
&lt;p align="center">
&lt;a name="fig5">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure5.png" alt="figure5">
&lt;br>
&lt;i>Figure 5 : Cost Function cβ((x1,x2),(y1,y2)) Combining Fairness and Efficiency&lt;/i>
&lt;/p>
&lt;ul>
&lt;li>β: A coefficient used to weight the importance of fairness relative to efficiency.&lt;/li>
&lt;li>π: This is the &amp;ldquo;moving plan.&amp;rdquo; It indicates how many items are moved from each location in House A to each location in House B.&lt;/li>
&lt;/ul>
&lt;p>The goal is to find the plan π that minimizes the total moving cost while ensuring an equitable distribution of items between the destinations.&lt;/p>
&lt;p>To arrive at the final formula, we start with the W&lt;sub>cβ&lt;/sub> distance, which measures the transport cost between the current distribution γ and the ideal distribution γ*. Using the Euclidean norm to measure the distance between the points (x1, x2) and (y1, y2), we factor and generalize the terms to obtain the following equality:&lt;/p>
&lt;p align="center">
&lt;a name="fig6">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure6.png" alt="figure6">
&lt;br>
&lt;i>Figure 6 : β-Fairness Metric: Combining Fairness and Efficiency in Distribution γ&lt;/i>
&lt;/p>
&lt;p>This formula combines both fairness (measured by |x1 - x2|) and efficiency (measured by |x1 + x2 - 2|), weighted by the parameter β. The term max{1, 2 - 2β} ensures that the metric remains normalized between 0 and 1.&lt;/p>
&lt;h4 id="short-example">&lt;strong>Short example&lt;/strong>&lt;/h4>
&lt;p>To better understand the impact of this metric, let&amp;rsquo;s examine several concrete cases of information distribution. We will compare different distributions and see how the mutual fairness metric allows us to evaluate them, considering both fairness and efficiency.&lt;/p>
&lt;p>Consider the cases where γ = δ&lt;sub>(0,0)&lt;/sub> and γ* = δ&lt;sub>(1,1)&lt;/sub>. We also add an intermediate example γ&lt;sub>ex&lt;/sub> = δ&lt;sub>(0.8, 0.2)&lt;/sub>, where 80% of group 1 has access to the information, compared to only 20% for group 2.&lt;/p>
&lt;p>By taking β = 0.6, which slightly favors fairness, we obtain the following results:&lt;/p>
&lt;ul>
&lt;li>fairness(γ*) &amp;gt; fairness(γ), confirming that the information is better distributed when everyone receives it.&lt;/li>
&lt;li>fairness(γ&lt;sub>ex&lt;/sub>) ≈ fairness(γ), because although γ&lt;sub>ex&lt;/sub> is twice as efficient (|0.8 + 0.2 - 2| = 1 versus |0 + 0 - 2| = 2), its score is heavily penalized by the lack of fairness. Indeed, one group is significantly favored over the other, making it as unbalanced as γ.&lt;/li>
&lt;/ul>
&lt;p align="center">
&lt;a name="fig7">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure7.png" alt="figure7">
&lt;br>
&lt;i>Figure 7 : Fairness Scores for Different Distributions: γ, γex, and γ∗&lt;/i>
&lt;/p>
&lt;h3 id="3-metric-in-practice">&lt;a href="#fairness-evaluation">3. Metric in practice&lt;/a>&lt;/h3>
&lt;h4 id="mutual-fairness-in-practice">&lt;strong>Mutual fairness in practice&lt;/strong>&lt;/h4>
&lt;p>We apply this metric to different datasets, which include social networks and communities partitioned into two groups. To do this, we load the dataset as a graph (V, E) and select a seedset S of size varying between 2 and 90. To diffuse the information, we use a probability p in the interval [0,1]. Since the process is stochastic, we repeat the operation 1000 times.&lt;/p>
&lt;p>The results in &lt;a href="#fig8">Figure 8&lt;/a> illustrate how information propagation varies depending on the probability p. In &lt;a href="#fig8">Figure 8&lt;/a>(a), with p = 0.5, the information is disseminated in a way that is both efficient and equitable, with the groups being highly connected. The behavior is deterministic, with similar results in each iteration. In &lt;a href="#fig8">Figure 8&lt;/a>(b), when p is reduced to 0.1, the propagation remains equitable but becomes less efficient, reaching on average only 20% of each group instead of nearly 100% as in (a).&lt;/p>
&lt;p>In &lt;a href="#fig8">Figure 8&lt;/a>(c), the outcomes are highly random: depending on the iteration, either one group receives all the information or the other, highlighting an inequity that is overlooked by traditional metrics. Finally, &lt;a href="#fig8">Figure 8&lt;/a>(d) reveals a slight bias, where the variance of the distribution extends but does not remain centered on the diagonal. Although some metrics might consider this situation fair, mutual fairness provides more insight by evaluating fairness in each realization rather than just averaging it.&lt;/p>
&lt;p align="center">
&lt;a name="fig8">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure8.png" alt="figure8">
&lt;br>
&lt;i>Figure 8 : Outreach Probability Distributions for Different Propagation Scenarios&lt;/i>
&lt;/p>
&lt;h4 id="impact-of--β">&lt;strong>Impact of β&lt;/strong>&lt;/h4>
&lt;p>The results of the metric as a function of β are illustrated in Figure 9. Yellow indicates a low transport cost, while blue signals a deviation from the ideal.&lt;/p>
&lt;p>When β = 0 &lt;a href="#fig9">Figure 9&lt;/a>(a), only efficiency is considered: the information reaches the maximum number of people, but without regard for fairness (low transport cost in (1,1)). In contrast, with β = 1 (&lt;a href="#fig9">Figure 9&lt;/a>(d)), only fairness is optimized, resulting in a perfectly equitable distribution but with reduced efficiency because the point (1,1) is no longer the only ideal one.&lt;/p>
&lt;p>An optimal balance is achieved for β = 0.66 (&lt;a href="#fig9">Figure 9&lt;/a>(c)), where the transport cost is minimized at the top-right of the plane. The further one moves away from this point, the higher the cost (blue areas), indicating a reduced level of optimization.&lt;/p>
&lt;p>In summary, as β increases, fairness is prioritized, and getting closer to the diagonal reduces the transport cost, ensuring an equitable and efficient diffusion of information. Thus, the formula dynamically adjusts the trade-off between fairness and efficiency, with β acting as the weighting parameter that influences the distribution of the cost.&lt;/p>
&lt;p align="center">
&lt;a name="fig9">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure9.png" alt="figure9">
&lt;br>
&lt;i>Figure 9 : Impact of β on Outreach Probability Distributions: Balancing Fairness and Efficiency&lt;/i>
&lt;/p>
&lt;p>Now that we have our metric to determine if a distribution is fair and efficient, we want to focus on an algorithm to select the right seeds.&lt;/p>
&lt;h3 id="4-improving-fairness-with-s3d">&lt;a href="#s3d-algorithm">4. Improving Fairness with S3D&lt;/a>&lt;/h3>
&lt;h4 id="stochastic-seed-selection-descent-s3d">Stochastic Seed Selection Descent (S3D)&lt;/h4>
&lt;p>While Mutual Fairness provides a way to measure fairness, Influence Maximization (IM) algorithms still need a method to optimize it without sacrificing outreach. S3D addresses this by dynamically adjusting seed selection to ensure a balanced information spread across all communities.&lt;/p>
&lt;p>Instead of selecting influential nodes solely based on popularity, S3D explores alternative seed sets, optimizing both fairness and outreach through an iterative process:&lt;/p>
&lt;p>The algorithm follows these key steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Initial Seed Selection&lt;/strong>:&lt;br>
Influential nodes are chosen using traditional heuristics (e.g., degree centrality, community detection).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Exploration of Neighboring States&lt;/strong>:&lt;br>
The algorithm tests alternative seed sets by adding, swapping, or removing nodes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Fairness Evaluation&lt;/strong>:&lt;br>
Each set is scored using the β-Fairness metric, which balances fairness and efficiency.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Acceptance Criteria (Metropolis-Hastings Selection Rule)&lt;/strong>:&lt;br>
The new seed set (S&amp;rsquo;) is accepted with a probability defined as:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p align="center">
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure12.png" alt="figure12">
&lt;/p>
&lt;p>ensuring fairness-improving modifications are favored while maintaining some randomness.&lt;/p>
&lt;ol start="5">
&lt;li>&lt;strong>Convergence&lt;/strong>:&lt;br>
The process runs until the fairness score stabilizes, achieving an optimal trade-off between fairness and outreach.&lt;/li>
&lt;/ol>
&lt;h4 id="experiment">Experiment&lt;/h4>
&lt;p>To evaluate the effectiveness of S3D, we compare it against traditional influence maximization algorithms on real-world social networks.&lt;/p>
&lt;p>Dataset : Here we can take for example job-matching networks where nodes represent individuals looking for job opportunities and edges represent connections between people (e.g., same school, same company, LinkedIn network).&lt;/p>
&lt;ul>
&lt;li>Traditional IM methods (bas_d, bas_g) → Select highly connected influencers, ignoring fairness.&lt;/li>
&lt;li>Fairness-aware heuristics (hrt_d, hrt_g) → Older fairness-based methods with static rules.&lt;/li>
&lt;li>S3D (our approach) → Dynamically selects seeds to balance fairness and efficiency.&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>Fairness (Mutual Fairness Score) – Measures how equally information is distributed across different groups.&lt;/li>
&lt;li>Efficiency (Total Outreach) – Measures how many people receive job opportunities in total.&lt;/li>
&lt;/ol>
&lt;h4 id="does-s3d-improve-fairness">Does S3D Improve Fairness?&lt;/h4>
&lt;p>Imagine a job-matching platform that aims to spread job opportunities equally across university graduates and vocational school graduates.&lt;/p>
&lt;ul>
&lt;li>Traditional IM methods pick influencers mostly from highly connected elite universities, leaving out vocational school graduates.&lt;/li>
&lt;li>S3D Solution: By dynamically adjusting seed selection, S3D ensures that both communities receive job postings more equitably.&lt;/li>
&lt;/ul>
&lt;p align="center">
&lt;a name="fig10">&lt;/a>
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure10.png" alt="figure10">
&lt;br>
&lt;i>Figure 10 : S3D Improvement Across Datasets and Parameters&lt;/i>
&lt;/p>
&lt;ul>
&lt;li>S3D (red points) significantly improves fairness compared to label-blind methods (blue).&lt;/li>
&lt;li>The outreach distribution shifts towards the diagonal, meaning both groups receive information more equally.
Without S3D, certain communities miss job postings entirely in some scenarios.&lt;/li>
&lt;/ul>
&lt;h4 id="how-does-s3d-balance-fairness-and-efficiency">How Does S3D Balance Fairness and Efficiency?&lt;/h4>
&lt;p align="center">
&lt;img src="\images\Burton_Elmo-MARIN_BERTIN\images\figure11.png" alt="figure11">
&lt;/p>
&lt;ul>
&lt;li>S3D achieves the highest fairness scores (y-axis).&lt;/li>
&lt;li>Minimal efficiency loss (x-axis) → Proves that fairness gains do not come at a high cost.&lt;/li>
&lt;li>Insight: S3D is most useful in moderately connected networks (e.g., workplaces, schools).&lt;/li>
&lt;/ul>
&lt;h4 id="impact-of-s3d">Impact of S3D&lt;/h4>
&lt;p>S3D improves information dissemination by ensuring a more equitable distribution between different groups, without sacrificing efficiency. In our job-matching example, it allows graduates of professional schools to access the same opportunities as those from prestigious universities, thus leading to inequalities linked to the social network. Thanks to its adaptive approach, S3D stands out as an effective solution to correct the biases of traditional algorithms while maintaining a broad reach.&lt;/p>
&lt;h3 id="5-conclusion">&lt;a href="#conclusion">5. Conclusion&lt;/a>&lt;/h3>
&lt;p>As a conclusion, we can say that fairness in Social Influence Maximization is crucial to prevent systemic exclusion in information dissemination. Traditional methods often favor well-connected individuals, reinforcing existing inequalities. By introducing Mutual Fairness and the S3D algorithm, we provide a framework that balances fairness and efficiency, ensuring a more equitable outreach. Through our job-matching case study for example, we demonstrated that S3D significantly reduces bias while maintaining high influence spread. These results confirm that fairness-aware approaches can be both practical and impactful, making them essential for real-world applications such as hiring, education, or public awareness campaigns.&lt;/p>
&lt;h3 id="references">References&lt;/h3>
&lt;ol>
&lt;li>Chowdhary, S., et al. (2024). Fairness in Social Influence Maximization via Optimal Transport. &lt;em>NeurIPS 2024&lt;/em>. Available here &lt;a href="https://neurips.cc/virtual/2024/poster/94521">https://neurips.cc/virtual/2024/poster/94521&lt;/a>.&lt;/li>
&lt;/ol></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/fairness-in-social-influence-maximization-via-optimal-transport/</guid><pubDate>Sat, 15 Mar 2025 17:27:56 +0100</pubDate></item><item><title>Knowledge Distillation: Boosting Interpretability in Deep Learning Models</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/impact-knowledge-distillation-model-interpretability/</link><description>&lt;style TYPE="text/css">
code.has-jax {font:inherit;
font-size:100%;
background: inherit;
border: inherit;}
&lt;/style>
&lt;script TYPE="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\$','\$']],
skipTags: ['script','noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script TYPE="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">
&lt;/script>
&lt;!DOCTYPE html>
&lt;html lang="fr">
&lt;head>
&lt;meta charset="UTF-8">
&lt;meta name="viewport" content="width=device-width, initial-scale=1.0">
&lt;/head>
&lt;h1 style="font-size: 28px;">Interpretability, the hidden power of knowledge distillation&lt;/h1>
&lt;p>Published March 15, 2025&lt;/p>
&lt;div>
&lt;a href="https://github.com/BryanBradfo/responsible-ai-datascience-ipParis.github.io" class="btn" style="text-decoration: none; display: inline-block; padding: 8px 16px; background-color: #f1f1f1; border: 1px solid #ddd; border-radius: 4px; color: black;">Update on GitHub&lt;/a>
&lt;/div>
&lt;div style="display: flex; margin-top: 20px;">
&lt;div style="display: flex; align-items: center; margin-right: 20px;">
&lt;img src="./images/Bryan_Remi/bryan.jpeg" alt="Bryan Chen" style="width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;">
&lt;div>
&lt;a href="https://github.com/BryanBradfo" style="text-decoration: none; color: #0366d6;">bryanbradfo&lt;/a>
&lt;p style="margin: 0;">Bryan Chen&lt;/p>
&lt;/div>
&lt;/div>
&lt;div style="display: flex; align-items: center; margin-right: 20px;">
&lt;img src="./images/Bryan_Remi/remi.jpg" alt="Rémi Calvet" style="width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;">
&lt;div>
&lt;a href="https://github.com/RemiCSK" style="text-decoration: none; color: #0366d6;">remicsk&lt;/a>
&lt;p style="margin: 0;">Rémi Calvet&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Knowledge distillation is a powerful technique to transfer the knowledge from a large &amp;ldquo;teacher&amp;rdquo; model to a &amp;ldquo;student&amp;rdquo; model. While it&amp;rsquo;s commonly used to improve performance and reduce computational costs by compressing large models, this blog post explores a fascinating discovery: knowledge distillation can also enhance model interpretability. We&amp;rsquo;ll dive into the paper &lt;a href="https://arxiv.org/abs/2305.15734">On the Impact of Knowledge Distillation for Model Interpretability&amp;quot;&lt;/a> (ICML 2023) by H. Han et al., which sheds light on this novel perspective.&lt;/p>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/better_meme.png" alt="Introduction" style="width: 60%; max-width: 500px; height: auto;">
&lt;/p>
&lt;p>Interpretability in AI allows researchers, engineers, and decision-makers to trust and control machine learning models. Recent models show impressive performance on many different tasks and often rely on deep learning models. Unfortunately, deep learning models are also know for the difficulty to interprete them and understand how they come to a result wich can be problematic in highly sensitive applications like autonomous driving or healthcare. The article we present in this blog shows that knowledge distillation can improve the interpretability of deep learning models.&lt;/p>
&lt;p>When AI is a black box, you&amp;rsquo;re just hoping for the best. But when you understand it, you become unstoppable.&lt;/p>
&lt;h2 style="font-size: 21px; display: flex; align-items: center;"> 0. Table of Contents &lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#i-crash-course-on-knowledge-distillation">I. Crash Course on Knowledge Distillation and Label Smoothing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ii-defining-interpretability-through-network-dissection">II. Defining Interpretability Through Network Dissection&lt;/a>&lt;/li>
&lt;li>&lt;a href="#iii-logit-distillation-feature-distillation">III. Logit Distillation &amp;amp; Feature Distillation: A Powerful Duo for Interpretability&lt;/a>&lt;/li>
&lt;li>&lt;a href="#iv-why-knowledge-distillation-enhances-interpretability">IV. Why Knowledge Distillation Enhances Interpretability&lt;/a>&lt;/li>
&lt;li>&lt;a href="#v-experimental-results-and-reproduction">V. Experimental Results and Reproduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#vi-beyond-network-dissection-other-interpretability-metrics">VI. Beyond Network Dissection: Other Interpretability Metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#join-the-discussion">Join the Discussion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="i-crash-course-on-knowledge-distillation" style="font-size: 21px; display: flex; align-items: center;"> I. Crash Course on Knowledge Distillation and Label Smoothing &lt;/h2>
&lt;h3 id="what-is-knowledge-distillation">What is Knowledge Distillation?&lt;/h3>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/knowledge_distillation.png" alt="Knowledge Distillation Overview" style="width: 70%; max-width: 500px; height: auto;">
&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/pdf/1503.02531">Knowledge distillation (KD)&lt;/a> is a model compression technique introduced by Hinton et al. (2015) that transfers knowledge from a complex teacher model to a simpler student model. Unlike traditional training where models learn directly from hard labels (one-hot encodings), KD allows the student to learn from the teacher&amp;rsquo;s soft probability distributions.&lt;/p>
&lt;h3 id="the-key-mechanics-of-knowledge-distillation">The Key Mechanics of Knowledge Distillation&lt;/h3>
&lt;p>The standard KD loss function combines the standard cross-entropy loss with a distillation loss term:&lt;/p>
&lt;p>$$\mathcal{L}_{KD}=(1-\alpha)\mathrm{CE}(y,\sigma(z_s))+\alpha T^2 \mathrm{CE}(\sigma(z_t^T),\sigma(z_s^T))$$&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$z_s$ and $z_t$ are the logits from the student and teacher models&lt;/li>
&lt;li>$T$ is the temperature parameter that controls softening of probability distributions&lt;/li>
&lt;li>$z_s^T \mathrel{:}= \frac{z_s}{T}$ and $z_t^T \mathrel{:}= \frac{z_t}{T}$&lt;/li>
&lt;li>$\sigma$ is the softmax function&lt;/li>
&lt;li>$\sigma(z_s^T) \mathrel{:}= \frac{\exp(z_s^T)}{\sum_j \exp(z_j^T)}$ and $\sigma(z_t^T) \mathrel{:}= \frac{\exp(z_t^T)}{\sum_j \exp(z_j^T)}$&lt;/li>
&lt;li>$\mathrm{CE}$ is cross-entropy loss&lt;/li>
&lt;li>$\alpha$ balances the importance of each loss component.&lt;/li>
&lt;/ul>
&lt;p>The first part of the loss $(1-\alpha)\mathrm{CE}(y,\sigma(z_s))$ is to incitate the student model to learn from one hot encoded ground truth label.&lt;/p>
&lt;p>The second part of the loss $\alpha T^2 \mathrm{CE}(\sigma(z_t^T),\sigma(z_s^T))$ is to incitate the student model to try to reproduce the ouputs of the teacher model. This is what permits the student to learn from the teacher.
The larger $\alpha$ is, the more the student will try to replicate the teacher model&amp;rsquo;s outputs and ignore the one hot encoded groundtruth and vice versa.&lt;/p>
&lt;h3 id="label-smoothing">Label Smoothing&lt;/h3>
&lt;p>Label smoothing (LS) is another technique that smooths hard targets by mixing them with a uniform distribution. In the cross entropy loss we replace the one hot encoded $y$ by $y_{LS} \mathrel{:}= (1-\alpha)y + \frac{\alpha}{K}$, where $K$ is the number of classes and $\alpha$ the smoothing parameter:&lt;/p>
&lt;script type="math/tex; mode=display">
\begin{align}
CE(y_{LS},\sigma(z)) &amp;= - \sum_{i=1}^{K} \left( (1 - \alpha) y_i + \frac{\alpha}{K} \right) \log \sigma(z_i) \\
&amp;= -(1 - \alpha) \sum_{i=1}^{K} y_i \log \sigma(z_i) - \alpha \sum_{i=1}^{K} \frac{1}{K}\log \sigma(z_i) \\
\end{align}
&lt;/script>
&lt;p>We obtain a loss that is similar to knowledge diffusion but there is a key difference important for interpretability that we will discuss later.
From the equation above, we get the label smoothing loss equation:
$$L_{LS} = (1-\alpha)\mathrm{CE}(y,\sigma(z)) + \alpha\mathrm{CE}(u,\sigma(z)) $$
Where $u$ is a uniform distribution over all the possible $K$ classes.&lt;/p>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/label_smoothing.jpg" alt="Label Smoothing" width="700">
&lt;/p>
&lt;h2 id="ii-defining-interpretability-through-network-dissection" style="font-size: 21px; display: flex; align-items: center;"> II. Defining Interpretability Through Network Dissection &lt;/h2>
&lt;p>The first thing to know is that there are different approaches to define and measure interpretability in machine learning.&lt;/p>
&lt;p>For image classification, the authors use &lt;a href="https://arxiv.org/pdf/1711.05611v2">network dissection&lt;/a> to quantitatively measure interpretability. The idea is to compare activation maps and see if areas with high activation correspond to an object or a meaningful concept on the image.&lt;/p>
&lt;p>The process can be better understood through the following illustration:&lt;/p>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/network_dissection.png" alt="Network Dissection Process" width="700">
&lt;/p>
&lt;p>Feed a neural network model an image, pick a deep layer and count the number of neurons that detects a concept like &amp;ldquo;cat&amp;rdquo; or &amp;ldquo;dog&amp;rdquo;.
We call those neurons concept detectors and will define them more precisely. The &lt;strong>number of concept detectors will be the primary metric to define the interpretability of a model&lt;/strong>, the higher the more we will consider it interpretable.&lt;/p>
&lt;p>&lt;strong>The easiest way to understand what is a concept detector is to look at the following pseudo code to compute the number of concept detectors:&lt;/strong>&lt;/p>
&lt;style>
body {
font-family: 'Arial', sans-serif;
line-height: 1.6;
}
.steps-container {
background: #f8f9fa;
border-left: 5px solid #007bff;
padding: 15px 20px;
margin: 20px 0;
border-radius: 5px;
}
.step {
font-weight: bold;
color: #007bff;
margin-top: 15px;
}
.math-expression {
font-family: 'Courier New', Courier, monospace;
background: #e9ecef;
padding: 5px;
border-radius: 3px;
}
.important {
background: #fff3cd;
color: #856404;
padding: 10px;
border-left: 4px solid #ffc107;
border-radius: 3px;
margin: 10px 0;
}
&lt;/style>
&lt;div class="steps-container">
&lt;h3 id="1-selecting-the-layer">&lt;span class="step">1. Selecting the Layer&lt;/span>&lt;/h3>
&lt;p>First, we need to choose a layer $\mathcal{l}$ to &lt;strong>dissect&lt;/strong>, typically deep in the network.&lt;/p>
&lt;h3 id="2-processing-each-image">&lt;span class="step">2. Processing Each Image&lt;/span>&lt;/h3>
&lt;p>For each image &lt;strong>x&lt;/strong> in the dataset:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Feedforward Pass&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Input an image &lt;strong>x&lt;/strong> of shape $ (n,n) $ into the neural network.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Activation Extraction&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>For each neuron in layer $\mathcal{l}$, collect the activation maps:
&lt;div class="math-expression">
\[ A_i(x) \in \mathbb{R}^{d \times d}, \quad \text{where } d &lt; n \text{ and } i \text{ is the neuron index.} \]
&lt;/div>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="3-defining-activation-distribution">&lt;span class="step">3. Defining Activation Distribution&lt;/span>&lt;/h3>
&lt;p>For each neuron &lt;strong>i&lt;/strong> in the layer $\mathcal{l}$:&lt;/p>
&lt;ul>
&lt;li>Define &lt;strong>a&lt;sub>i&lt;/sub>&lt;/strong> as the empirical distribution of activation values across different images &lt;strong>x&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;h3 id="4-computing-activation-threshold">&lt;span class="step">4. Computing Activation Threshold&lt;/span>&lt;/h3>
&lt;ul>
&lt;li>Compute a threshold &lt;strong>T&lt;sub>i&lt;/sub>&lt;/strong> such that:
&lt;div class="math-expression">
\[ P(a_i \geq T_i) = 0.005 \]
&lt;/div>
- This ensures only the **top 0.5%** activations are retained.
&lt;/li>
&lt;/ul>
&lt;h3 id="5-resizing-activation-maps">&lt;span class="step">5. Resizing Activation Maps&lt;/span>&lt;/h3>
&lt;ul>
&lt;li>Interpolate &lt;strong>A&lt;sub>i&lt;/sub>&lt;/strong> to match the dimension $ (n,n) $ for direct comparison with input images.&lt;/li>
&lt;/ul>
&lt;h3 id="6-creating-binary-masks">&lt;span class="step">6. Creating Binary Masks&lt;/span>&lt;/h3>
&lt;p>For each image &lt;strong>x&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Generating Activation Masks&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Create a &lt;strong>binary mask&lt;/strong> $ A_i^{\text{mask}}(x) $ of shape $ (n,n) $:
&lt;div class="math-expression">
\[ A_i^{\text{mask}}(x)[j,k] = \begin{cases} 1, &amp; \text{if } A_i(x)[j,k] \geq T_i \\ 0, &amp; \text{otherwise} \end{cases} \]
&lt;/div>
&lt;/li>
&lt;li>This retains only the highest activations.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Using Ground Truth Masks&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Given a &lt;strong>ground truth mask&lt;/strong> $ M_c(x) $ of shape $ (n,n) $, where:
&lt;ul>
&lt;li>$ M_c(x)[j,k] = 1 $ if the pixel in &lt;strong>x&lt;/strong> belongs to class &lt;strong>c&lt;/strong>, otherwise &lt;strong>0&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Computing Intersection over Union (IoU)&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Calculate the IoU between &lt;strong>A&lt;sub>i&lt;/sub>&lt;sup>mask&lt;/sup>(x)&lt;/strong> and &lt;strong>M&lt;sub>c&lt;/sub>(x)&lt;/strong>:
&lt;div class="math-expression">
\[ \text{IoU}_{i,c} = \frac{|A_i^{\text{mask}}(x) \cap M_c(x)|}{|A_i^{\text{mask}}(x) \cup M_c(x)|} \]
&lt;/div>
&lt;/li>
&lt;li>If $\text{IoU}_{i,c} &amp;gt; 0.05$, the neuron &lt;strong>i&lt;/strong> is considered a &lt;strong>concept detector&lt;/strong> for concept &lt;strong>c&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;h3 id="if-you-prefer-to-understand-with-code-here-is-an-implementation-of-the-procedure-described-above">If you prefer to understand with code, here is an implementation of the procedure described above:&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">identify_concept_detectors&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">layer_name&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">concept_masks&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> Identify neurons that act as concept detectors in a specific layer.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> model: Neural network model
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> layer_name: Name of the layer to analyze
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> dataset: Dataset with images
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> concept_masks: Dictionary mapping images to concept segmentation masks
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> Returns:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> Dictionary mapping neurons to detected concepts
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Step 1: Collect activation maps for each image&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">activation_maps&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">image&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Forward pass and extract activation at specified layer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">activations&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">get_layer_activation&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">layer_name&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">image&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">activation&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">enumerate&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">activations&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">neuron_idx&lt;/span> &lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">activation_maps&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">activation_maps&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">activation_maps&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">]&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">append&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">activation&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Step 2: Compute threshold for top 0.5% activations for each neuron&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">thresholds&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">activations&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">activation_maps&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">items&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Flatten all activations for this neuron&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">all_activations&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">cat&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#111">act&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">flatten&lt;/span>&lt;span style="color:#111">()&lt;/span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">act&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">activations&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Compute threshold for top 0.5%&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">threshold&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">quantile&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">all_activations&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.995&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">thresholds&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">threshold&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Step 3: Create binary masks and compute IoU with concept masks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">concept_detectors&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">image_idx&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">image&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">enumerate&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">image_concepts&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">concept_masks&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">image_idx&lt;/span>&lt;span style="color:#111">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">activations&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">activation_maps&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">items&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Get activation for this neuron on this image&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">activation&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">activations&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">image_idx&lt;/span>&lt;span style="color:#111">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Create binary mask using threshold&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">binary_mask&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">activation&lt;/span> &lt;span style="color:#f92672">&amp;gt;=&lt;/span> &lt;span style="color:#111">thresholds&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">])&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">float&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Resize to match image dimensions&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">binary_mask&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">F&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">interpolate&lt;/span>&lt;span style="color:#111">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">binary_mask&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">unsqueeze&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">unsqueeze&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">image&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">shape&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">:],&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">mode&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#d88200">&amp;#39;bilinear&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">)&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">squeeze&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Compute IoU with each concept mask&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">concept&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">mask&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">image_concepts&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">items&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">intersection&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sum&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">binary_mask&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#111">mask&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">union&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sum&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">binary_mask&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sum&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">mask&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#111">intersection&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">iou&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">intersection&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#111">union&lt;/span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">union&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#00a8c8">else&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># If IoU exceeds threshold (typically 0.05), consider it a detector&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">iou&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">0.05&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">neuron_idx&lt;/span> &lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">concept_detectors&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">concept_detectors&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">set&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">concept_detectors&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">neuron_idx&lt;/span>&lt;span style="color:#111">]&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">add&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">concept&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#111">concept_detectors&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="iii-logit-distillation-feature-distillation" style="font-size: 21px; display: flex; align-items: center;"> III. Logit Distillation &amp; Feature Distillation: A Powerful Duo for Interpretability &lt;/h2>
&lt;p>Combining logit distillation with feature distillation not only boosts performance but also enhances the interpretability of student models. This improvement is measured by an increase in the number of concept detectors, which represent units aligned with human-interpretable concepts.&lt;/p>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/feature_logit_distillation.png" alt="Feature_Logit_Distillation" width="700">
&lt;/p>
&lt;p>where Attention Transfer (AT), Factor Transfer (FT), Contrastive Representation Distillation (CRD), and Self-Supervised Knowledge Distillation (SSKD) are all variations of knowledge distillation techniques, each designed to transfer knowledge from teacher models to student models in unique ways.&lt;/p>
&lt;h3 id="how-they-work-together">How they work together?&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Logit Distillation:&lt;/strong>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Transfers class-similarity information from the teacher to the student through softened logits.&lt;/li>
&lt;li>Helps the student model understand the relationships between semantically similar classes, making activation maps more object-centric.&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>&lt;strong>Feature Distillation:&lt;/strong>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Focuses on aligning intermediate layer features between the teacher and student.&lt;/li>
&lt;li>Improves the student model&amp;rsquo;s ability to replicate the teacher’s feature representations, supporting richer internal representations.&lt;/li>
&lt;/ul>
&lt;h2 id="iv-why-knowledge-distillation-enhances-interpretability" style="font-size: 21px; display: flex; align-items: center;"> IV. Why Knowledge Distillation Enhances Interpretability &lt;/h2>
&lt;p>The key insight from the paper is that knowledge distillation transfers not just the ability to classify correctly, but also class-similarity information that makes the model focus on more interpretable features.&lt;/p>
&lt;h3 id="transfer-of-class-similarities">Transfer of Class Similarities&lt;/h3>
&lt;p>When a teacher model sees an image of a dog, it might assign:&lt;/p>
&lt;ul>
&lt;li>85% probability to &amp;ldquo;Golden Retriever&amp;rdquo;&lt;/li>
&lt;li>10% probability to other dog breeds&lt;/li>
&lt;li>5% probability to other animals and objects&lt;/li>
&lt;/ul>
&lt;p>These &amp;ldquo;soft targets&amp;rdquo; (consequence of logit distillation) encode rich hierarchical information about how classes relate. The student model distilling this knowledge learns to focus on features that are common to similar classes (e.g., general &amp;ldquo;dog&amp;rdquo; features).&lt;/p>
&lt;h3 id="label-smoothing-vs-knowledge-distillation">Label Smoothing vs. Knowledge Distillation&lt;/h3>
&lt;p>By looking at the KD and label smoothing losses, we can see that they are similar. When $T=1$ they only differ in the second member where we have a $\sigma(z_t^T)$ that contains class-similarity information instead of $u$ that doesn&amp;rsquo;t contain any information.&lt;/p>
&lt;ul>
&lt;li>$\mathcal{L}_{KD}=(1-\alpha)\mathrm{CE}(y,\sigma(z_s))+\alpha T^2 \mathrm{CE}(\sigma(z_t^T),\sigma(z_s^T))$&lt;/li>
&lt;li>$L_{LS} = (1-\alpha)\mathrm{CE}(y,\sigma(z)) + \alpha\mathrm{CE}(u,\sigma(z)) $&lt;/li>
&lt;/ul>
&lt;p>So, if there is a difference in interpretability, it is likely that it comes from the fact that distillation permits to get class similarity knowledge from the teacher model. This is exactly what is shown in the figure below. Knowledge distillation guides student models to focus on more object-centric features rather than background or contextual features. This results in activation maps that better align with the actual objects in images.&lt;/p>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/comparisons_dog.png" alt="ObjectCentricActivation" width="700">
&lt;/p>
&lt;p>The next figure also highlights the loss of interpretability (less concept detectors) when using label smoothing and the improvement of interpretability (more concept detectors) for KD:&lt;/p>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/NbConceptDetDiffModels.png" alt="KD vs LS Distributions" width="600">
&lt;/p>
&lt;p>While label smoothing can improve accuracy, it often reduces interpretability by erasing valuable class relationships while KD keeps class relationship information and improves both accuracy and interpretability.&lt;/p>
&lt;h2 id="v-experimental-results-and-reproduction" style="font-size: 21px; display: flex; align-items: center;"> V. Experimental Results and Reproduction &lt;/h2>
&lt;p>Let&amp;rsquo;s implement a reproduction of one of the paper&amp;rsquo;s key experiments to see knowledge distillation&amp;rsquo;s effect on interpretability in action.&lt;/p>
&lt;h3 id="setting-up-the-experiment">Setting Up the Experiment&lt;/h3>
&lt;p>We are going to replicate the experiment by using the &lt;a href="https://github.com/Rok07/KD_XAI">GitHub repository provided by the authors&lt;/a>. The repository contains the code to train the models, compute the concept detectors, and evaluate the interpretability of the models.&lt;/p>
&lt;p>As it is often the case with a machine learning paper, running the code to reproduce results requires some struggle.
To reproduce the results, you could use a virtual environment (e.g. &lt;a href="https://datalab.sspcloud.fr/">SSP Cloud Datalab&lt;/a>) and then do the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git clone https://github.com/Rok07/KD_XAI.git
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">cd&lt;/span> torchdistill
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install -e .
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">cd&lt;/span> ..
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash script/dlbroden.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nano torchdistill/torchdistill/models/custom/bottleneck/__init__.py
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~ comment the first line
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install opencv-python
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install imageio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo apt update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo apt install -y libgl1-mesa-glx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nano util/vecquantile.py
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~ change NaN by nan
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nano loader/data_loader.py
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~ add out&lt;span style="color:#f92672">[&lt;/span>i&lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> rgb&lt;span style="color:#f92672">[&lt;/span>:,:,0&lt;span style="color:#f92672">]&lt;/span> + &lt;span style="color:#f92672">(&lt;/span>rgb&lt;span style="color:#f92672">[&lt;/span>:,:,1&lt;span style="color:#f92672">]&lt;/span>.astype&lt;span style="color:#f92672">(&lt;/span>np.uint16&lt;span style="color:#f92672">)&lt;/span> * 256&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">cd&lt;/span> ..
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nano settings.py
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~ change &lt;span style="color:#111">TEST_MODE&lt;/span> &lt;span style="color:#f92672">=&lt;/span> False to True
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">cd&lt;/span> dataset/broden1_224
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp index.csv index_sm.csv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~ keep the &lt;span style="color:#ae81ff">4000&lt;/span> first lines
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">cd&lt;/span> ../..
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nano visualize/bargraph.py
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~ change parameter threshold of bar_graph_svg&lt;span style="color:#f92672">()&lt;/span> to 0.001
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python main.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Network Dissection quantifies the interpretability of hidden units by measuring their alignment with human-interpretable concepts. The following results reveal several interesting findings:&lt;/p>
&lt;h4 id="1-concept-distribution-from-bargraphsvg">1. Concept Distribution (from bargraph.svg):&lt;/h4>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/class_distribution.png" alt="Class Distribution" width="600">
&lt;/p>
&lt;ul>
&lt;li>~6 units detecting object concepts&lt;/li>
&lt;li>~2 units detecting scene concepts&lt;/li>
&lt;li>1 unit detecting material properties&lt;/li>
&lt;li>~13 units detecting textures&lt;/li>
&lt;li>~6 units detecting colors&lt;/li>
&lt;/ul>
&lt;h4 id="2-specific-units-layer4-0xxxjpg">2. Specific Units: (layer4-0xxx.jpg)&lt;/h4>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/unit_grid.png" alt="Unit Grid" width="1600">
&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Unit 330&lt;/strong> has specialized in detecting grid and regular pattern textures&lt;/li>
&lt;/ul>
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/unit_sky.png" alt="Unit Sky" width="1600">
&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Unit 202&lt;/strong> detects sky regions in images&lt;/li>
&lt;/ul>
&lt;p>The network dissection approach reveals interpretable neurons of a distilled ResNet18.&lt;/p>
&lt;h2 id="vi-beyond-network-dissection-other-interpretability-metrics" style="font-size: 21px; display: flex; align-items: center;"> VI. Beyond Network Dissection: Other Interpretability Metrics &lt;/h2>
&lt;p>While the paper emphasizes the use of &lt;strong>Network Dissection&lt;/strong> to measure model interpretability by quantifying concept detectors, it also explores several additional metrics to confirm the broader impact of &lt;strong>Knowledge Distillation (KD)&lt;/strong> on interpretability:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://arxiv.org/pdf/2009.02899">Five-Band Scores&lt;/a>, proposed by Tjoah &amp; Guan (2020):&lt;/strong> This metric assesses interpretability by evaluating pixel accuracy (accuracy of saliency maps in identifying critical features), precision (how well the saliency maps match the actual distinguishing features), recall, and false positive rates (FPR, lower FPR indicates better interpretability) using a synthesized dataset with heatmap ground truths. KD-trained models consistently show higher accuracy and lower FPR compared to other methods.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://arxiv.org/pdf/2102.12781">DiffROAR Scores&lt;/a>, proposed by Shah et al. (2021):&lt;/strong> This evaluates the difference in predictive power on a model trained on a dataset and a model trained on a version of the dataset where we removed top and bottom x% of the pixel according to their importance for the task. The authors find that KD has a higher DiffROAR score than a model trained from scratch. It means that KD makes the model use more relevant features and thus more interpretable in that sense.
&lt;li>&lt;strong>Loss Gradient Alignment:&lt;/strong> This metric measures the alignment of model gradients with human-perceived important features. KD models exhibit better alignment, indicating greater interpretability as we can see on this figure:
&lt;p align="center">
&lt;img src="./images/Bryan_Remi/gradient_interpre.png" alt="ObjectCentricActivation" width="700">
&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>
&lt;p>These metrics collectively show that KD can enhance interpretability. The consistent results showing that knowledge distillation can enhance interpretability for different metrics of interpretability provide strong arguments to believe that KD could be broadly used for better interpretability of deep learning models. &lt;/p>
&lt;h2 id="conclusion" style="font-size: 21px; display: flex; align-items: center;"> Conclusion &lt;/h2>
&lt;p align="center"> &lt;img src="./images/Bryan_Remi/pinguins_studying.gif" alt="Feeling strong with interpretable AI" style="width: 30%; max-width: 500px; height: auto;"> &lt;/p>
&lt;p>The article showed that knowledge distillation can improve both accuracy and interpretability. They attribute the improvement in interpretability to the transfer of class similarity knowledge from the teacher to the student model. They compare label smoothing (LS) that is similar to KD but LS does not benefit from class-similarity information. The empirical experiments shows better accuracy for LS and KD but the interpretability of LS decreases whereas it increases for KD confirming the hypothesis that class similarity knowledge has a role in interpretability. The authors obtain consistent results when using other metrics than the number of concept detectors for interpretability showing that their approach is robust to different definitions of interpretability.&lt;/p>
&lt;p>Those encouraging results could lead to applications of knowledge distillation to improve the interpretability of deep learning models in highly sensitive areas like autonomous systems and healthcare.&lt;/p>
&lt;h2 id="join-the-discussion" style="font-size: 21px; display: flex; align-items: center;"> Join the Discussion &lt;/h2>
&lt;p>We’d love to hear your thoughts! What are your experiences with Knowledge Distillation (KD)? Have you found it to improve not just performance but also interpretability in your projects? Feel free to share your ideas, questions, or insights in the comments section or engage with us on &lt;a href="https://github.com/BryanBradfo/responsible-ai-datascience-ipParis.github.io">GitHub&lt;/a>!&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>Hinton, G., Vinyals, O., &amp;amp; Dean, J. (2015). &lt;a href="https://arxiv.org/abs/1503.02531">Distilling the knowledge in a neural network.&lt;/a> arXiv:1503.02531.&lt;/li>
&lt;li>Han, H., Kim, S., Choi, H.-S., &amp;amp; Yoon, S. (2023). &lt;a href="https://arxiv.org/pdf/2305.15734">On the Impact of Knowledge Distillation for Model Interpretability.&lt;/a> arXiv:2305.15734.&lt;/li>
&lt;li>Bau, D., Zhou, B., Khosla, A., Oliva, A., &amp;amp; Torralba, A. (2017). &lt;a href="https://arxiv.org/pdf/1704.05796">Network dissection: Quantifying interpretability of deep visual representations.&lt;/a> arXiv:1704.05796.&lt;/li>
&lt;li>Tjoa, E., &amp;amp; Guan, M. Y. (2020). &lt;a href="https://arxiv.org/pdf/2009.02899"> Quantifying explainability of saliency methods in deep neural networks.&lt;/a> arXiv:2009.02899.&lt;/li>
&lt;li>Shah, H., Jain, P., &amp;amp; Netrapalli, P. (2021). &lt;a href="https://arxiv.org/pdf/2102.12781">Do input gradients highlight discriminative features?&lt;/a> arXiv:2102.12781, NeurIPS 2021.&lt;/li>
&lt;/ul>
&lt;/html></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/impact-knowledge-distillation-model-interpretability/</guid><pubDate>Sat, 15 Mar 2025 12:16:21 +0100</pubDate></item><item><title>Understanding Visual Feature Reliance Through the Lens of Complexity</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/understanding_visual_feature_reliance_through_the_lens_of_complexity/</link><description>&lt;hr>&lt;/hr>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\$','\$']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h1 style="font-size: 36px;">Understanding Visual Feature Reliance through the Lens of Complexity&lt;/h1>
&lt;p>&lt;strong>Authors: DIB Caren, SABA Jean Paul, WANG Romain&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Article: &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/819977c0a95458911bbfd9e5b5115018-Paper-Conference.pdf">Understanding Visual Feature Reliance through the Lens of Complexity&lt;/a>&lt;/strong>&lt;/p>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#feature-extraction">Feature Extraction via Dictionary Learning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#detecting-complexity">Detecting Complexity&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#redundancy">Relations with Redundancy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#robustness">Relations with Robustness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#importance">Importance Measure&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#replication">Feature Flow and Information Theory&lt;/a>&lt;/li>
&lt;li>&lt;a href="#experiment">Experiment Summary: Exploring Feature Complexity in ResNet18&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#experiment_1">How the Code Was Structured and What Was Done&lt;/a>&lt;/li>
&lt;li>&lt;a href="#experiment_2">Experiment Results and Interpretation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h1 id="introduction">1. Introduction&lt;/h1>
&lt;p>In this blog, we’ll take a deep dive into an insightful and thought-provoking paper authored by Thomas Fel, Louis Béthune, Andrew Kyle Lampinen, Thomas Serre, and Katherine Hermann. Their work explores the intricate mechanisms underlying how deep neural networks—specifically ResNet50—learn and represent complex features. This research, rooted in both theoretical and empirical analysis, investigates the nature of feature complexity, how features evolve over the course of training, and the computational structures that enable neural networks to generalize effectively.&lt;/p>
&lt;p>The authors&amp;rsquo; central motivation is to understand how models balance computational efficiency with representational richness. They explore why deep networks exhibit a preference for simpler features (simplicity bias), how complex features are supported within a network, and the trade-offs between redundancy, robustness, and importance of these features. V-information serves as the main complexity metric used throughout their study, offering a principled approach to quantifying how computationally accessible features are. In addition to V-information, they employ several other quantitative measures—such as importance scores derived from Gradient × Input, and redundancy and robustness metrics informed by prior work—to provide an exhaustive and structured analysis of feature learning dynamics.&lt;/p>
&lt;p>Their findings have implications for model interpretability, robustness, and generalization, offering deep insights into the practical and theoretical aspects of modern deep learning systems. In this blog, we break down their study into a comprehensive guide for easier understanding.&lt;/p>
&lt;hr>
&lt;h1 id="feature-extraction">2. Feature Extraction and Visualization&lt;/h1>
&lt;p>In this section, the authors explore the nature and diversity of features learned by the network. First, some general information:&lt;/p>
&lt;p>&lt;strong>Simple Features&lt;/strong>: These are easy-to-decode, frequently occurring concepts like &lt;em>sky&lt;/em>, &lt;em>grass&lt;/em>, and &lt;em>watermarks&lt;/em>. They typically emerge early in the network and are transported through residual connections with little modification. Such features are aligned with simplicity bias and often serve as shortcuts for the model.&lt;/p>
&lt;p>&lt;strong>Medium Complexity Features&lt;/strong>: These include concepts like &lt;em>human-related elements&lt;/em>, &lt;em>low-pixel quality detectors&lt;/em>, and &lt;em>trademarks&lt;/em>. They often represent slightly more abstract or nuanced properties and require more layers and computational effort to emerge.&lt;/p>
&lt;p>&lt;strong>Complex Features&lt;/strong>: Highly intricate concepts like &lt;em>insect legs&lt;/em>, &lt;em>whiskers&lt;/em>, and &lt;em>filament structures&lt;/em> represent the most complex features. These require extensive processing across multiple layers and involve both the main and residual network branches to form progressively.&lt;/p>
&lt;p>To be able to extract those features, the authors introduced an overcomplete dictionary as a solution to a key challenge in understanding deep neural networks: &lt;strong>the superposition problem&lt;/strong>, where multiple features are entangled within single neurons, making it difficult to isolate and analyze individual features. In standard neural networks, activations $f_n(x)$ in the penultimate layer represent complex, often overlapping features, and the number of distinct features may far exceed the number of neurons $|A_\ell|$. To address this, the authors leveraged dictionary learning to build an overcomplete dictionary $D^*$, where $k \gg |A_\ell|$, with $k$ representing the number of atoms (or basis elements) in the dictionary. This allowed them to extract a richer set of disentangled features—up to 10,000, far more than the neuron count.&lt;/p>
&lt;p>Each activation $f_n(x)$ is approximated as a linear combination of atoms from the overcomplete dictionary $D^*$, weighted by sparse coefficients $z$:&lt;/p>
&lt;p>$$
f_n(x) \approx z D^* = \sum_{i=1}^k z_i d_i
$$&lt;/p>
&lt;p>This overcomplete setup allows for disentangling features beyond what individual neurons can represent. The dictionary $D^*$ is learned using Non-Negative Matrix Factorization (NMF), which aligns with the non-negative nature of ReLU activations. The optimization minimizes reconstruction error with non-negativity constraints:&lt;/p>
&lt;p>$$
(Z, D^*) = \arg \min_{Z \ge 0, D^* \ge 0} | f_n(X) - Z D^* |_F
$$&lt;/p>
&lt;p>Trained on ImageNet with 58 million samples, the dictionary preserves over 99% of the model’s predictive accuracy. Once $D^*$ is fixed, features for new inputs are extracted by solving:&lt;/p>
&lt;p>$$
z = \arg \min_{z \ge 0} | f_n(x) - z D^* |_F
$$&lt;/p>
&lt;p>The authors used MACO, an advanced feature visualization technique, to produce clearer and more realistic images of the network’s learned features. These visualizations are sorted by complexity, from the simplest to the most complex features, highlighting the increasing detail and intricacy as complexity grows.&lt;/p>
&lt;p>&lt;img
src="./images/JeanPaul_Saba/VIZ.png"
alt="Meta-Feature Visualization"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;em>Caption: Feature Viz using MACO&lt;/em>&lt;/p>
&lt;p>&lt;img
src="./images/JeanPaul_Saba/UMAP.png"
alt="UMAP Visualization"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;em>Caption: Feature complexity using UMAP&lt;/em>&lt;/p>
&lt;hr>
&lt;h1 id="detecting-complexity">3. Detecting Complexity&lt;/h1>
&lt;p>The authors propose &lt;strong>V-information&lt;/strong> as their primary metric to quantify feature complexity. They focus on a setting where the predictive family $V$ consists of linear classifiers with Gaussian posteriors. In this context, V-information measures how much information a representation $x$ provides about a feature $z$ under computational constraints. The authors derive a closed-form solution for V-information when $V$ consists of these linear Gaussian models:&lt;/p>
&lt;p>$$
I_V(x \to z) = H_V(z) - H_V(z|x)
$$&lt;/p>
&lt;p>Here, $H_V(z)$ represents the V-entropy, which measures the uncertainty about $z$ when using the best possible predictor from the restricted family $V$. Similarly, $H_V(z|x)$ is the V-conditional entropy, which measures the remaining uncertainty about $z$ after observing $x$, again under the constraint of using predictors from $V$.&lt;/p>
&lt;p>Which leads to having:&lt;/p>
&lt;p>$$
0 \le I_V(x \to z) \le \text{Var}(z)
$$&lt;/p>
&lt;p>Since the input data are centered and scaled, $\text{Var}(z)$ is typically close to 1. The authors define feature complexity $K(z, x)$ as the inverse of the average V-information across network layers, quantifying how difficult it is to decode a feature as it propagates through the network:&lt;/p>
&lt;p>$$
K(z, x) = 1 - \frac{1}{n} \sum_{\ell=1}^{n} I_V(f_\ell(x) \to z)
$$&lt;/p>
&lt;p>They note that a higher $K(z, x)$ score indicates a more complex feature, harder to decode until later in the model. Empirically, they observed that $K(z, x)$ generally falls within $[0, 1]$, with 1 representing high complexity.&lt;/p>
&lt;h2 id="redundancy">3.1. Relations with Redundancy&lt;/h2>
&lt;p>To explore the relationship between complexity and redundancy, the authors employed a redundancy measure based on &lt;strong>Centered Kernel Alignment (CKA)&lt;/strong>. In their analysis, they compared the similarity between a feature $z$ and the network activations $f_n(X)$, both before and after masking parts of the activations. A &lt;strong>binary mask $m$&lt;/strong> was applied to the activations, where $m \in {0,1}^{|A_\ell|}$ selects which neurons remain active (1) and which are deactivated (0). If masking neurons didn’t change the similarity much, it meant the feature was redundant, spread over many neurons. If the similarity dropped a lot, the feature was localized, relying on fewer neurons. The redundancy score was calculated as:&lt;/p>
&lt;p>$$
\text{Redundancy} = \mathbb{E}_m \left[ \frac{CKA(f_n(X) \odot m, z)}{CKA(f_n(X), z)} \right]
$$&lt;/p>
&lt;p>Where:&lt;/p>
&lt;p>$$
\text{CKA}(A, B) = \frac{|K_A K_B|_F^2}{|K_A K_A|_F \cdot |K_B K_B|_F}
$$&lt;/p>
&lt;p>They found that &lt;strong>complex features are less redundant&lt;/strong>, meaning they depend on specific neurons and are more fragile.&lt;/p>
&lt;h2 id="robustness">3.2. Complexity and Robustness&lt;/h2>
&lt;p>The authors also investigated how feature complexity relates to robustness. They found that complex features are less robust, meaning they are more sensitive to input perturbations.&lt;/p>
&lt;p>They quantified robustness by measuring the variance in a feature’s response $z(x)$ when the input $x$ was perturbed with Gaussian noise. For each input, they generated perturbed versions:&lt;/p>
&lt;p>$$
\tilde{x} = x + \mathcal{N}(0, \sigma^2 I)
$$&lt;/p>
&lt;p>and computed the sensitivity score as the variance of $z(\tilde{x})$:&lt;/p>
&lt;p>$$
\text{Sensitivity}(z) = \text{Var}(z(\tilde{x}))
$$&lt;/p>
&lt;p>They tested this over 100 noise samples and three noise levels $\sigma \in {0.01, 0.1, 0.5}$ across 2,000 validation images. Regardless of the metric used (variance or range), complex features consistently showed higher sensitivity to noise. This suggests complex features are more fragile and less robust compared to simpler ones.&lt;/p>
&lt;h2 id="importance">3.3. Importance Measure&lt;/h2>
&lt;p>By trying to find a relation with the importance of the features, the authors focused on the penultimate layer of the network, where the extracted features $z$ are directly connected to the logits $y$. A logit is the raw output of the model before applying softmax to get class probabilities. It represents the model’s confidence for each class.&lt;/p>
&lt;p>To measure how much each feature $z_i$ influences the logit $y$, the authors used the Gradient × Input method. Specifically, they computed:&lt;/p>
&lt;p>$$
\Gamma(z_i) = \mathbb{E} \left[ \left| \frac{\partial y}{\partial z_i} \cdot z_i \right| \right]
$$&lt;/p>
&lt;p>This gives the importance score for each feature, showing how sensitive the model&amp;rsquo;s output is to changes in $z_i$. A higher score means the feature has a bigger impact on the prediction.&lt;/p>
&lt;p>The authors found that simple features often have higher importance scores. These features are more directly used by the network to make decisions. On the other hand, complex features tend to have lower direct importance but may still play &lt;strong>supporting roles&lt;/strong> in the model’s reasoning.&lt;/p>
&lt;hr>
&lt;h1 id="replication">4. Feature Flow and Information Theory&lt;/h1>
&lt;p>The authors validated their hypothesis on how simple and complex features propagate through neural networks by replicating their earlier analysis with a different complexity measure. While they initially used Centered Kernel Alignment (CKA), they later confirmed their findings with &lt;strong>V-Information&lt;/strong>, their primary complexity metric.
The results were consistent: simple features showed high V-Information early and were efficiently passed through residual connections, while complex features accumulated V-Information gradually, being constructed layer by layer. This replication, done with a different ResNet50 model (Keras) and on ImageNet validation data, reinforced the idea that complex features are built progressively rather than carried intact through the network.&lt;/p>
&lt;p>The authors then connected their empirical findings to concepts from algorithmic information theory, particularly &lt;strong>Kolmogorov Complexity&lt;/strong> and &lt;strong>Levin Complexity&lt;/strong>, to offer a theoretical foundation for their observations.&lt;/p>
&lt;p>&lt;strong>Kolmogorov Complexity&lt;/strong> measures the length of the shortest program capable of producing a given output sequence $(u_n)$ over some finite alphabet $\Sigma$:&lt;/p>
&lt;p>$$
K^{(\infty)}_L(u_n) = \min_{P(n) = u_n} |P|
$$&lt;/p>
&lt;p>Intuitively, sequences that follow simple patterns (like $1, 2, 3, 4, &amp;hellip;$) have low Kolmogorov complexity because they can be described by a short program. Conversely, truly random sequences have high complexity because they lack shorter descriptions. While Kolmogorov complexity captures an idealized notion of simplicity, it is not computable—there’s no general algorithm that can calculate it.&lt;/p>
&lt;p>To make this concept tractable, &lt;strong>Levin Complexity&lt;/strong> was introduced. Levin adds a penalty based on the runtime of the program, balancing the program’s length and the time it takes to run:&lt;/p>
&lt;p>$$
K^{(T)}_L(u_n) = \min_{P(n) = u_n} |P| + \log |\Sigma| \cdot T(P, n)
$$&lt;/p>
&lt;p>Here, $T(P, n)$ represents the time the program $P$ takes to compute $u_n$. This makes it possible to compute Levin complexity through Levin Universal Search, an algorithm that runs all programs of increasing length in parallel, for one step at a time, until one halts and produces the output:&lt;/p>
&lt;pre tabindex="0">&lt;code>Algorithm 1 : Levin Universal Search
Input: sequence (u_n) ∈ Σ*
Output: program P minimizing K(T)_L
1: S ← ∅
2: for i ∈ N do
3: for each program P ∈ (Σ^i ∩ L) do
4: S ← S ∪ {P}
5: for each P ∈ S in parallel do
6: Run P for exactly 1 step
7: if P halts on u_n then
8: return P
&lt;/code>&lt;/pre>&lt;p>This algorithm tends to find the simplest and fastest programs first, illustrating a simplicity bias: simpler solutions are discovered before more complex ones.&lt;/p>
&lt;p>The authors argue that deep learning models exhibit a similar simplicity bias. Neural networks are effectively programs composed of weights and computations. Features that are decoded early in the network—those requiring fewer layers and simpler computation—are analogous to low Kolmogorov or Levin complexity. Features that require deeper, more complex computations align with higher complexity measures.&lt;/p>
&lt;p>Their &lt;strong>V-Information&lt;/strong> metric formalizes this intuition. It quantifies how much useful information about a feature $z$ can be extracted from an input $x$ at different layers of the network:&lt;/p>
&lt;p>$$
I_V(x \rightarrow z) = H_V(z) - H_V(z | x)
$$&lt;/p>
&lt;p>A higher V-Information indicates a feature is easier to decode (simpler), while lower V-Information implies a feature is harder to access and thus more complex. This mirrors the relationship between program length and runtime in Kolmogorov and Levin complexities.&lt;/p>
&lt;hr>
&lt;h1 id="experiment">5. Experiment Summary: Exploring Feature Complexity in ResNet18&lt;/h1>
&lt;p>In this experiment, we set out to explore how complex and simple features are learned and represented in a deep convolutional neural network, specifically using &lt;strong>ResNet18&lt;/strong> trained on &lt;strong>CIFAR-10&lt;/strong>. The goal was to understand feature complexity, redundancy, robustness, and importance, similar to the methods and insights presented in the research paper.&lt;/p>
&lt;h2 id="experiment_1">5.1. How the Code Was Structured and What Was Done&lt;/h2>
&lt;p>In this experiment, we worked with the CIFAR-10 dataset, which contains 60,000 color images across 10 classes, such as airplanes, cars, and birds. We split the dataset into training and validation sets and normalized the images for consistent input.&lt;/p>
&lt;p>We trained a ResNet18 model from scratch on CIFAR-10 over 5 epochs using Stochastic Gradient Descent (SGD) and cross-entropy loss. The trained model achieved reasonable accuracy, making it suitable for analyzing feature representations.&lt;/p>
&lt;p>After training, we extracted features from the penultimate layer and pre-pooling feature maps, which capture high-level concepts learned by the network. To further analyze these features, we applied Non-Negative Matrix Factorization (NMF), creating an overcomplete dictionary that helped disentangle individual features and overcome the issue of feature superposition.&lt;/p>
&lt;p>We computed V-Information scores to measure the complexity of each feature, showing how difficult they are to decode at different layers of the network. In addition, we analyzed feature importance through logistic regression, assessed redundancy by examining feature correlations, and measured robustness by testing feature stability under noise perturbations.&lt;/p>
&lt;p>Finally, we visualized the features using UMAP, projecting them into 2D space to reveal clusters based on complexity and semantic similarity. HDBSCAN clustering helped identify groups of related features, offering insights into their complexity and roles in the network’s decision-making process.&lt;/p>
&lt;h2 id="experiment_2">5.2. Experiment Results and Interpretation&lt;/h2>
&lt;ol>
&lt;li>We plotted the features using &lt;strong>UMAP&lt;/strong> and colored them by their &lt;strong>complexity scores&lt;/strong> (based on V-Information).&lt;/li>
&lt;/ol>
&lt;p>&lt;img
src="./images/JeanPaul_Saba/experimentUMAPlabeled.png"
alt="Feature Complexity UMAP"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;em>Caption: UMAP representation of the Features&amp;rsquo; Complexity on CIFAR10 Dataset&lt;/em>&lt;/p>
&lt;p>&lt;img
src="./images/JeanPaul_Saba/clusters.png"
alt="Examples Of Clusters"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;em>Caption: Clusters that are labeled on the UMAP Map&lt;/em>&lt;/p>
&lt;p>By applying V-Information, we clustered features based on their complexity, and visualized them using UMAP. In the second UMAP plot (Figure 2), we see clear groupings: clusters with images of planes, cars, and sky are associated with low complexity. These images are simple, with uniform regions and clear shapes, making them easier to process for the model.&lt;/p>
&lt;p>In contrast, clusters labeled Animals, Deers, and Horses contain high complexity images. These images feature more visual detail, like fur, leaves, and branches, which require deeper processing and more complex feature representations.&lt;/p>
&lt;p>Even with CIFAR-10’s low-resolution, the network distinguishes simple from complex features effectively. Images dominated by sky are less complex, while those with animals and rich textures show up as more complex in both the V-Information measure and the UMAP clustering.&lt;/p>
&lt;ol start="2">
&lt;li>After analyzing the UMAP, we plotted how feature complexity relates to feature importance for classification.&lt;/li>
&lt;/ol>
&lt;p>&lt;img
src="./images/JeanPaul_Saba/complexityVSimportance.png"
alt="Complexity vs Importance"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>In our experiment, we plotted the relationship between feature complexity and importance to investigate how these two properties interact within the network. The scatter plot shows each feature as a blue dot, with complexity on the x-axis (measured by V-Information) and importance on the y-axis (derived from a logistic regression model trained on the features). We also added a red regression line to highlight the overall trend.&lt;/p>
&lt;p>From this visualization, we found a slight negative correlation between complexity and importance. This means that simpler features, which are easier for the network to decode, tend to have higher importance in the model’s decisions. On the other hand, more complex features, which require deeper computation and are harder to extract, tend to have lower importance on their own. This result supports the findings in the paper, which argue that neural networks exhibit a simplicity bias, relying more heavily on simpler, easily accessible features during inference.&lt;/p>
&lt;h1 id="conclusion">6. Conclusion&lt;/h1>
&lt;p>In this work, we explored the concept of feature complexity in deep neural networks, based on the paper &lt;em>&amp;ldquo;Understanding Visual Feature Reliance through the Lens of Complexity.&amp;rdquo;&lt;/em> The authors introduced V-Information as a metric to measure how accessible and complex a feature is within a model. Their findings highlight a simplicity bias in networks, where simpler features are prioritized due to their ease of extraction, robustness, and importance for decision-making.&lt;/p>
&lt;p>In our experiment with ResNet18 on CIFAR-10, we replicated key aspects of their methodology. Using NMF and V-Information, we analyzed feature complexity and visualized the feature space with UMAP and HDBSCAN clustering. Our results showed that simple features, such as planes and skies, clustered together and had lower complexity, while more detailed images, like animals, exhibited higher complexity. We also observed a slight negative correlation between feature complexity and importance, reinforcing the idea that networks rely more on simple features.&lt;/p>
&lt;p>Overall, our experiments support the paper’s conclusions: deep networks tend to favor simple, robust features while complex ones play a secondary, less stable role in the decision process.&lt;/p></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/understanding_visual_feature_reliance_through_the_lens_of_complexity/</guid><pubDate>Wed, 12 Mar 2025 16:28:12 +0100</pubDate></item><item><title>When Fairness Meets Privacy</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/when_fairness_meets_privacy/</link><description>&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\\[','\\]']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h1 style="font-size: 36px;">When Fairness Meets Privacy: A Double-Edged Sword in Machine Learning&lt;/h1>
&lt;p>&lt;em>This blog is based on and aims to present the key insights from the paper: &lt;strong>When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks&lt;/strong> by Tian et al. (2023) &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The study investigates how fairness-aware models can introduce new privacy risks, specifically through membership inference attacks. By summarizing the main findings and implications, this blog provides an accessible overview of the paper’s contributions and their significance for machine learning security and ethical AI development. For full details, refer to the original publication &lt;a href="https://arxiv.org/pdf/2311.03865">here&lt;/a>.&lt;/em>&lt;/p>
&lt;h1 style="font-size: 24px;">Authors: Lagarde Vincent, Boyenval Thibaut, Leurquin Daniel&lt;/h1>
&lt;div style="text-align: center;">
&lt;img src="./images/image_fairness_privacy/figure2.webp" alt="FD-MIA: Prediction Difference Distribution" style="width:80%; display:block; margin:auto;">
&lt;p style="font-style: italic; font-size: 14px;">This image was generated using artificial intelligence.&lt;/p>
&lt;/div>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="#1-introduction-fairness-or-privacy-pick-your-poison">Introduction: Fairness or Privacy—Pick Your Poison?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-algorithmic-fairness-a-noble-goal-that-cuts-both-ways">Algorithmic Fairness: A Noble Goal That Cuts Both Ways&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-membership-inference-attacks-the-silent-thief-of-privacy">Membership Inference Attacks: The Silent Thief of Privacy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-the-birth-of-a-new-threat-fairness-discrepancy-membership-inference-attacks-fd-mia">The Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-reproducible-code-experiments-illustrating-fd-mia">Reproducible Code Experiments: Illustrating FD-MIA&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-experimental-findings-how-fairness-opens-the-door-to-attackers">Experimental Findings: How Fairness Opens the Door to Attackers&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-the-future-of-fairness-and-privacy-can-we-have-both">The Future of Fairness and Privacy: Can We Have Both?&lt;/a>&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="1-introduction-fairness-or-privacy-pick-your-poison">1. Introduction: Fairness or Privacy Pick Your Poison&lt;/h2>
&lt;blockquote>
&lt;p>&lt;em>It is double pleasure to deceive the deceiver.&lt;/em> — Niccolò Machiavelli.&lt;/p>&lt;/blockquote>
&lt;p>This paradox of attack and defense perfectly applies to the interplay between fairness and privacy in machine learning.&lt;/p>
&lt;p>Imagine stepping into a high-tech courtroom. The AI judge, designed to be perfectly fair, renders unbiased decisions. But then, a hacker in the back row smirks—because that same fairness-enhancing mechanism just leaked private data about every case it trained on.&lt;/p>
&lt;p>Fairness and privacy in AI are like the two ends of a seesaw: push too hard on one side, and the other rises uncontrollably. &lt;strong>Recent research reveals a disturbing paradox: making a model fairer can also make it leak more private information.&lt;/strong>&lt;/p>
&lt;p>This blog explores how fairness in machine learning, despite its good intentions, can introduce &lt;strong>Membership Inference Attacks (MIAs)&lt;/strong>. Worse still, it uncovers a devastating new attack—&lt;strong>Fairness Discrepancy Membership Inference Attack (FD-MIA)&lt;/strong>—that exploits fairness interventions to &lt;strong>make privacy breaches even more effective&lt;/strong>.&lt;/p>
&lt;hr>
&lt;h2 id="2-algorithmic-fairness-a-noble-goal-that-cuts-both-ways">2. Algorithmic Fairness: A Noble Goal That Cuts Both Ways&lt;/h2>
&lt;p>Fairness in AI is like forging a perfect sword—it must be balanced, precise, and just. Researchers have developed &lt;strong>in-processing fairness interventions&lt;/strong>, which modify the training process to remove biases in model predictions. These methods act like master swordsmiths, hammering out the unwanted imperfections in AI decision-making.&lt;/p>
&lt;p>However, every sword has two edges. These fairness techniques do not just eliminate biases—they also alter how models respond to data. This change in behavior can create exploitable patterns that adversaries can use to infer whether a specific individual was part of the training data. In short, while fairness dulls one blade (bias), it sharpens another (privacy risk).&lt;/p>
&lt;p>Mathematically, fairness interventions often involve introducing constraints into the loss function:&lt;/p>
&lt;p>$$L_\text{fair} = L_\text{orig} + \lambda \cdot \mathcal{L}_{\text{fairness}}$$&lt;/p>
&lt;p>where&lt;/p>
&lt;ul>
&lt;li>$\mathcal{L}_{\text{orig}}$ is the original loss function (e.g., cross-entropy loss for classification tasks).&lt;/li>
&lt;li>$\mathcal{L}_{\text{fairness}}$ is a fairness penalty term, which ensures that predictions are balanced across different demographic groups.&lt;/li>
&lt;li>$\lambda$ is a hyperparameter controlling the trade-off between accuracy and fairness.&lt;/li>
&lt;/ul>
&lt;p>Common fairness constraints include &lt;strong>Equalized Odds&lt;/strong>, which ensures that true positive and false positive rates are equal across groups:&lt;/p>
&lt;p>$$P(\hat{Y} = 1 | Y = 1, S = s_0) = P(\hat{Y} = 1 | Y = 1, S = s_1)$$&lt;/p>
&lt;p>where $S$ represents a sensitive attribute (e.g., gender or race).&lt;/p>
&lt;p>The fairness penalty can be incorporated during model training by adding it to the loss function, as shown in this training function. A penalty coefficient can be specified to control the impact of the fairness term. Setting the coefficient to 0 results in no penalty being applied to the loss:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">train_model&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">data_loader&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">optimizer&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">criterion&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fairness_weight&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.0&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">model&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">train&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">total_loss&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">X_batch&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">y_batch&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">sensitive_batch&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">data_loader&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">optimizer&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">zero_grad&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">logits&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">model&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">X_batch&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">criterion&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">logits&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">y_batch&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Fairness regularization: penalize different confidence (softmax probabilities)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">fairness_weight&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">probs&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">functional&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">softmax&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">logits&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Compute average probability for each sensitive group&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">group0_mask&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">sensitive_batch&lt;/span> &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">group1_mask&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">sensitive_batch&lt;/span> &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">group0_mask&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sum&lt;/span>&lt;span style="color:#111">()&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#f92672">and&lt;/span> &lt;span style="color:#111">group1_mask&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sum&lt;/span>&lt;span style="color:#111">()&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">avg_prob0&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">probs&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">group0_mask&lt;/span>&lt;span style="color:#111">]&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">mean&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">avg_prob1&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">probs&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">group1_mask&lt;/span>&lt;span style="color:#111">]&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">mean&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># L2 difference between average prediction distributions&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">fairness_penalty&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">norm&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">avg_prob0&lt;/span> &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#111">avg_prob1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">p&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span> &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#111">fairness_weight&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#111">fairness_penalty&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">backward&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">optimizer&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">step&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">total_loss&lt;/span> &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#111">loss&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">item&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#111">total_loss&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#111">len&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">data_loader&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While these interventions improve fairness, they also alter the confidence distribution of model predictions—&lt;strong>a fact that attackers can exploit&lt;/strong>.&lt;/p>
&lt;hr>
&lt;h2 id="3-membership-inference-attacks-the-silent-thief-of-privacy">3. Membership Inference Attacks: The Silent Thief of Privacy&lt;/h2>
&lt;p>🔐 A Parallel with Cryptography
&lt;strong>Membership inference attacks (MIAs)&lt;/strong> are to privacy what brute-force attacks are to passwords. Instead of guessing a password, they test thousands of combinations to see which one is good.&lt;/p>
&lt;p>MIAs work the same way: they analyze a model’s outputs to determine if a given data point was part of its training set.&lt;/p>
&lt;p>A traditional MIA exploits &lt;strong>confidence scores&lt;/strong>—the probabilities that a model assigns to different predictions. The intuition is simple: models tend to be more confident on data they have seen during training. Given a target model $T$ and a queried sample $x$, an attacker computes:&lt;/p>
&lt;p>$$M(x) = 1 \text{ if } A(T(x)) &amp;gt; \tau$$&lt;/p>
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$A(T(x))$ is a decision function (often a threshold on the confidence score).&lt;/li>
&lt;li>$\tau$ is a predefined threshold.&lt;/li>
&lt;/ul>
&lt;h3 id="why-traditional-mias-fail-on-fair-models">Why Traditional MIAs Fail on Fair Models&lt;/h3>
&lt;p>Fairness interventions introduce &lt;strong>more uncertainty&lt;/strong> into the model’s predictions. This causes:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Lower confidence scores overall&lt;/strong>, making it harder for attackers to distinguish between training and non-training samples.&lt;/li>
&lt;li>&lt;strong>More uniform confidence distributions&lt;/strong>, which means attackers lose their key signal.&lt;/li>
&lt;/ul>
&lt;p>Thus, fairness-enhanced models resist traditional MIAs. But this protection is not foolproof—&lt;strong>a new, more dangerous attack lurks in the shadows&lt;/strong>.&lt;/p>
&lt;hr>
&lt;h2 id="4-the-birth-of-a-new-threat-fairness-discrepancy-membership-inference-attacks-fd-mia">4. The Birth of a New Threat: Fairness Discrepancy Membership Inference Attacks (FD-MIA)&lt;/h2>
&lt;p>If traditional MIAs are blunt weapons, &lt;strong>FD-MIA is a scalpel.&lt;/strong> It exploits the discrepancies between a biased model and a fairness-enhanced one.&lt;/p>
&lt;h3 id="how-does-fd-mia-work">How does FD-MIA work?&lt;/h3>
&lt;p>Fairness interventions shift model predictions differently for training and non-training data. This creates a &lt;strong>gap&lt;/strong> between how biased and fair models behave for the same inputs. An attacker, armed with knowledge of both models, can exploit this difference to infer membership with high accuracy.&lt;/p>
&lt;p>Mathematically, FD-MIA extends membership prediction by comparing prediction shifts between biased and fair models:&lt;/p>
&lt;p>$$M(x) = 1 \text{ if } |T_{\text{bias}}(x) - T_{\text{fair}}(x)| &amp;gt; \tau$$&lt;/p>
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$T_{\text{bias}}(x)$ and $T_{\text{fair}}(x)$ are the predictions from the biased and fair models, respectively.&lt;/li>
&lt;li>$\tau$ is a threshold chosen by the attacker.&lt;/li>
&lt;/ul>
&lt;div style="text-align: center;">
&lt;img src="./images/image_fairness_privacy/figure3.png" alt="FD-MIA: Prediction Difference Distribution" style="width:80%; display:block; margin:auto;">
&lt;p style="font-style: italic; font-size: 14px;">Figure 1: FD-MIA exploits the predictions from both models to achieve efficient attacks. From original paper.&lt;/p>
&lt;/div>
&lt;p>The key insight is that &lt;strong>fairness interventions cause systematic shifts&lt;/strong> in model confidence, creating a measurable pattern that attackers can exploit.&lt;/p>
&lt;p>Here is an example implementation of the FD-MIA attack using a function that compares the prediction difference between two models against a user-defined threshold:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">fd_mia_attack&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">sample&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">threshold&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> Given a sample, compute the absolute difference between the
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> biased and fair model&amp;#39;s softmax outputs for the positive class.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> If the difference exceeds the threshold, predict membership.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">eval&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">eval&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">with&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">no_grad&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">logits_b&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">sample&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">unsqueeze&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">logits_f&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">sample&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">unsqueeze&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">prob_b&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">functional&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">softmax&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">logits_b&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">)[&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">prob_f&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">functional&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">softmax&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">logits_f&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">)[&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">diff&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">abs&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">prob_b&lt;/span> &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#111">prob_f&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># In practice, the threshold can be tuned via shadow models or validation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">diff&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#111">threshold&lt;/span> &lt;span style="color:#00a8c8">else&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">diff&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">item&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="5-reproducible-code-experiments-illustrating-fd-mia">5. Reproducible Code Experiments: Illustrating FD-MIA&lt;/h2>
&lt;p>In this section, we reproduce the FD-MIA attack described in the original paper:&lt;/p>
&lt;p>&lt;strong>Data Generation and Splitting&lt;/strong>&lt;br>
The process starts with generating a synthetic dataset where a binary sensitive attribute influences the feature distribution. The target labels are computed using a logistic function, and the dataset is then split into training (member) and testing (non-member) sets to simulate membership inference scenarios.&lt;/p>
&lt;p>&lt;strong>Classifier Architectures&lt;/strong>&lt;br>
Two identical neural network architectures are defined:&lt;/p>
&lt;ul>
&lt;li>A &lt;strong>biased baseline model&lt;/strong> trained without fairness constraints.&lt;/li>
&lt;li>A &lt;strong>fairness-enhanced model&lt;/strong>, which incorporates a fairness penalty to balance prediction distributions across sensitive groups.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Training with Fairness Regularization&lt;/strong>&lt;br>
The function &lt;code>train_model&lt;/code> allows the inclusion of a fairness penalty during training. For the fair model, this penalty—weighted by &lt;code>fairness_weight&lt;/code>—is added to the standard cross-entropy loss to encourage prediction consistency across sensitive groups.&lt;/p>
&lt;p>&lt;strong>FD-MIA Attack Implementation&lt;/strong>&lt;br>
The attack, implemented in &lt;code>fd_mia_attack&lt;/code>, exploits the absolute difference in predicted probabilities (for the positive class) between the biased and fair models. If this difference exceeds a given threshold, the sample is inferred as a training member. This approach leverages the core principle of FD-MIA: fairness interventions create prediction discrepancies that can be used for membership inference.&lt;/p>
&lt;p>&lt;strong>Evaluation and Visualization&lt;/strong>&lt;br>
The attack is evaluated by comparing prediction differences between member and non-member data. We visualize the prediction difference distributions to highlight how fairness-driven adjustments can unintentionally expose membership information.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch.nn&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch.optim&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">optim&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torch.utils.data&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">Dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">DataLoader&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">random_split&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">numpy&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">np&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">matplotlib.pyplot&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">sklearn.metrics&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">roc_auc_score&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Set random seeds for reproducibility&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">manual_seed&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">42&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">random&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">seed&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">42&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 1. Create a synthetic dataset with a sensitive attribute&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">class&lt;/span> &lt;span style="color:#75af00">SyntheticFairDataset&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">Dataset&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#111">__init__&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">n_samples&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1000&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Features: two-dimensional points drawn from different distributions&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">n_samples&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">n_samples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">X&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">y&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sensitive&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[]&lt;/span> &lt;span style="color:#75715e"># sensitive attribute: 0 or 1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">i&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">range&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">n_samples&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Randomly assign a sensitive group (imbalance can be introduced here)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">s&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">random&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">choice&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">],&lt;/span> &lt;span style="color:#111">p&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#ae81ff">0.7&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.3&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Generate features from group-dependent distributions&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">s&lt;/span> &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">x&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">random&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">normal&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">loc&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">scale&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">else&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">x&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">random&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">normal&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">loc&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.5&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">scale&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Label is determined by a linear rule (with some noise)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">y_prob&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">exp&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">x&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#111">x&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">0.5&lt;/span>&lt;span style="color:#111">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">y_label&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">random&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">binomial&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">y_prob&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">X&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">append&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">x&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">y&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">append&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">y_label&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sensitive&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">append&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">s&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">X&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">tensor&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">X&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dtype&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">float32&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">y&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">tensor&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">y&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dtype&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">long&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sensitive&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">tensor&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sensitive&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dtype&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">long&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#111">__len__&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">n_samples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#111">__getitem__&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">idx&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">X&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">idx&lt;/span>&lt;span style="color:#111">],&lt;/span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">y&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">idx&lt;/span>&lt;span style="color:#111">],&lt;/span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">sensitive&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">idx&lt;/span>&lt;span style="color:#111">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">dataset&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">SyntheticFairDataset&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">n_samples&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2000&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Split into training (for model training) and attack evaluation (simulate member vs. non-member)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">train_size&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">int&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#111">len&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">test_size&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">len&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#111">train_size&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">train_dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">test_dataset&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">random_split&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">train_size&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">test_size&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">train_loader&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">DataLoader&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">train_dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">batch_size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">64&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">shuffle&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">True&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">test_loader&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">DataLoader&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">test_dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">batch_size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">64&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">shuffle&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">False&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 2. Define the classifier architectures&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">class&lt;/span> &lt;span style="color:#75af00">SimpleClassifier&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Module&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#111">__init__&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">input_dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">hidden_dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">16&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">output_dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">super&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">SimpleClassifier&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">__init__&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">fc1&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Linear&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">input_dim&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">hidden_dim&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">relu&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">ReLU&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">fc2&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Linear&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">hidden_dim&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">output_dim&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">forward&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">x&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">out&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">relu&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">fc1&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">x&lt;/span>&lt;span style="color:#111">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#111">self&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">fc2&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">out&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#75715e"># raw logits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Baseline (biased) model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">biased_model&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">SimpleClassifier&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Fairness-enhanced model: we add a fairness penalty to the loss (for demonstration)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">fair_model&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">SimpleClassifier&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 3. Train both models&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">criterion&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">CrossEntropyLoss&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Train biased model without fairness penalty&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">optimizer_biased&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">optim&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Adam&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">parameters&lt;/span>&lt;span style="color:#111">(),&lt;/span> &lt;span style="color:#111">lr&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.01&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">epoch&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">range&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">20&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">train_model&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">train_loader&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">optimizer_biased&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">criterion&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fairness_weight&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.0&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Uncomment to print loss: print(f&amp;#34;Biased Model Epoch {epoch+1}: Loss {loss:.4f}&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Train fair model with a fairness penalty (fairness_weight &amp;gt; 0)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">optimizer_fair&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">optim&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Adam&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">parameters&lt;/span>&lt;span style="color:#111">(),&lt;/span> &lt;span style="color:#111">lr&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.01&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">fairness_weight&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1.0&lt;/span> &lt;span style="color:#75715e"># adjust to control trade-off&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">epoch&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">range&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">20&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">train_model&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">train_loader&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">optimizer_fair&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">criterion&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fairness_weight&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">fairness_weight&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Uncomment to print loss: print(f&amp;#34;Fair Model Epoch {epoch+1}: Loss {loss:.4f}&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 4. Membership Inference Attack using FD-MIA&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Evaluate the attack on both training (members) and test (non-members) samples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">evaluate_attack&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">threshold&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">attack_labels&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">attack_scores&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Assuming samples in train_dataset are members and test_dataset non-members&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">sample&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">_&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">_&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">DataLoader&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">batch_size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">shuffle&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">False&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">pred&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">diff&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">fd_mia_attack&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">sample&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">],&lt;/span> &lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">threshold&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">attack_labels&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">append&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">pred&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">attack_scores&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">append&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">diff&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">array&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">attack_labels&lt;/span>&lt;span style="color:#111">),&lt;/span> &lt;span style="color:#111">np&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">array&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">attack_scores&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># For demonstration, we use the entire training set as member data and test set as non-member data.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">member_labels&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">member_diffs&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">evaluate_attack&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">train_dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">threshold&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">nonmember_labels&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">nonmember_diffs&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">evaluate_attack&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">test_dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">biased_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">fair_model&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">threshold&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h3 id="6-experimental-findings-how-fairness-opens-the-door-to-attackers">&lt;strong>6. Experimental Findings: How Fairness Opens the Door to Attackers&lt;/strong>&lt;/h3>
&lt;p>The study conducted extensive experiments across &lt;strong>six datasets, three attack methods, and five fairness approaches&lt;/strong>, testing over &lt;strong>160 models&lt;/strong>.
To do this, they performed a comprehensive set of experiments involving:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Multiple datasets with different sensitive attributes (like gender or race),&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Biased vs. fair model variants,&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Three types of MIAs (including their novel FD-MIA),&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multiple fairness interventions&lt;/p>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>Number of Fairness Settings&lt;/th>
&lt;th>Target Tasks&lt;/th>
&lt;th>FD-MIA Effectiveness&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CelebA&lt;/td>
&lt;td>3 settings (smile, hair, makeup)&lt;/td>
&lt;td>3 binary targets × 2 sensitive attributes&lt;/td>
&lt;td>FD-MIA consistently outperformed other methods.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UTKFace&lt;/td>
&lt;td>2 settings (race prediction, gender prediction)&lt;/td>
&lt;td>Race or gender, sensitive to the other&lt;/td>
&lt;td>FD-MIA revealed privacy leaks even with balanced groups.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FairFace&lt;/td>
&lt;td>2 settings (race prediction, gender prediction)&lt;/td>
&lt;td>Same as UTKFace&lt;/td>
&lt;td>Most vulnerable dataset—biggest fairness shift = biggest privacy leak.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The results were shocking:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Fair models were significantly harder to attack using traditional MIAs.&lt;/strong>&lt;/li>
&lt;li>&lt;strong>FD-MIA, however, dramatically increased attack success rates—fairness actually made models more vulnerable!&lt;/strong>&lt;/li>
&lt;li>The greater the fairness intervention, the wider the discrepancy between biased and fair models, making FD-MIA even more effective.&lt;/li>
&lt;/ul>
&lt;p>Our synthetic experiment further supports these findings. The histogram below illustrates the absolute difference between the predictions of the biased and fair models for both &lt;em>members&lt;/em> (training data) and &lt;em>non-members&lt;/em> (test data).&lt;/p>
&lt;div style="text-align: center;">
&lt;img src="./images/image_fairness_privacy/figure1.png" alt="FD-MIA: Prediction Difference Distribution">
&lt;p style="font-style: italic; font-size: 14px;">Figure 2: FD-MIA: Prediction Difference Distribution.&lt;/p>
&lt;/div>
&lt;p>As expected, members exhibit a significantly higher prediction discrepancy compared to non-members. This clear separation highlights how fairness constraints alter model confidence differently for training and test samples—providing an exploitable signal for membership inference. &lt;strong>In simple terms: making a model fairer may paradoxically make it leak more private information.&lt;/strong> A cruel irony for those trying to do the right thing.&lt;/p>
&lt;hr>
&lt;h2 id="7-the-future-of-fairness-and-privacy-can-we-have-both">7. The Future of Fairness and Privacy: Can We Have Both?&lt;/h2>
&lt;p>The million-dollar question: &lt;strong>Can we balance fairness and privacy without sacrificing one for the other?&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>If you know the enemy and know yourself, you need not fear the result of a hundred battles.&lt;/em> — Sun Tzu.&lt;/p>&lt;/blockquote>
&lt;p>A better understanding of the link between fairness and privacy, as well as the potential and new attacks introduced by unbiased models, is already a solid step toward defending against threats. By understanding the underlying mechanisms, it becomes possible to counteract them.&lt;/p>
&lt;p>The researchers propose two key defenses:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Restricting Information Access&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Limiting confidence score outputs reduces the information available to attackers.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Differential Privacy (DP)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>By injecting noise into model training, DP-SGD (Differentially Private Stochastic Gradient Descent) helps obscure membership information:&lt;/p>
&lt;p>$$\tilde{g}_t = g_t + \mathcal{N}(0, \sigma^2 I)$$&lt;/p>
&lt;p>where $g_t$ is the original gradient and $\mathcal{N}(0, \sigma^2 I)$ is Gaussian noise added to prevent membership inference.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>While these methods help, they come with trade-offs: &lt;strong>too much privacy protection can lower fairness and accuracy, while too little leaves models vulnerable.&lt;/strong> The challenge ahead is designing AI systems that can balance both.&lt;/p>
&lt;p>💡 Alternative approach: Fairness-Aware Differential Privacy (FADP) adapts noise levels based on protected groups, balancing privacy and fairness.&lt;/p>
&lt;hr>
&lt;h2 id="references">References&lt;/h2>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>H. Tian, G. Zhang, B. Liu, T. Zhu, M. Ding, and W. Zhou, &amp;ldquo;When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers via Membership Inference Attacks&amp;rdquo;, &lt;em>arXiv e-prints&lt;/em>, Art. no. &lt;a href="https://arxiv.org/pdf/2311.03865">arXiv.2311.03865&lt;/a>.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/when_fairness_meets_privacy/</guid><pubDate>Mon, 10 Mar 2025 12:22:37 +0100</pubDate></item><item><title>Get a calibrated and efficient model with tailored data augmentation.</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/mixupdatacalibration/</link><description>&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\\[','\\]']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h2 id="authors--tristan-waddington-fabien-lagnieu--dimitri-henrard-iratchet">Authors : &lt;em>Tristan Waddington, Fabien Lagnieu &amp;amp; Dimitri Henrard-Iratchet&lt;/em>&lt;/h2>
&lt;h2 id="comment-on-the-research-paper-tailoring-mixup-to-data-for-calibration-written-by-quentin-bouniot-pavlo-mozharovskyi--florence-dalché-buc-from-ltci-télécom-paris-institut-polytechnique-de-paris-france">Comment on the research paper: &lt;a href="https://arxiv.org/abs/2311.01434">&lt;strong>Tailoring Mixup to Data for Calibration&lt;/strong>&lt;/a>, written by &lt;em>Quentin Bouniot, Pavlo Mozharovskyi &amp;amp; Florence d’Alché-Buc&lt;/em>, from LTCI, Télécom Paris, Institut Polytechnique de Paris, France&lt;/h2>
&lt;h1 id="table-of-contents">Table of contents&lt;/h1>
&lt;ol>
&lt;li>&lt;a href="#1-existing-data-augmentation-methods">Existing Data Augmentation Methods&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-understanding-calibration">Understanding Calibration&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-best-of-both-worlds-tailoring-mixup-to-data-for-calibration">Best of both worlds: Tailoring Mixup to Data for Calibration&lt;/a>&lt;/li>
&lt;/ol>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;blockquote>
&lt;p>&amp;ldquo;But it works well on the training set!&amp;rdquo; is the machine learning equivalent to the classic &amp;ldquo;But it works on my computer!&amp;rdquo;&lt;/p>&lt;/blockquote>
&lt;p>The basic workflow of machine learning has two steps:&lt;/p>
&lt;ul>
&lt;li>First, &lt;strong>train&lt;/strong> your model to perform a task from an available dataset.&lt;/li>
&lt;li>Second, &lt;strong>generalize&lt;/strong> and predict the results from unseen data.&lt;/li>
&lt;/ul>
&lt;p>How can data scientists be sure and &lt;em>confident&lt;/em> that their model will infer a correct result on this new data?&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/WoMM.jpg"
alt="Does not know why it (don't) works."
width=300>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>We know that deep learning models need vast amounts of data to be efficient.
So, when there is not enough, researchers simply… create more data:
this is the concept of &lt;strong>data augmentation&lt;/strong>.&lt;/p>
&lt;p>However, this technique tends to exarcerbate the models&amp;rsquo; &lt;strong>overconfidence&lt;/strong> in their
predictions.
Discrepancies between confidence and prediction accuracy are acceptable in domains such as e-commerce recommendations,
but high stake applications such as medical diagnosis or nuclear safety require an accurate &lt;strong>confidence score&lt;/strong>, where robustness and reliability are critical.&lt;/p>
&lt;p>&lt;strong>This is the idea behind calibration: the model&amp;rsquo;s confidence in its prediction must truly reflect its own prediction accuracy.&lt;/strong>
Well-calibrated models do not just inspire trust; they also contribute to &lt;strong>fairer decision-making&lt;/strong>, by ensuring that predictions are accompanied by reliable confidence estimates, which can help reduce biased or unjust outcomes.&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/CalibratedClassifier.png"
alt="Difference between a calibrated classifier (right) and a bad one (left)."
width=600>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Illustration of the confidence of a calibrated classifier (right) with
accurate prediction probabilities and a more brutal and under-confident one (left).&lt;/em>&lt;/p>
&lt;p>Merging data augmentation and calibration is challenging. The first is prone to
create &lt;strong>manifold intrusion&lt;/strong>, where synthetic data with a given label conflicts
with original data of another class. The rise of the size of the dataset also
increases the computational cost of the training. This contradicts the potential objective of frugality. The second is known to &lt;strong>constrain the accuracy&lt;/strong>.&lt;/p>
&lt;p>To handle these challenges, Quentin Bouniot and Pavlo Mozharovskyi have conducted
under the direction of Florence d&amp;rsquo;Alché-Buc an extensive study on one of the
technique of data augmentation, the &lt;strong>linear interpolation of training samples&lt;/strong>,
also called &lt;strong>Mixup&lt;/strong>. They have found an efficient way to tune this process to
both improve the performance &lt;strong>and&lt;/strong> the calibration of models, while being
much more efficient than previous methods.&lt;/p>
&lt;p>Let&amp;rsquo;s dig step by step into it.&lt;/p>
&lt;hr>
&lt;h1 id="1-existing-data-augmentation-methods">1. Existing Data Augmentation Methods&lt;/h1>
&lt;p>Deep learning methods rely on vast amounts of data, so if you do not have enough, make
it yourself. This is the first conclusion of the
study of a Microsoft Research team, lead by Patrice Simard in 2003 aimed to list the
current best practices of neural networks training:&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>&amp;ldquo;The most important practice is getting a training set as large as possible: we
expand the training set by adding a new form of distorted data.&amp;rdquo;&lt;/em> [Simard et al. 2003] &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>&lt;/blockquote>
&lt;p>The good results of subsequent models have proved them right. And numerous
techniques have been developed since. Let&amp;rsquo;s review some of them.&lt;/p>
&lt;h2 id="11-create-new-images-for-classification">1.1. Create new images for classification&lt;/h2>
&lt;p>The most visual example of data augmentation is the way image classifiers are trained.
To make them more robust and efficient, scientists have transformed the input images
to drastically increase the size of the training set (by up to 2048 times).&lt;/p>
&lt;p>The most commonly used transformations
are random cropping and resizing, flipping, and color distortion.
This is now so common that it can be done in a few lines in &lt;code>pytorch&lt;/code> (see next code
snippet), and automatic recipes
such as &lt;code>AutoAugment&lt;/code>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> are readily available to augment common datasets.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torchvision&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">transforms&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Definition of transformations for an image dataset&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">transformTrain&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">transforms&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Compose&lt;/span>&lt;span style="color:#111">([&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">transforms&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">RandomResizedCrop&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">224&lt;/span>&lt;span style="color:#111">),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">transforms&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">RandomHorizontalFlip&lt;/span>&lt;span style="color:#111">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">transforms&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">ColorJitter&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">brightness&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">.5&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">hue&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">.3&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Additional transformations for images are illustrated
on the next figure. We expect the neural networks to &amp;ldquo;see&amp;rdquo; these 10 new images
as being close in their latent space. With this example, one original labeled image
is processed 10 times in different versions during the training of the model.&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/Data-Augmentation.png"
alt="Different methods of data augmentation operators."
width=600>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Illustration of different data augmentation operators, taken from the paper of Chen 2020 &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/em>&lt;/p>
&lt;h2 id="12-linear-interpolation-or-mixup">1.2. Linear interpolation or Mixup&lt;/h2>
&lt;p>Another idea is to create a virtual sample from a &lt;strong>vicinity&lt;/strong> around the true
training data—like we did in high school when we added epsilon to a number to see in witch direction
the function is moving. This principle has been demonstrated to help models
generalize. However, the method of creation is often hand-crafted and only mimic
natural perturbations.&lt;/p>
&lt;p>To scale up this process, [Zhang et al., 2018]&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> imagined the &lt;strong>Mixup&lt;/strong> process,
which is a linear interpolation, or mixing, of two or more training datapoints.&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/mixup_figures-vanilla.png"
alt="Example of vanilla Mixup"
width=300>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Illustration of a linear interpolation of Mixup. New points $\tilde{\mathrm{x}}_1$ and $\tilde{\mathrm{x}}_2$ are respectively drawn from the segment $[\mathrm{x}_1, \mathrm{x}_2]$ and $[\mathrm{x}_1, \mathrm{x}_3]$&lt;/em>&lt;/p>
&lt;p>The process of data augmentation during training with Mixup consists of three phases:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>selecting tuples&lt;/strong> (most often pairs) of points to mix together,&lt;/li>
&lt;li>&lt;strong>sampling coefficients&lt;/strong> that will govern the interpolation to generate synthetic points,&lt;/li>
&lt;li>applying a specific &lt;strong>interpolation procedure&lt;/strong> between the points weighted by the coefficients sampled.&lt;/li>
&lt;/ol>
&lt;p>However, the literature explores the drawbacks of this process:&lt;/p>
&lt;ul>
&lt;li>Mixing carelessly different points can result in
&lt;strong>incorrect labels and hurt generalization&lt;/strong> [Guo et al., 2019]&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>, while
mixing similar points helps in diversity [Dablain et al., 2022].&lt;/li>
&lt;li>Furthermore, several previous work have highlighted a &lt;strong>trade-off between performance and calibration&lt;/strong> in Mixup [Wang et al., 2023].&lt;/li>
&lt;/ul>
&lt;p>Before digging further into the Mixup process, it is time to understand what
exactly is the calibration of a model and why it can be worth of a trade-off with performance.&lt;/p>
&lt;hr>
&lt;h1 id="2-understanding-calibration">2. Understanding Calibration&lt;/h1>
&lt;p>Modern (post 2016) neural networks have a high accuracy but are overconfident
in their predictions, outputting softmax scores of
above 99.9% for the dominant class, hence misleading the user into a false sense of confidence.
This is why we need &lt;strong>calibration&lt;/strong>.&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/Over_confident_IA.jpg"
alt="Failed object detection with high confidence"
width=400>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Meme about the overconfidence of an AI agent (obviously uncalibrated) over a failed prediction.&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Calibration&lt;/em> is a metric to quantify uncertainty, measuring the difference between a model’s confidence
in its predictions and the actual probability of those predictions being correct.&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/p>&lt;/blockquote>
&lt;p>In other words, if a &lt;em>calibrated model&lt;/em> predicts the image as a cat with a confidence
of 0.3, this prediction has a 30% chance of being correct.
&lt;strong>The actual aim is not exactly to explain the results, but confidence calibration prevents
some mistakes by associating a prediction with its confidence score.&lt;/strong>&lt;/p>
&lt;p>Let&amp;rsquo;s explore further the motivations of calibration and the way to measure
it and the potential draw backs.&lt;/p>
&lt;h3 id="21-importance-of-calibration">2.1. Importance of calibration&lt;/h3>
&lt;p>The gap in confidence calibration has been spotted by [Guo et al. (2017)]&lt;sup id="fnref1:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>,
and is linked to the actual use cases of neural networks, where the calibration is not crucial.
In LLMs or online recommender systems, a 90% quality of predictions is enough and occasional mistakes are acceptable. For further use however, like in medical diagnosis prediction or
in defense systems, an overconfident model can lead to tragic consequences.&lt;/p>
&lt;p>What would be the benefits of a well calibrated model ?&lt;/p>
&lt;ul>
&lt;li>It can &lt;strong>filter out the poor predictions&lt;/strong>, and not provide a wrong prediction to the user.&lt;/li>
&lt;li>It can &lt;strong>reinforce the continuous training&lt;/strong>, by asking for the actual label of the low confidence prediction.&lt;/li>
&lt;li>It can &lt;strong>detect outliers&lt;/strong> and warn the user that something strange is happening.&lt;/li>
&lt;li>It can improve the &lt;strong>robustness of the model&lt;/strong> by ensuring that prediction confidence accurately reflects the underlying uncertainty, leading to more reliable decisions in critical situations.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>To sum it up, a well calibrated model is a reliable coworker aware of its own capacities.&lt;/strong>&lt;/p>
&lt;h3 id="22-calibration-metrics">2.2. Calibration Metrics&lt;/h3>
&lt;p>To compare the calibration of models, specific metrics are required. Unlike simple accuracy on a dataset, various metrics have been proposed in the literature, each focusing on different characteristics.&lt;/p>
&lt;h4 id="221-the-brier-score">2.2.1. The Brier Score&lt;/h4>
&lt;p>The Brier score [Brier, 1050] is the mean square error between predicted confidence and target.
Here the target has the form of a one-hot encoded vector.&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/BrierScore_Wolfe.png"
alt="Brier Score illustration"
width=600>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Computing the Brier Score on classification task (image by Wolfe)&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/em>&lt;/p>
&lt;p>Intuitively, the &lt;strong>Brier Score measures the accuracy of predicted probabilities&lt;/strong>. It can be decomposed into three components — uncertainty (marginal uncertainty over labels), resolution (deviations of individual predictions against the marginal), and reliability (average violation of true label frequencies)&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Brier Score = uncertainty - resolution + reliability&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>The Brier Score is insensitive to the low frequencies events, hence it can be used in combination
with one of the other following metrics to provide useful insights.
Basically, the score is low when the predictions reflect the confidence, i.e. when the model is calibrated.&lt;/p>
&lt;p>&lt;em>The following code is a dummy example of Brier score computation of a single classification probabilities over 3 classes. The same probabilities will be used on different metrics.&lt;/em>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">sklearn.metrics&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">brier_score_loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Example of prediction outputs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">prob_u&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#ae81ff">0.34&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.33&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.33&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">prob_l&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.25&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.25&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">prob_h&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#ae81ff">0.9&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.07&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.03&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">target&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Compute brier score&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;Brier score for uniform:&lt;/span>&lt;span style="color:#8045ff">\t&lt;/span>&lt;span style="color:#d88200"> &lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">brier_score_loss&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">target&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">prob_u&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#d88200">:&lt;/span>&lt;span style="color:#d88200">.4f&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;Brier score for low confidence:&lt;/span>&lt;span style="color:#8045ff">\t&lt;/span>&lt;span style="color:#d88200"> &lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">brier_score&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">target&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">prob_l&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#d88200">:&lt;/span>&lt;span style="color:#d88200">.4f&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;Brier score for high confidence: &lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">brier_score&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">target&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">prob_h&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#d88200">:&lt;/span>&lt;span style="color:#d88200">.4f&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Output&lt;/em>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Brier score &lt;span style="color:#00a8c8">for&lt;/span> uniform: 0.2178
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Brier score &lt;span style="color:#00a8c8">for&lt;/span> low confidence: 0.1250
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Brier score &lt;span style="color:#00a8c8">for&lt;/span> high confidence: 0.0053
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="222-the-expected-calibration-error-ece">2.2.2. The Expected Calibration Error (ECE)&lt;/h4>
&lt;p>The Expected Calibration Error [Guo et al, 2017] approximates the difference between &lt;strong>accuracy&lt;/strong> and &lt;strong>confidence&lt;/strong> by grouping samples into equally spaced &lt;strong>bins&lt;/strong> with respect to their confidence scores.
Because it is both simple and interpretable, ECE is a popular metric to evaluate calibration on classification tasks in practice.
ECE computes the difference between average confidence and accuracy within each
bin, then takes a &lt;strong>weighted average of these values based upon the relative
size of each bin.&lt;/strong>&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/ECE_Wolfe.png"
alt="ECE illustration "
width=600>
&lt;/img>
&lt;figcaption>&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Computing ECE over a group of prediction, (image by Wolfe)&lt;sup id="fnref1:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/em>&lt;/p>
&lt;p>ECE measures how well a model’s estimated &amp;ldquo;probabilities&amp;rdquo; match the observed
probabilities by taking a weighted average over the absolute difference between
accuracy and estimated probabilities (confidence). This measure involves splitting
the predictions into $M$ equally spaced bins.&lt;/p>
&lt;p>$$ECE = \sum_{bins}^M \frac{\text{bin size}}{\text{nb samples}} | \text{accuracy per bin} - \text{average bin probability}| $$&lt;/p>
&lt;p>A very good example on how to compute ECE by hand can be found in the article
of &lt;a href="https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/">Maja Pavlovic&lt;/a>
on the blog TowardsDataScience&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Variants&lt;/strong>: &lt;em>Adaptative ECE&lt;/em> (AECE) is simmilar to ECE, but with each bin having the same number of samples. Other extensions of ECE can
be used to estimate the variance over the bins, the &lt;em>Uncertainty Calibration Error&lt;/em>
(UCE) or the &lt;em>Expected Normalize Calibration Error&lt;/em> (ENCE). They will not be
detailed further here.&lt;/p>
&lt;h4 id="223-the-negative-log-likelihood-nll">2.2.3. The Negative Log Likelihood (NLL)&lt;/h4>
&lt;p>The Negative Log Likelihood (NLL) is the typical objective function for training neural networks in multi-class classification. It characterizes the disparity
between the predicted and the actual confidence for the true label.
It reaches a perfect score of $0$ when all data is correctly predicted with 100% confidence,
and rises as soon as some are misclassified. Hence lower scores correspond to better calibration.&lt;/p>
&lt;p>&lt;em>Dummy example of NLL computation of a single prediction over 3 classes&lt;/em>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch.nn&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">log_softmax&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">LogSoftmax&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dim&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">loss_fn&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">NLLLoss&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># input to NLLLoss is of size (batch_size x nb_classes) = 1 x 3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">target&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">])&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">long&lt;/span>&lt;span style="color:#111">()&lt;/span> &lt;span style="color:#75715e"># correct class is at index O&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># different examples of logits from a classifier&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">logits_u&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([[&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">]])&lt;/span> &lt;span style="color:#75715e"># uniform prediction&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">logits_l&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([[&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.2&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.1&lt;/span>&lt;span style="color:#111">]])&lt;/span> &lt;span style="color:#75715e"># low confidence prediction&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">logits_h&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Tensor&lt;/span>&lt;span style="color:#111">([[&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">0.1&lt;/span>&lt;span style="color:#111">]])&lt;/span> &lt;span style="color:#75715e"># high confidence prediction&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;nll uniform: &lt;/span>&lt;span style="color:#8045ff">\t\t&lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">loss_fn&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">log_softmax&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">logits_u&lt;/span>&lt;span style="color:#111">),&lt;/span> &lt;span style="color:#111">target&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#d88200">:&lt;/span>&lt;span style="color:#d88200">.4f&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;nll low confidence: &lt;/span>&lt;span style="color:#8045ff">\t&lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">loss_fn&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">log_softmax&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">logits_l&lt;/span>&lt;span style="color:#111">),&lt;/span> &lt;span style="color:#111">target&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#d88200">:&lt;/span>&lt;span style="color:#d88200">.4f&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;nll high confidence: &lt;/span>&lt;span style="color:#8045ff">\t&lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">loss_fn&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">log_softmax&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">logits_h&lt;/span>&lt;span style="color:#111">),&lt;/span> &lt;span style="color:#111">target&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#d88200">:&lt;/span>&lt;span style="color:#d88200">.4f&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Output&lt;/em>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nll uniform: 1.0986
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nll low confidence: 0.6184
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nll high confidence: 0.0001
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>However, NNL also causes overconfidence in modern neural networks.&lt;/strong>
They are purposely trained to minimize it by making high confidence predictions, which actually lowers the exponential sum of the soft max, as in our
high_confidence example above.&lt;/p>
&lt;p>This kind of behavior can be exhibited by drawing the calibration curve of the predictor.&lt;/p>
&lt;h4 id="23-calibration-curves---reliability-diagrams">2.3. Calibration Curves - Reliability diagrams&lt;/h4>
&lt;p>The calibration curves [Wilks, 1995] compare how well the probabilistic predictions of a
binary classifier are calibrated. It shows the frequency of the predicted label against the
predicted probability. It is easily drawn with the method &lt;code>model.predict_proba()&lt;/code> of scikit-learn.&lt;/p>
&lt;p>&lt;a href="https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html#calibration-curves">Scikit-Learn&amp;rsquo;s documentation&lt;/a>
provides a very insightful illustration to better understand these curves. They
have fitted 4 different classifiers on a very small training set and plot the
calibration curve along with the histogram showing the related distribution of
the predicted probabilities on each of the 10 bins. On this specific example,
we can observe the following behaviors:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Logistic Regression&lt;/strong>: not perfect, but well calibrated because the
optimized log loss is also the scoring rule (as seen in previous section).&lt;/li>
&lt;li>&lt;strong>Gaussian Naive Bayes&lt;/strong>: its tendency to push probabilities to 0 or 1 is well
shown on histogram orange. This means an overconfident model.&lt;/li>
&lt;li>&lt;strong>Support vector Classifier&lt;/strong> displays a typical sigmoid calibration curve.
This under-confident result is typical of maximum-margin methods.&lt;/li>
&lt;li>&lt;strong>Random Forest&lt;/strong> averages the predictions over a set of models, meaning exact predictions of 0 or 1 are rare, hence the shift towards 0.2 and 0.9. The whole
model seems under-confident, but since each tree is fitted in minimizing a scoring
rule (Brier score of log-loss) the pink calibration curve is pretty close to the dot line.&lt;/li>
&lt;/ul>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/sphx_glr_plot_compare_calibration_001.png"
alt="Calibration plot comparison"
width=600>
&lt;/img>
&lt;figcaption>
&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Behavior of regular classifier on a standard task.
Upper: Calibration curves with &lt;code>sklearn.calibration.CalibrationDisplay.from_estimator&lt;/code>.
Bottom: Histograms of the number of samples per bins of &lt;code>predict_proba&lt;/code> values.&lt;/em>&lt;/p>
&lt;p>Remember: a &lt;strong>perfectly calibrated estimator&lt;/strong> will get the doted diagonal line
and its histogram will be flat.&lt;/p>
&lt;h3 id="24-drawbacks">2.4 Drawbacks&lt;/h3>
&lt;p>To provide an insight of the side effects of the calibration, we will study the
impact of the provided method &lt;code>CalibratedClassifierCV&lt;/code> in Scikit-Learn.
It uses cross-validation to obtain unbiased predictions, which are then used for calibration.
The sigmoid method here is a simple logistic regression model. We experiment
the effect of the calibration on the accuracy and the Brier Score of 4 classifiers
fitted on the titanic dataset. We will only display the code to instantiate and
calibrate the models.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">sklearn.linear_model&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">LogisticRegression&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">sklearn.naive_bayes&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">GaussianNB&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">sklearn.svm&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">SVC&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">sklearn.ensemble&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">RandomForestClassifier&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">sklearn.calibration&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">CalibratedClassifierCV&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># [...] load titanic dataset, split data, Skub automatically the data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">X_train&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">X_test&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">y_train&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">y_test&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">train_test_split&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#f92672">...&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Initialize classifiers&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">classifiers&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;Logistic Regression&amp;#34;&lt;/span>&lt;span style="color:#111">:&lt;/span> &lt;span style="color:#111">LogisticRegression&lt;/span>&lt;span style="color:#111">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;Naive Bayes&amp;#34;&lt;/span>&lt;span style="color:#111">:&lt;/span> &lt;span style="color:#111">GaussianNB&lt;/span>&lt;span style="color:#111">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;Support Vector Classifier&amp;#34;&lt;/span>&lt;span style="color:#111">:&lt;/span> &lt;span style="color:#111">SVC&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">probability&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">True&lt;/span>&lt;span style="color:#111">),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;Random Forest&amp;#34;&lt;/span>&lt;span style="color:#111">:&lt;/span> &lt;span style="color:#111">RandomForestClassifier&lt;/span>&lt;span style="color:#111">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Calibrate with the simgmoid regression&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">calibrated_classifiers&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">calibrated_proba&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">name&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">classifier&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">classifiers&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">items&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">calibrated_classifiers&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">name&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">CalibratedClassifierCV&lt;/span>&lt;span style="color:#111">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">classifier&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">cv&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">method&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#d88200">&amp;#34;sigmoid&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">)&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">fit&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">X_train&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">y_train&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">calibrated_proba&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">name&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">classifier&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">predict_proba&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">X_test&lt;/span>&lt;span style="color:#111">)[:,&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># [...] Compute metrics and display as bar plots&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/Calibration_side_effect.png"
alt="Calibration effect on metrics"
width=600>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: effect of calibration on classifiers&amp;rsquo; metrics. We expect a small drop in
accuracy and a reduction (improvement) of the Brier Score. But our method is not
efficient here.&lt;/em>&lt;/p>
&lt;p>We can see that this method has moderate and sometimes counterintuitive effects.
&lt;strong>This suggests that the training set is not sufficient to
fit a calibrated estimator.&lt;/strong>&lt;/p>
&lt;hr>
&lt;h1 id="3-best-of-both-worlds-tailoring-mixup-to-data-for-calibration">3. Best of Both Worlds: Tailoring Mixup to Data for Calibration&lt;/h1>
&lt;p>We have reached the core of the paper of Bouniot et al. They show that, by
&lt;strong>taking the distance of points into account when sampling the coefficients&lt;/strong> in the
second phase of Mixup, we can (i) avoid a loss in diversity, and (ii) reduce manifold intrusion and label noise.&lt;/p>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/Mixup-ours.png"
alt="Similarity Mixup"
width=300>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Illustration of a Blouniot et al.&amp;rsquo;s process of &lt;strong>similarity&lt;/strong> in interpolation. New point $\tilde{\mathrm{x}}_2$ have to be closer to $\mathrm{x}_1$ because $\mathrm{x}_3$ has a different label but still preserve diversity.&lt;/em>&lt;/p>
&lt;p>If we had only used a selection of samples with similar labels, we would have
lost the possible exploration of new directions of the latent space.
&lt;strong>With this similarity process, at the end of the day, we have avoided restricting&lt;/strong>
&lt;strong>possible direction of mixing while staying in the vicinity of original points,&lt;/strong>
&lt;strong>hence preventing manifold intrusion.&lt;/strong>&lt;/p>
&lt;h2 id="31-linear-interpolation-of-training-samples-mixup">3.1 Linear interpolation of training samples: Mixup&lt;/h2>
&lt;p>Mixing samples through linear interpolation is the easiest and most efficient way
to create new data from a computational point of view. Combining data from the same
batch also avoids additional sampling during training.&lt;/p>
&lt;p>Specific techniques have been proposed since 2018 to compute linear interpolation
but often at the cost of more complex training or loss of diversity.
The selection process of samples to interpolate from may be computationally
expensive.
Furthermore
such studies have been conducted with the aim of improving models&amp;rsquo; generalization, not their calibration, and will not solve our issue.&lt;/p>
&lt;p>In the original mixup method of [Zhang et al. 2018]&lt;sup id="fnref1:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>, at each training iteration
of the model, each input is mixed with another input randomly selected from the
same batch, with a random strength drawn form a Beta law.&lt;/p>
&lt;p>But how can we be sure of the label of these new datapoints?&lt;/p>
&lt;h2 id="32-weighting-to-prevent-manifold-intrusion">3.2 Weighting to prevent manifold intrusion&lt;/h2>
&lt;p>The real danger of mixup is &lt;strong>manifold intrusion&lt;/strong>, where the interpolated
sample between two identical label points falls into an other class.&lt;/p>
&lt;blockquote>
&lt;p>The likelihood of conflict in the synthetic label
increases with the distance between the two points. As
data live in manifolds of the representation space, the linear combination of
two points far from each other can lie in a different manifold than the linear
combination of the labels. The further away the
points are, the more manifolds can exist in between.&lt;/p>&lt;/blockquote>
&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/Manifold_mixup.png"
alt="Risk of manifold intrusion when mixing samples"
width=600>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Illustration from [Baena, 2022]&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> of a &lt;strong>manifold intrusion&lt;/strong> (right) when drawing samples as mixup existing points. The linear interpolation (red line) crosses the blue classe leading to conflict.&lt;/em>&lt;/p>
&lt;p>Bouniot et al. have conduct extensive experiments to show that there is a
&lt;strong>trade-off between adding diversity&lt;/strong> by increasing the proportion
of elements to mix, &lt;strong>and uncertainty&lt;/strong> by mixing elements far from each other.
Furthermore, it shows that we cannot restrict pairs to mix by selecting data
solely based on distance, as &lt;strong>it can degrade performance by reducing diversity of synthetic samples&lt;/strong>.&lt;/p>
&lt;p>To better control this trade-off with Mixup, they suggest to tailor interpolation
coefficients based on the distance of training data. The final part will detail this process.&lt;/p>
&lt;h2 id="33-the-power-of-the-similarity-kernel">3.3 The power of the similarity kernel&lt;/h2>
&lt;p>Bouniot et al. &lt;strong>used a similarity kernel to mix more strongly
similar data and avoid mixing less similar ones&lt;/strong>, to preserve label
quality and confidence of the network.&lt;/p>
&lt;p>To do so, &lt;strong>they needed to change the interpolation coefficient depending on the
similarity between the points&lt;/strong>. They have found a way to preserve the type of
distribution of samples by warping these coefficients at every iteration to
govern the strength and direction of the mixup. Curious readers can refer
to section 3.2 of &lt;sup id="fnref1:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> for technical details. In summary, they only
need the parameter $\tau$ of a Beta distribution $B(\tau, \tau)$ that behaves
logarithmically with this parameter. Hence, $\tau$ should be exponentially
correlated with the distance of points to finally obtain a linear interpolation.&lt;/p>
&lt;p>&lt;strong>To this end, they define a class of similarity kernels, based on a normalized and
centered Gaussian kernel, that outputs the correct warping parameter $\tau$ for the given pair of points.&lt;/strong>
This similarity kernel is defined by the amplitude and the standard derivation
of the Gaussian, two additional parameters to tune separately. The computation
of $\tau$ also depends on the average distance of samples in the same batch.
More specifically, for classification
tasks, they use the $L_2$ distance between embeddings, while for regression
tasks, they use the distance between labels.&lt;/p>
&lt;p>The algorithm to compute this parameter is described bellow in pseudo-code:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-code" data-lang="code">Input: (Batch (x_i, y_i) of size n, kernel similarity parameters, current model parameters)
Sample random permutation sigma
For i in [1, n], do
# Compute the interpolated points from x_i and x_sigma(i):
Compute warping parameter tau using a Beta coefficient and the similarity kernel
Generate new point x_tilde as a linear interpolation of x_i and x_sigma(i), weighted by tau
Generate new label y_tilde as a linear interpolation of y_i and y_sigma(i), weighted by tau
Aggregate new data to batch
Optimize loss over this augmented batch
Output: the updated model parameters
&lt;/code>&lt;/pre>&lt;p align="center">
&lt;figure>
&lt;img src="./images/MixUpDataCalibration/heatmap_density_dist_inverse_warp_v2.png"
alt="Evolution of the similarity kernel"
width=300>
&lt;/img>
&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Figure: Illustration of the effect of the similarity kernel on two points $x_i$ and $x_{\sigma{(i)}}$, additional description bellow. Figure from [Bouniot, 2024]&lt;sup id="fnref2:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>The motivation behind this kernel is to have $\tau &amp;gt;1$ when the two points to mix are similar, i.e., the
distance is lower than average, to increase the mixing effect, and $\tau &amp;lt; 1$ otherwise, to reduce the
mixing. Above Figure illustrates the evolution of the density of warped interpolation coefficients $ω_τ(λ)$,
depending on the distance between the points to mix. Close distances (left part of the heatmap)
induce strong interpolations, while far distances (right part of the heatmap) reduce interpolation.
&lt;strong>Using this similarity kernel to find the correct τ to parameterize the Beta distribution defines our full&lt;/strong>
&lt;strong>Similarity Kernel Mixup framework.&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h2 id="34-going-further">3.4 Going further&lt;/h2>
&lt;p>Extensive experiments have been conducted by the authors on image classification and regression tasks.
They have reproduced the protocol of the literature and &lt;strong>their framework displays an improvement in
both accuracy and calibration across the 3 metrics described above (ECE, Brier and NLL)&lt;/strong>.&lt;/p>
&lt;p>It is important to note, however, that the hyper-parameters have been tuned, and
the best results across different metrics do not share the same values for the kernel standard deviation.&lt;/p>
&lt;p>During the experiment process, the authors have compared the final results after
&lt;em>temperature scaling&lt;/em>, following [Guo, 2017]&lt;sup id="fnref2:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> process. This temperature
is also a learnable parameter that have been optimized during
the training of the models.&lt;/p>
&lt;p>In addition to improving calibration and performance, this approach demonstrates &lt;strong>greater frugality&lt;/strong> by reducing computational overhead compared to other calibration-driven data augmentation methods. By efficiently tailoring the interpolation process, it lowers the number of unnecessary computations and memory usage, contributing to &lt;strong>more sustainable and energy-efficient&lt;/strong> machine learning practices.&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>With similarity kernel, we get a more accurate and better calibrated model because
the coefficients governing the interpolation are warped to change their underlying distribution
depending on the similarity between the points to mix, so that
&lt;strong>similar datapoints are mixed more strongly than less similar ones&lt;/strong>,
&lt;strong>preserving calibration by avoiding manifold intrusion and label noise&lt;/strong>.&lt;/p>
&lt;p>As seen in the pseudo-code, this provides a more efficient data augmentation
approach than Calibration-driven Mixup methods, both in
terms of time and memory, offering a &lt;strong>better trade-off between performance and calibration improvement&lt;/strong>, while promoting frugality by reducing unnecessary computational resources.&lt;/p>
&lt;p>Concurrently, [Verma et al. 2018]&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> have proposed the &lt;strong>Manifold Mixup&lt;/strong>
framework that encourages neural networks to predict less confidently on
interpolations of hidden representation via a simple regularizer. This
training method leads to class-representations with fewer directions of variance.
But even with the actual purpose the reduce the over confidence, the word
&amp;ldquo;Calibration&amp;rdquo; never occurs in the paper&amp;hellip;&lt;/p>
&lt;p>&lt;strong>This highlights the necessity of raising awareness about calibration and establishing a standard process for evaluating models.&lt;/strong>&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to
visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis
and Recognition, volume 2, pages 958–962, 2003.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Ekin D. Cubuk, Barret Zoph , Dandelion Mané, Vijay Vasudevan, Quoc V. Le, Google Brain (2018).
AutoAugment: Learning Augmentation Strategies from Data &lt;a href="https://arxiv.org/abs/1805.09501">arXiv&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, (2020).
A Simple Framework for Contrastive Learning of Visual Representations &lt;a href="https://arxiv.org/abs/2002.05709">arXiv&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2018). mixup: Beyond empirical risk
minimization. In International Conference on Learning Representations.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>Guo, Chuan, et al. “On calibration of modern neural networks.” International Conference on Machine Learning. PMLR, 2017.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref2:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>Bouniot, Q., Mozharovskyi P., d&amp;rsquo;Alché-Buc, F. (2023).
Tailoring Mixup to Data for Calibration &lt;a href="https://arxiv.org/abs/2311.01434">arXiv&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>Cameron R. Wolfe, Ph.D. Confidence Calibration for Deep Networks: Why and How? &lt;a href="https://medium.com/towards-data-science/confidence-calibration-for-deep-networks-why-and-how-e2cd4fe4a086">medium/TowardsDataScience blogpost&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>Maja Pavlovicic, Expected Calibration Error (ECE): A Step-by-Step Visual Explanation &lt;a href="https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/">link&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>Raphael Baena, Lucas Drumetz, Vincent Gripon (2022)
Preventing Manifold Intrusion with Locality: Local Mixup &lt;a href="https://arxiv.org/abs/2201.04368">arXiv&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10">
&lt;p>Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D., and Bengio, Y. (2019).
Manifold mixup: Better representations by interpolating hidden states. In Chaudhuri, K. and
Salakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pages 6438–6447. PMLR.&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/mixupdatacalibration/</guid><pubDate>Sun, 09 Mar 2025 21:03:13 +0100</pubDate></item><item><title>BitFit: BIas-Term FIne-Tuning</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/bitfit/</link><description>&lt;style
TYPE="text/css">
code.has-jax {font:
inherit;
font-size:
100%;
background:
inherit;
border:
inherit;}
&lt;/style>
&lt;script
type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script
type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h1 style="font-size: 24px;">BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers&lt;/h1>
&lt;h3 id="authors--abdoul-r-zeba-nour-yahya-nourelhouda-klich">Authors : Abdoul R. Zeba, Nour Yahya, Nourelhouda Klich&lt;/h3>
&lt;h2 style="font-size: 20px;"> 1. Introduction &lt;/h2>
&lt;p>Fine-tuning large transformer models like BERT has become the gold standard for adapting them to specific tasks. However, this process is often computationally expensive, requiring vast amounts of memory, making it impractical for many real-world applications. What if there was a way to adapt these models with minimal computational overhead while maintaining competitive performance?&lt;/p>
&lt;p>Through this blog post, we will discuss &lt;i>&lt;strong>BitFit&lt;/strong>&lt;/i> — a novel parameter-efficient fine-tuning technique proposed in the paper BitFit: A Simpler and More Efficient Approach to Fine-tuning Transformers (&lt;a href="#benzaken">Ben Zaken et al., 2022&lt;/a>).&lt;/p>
&lt;h2 style="font-size: 20px;"> 2. Why Fine-tuning Needs Optimization &lt;/h2>
&lt;p>Fine-tuning NLP models typically involves updating all model parameters, but this poses some major challenges:&lt;/p>
&lt;ol>
&lt;li>Computational Cost: Training a full BERT-large model requires high-end GPUs and significant memory.&lt;/li>
&lt;li>Deployment Complexity: Every task-specific fine-tuned model requires a separate copy of the large model.&lt;/li>
&lt;li>Transfer Learning Issues: Modifying too many parameters can lead to overfitting on small datasets.&lt;/li>
&lt;/ol>
&lt;p>Wouldn&amp;rsquo;t it be great if we could fine-tune just a small subset of the parameters and get similar results? This is exactly where &lt;i>&lt;strong>BitFit&lt;/strong>&lt;/i> comes in.&lt;/p>
&lt;h2 style="font-size: 20px;"> 3. How BitFit Works &lt;/h2>
&lt;p>Traditional fine-tuning updates &lt;strong>all&lt;/strong> the parameters in a Transformer model, which is computationally expensive. BitFit, on the other hand, &lt;strong>only updates the bias terms&lt;/strong> in the model while keeping all other weights &lt;strong>frozen&lt;/strong>.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Bias terms&lt;/strong> are small but essential parameters in neural networks. They adjust activations before applying transformations, helping models adapt to new tasks with minimal updates.&lt;/p>&lt;/blockquote>
&lt;h3 style="font-size: 18px;"> 3.1 Why Focus on Bias Terms ? &lt;/h3>
&lt;p>Bias terms &lt;strong>&lt;i>b&lt;/i>&lt;/strong> play a crucial role in neural networks because:&lt;/p>
&lt;ul>
&lt;li>They allow neurons to fire (activate) even when inputs are zero.&lt;/li>
&lt;li>Adjusting bias values can shift outputs without requiring full retraining.&lt;/li>
&lt;li>Updating biases is a lightweight operation, meaning less memory and faster adaptation.&lt;/li>
&lt;/ul>
&lt;p>A key finding in the BitFit paper is that bias terms contribute uniquely to fine-tuning. When researchers randomly selected the &lt;strong>same number&lt;/strong> of non-bias parameters for fine-tuning, the model performed significantly worse than BitFit → This suggests that bias parameters are not just a small subset but play an important role in model adaptation.&lt;/p>
&lt;h3 style="font-size: 18px;"> 3.2 What Layers Does BitFit Modify ? &lt;/h3>
&lt;p>BitFit updates the bias terms in &lt;strong>key layers&lt;/strong> of Transformer models like BERT:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Self-Attention Layers&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Transformers use self-attention to focus on important words in a sentence. Each attention head contains three transformations: &lt;strong>Query&lt;/strong> (&lt;code>Q&lt;/code>), &lt;strong>Key&lt;/strong> (&lt;code>K&lt;/code>), &lt;strong>Value&lt;/strong> (&lt;code>V&lt;/code>). And each transformation has its own bias term (&lt;code>bQ&lt;/code>, &lt;code>bK&lt;/code>, &lt;code>bV&lt;/code>).&lt;/p>
&lt;p>→ BitFit &lt;strong>updates only these biases&lt;/strong> (&lt;code>bQ&lt;/code>, &lt;code>bK&lt;/code>, &lt;code>bV&lt;/code>), while the main attention weights (&lt;code>WQ&lt;/code>, &lt;code>WK&lt;/code>, &lt;code>WV&lt;/code>) remain &lt;strong>unchanged&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Feedforward Layers (MLPs)&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Transformers contain fully connected layers that transform intermediate representations. These layers consist of two main &lt;strong>weight matrices&lt;/strong> (&lt;code>W1&lt;/code>, &lt;code>W2&lt;/code>) and &lt;strong>bias terms&lt;/strong> (&lt;code>b1&lt;/code>, &lt;code>b2&lt;/code>).&lt;/p>
&lt;p>→ BitFit &lt;strong>updates only the bias terms&lt;/strong> (&lt;code>b1&lt;/code>, &lt;code>b2&lt;/code>), keeping the weight matrices (&lt;code>W1&lt;/code>, &lt;code>W2&lt;/code>) &lt;strong>frozen&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Layer Normalization (LN) Layers&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Transformers use Layer Normalization to stabilize training and prevent exploding gradients. Each LN layer has two learnable parameters: the &lt;strong>scale factor&lt;/strong> &lt;code>$\gamma$&lt;/code> to control the output scaling and the &lt;strong>bias&lt;/strong> &lt;code>$\beta$&lt;/code> to adjust the mean shift.&lt;/p>
&lt;p>→ BitFit &lt;strong>modifies only the bias term&lt;/strong> &lt;code>$\beta$&lt;/code>, while keeping the scale factor &lt;code>$\gamma$&lt;/code> &lt;strong>fixed&lt;/strong>.&lt;/p>
&lt;h3 style="font-size: 18px;"> 3.3 Mathematical Explanation &lt;/h3>
&lt;h4 style="font-size: 16px;"> 3.3.1 Standard Fine-Tuning &lt;/h4>
&lt;p>In traditional fine-tuning, we update &lt;strong>both&lt;/strong> the weights ($W$) and bias terms ($b$):&lt;/p>
&lt;p>$$
W_{\text{fine-tuned}} = W_{\text{pretrained}} + \Delta W
$$&lt;/p>
&lt;p>$$
b_{\text{fine-tuned}} = b_{\text{pretrained}} + \Delta b
$$&lt;/p>
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$W_{\text{pretrained}}$ is the original weight matrix from the pre-trained model.&lt;/li>
&lt;li>$\Delta W$ is the learned update during fine-tuning.&lt;/li>
&lt;li>$b_{\text{pretrained}}$ is the original bias vector.&lt;/li>
&lt;li>$\Delta b$ is the learned bias update.&lt;/li>
&lt;/ul>
&lt;h4 style="font-size: 16px;"> 3.3.2 BitFit: BIas-Term FIne-Tuning &lt;/h4>
&lt;p>Instead of updating all weights, BitFit &lt;strong>freezes&lt;/strong> $W$ and &lt;strong>only updates&lt;/strong> $b$:&lt;/p>
&lt;p>$$
W_{\text{fine-tuned}} = W_{\text{pretrained}}
$$&lt;/p>
&lt;p>$$
b_{\text{fine-tuned}} = b_{\text{pretrained}} + \Delta b
$$&lt;/p>
&lt;p>Here, $\Delta b$ represents the learned adjustments needed for the new task.&lt;/p>
&lt;h3 style="font-size: 18px;"> 3.4 Are all Bias Terms Equal ? &lt;/h3>
&lt;p>Not all bias parameters contribute equally to fine-tuning. Researchers found that two types of bias terms are especially important:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Query Biases&lt;/strong> &lt;code>bQ&lt;/code>: Found in self-attention layers, responsible for selecting relevant words.&lt;/li>
&lt;li>&lt;strong>Middle-Layer MLP Biases&lt;/strong> &lt;code>b2&lt;/code>: Found in feedforward layers, responsible for transforming representations.&lt;/li>
&lt;/ul>
&lt;p>By only fine-tuning these two subsets, performance remained almost identical to full BitFit while updating half the parameters.&lt;/p>
&lt;blockquote>
&lt;p>BitFit typically fine-tunes &lt;strong>0.08%&lt;/strong> of model parameters, but using only &lt;code>bQ&lt;/code> and &lt;code>b2&lt;/code>, this number drops to &lt;strong>0.04%&lt;/strong> with no major accuracy loss!&lt;/p>&lt;/blockquote>
&lt;p>This means fine-tuning can be made even more efficient by selecting only the most impactful bias terms.&lt;/p>
&lt;h2 style="font-size: 20px;"> 4. How Well Does BitFit Perform? &lt;/h2>
&lt;p>Compared to other parameter-efficient fine-tuning techniques such as Diff-Pruning and Adapters, BitFit achieves competitive performance with significantly fewer trainable parameters.&lt;/p>
&lt;p>BitFit outperforms Diff-Pruning on 4 of the 9 tasks of the GLUE benchmark using the BERTLARGE model and with 6 times fewer trainable parameters. On the test set, BitFit decisively beats Diff-Pruning over two tasks and Adapters over four tasks with 45 times fewer trainable parameters.&lt;/p>
&lt;p>The performance trends of BitFit remain consistent across different base models, e.g., BERTBASE and RoBERTaBASE. The performance of BitFit is not simply due to its adaptation of a collection of parameters, but rather the specific choice of bias parameters. Random selection of an identical number of parameters yields significantly poorer performance, which means that bias parameters have a unique critical contribution to fine-tuning.
Moreover, further analysis reveals that not all bias parameters are equally important as some of them contribute more to the model&amp;rsquo;s performance than others.&lt;/p>
&lt;p>BitFit also demonstrates a smaller generalization gap compared to full fine-tuning, suggesting better generalization capabilities. In token-level tasks such as POS-tagging, BitFit achieves comparable results to full fine-tuning.&lt;/p>
&lt;p>Finally, BitFit&amp;rsquo;s performance also appears to rely on training set size. In experiment with the Stanford Question Answering Dataset, BitFit outperforms full fine-tuning in small-data regimes, but the trend reverses as the training set size increases. What that means is that BitFit is particularly useful when it comes to targeted fine-tuning under small-to-mid-sized data conditions.&lt;/p>
&lt;h2 style="font-size: 20px;"> 5. Why Does BitFit Work? &lt;/h2>
&lt;p>BitFit&amp;rsquo;s success can be attributed to several key factors that challenge traditional assumptions about fine-tuning large language models. Rather than retraining all parameters, BitFit selectively updates only the bias terms, leading to efficient adaptation without sacrificing performance. But why is this approach effective?&lt;/p>
&lt;h3 style="font-size: 18px;"> 5.1 Fine-Tuning as Knowledge Exposure, Not Learning &lt;/h3>
&lt;p>A crucial insight is that fine-tuning large pre-trained transformers is often less about &amp;ldquo;learning new knowledge&amp;rdquo; and more about &amp;ldquo;exposing&amp;rdquo; the knowledge already embedded in the model. Since transformer-based models like BERT have already learned a vast range of linguistic patterns during their unsupervised pre-training phase, adjusting a small number of parameters—specifically the bias terms—can be enough to bring out task-specific information without reworking the entire model.&lt;/p>
&lt;h3 style="font-size: 18px;"> 5.2 Bias Terms and Their Unique Role &lt;/h3>
&lt;p>Bias terms in neural networks serve as offset values, allowing neurons to activate even when input features are zero. Unlike weights, which define relationships between features, bias terms shift outputs in a task-specific manner.&lt;/p>
&lt;p>BitFit leverages the fact that bias terms interact across layers in a way that can subtly adjust how information flows through the model without needing to modify the main weight matrices. This enables significant changes in task-specific performance with minimal modifications to the model structure.&lt;/p>
&lt;h3 style="font-size: 18px;"> 5.3 A Targeted and Structured Approach &lt;/h3>
&lt;p>Not all bias parameters contribute equally to model adaptation. The study found that:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Query Biases (bQ)&lt;/strong>: Found in self-attention layers, crucial for determining which words receive attention.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Middle-Layer MLP Biases (b2)&lt;/strong>: Found in feedforward layers, responsible for transforming hidden representations.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>By focusing updates on these specific biases, BitFit achieves near full fine-tuning performance while modifying only 0.04% of model parameters.&lt;/p>
&lt;h3 style="font-size: 18px;"> 5.4 The Generalization Advantage &lt;/h3>
&lt;p>Another reason BitFit works well is its effect on generalization. Traditional fine-tuning tends to overfit on small datasets because it updates many parameters, potentially memorizing noise rather than learning transferable patterns. BitFit, by contrast, updates only a fraction of the parameters, leading to a smaller generalization gap.&lt;/p>
&lt;p>In fact, experiments show that BitFit performs better than full fine-tuning in small-data regimes. This suggests that limiting parameter updates can sometimes lead to better robustness and generalization.&lt;/p>
&lt;h2 style="font-size: 20px;"> 6. Implications and Future Directions &lt;/h2>
&lt;p>BitFit opens new avenues for efficient fine-tuning, making it particularly relevant in scenarios where computational resources are limited, such as:&lt;/p>
&lt;h3 style="font-size: 18px;"> 6.1 Efficient Deployment in Real-World Applications &lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Low-Resource AI Systems&lt;/strong>: BitFit’s lightweight approach is ideal for deploying NLP models in mobile applications, IoT devices, and embedded AI systems where computational efficiency is critical.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Multi-Task Learning&lt;/strong>: Since only bias terms need updating, multiple tasks can share the same base model, reducing memory overhead and increasing flexibility in production systems.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Scalable NLP Services&lt;/strong>: Cloud-based NLP services that handle multiple tasks (e.g., chatbots, automated translations) can benefit from BitFit by reducing the need to store and load separate models for each task.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 style="font-size: 18px;"> 6.2 Understanding Model Adaptation &lt;/h3>
&lt;p>The success of BitFit raises deeper questions about the nature of transfer learning and fine-tuning:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Do large transformers truly need full fine-tuning, or is most of their knowledge already latent?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Could bias-only tuning be the key to unlocking efficient continual learning strategies?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Are there other small but critical subsets of parameters that can be updated to achieve similar efficiency gains?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 style="font-size: 18px;"> 6.3 Potential Enhancements &lt;/h3>
&lt;p>While BitFit is a promising step forward, future research could explore:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Selective bias tuning&lt;/strong>: Further analyzing which specific bias terms contribute most to adaptation and whether additional optimization can reduce the number of updates even further.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Hybrid approaches&lt;/strong>: Combining BitFit with methods like adapters or LoRA (Low-Rank Adaptation) to achieve even better efficiency.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Application to other architectures&lt;/strong>: Investigating whether BitFit’s principles extend beyond BERT to models like GPT, T5, and multimodal transformers.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 style="font-size: 20px;"> 7. Conclusion &lt;/h2>
&lt;p>In conclusion, BitFit offers a desirable compromise between effectiveness and efficiency, making it a valuable tool for fine-tuning transformer-based models, especially in resource-constrained environments or with limited amounts of training data. Having the capability to achieve competitive performance using significantly fewer trainable parameters, coupled with its achievement in low data regimes, bodes well for other NLP tasks and applications.&lt;/p>
&lt;p>BitFit defies the usual wisdom concerning universal fine-tuning by illustrating how slight tweaks in only a very small percentage of model parameters yield high-performance. This efficient approach makes AI models more accessible, scalable, and cost-effective.&lt;/p>
&lt;h3 id="references">References&lt;/h3>
&lt;ul>
&lt;li>&lt;a id="#benzaken">&lt;/a> &lt;a href="https://aclanthology.org/2022.acl-short.1/">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models&lt;/a> (Ben Zaken et al., ACL 2022)&lt;/li>
&lt;/ul></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/bitfit/</guid><pubDate>Wed, 19 Feb 2025 15:20:48 +0100</pubDate></item><item><title>Axiomatic Explanations for Visual Search, Retrieval and Similarity Learning</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/axiomatic_explanations/</link><description>&lt;style
TYPE="text/css">
code.has-jax {font:
inherit;
font-size:
100%;
background:
inherit;
border:
inherit;}
&lt;/style>
&lt;script
type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script
type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;!DOCTYPE html>
&lt;html lang="en">
&lt;head>
&lt;meta charset="UTF-8">
&lt;meta name="viewport" content="width=device-width, initial-scale=1.0">
&lt;title>Styled Table&lt;/title>
&lt;style>
table {
border-collapse: collapse;
width: 100%;
}
th, td {
padding: 8px;
text-align: center;
border-bottom: 1px solid #ddd;
}
th {
background-color: #f2f2f2;
}
tr:hover {
background-color: #f5f5f5;
}
&lt;/style>
&lt;/head>
&lt;/html>
&lt;h1 style="font-size: 24px;">AXIOMATIC EXPlanATIONS FOR VISUAL SEARCh, RETRIEVAL, AND SIMILARITY LEARNING &lt;/h1>
&lt;h1 style="font-size: 13px;">Authors:Mark Hamilton ${ }^{1,2}$, Scott Lundberg ${ }^{2}$, Stephanie Fu ${ }^{1}$, Lei Zhang ${ }^{2}$, William T. Freeman ${ }^{1,3}$&lt;br>${ }^{1}$ MIT, ${ }^{2}$ Microsoft, ${ }^{3}$ Google&lt;br>markth@mit.edu
&lt;br/>
**Authors of the blogpost**: Yassine Beniguemim and Noureddine BOULLAM.
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0.0">Abstract&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-0.1">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">Exploring Visual Search Algorithm Explanations&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#section-1.1">First-Order Explanations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1.2">Unifying First-Order Search Interpretation Techniques&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1.3">Second-Order Explanations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1.4">A Fast Shapley-Taylor Approximation Kernel&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1.5">Second-Order Search Activation Maps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-2">Implementing Second-Order Explanations in Practice&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="section-0.0">Abstract&lt;/h2>
&lt;p>Visual search, recommendation, and contrastive similarity learning are pivotal technologies shaping user experiences in the digital age. However, the complexity of modern model architectures often obscures their inner workings, making them challenging to interpret. In our blog, we delve into a groundbreaking paper titled &amp;ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING&amp;rdquo; authored by Mark Hamilton et al. This paper introduces a novel framework grounded in the theory of fair credit assignment, providing axiomatic solutions that generalize existing explanation techniques and address fairness concerns in recommendation systems. Through our exploration, we aim to demystify the complexities of visual search algorithms, offering readers insights into their operation and implications for various domains.&lt;/p>
&lt;div style="display: inline-block; width:">
&lt;img src="https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-02.jpg?height=600&amp;width=1354&amp;top_left_y=282&amp;top_left_x=382" alt="Figure 5" width="100%">
&lt;p style="text-align: center; font-size: 10px;">Figure 1: Architectures for search engine interpretability. Like classifier explanations, First-order search explanations yield heatmaps of important pixels for similarity (bottom row third column). Second order search interpretation methods yield a dense correspondence between image locations (last two columns). CAM (second column) is a particular case of Shapley value approximation, and we generalize it to yield dense correspondences (last column).&lt;/p>
&lt;/div>
&lt;h2 id="section-0.1">Introduction&lt;/h2>
&lt;p>Welcome to our blog, where we embark on a journey to demystify the intricate world of visual search technology. In today&amp;rsquo;s digital age, recommendation systems play a pivotal role in guiding users through a vast sea of information, aiding in everything from online shopping to content discovery.&lt;/p>
&lt;p>Yet, behind the scenes, these recommendation engines operate using sophisticated algorithms that can seem like a black box to many users. How do they decide which products to suggest, or which images are most similar to a given query? These questions lie at the heart of our exploration.&lt;/p>
&lt;p>Inspired by the groundbreaking paper &amp;ldquo;AXIOMATIC EXPLANATIONS FOR VISUAL SEARCH, RETRIEVAL, AND SIMILARITY LEARNING&amp;rdquo; authored by Mark Hamilton et al., we embark on a quest to unravel the inner workings of these recommendation systems. By delving into the concepts of fair credit assignment, Shapley values, and Harsanyi Dividends, we aim to shed light on the underlying principles that govern visual search algorithms.&lt;/p>
&lt;h2 id="section-1">1. Exploring Visual Search Algorithm Explanations&lt;/h2>
&lt;p>In our exploration of visual search algorithm explanations, we delve into the fundamental concepts introduced in the paper by Mark Hamilton et al. Our journey begins with an examination of the two distinct classes of explanation methods: &amp;ldquo;first order&amp;rdquo; and &amp;ldquo;second order.&amp;rdquo; First-order approaches focus on highlighting important pixels contributing to object similarity, while second-order explanations provide a comprehensive correspondence between query and retrieved images.&lt;/p>
&lt;h3 id="section-1.1">1.1 First-Order Explanations&lt;/h3>
&lt;p>First-order interpretations are rooted in classifier explainability theory, offering insights into the importance of individual pixels or features in determining object similarity. We explore the theoretical underpinnings of these explanations, drawing parallels to existing techniques such as Class Activation Maps (CAM), GradCAM, and LIME.&lt;/p>
&lt;h4 id="formalizing-first-order-interpretations">Formalizing First-Order Interpretations&lt;/h4>
&lt;p>The core of first-order explanations lies in the formalization of the value function, typically represented as $v_1(S)$, where $S$ represents subsets of features or pixels. This function allows us to quantify the contribution of each subset to the overall similarity score between query and retrieved images.&lt;/p>
&lt;p>$$
v_1(S): 2^N \rightarrow \mathbb{R} := d(x, \text{mask}(y, S))
$$&lt;/p>
&lt;h3 id="section-1.2">1.2 Unifying First-Order Search Interpretation Techniques&lt;/h3>
&lt;p>Building upon existing classifier explainability methods, we introduce an approach to transform opaque and grey-box classification explainers into search engine explainers. By formalizing the value function and leveraging concepts like Shapley values, we extend existing approaches such as LIME and SHAP to the realm of visual search.&lt;/p>
&lt;h4 id="leveraging-shapley-values">Leveraging Shapley Values&lt;/h4>
&lt;p>Shapley values provide a principled way to assign credit to individual features or pixels based on their contribution to the similarity function. By applying Shapley values to the search engine context, we can identify the most influential elements in both query and retrieved images.&lt;/p>
&lt;p>$$
\phi_{v_1}(S) = \sum_{T: S \subset T} \frac{d_v(T)}{\binom{|T|}{|S|}}
$$&lt;/p>
&lt;h3 id="section-1.3">1.3 Second-Order Explanations&lt;/h3>
&lt;p>Moving beyond pixel-level interpretations, we delve into second-order explanations that capture the interactions between areas of query and retrieved images. Drawing inspiration from Harsanyi Dividends and Shapley-Taylor indices, we explore how these concepts generalize to provide richer insights into image similarity.&lt;/p>
&lt;h4 id="understanding-second-order-interpretations">Understanding Second-Order Interpretations&lt;/h4>
&lt;p>Second-order explanations go beyond individual features to capture the interaction strength between different parts of query and retrieved images. We introduce the concept of Harsanyi Dividends, which provide a detailed view of the function&amp;rsquo;s behavior at every coalition of features.&lt;/p>
&lt;p>$$
d_v(S) = \begin{cases} v(S) &amp;amp; \text{if } |S|=1 \
v(S) - \sum_{T \subsetneq S} d_v(T) &amp;amp; \text{if } |S| &amp;gt; 1 \end{cases}
$$&lt;/p>
&lt;h3 id="section-1.4">1.4 A Fast Shapley-Taylor Approximation Kernel&lt;/h3>
&lt;p>While Harsanyi Dividends and Shapley-Taylor indices offer robust credit assignment mechanisms, their computation can be challenging. We introduce a novel weighting kernel for second-order Shapley-Taylor indices, significantly reducing computational complexity while maintaining accuracy.&lt;/p>
&lt;div style="display: inline-block; width: 45%;">
&lt;img src="https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-07.jpg?height=455&amp;width=610&amp;top_left_y=282&amp;top_left_x=405" alt="Figure 5" width="100%">
&lt;p style="text-align: center; font-size: 10px;">Figure 5: Convergence of Shapley-Taylor estimation schemes with respect to the Mean Squared Error (MSE) on randomly initialized deep networks with 15 dimensional input. Our strategies (Kernel) converge with significantly fewer function evaluations.&lt;/p>
&lt;/div>
&lt;div style="display: inline-block; width: 45%;">
&lt;img src="https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-07.jpg?height=455&amp;width=656&amp;top_left_y=282&amp;top_left_x=1079" alt="Figure 6" width="100%">
&lt;p style="text-align: center; font-size: 10px;">Figure 6: Our Second-order explanation evaluation strategy. A good method should project query objects (top left and middle) to corresponding objects in the retrieved image (bottom left and middle). When censoring all but these shared objects (right column) the search engine should view these images as similar.&lt;/p>
&lt;/div>
&lt;h4 id="efficient-computation-with-kernel-approximation">Efficient Computation with Kernel Approximation&lt;/h4>
&lt;p>The proposed weighting kernel allows for efficient approximation of Shapley-Taylor indices, enabling faster computation without sacrificing accuracy. By sampling random coalitions and aggregating information into a weighted quadratic model, we achieve a significant reduction in computational overhead.&lt;/p>
&lt;h3 id="section-1.5">1.5 Second-Order Search Activation Maps&lt;/h3>
&lt;p>Applying the Shapley-Taylor framework, we derive second-order search activation maps, offering dense correspondences between query and retrieved image locations. These maps provide a deeper understanding of image similarity, facilitating more nuanced interpretations of visual search results.&lt;/p>
&lt;h4 id="visualizing-second-order-explanations">Visualizing Second-Order Explanations&lt;/h4>
&lt;p>Using the derived Shapley-Taylor indices, we construct matrices representing the interaction strength between query and retrieved image locations. These matrices allow us to visualize how different parts of the query image correspond to parts of the retrieved image, providing intuitive insights into the similarity judgments made by the search algorithm.&lt;/p>
&lt;div style="display: inline-block; width:">
&lt;img src="https://cdn.mathpix.com/cropped/2024_03_29_e6cbedd60806617ef5aeg-08.jpg?height=1003&amp;width=1312&amp;top_left_y=377&amp;top_left_x=404" alt="Figure 5" width="100%">
&lt;p style="text-align: center; font-size: 10px;">Table 1: Comparison of performance of first- and second-order search explanation methods.&lt;/p>
&lt;/div>
&lt;h2 id="section-2">2. Implementing Second-Order Explanations in Practice&lt;/h2>
&lt;p>With a solid theoretical foundation in place, we now turn our attention to practical implementation steps for incorporating second-order explanations into existing visual search systems.&lt;/p>
&lt;h3 id="section-2.1">2.1 Data Preparation and Preprocessing&lt;/h3>
&lt;p>Before integrating second-order explanations, it&amp;rsquo;s crucial to preprocess and structure the data appropriately. This involves organizing the dataset into query-image pairs, ensuring consistency in image format, resolution, and preprocessing steps such as normalization and resizing.&lt;/p>
&lt;h4 id="data-pipeline-overview">Data Pipeline Overview&lt;/h4>
&lt;p>We design a robust data pipeline encompassing data loading, preprocessing, and augmentation stages. Leveraging popular libraries like TensorFlow and PyTorch, we streamline the process of preparing the dataset for training and evaluation.&lt;/p>
&lt;h3 id="section-2.2">2.2 Model Modification and Integration&lt;/h3>
&lt;p>To enable the computation of second-order explanations, we modify the existing visual search model architecture. This adaptation involves incorporating additional layers or modules to capture the interactions between query and retrieved images.&lt;/p>
&lt;h4 id="architectural-adjustments">Architectural Adjustments&lt;/h4>
&lt;p>We introduce novel components such as interaction modules or attention mechanisms to facilitate the computation of second-order explanations. These architectural adjustments enable the model to learn and represent the complex relationships between different regions of query and retrieved images.&lt;/p>
&lt;h3 id="section-2.3">2.3 Training and Evaluation Procedures&lt;/h3>
&lt;p>Training a visual search model with second-order explanations requires careful consideration of training objectives, loss functions, and evaluation metrics. We devise training procedures that optimize both the primary search task and the secondary objective of generating accurate explanations.&lt;/p>
&lt;h4 id="objective-function-formulation">Objective Function Formulation&lt;/h4>
&lt;p>We define a composite objective function that combines the primary search task loss with a regularization term for encouraging meaningful second-order explanations. This formulation ensures that the model learns to balance between search accuracy and explanation fidelity during training.&lt;/p>
&lt;h3 id="section-2.4">2.4 Validation and Interpretation&lt;/h3>
&lt;p>Once trained, we validate the effectiveness of the model&amp;rsquo;s second-order explanations through comprehensive evaluation procedures. This involves qualitative analysis of explanation maps, quantitative assessment of explanation quality, and user studies to gauge the interpretability of the generated explanations.&lt;/p>
&lt;h4 id="evaluation-metrics">Evaluation Metrics&lt;/h4>
&lt;p>We define metrics such as explanation fidelity, coherence, and relevance to quantitatively evaluate the quality of second-order explanations. By comparing against baseline methods and human annotations, we assess the model&amp;rsquo;s ability to capture meaningful interactions between query and retrieved images.&lt;/p>
&lt;h3 id="section-2.5">2.5 Deployment Considerations&lt;/h3>
&lt;p>Deploying a visual search system with second-order explanations requires careful planning and integration into existing infrastructure. We address scalability, latency, and user experience considerations to ensure seamless deployment in real-world applications.&lt;/p>
&lt;h4 id="scalable-inference-architecture">Scalable Inference Architecture&lt;/h4>
&lt;p>We design an inference pipeline optimized for efficient computation of second-order explanations in production environments. This involves leveraging distributed computing frameworks and model optimization techniques to minimize latency and maximize throughput.&lt;/p>
&lt;h2 id="section-3">3. Conclusion&lt;/h2>
&lt;p>By following these implementation steps, we bridge the gap between theoretical insights and practical deployment of second-order explanations in visual search systems. Our approach empowers users to gain deeper insights into the underlying mechanisms driving search results, paving the way for more transparent and interpretable AI systems.&lt;/p>
&lt;h2 id="additional-resources">Additional Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Video Description&lt;/strong>: Dive deeper into the concepts with a detailed video overview available &lt;a href="https://aka.ms/axiomatic-video">here&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Code Repository&lt;/strong>: Access the training and evaluation code to explore the implementation details &lt;a href="https://aka.ms/axiomatic-code">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>For a comprehensive exploration of the technical details and experimental results, refer to the &lt;a href="https://arxiv.org/pdf/2103.00370.pdf">full paper&lt;/a>.&lt;/p>
&lt;h2 id="references">REFERENCES&lt;/h2>
&lt;p>Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels. Technical report, 2010.&lt;/p>
&lt;p>Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation with inter-pixel relations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2209-2218, 2019.&lt;/p>
&lt;p>Marco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. In International Conference on Machine Learning, pp. 272-281. PMLR, 2019.&lt;/p>
&lt;p>Robert J Aumann and Lloyd S Shapley. Values of non-atomic games. Princeton University Press, 2015.&lt;/p>
&lt;p>Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.&lt;/p>
&lt;p>Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.&lt;/p>
&lt;p>Bing. Beyond text queries: Searching with bing visual search, Jun 2017. URL https://aka. ms/AAas 7 jg.&lt;/p>
&lt;p>Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 12091218, 2018.&lt;/p>
&lt;p>Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.&lt;/p>
&lt;p>Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 782-791, 2021.&lt;/p>
&lt;p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.&lt;/p>
&lt;p>Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020 b.&lt;/p>
&lt;p>Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang. Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020c.&lt;/p>
&lt;hr>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/axiomatic_explanations/</guid><pubDate>Thu, 28 Mar 2024 05:58:39 +0100</pubDate></item><item><title>Privacy Amplification by Decentralization</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/privacy-amplification/</link><description>&lt;h1 style="font-size: 36px;">Privacy Amplification by Decentralization&lt;/h1>
&lt;h1 style="font-size: 24px;">Author: Sarah ABBANA BENNANI &lt;/h1>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-1">Introduction - the challenge of data privacy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Theoretical Aspects on Differential Privacy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">First Case: walk on a ring&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">Generalisation: walk on a complete graph&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">Experiments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">Perspectives&lt;/a>&lt;/li>
&lt;/ul>
&lt;br />
&lt;p>This is a blogpost about the paper Privacy Amplification by Decentralization, published by E. Cyffers et al. in 2022 and available &lt;a href="https://proceedings.mlr.press/v151/cyffers22a/cyffers22a.pdf">here&lt;/a>.&lt;/p>
&lt;h1 id="section-1">&lt;h1 style="font-size: 20px;">Introduction - the challenge of data privacy&lt;/h1>&lt;/h1>
&lt;p>In recent years, the concept of privacy has gained significant attention due to the proliferation of data collection practices and the need to safeguard individuals&amp;rsquo; personal information. &lt;br>
There has been a notable shift towards implementing regulations to govern the gathering of data from individuals, underscoring the pressing demand for privacy measures that are not only effective and robust against potential attacks but also transparent and firmly grounded in logic and mathematics.&lt;/p>
&lt;p>&lt;strong>A current way to define privacy in the context of data sharing is the promess of the dataholder (the person or entity managing the data) towards the users, that there will be no consequences (positive or negative) induced by their consent to sharing their data.&lt;/strong>&lt;/p>
&lt;br />
&lt;p>&lt;em>Let us take a small example to illustrate and to understand the underlying complexity of this notion: we consider an entity that desires to conduct a study on the correlation between smoking and cancer risks. &lt;br>
Should a smoker participate, and the study concludes that smoking indeed increases the likelihood of cancer, the repercussions for the smoker could vary.&lt;/em>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Some negative impacts: insurance premiums could increase&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Some positive impacts: motivation to quit smoking&lt;/em>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;em>We could therefore think that privacy in this case is broken for the participant, however there is a subtility which is one of the keys to capture the nuance between the privacy of an individiual and that of a group. In this case, crucially, we cannot say privacy is breached, as the participation of the smoker should not alter the study&amp;rsquo;s outcome, i.e. from a probabilistic standpoint, whether or not the individual participates in the study will not significantly change the likelihood of the conclusion of the study. &lt;br>
Formally, and to introduce some probabilities, which we will delve into further later on:&lt;/em>&lt;/p>
&lt;h1 style="font-size: 13px;">$\mathbb{P}(result=smoking\ bad | individual\ participates) \approx \mathbb{P}(result=smoking\ bad | individual\ does\ not\ participate) $&lt;/h1>
&lt;br />
&lt;p>Privacy has become a real challenge for all parties, as &lt;strong>it is necesssary to find a balance between the utility of the data and the privacy guarantees of the users&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>For the dataholder, the aim is to retain the wealth of data to derive useful insights. They must be able to analyse enough of the data to learn about the population without revealing any individual-specific information.&lt;/li>
&lt;li>For the users, they must believe that their data will be protected and that they will not be hurt by giving them. This trust in the dataholder is important to incite the users to give their data.&lt;/li>
&lt;/ul>
&lt;br />
&lt;p>In this paper, the aim of the authors, E. Cyffers and A. Bellet, was to show some algorithms and methods that allow to improve the privacy-utility trade-offs and therefore reinforce privacy around the data, while keeping scalability.&lt;/p>
&lt;p>The proposed algorithms are based on &lt;strong>full decentralization&lt;/strong>, and &lt;strong>newtork differential privacy (DP)&lt;/strong>, two notions that we will explain right below.&lt;/p>
&lt;br />
&lt;h1 id="section-2">&lt;h1 style="font-size: 20px;">Theoretical Aspects on Differential Privacy&lt;/h1>&lt;/h1>
&lt;h2 id="mathematical-context">&lt;h1 style="font-size: 18px;">Mathematical context&lt;/h1>&lt;/h2>
&lt;p>We must introduce some key mathematical definitions to understand the problem we want to tackle.&lt;/p>
&lt;h2 id="users-space">Users space&lt;/h2>
&lt;p>We consider a set of $n$ users (e.g. a population responding to a survey), each holding a private dataset that we note $D_u$ (e.g. their answer to the questions of the survey).&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/users-space.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;h2 id="neighboring-relation">Neighboring relation&lt;/h2>
&lt;p>We write $D=D_1 \cup \cdots \cup D_n$ the union of all users datasets.&lt;/p>
&lt;p>We can define a &lt;strong>neighboring relation&lt;/strong> over these datasets, that we call user-level Differential Privacy: &lt;br>
For two datasets $D$ and $D&amp;rsquo;$ of the same size, we denote by $D \sim_u D^{\prime}$ the fact that $D$ and $D&amp;rsquo;$ are neighbors, in the sense that they only differ on user $u$&amp;rsquo;s data.&lt;/p>
&lt;p>&lt;em>For example, $D$ and $D&amp;rsquo;$ could be two datasets corresponding to the answers of a survey from 10 users. For nine of these users the answers are the same for the two datasets. But for one user $u$, the answers are different (e.g. in $D$ user $u$ smokes, in $D&amp;rsquo;$ he doesn&amp;rsquo;t smoke).&lt;/em>&lt;/p>
&lt;p>The inuition between this definition relatively to privacy is that compared to traditional differential privacy, which considers changes in individual data points, user-level DP provides stronger privacy guarantees. By hiding the influence of an entire user&amp;rsquo;s dataset, rather than just a single data point, it ensures that individual user contributions are not discernible, thus enhancing overall privacy protection.&lt;/p>
&lt;h2 id="decentralization">Decentralization&lt;/h2>
&lt;p>We will set ourselves in a fully decentralized system. In this configuration, each user only communicates with a small number of other users at each step, and there is no central coordinator processing all the data. The aim of this setting is to limit the concentration of sensitive information in one place, reducing the risk of data breaches and unauthorized access.&lt;/p>
&lt;p>The users and their communications are represented by a network (directed or undirected) graph $G = (V, E)$, where $V$ is the users ensemble defined above, and $E$ is the set of edges: $(u, v) \in E$ indicates that user $u$ can send messages to user $v$.&lt;/p>
&lt;br />
&lt;p>In this case, a randomised decentralized algorithm is defined as a mapping that from a dataset, returns that transcript of all messages exchanges between the users over the network. In formal terms, $A: D \longmapsto {(u, m, v): u \text{ sent message with content } m \text{ to } v }$.&lt;/p>
&lt;br />
&lt;p>The aim of decentralization in this representation, is to give users the fewer information possible, i.e. only the messages they are involved in, and not the full transcript $A(D)$.&lt;/p>
&lt;p>We introduce this view of a user $u$: $\mathcal{O}_u(\mathcal{A}(D))=\left(\left(v, m, v^{\prime}\right) \in \mathcal{A}(D): v=u \text { or } v^{\prime}=u\right)$&lt;/p>
&lt;br />
&lt;h2 id="differential-privacy">&lt;h1 style="font-size: 18px;">Differential Privacy&lt;/h1>&lt;/h2>
&lt;p>We will take a step back on this representation to introduce in a more global way the mathematical notion of Differential Privacy (DP).&lt;/p>
&lt;p>Let us consider a randomised algorithm $M$. $M$ is said to be &amp;ldquo;$\alpha$-differentially private&amp;rdquo; if, for any event $A$:&lt;/p>
&lt;p>$$\mathbb{P}[M(D)\in A]\leq e^{\alpha} \cdot \mathbb{P}[M(D&amp;rsquo;)\in A]$$&lt;/p>
&lt;p>where $D$ and $D&amp;rsquo;$ are two datasets differing on a single element.&lt;/p>
&lt;br />
&lt;p>To make this more intuitive, a randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm must treat the data so that the output is not overly depend on the data of any one individual.&lt;/p>
&lt;p>Let&amp;rsquo;s consider the event &amp;ldquo;Smoking is correlated to cancer&amp;rdquo;, and $D$ and $D&amp;rsquo;$ differing on the user $u$&amp;rsquo;s data, whether or not that individual that has cancer smokes or not.&lt;/p>
&lt;p>We can rewrite the definition as: $\frac{\mathbb{P}\left[M\left(D\right) \in A\right]}{\mathbb{P}\left[M\left(D&amp;rsquo;\right) \in A\right]} \leq e^{\alpha}$&lt;/p>
&lt;p>We can see that $\alpha$, the privacy factor, represents the lost of privacy:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>When $\alpha \rightarrow 0$: the two probabilities are equal, meaning that whether user $u$ participates or not to the survey, the result is the same, i.e. privacy is at its maximum, but the statistical utility is null.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When $\alpha \rightarrow+\infty$: there are no constraints on the probabilities and therefore no constraints on privacy.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Thus it is the intermediate case for $\alpha$ that is the most interesting and that can allow a good trade-off between privacy and utility.&lt;/p>
&lt;br />
&lt;h2 id="network-differential-privacy">Network Differential Privacy&lt;/h2>
&lt;p>In this paper, the definition used for Differential Privacy is a bit different, actually relaxed as the algorithm is decentralized.&lt;/p>
&lt;p>An algorithm $A$ is said to be $(\varepsilon, \delta)$-network Differentially Private if for all pairs of distinct user $u, v \in V$ and all pairs of neighboring datasets $D \sim_u D^{\prime}$, we have:&lt;/p>
&lt;p>$$ \mathbb{P}\left(\mathcal{O}_v(\mathcal{A}(D))\right) \leq e^{\varepsilon} \mathbb{P}\left(\mathcal{O}_v\left(\mathcal{A}\left(D^{\prime}\right)\right)\right)+\delta $$&lt;/p>
&lt;p>We can interpret this as the need that the information gathered by $v$ during the execution of $A$ must not depend too much on $u$&amp;rsquo;s data.&lt;/p>
&lt;br />
&lt;p>Furthermore, the definition can be extended in the case of collusion between the users, i.e. if multiple individuals collaborate or conspire to exploit or manipulate a system or process for their collective benefit.&lt;/p>
&lt;p>An algorithm $A$ is $(c, \varepsilon, \delta)$-network DP if for each user $u$, all subsets $W \subset V$ such that $\left|W\right| \leq c$, and all pairs of neighboring datasets $D \sim_u D^{\prime}$, we have:&lt;/p>
&lt;p>$$ \mathbb{P}\left(\mathcal{O}_W(\mathcal{A}(D))\right) \leq e^{\varepsilon} \mathbb{P}\left(\mathcal{O}_W\left(\mathcal{A}\left(D^{\prime}\right)\right)\right)+\delta $$&lt;/p>
&lt;p>Here $\mathcal{O}_W$ represents the aggregated information of the collusion: $\mathcal{O}_W = \cup _{w \in W} \mathcal{O}_w$.&lt;/p>
&lt;br />
&lt;h2 id="decentralized-computation-model">Decentralized computation model&lt;/h2>
&lt;p>The algorithms studied in this paper are meant to perform computations by using a token that will walk through the nodes of the network graph. The purpose of the token is to facilitate sequential updates across the nodes in the network. As it traverses through the nodes following the edges of the graph, it carries information and updates its states based on local computations performed at each node from the values obtainable from the corresponding user.&lt;/p>
&lt;p>If the token $\tau$ resides at some node $u$, it will be:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Updated by: $\tau \leftarrow \tau+x_u^k, \quad$ with $x_u^k=g^k\left(\tau ; D_u\right)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Sent to another user $v$ with $(u, v) \in E$&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Here, $x_u^k$ denotes the contribution of user $u$ to the computation. It depends both on the current value of $\tau$ and on the number of times $k$ that the token visited $u$ so far.&lt;/p>
&lt;p>This model of computation allows to optimize the combination of local costs within the network, which is useful for tasks like training machine learning models. The token holds the model&amp;rsquo;s parameters and is updated based on the local information at each point it visits. This decentralized approach can also be used to calculate summaries of data contributed by users, such as finding totals or averages.&lt;/p>
&lt;br />
&lt;p>&lt;em>The idea of the following parts is to study different graph achitectures and computation protocols, based on the formalization explained above, to achieve good utility-privacy trade-offs&lt;/em>&lt;/p>
&lt;br />
&lt;h1 id="section-3">&lt;h1 style="font-size: 20px;">First case: walk on a ring&lt;/h1>&lt;/h1>
&lt;p>We consider here a graph architecture of a directed ring, i.e. $E = {(u, u+1)}_{u=1}^{n-1} \cup{(n, 1)}$, meaning that the token, starting from the first user, will travel around the ring multiple times, and more precisely go through every user $K$ times.&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/walk-on-ring.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>This is a simple case that is meant to show how we can achieve suitable results without relying on a centralised agregator.&lt;/p>
&lt;p>We are going to explain how this architecture can perform for privacy guarantees on the task of &lt;em>Real Summation&lt;/em>, and then on &lt;em>Discrete Histogram Computation&lt;/em>.&lt;/p>
&lt;br />
&lt;h2 id="real-summation">Real Summation&lt;/h2>
&lt;p>Each user will contribute a value during each round of the token&amp;rsquo;s journey. The task of &lt;em>real summation&lt;/em> aims to estimate the sum of all contributions made by users.&lt;/p>
&lt;p>&lt;em>For example, we can imagine a scenario where users of a health monitoring app report their daily step counts. The app&amp;rsquo;s goal is to calculate the total number of steps taken by all users, without revealing individual step counts. Each user&amp;rsquo;s daily step count is considered a contribution, and the app needs to aggregate these contributions while preserving user privacy.&lt;/em>&lt;/p>
&lt;p>Indeed to preserve privacy in this case, a common method is to add random noise, an abstract perturbation mechanism, which usually consist in a standard Gaussian or Laplace deviation to the contribution. We won&amp;rsquo;t go into further details on the perturbation, but we assume that it satisfies traditional local differntial privacy (LDP).&lt;/p>
&lt;p>Furthermore, here the decentralized protocol proposes to add this noise only once every few hops of the token, and in fact every $n-1$ hops of the token as shown in the algorithm below:&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/algo1.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>They prove the following theorem:&lt;/p>
&lt;p>&lt;strong>Theorem:&lt;/strong> Let $\varepsilon, \delta&amp;gt;0$. Algorithm 1 outputs an unbiased estimate of $\bar{x}$ with standard deviation $\sqrt{\left\lfloor \frac{Kn}{n - 1} \right\rfloor} \sigma_{\text{loc}}$, and is $\sqrt{2K \ln\left(\frac{1}{\delta&amp;rsquo;}\right)\varepsilon}$ $+ K\epsilon(e^\varepsilon - 1), K\delta + \delta&amp;rsquo;$-network DP for any $\delta&amp;rsquo; &amp;gt; 0$&lt;/p>
&lt;p>The Algorithm 1 proposed actually provides a gain on the error of $O\left(\frac{1}{\sqrt{n}}\right)$ compared to a LDP achieving the same privacy guarantees. This means it achieves a similar balance between privacy and utility as a centralized aggregator would, if they itratively aggregated user contributions then perturb the results before sending it to the users, buy here without the need for this centralized party.&lt;/p>
&lt;br />
&lt;h2 id="discrete-histogram-computation">Discrete Histogram Computation&lt;/h2>
&lt;p>Here we focus on another task that is computing histograms over a discrete domain.&lt;/p>
&lt;p>&lt;em>With the same example as above, it could be such as counting the frequency of steps in different ranges for a health monitoring app.&lt;/em>&lt;/p>
&lt;p>Traditional local differential privacy (LDP) methods use L-ary randomized response, where each user submits their true value with probability $1-\gamma$ and a random value with probability $\gamma$. However, in the decentralized approach with a ring network, they propose Algorithm 2. This algorithm randomizes each user&amp;rsquo;s contribution using L-ary randomized response before adding it to the token, which maintains a partial histogram representing the shuffled contributions, thus enhancing privacy through shuffling, as demonstrated in previous studies.&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/algo2.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>As for the case of real summation, a theorem proves that to achieve the same privacy in LDP, it would need $\sqrt n$ times more random responses, and when achieving the same utility (meaning to fix $\gamma$), Algorithm 2 provides a gain of privacy of $O\left(\frac{1}{\sqrt{n}}\right)$.&lt;/p>
&lt;br />
&lt;p>We see that decentralized computation over a ring enables comparable utility to a trusted aggregator by sequentially hiding previous users&amp;rsquo; contributions, without relying on a central server or requiring costly multi-party computation protocols.&lt;/p>
&lt;p>However this simple topology presents limitations including vulnerability to collusions, which compromises differential privacy guarantees, and inadequacy for extensions to gradient descent due to the lack of privacy amplification between users with fixed positions in the ring.&lt;/p>
&lt;p>This is why we shall now consider random walks over a complete graph.&lt;/p>
&lt;br/>
&lt;h1 id="section-4">&lt;h1 style="font-size: 20px;">Generalisation: walk on a complete graph&lt;/h1>&lt;/h1>
&lt;p>Random walk on a complete graph assumes the token is randommly sent to a user at each step. The walk consists of fixed-length random walks, ensuring that each user&amp;rsquo;s contributions are random, and their path is concealed, allowing only the messages sent and received to be known by a user.&lt;/p>
&lt;br />
&lt;h2 id="real-summation-1">Real Summation&lt;/h2>
&lt;p>Algorithm 3 shows the protocol, naturally extended from the ring topology, where each user updates the token with its contribution and a perturbation. The secrecy of the path taken by the token and the aggregations of the contributions between two visits of the token guarantee the network DP property.&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/algo3.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>Again, a theorem proves that asymptotically, network DP offers a privacy amplification of $O\left(\frac{1}{\sqrt{n}}\right)$ over LDP for the same conditions, which aligns with the privacy-utility trade-off of a trusted aggregtor.&lt;/p>
&lt;br />
&lt;p>The same analysis can be done for the discrete histogram computation case.&lt;/p>
&lt;br />
&lt;h2 id="stochastic-gradient-descent">Stochastic Gradient Descent&lt;/h2>
&lt;p>In this section, we address the challenge of private convex optimization using stochastic gradient descent (SGD). We consider a convex set $(W \subseteq \mathbb{R}^d)$ and a collection of convex functions $(f(\cdot; D_1), \ldots, f(\cdot; D_n))$, each associated with a user, being L-Lipschitz and $(\beta)$-smooth over $(W)$. Our goal is to privately solve the optimization problem to find $(w^*)$ minimizing the average of these functions over $(W)$:&lt;/p>
&lt;p>$$w^* \in \arg \min_{w \in \mathcal{W}} \left( F(w):=\frac{1}{n} \sum_{u=1}^n f\left(w ; D_u\right) \right)$$&lt;/p>
&lt;p>This equation encapsulates various machine learning tasks, such as ridge and logistic regression, and others. This is significant because it addresses the need for private optimization in machine learning, ensuring that sensitive data remains protected while training models on distributed datasets.&lt;/p>
&lt;p>The algorithm below proposes a method to privately approximate $w^*$, where the token represents the current iterate. At each step, the user $u$ holding the token performs a projected noisy gradient step and sends the updated token to a random user. The variance in the Gaussian mechanism of line 4 is deduced from the Lipschitz property of the functions.&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/algo4.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>A theorem based on the evolution of the privacy loss proves the differential privacy guarantees, and again the results are satisfactory. Compared to traditional local differential privacy methods, we obtain a privacy amplification of $O\left(\frac{\ln n}{\sqrt{n}}\right)$ for a specific number of iterations, with the same level of privacy-utility trade-off.&lt;/p>
&lt;p>With a fixed privacy budget and a large number of iteration, the expected error of this algorithm is smaller with this network DP than with LDP.&lt;/p>
&lt;br />
&lt;p>Compared to the ring case, this random walk approach has better robustness to collusion, as colluding users can be treated as a single node with adjusted transition probabilities, leading to equivalent privacy guarantees as for non-colluding users.&lt;/p>
&lt;br/>
&lt;h1 id="section-5">&lt;h1 style="font-size: 20px;">Experiments&lt;/h1>&lt;/h1>
&lt;p>To show the efficiency of the privacy amplification methods explained in this article, some experiments have been made on the complete graph, first for the Real Summation task, and then for Machine Learning with Stochastic Gradient Descent (SGD).&lt;/p>
&lt;p>The code is available here: &lt;a href="https://github.com/totilas/privacy-amplification-by-decentralization/tree/main">Github Link&lt;/a>&lt;/h1>&lt;/p>
&lt;br />
&lt;h2 id="real-summation-2">Real Summation&lt;/h2>
&lt;p>We reproduced the first experiment from the paper, comparing th analytical bounds of LDP and NDP on the real summation task.&lt;/p>
&lt;p>To do so, we only need to run the &lt;em>main_a.py&lt;/em> and &lt;em>main_b.py&lt;/em> files with python from the &lt;em>fig1&lt;/em> folder to display the corresponding figures (a) and (b). It works, for instance, with Python version 3.8, with the prerequisite of having installed the packages &lt;em>numpy&lt;/em> and &lt;em>matplotlib&lt;/em>, only taking a few seconds to execute.&lt;/p>
&lt;p>It gives the following results:&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/results1.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>As we may see from the theoretical bounds, privacy is amplified with network differential privacy over LDP when the numer of users $n$ is greater or equal to 20, with increaingly substancial improvements as $n$ grows.&lt;/p>
&lt;p>In practice by making some simulations, the gains are even more significant and even for a smaller number of users, as we see in figure (b).&lt;/p>
&lt;br/>
&lt;h2 id="machine-learning-with-sgd">Machine Learning with SGD&lt;/h2>
&lt;p>For this second experiment, the task is to train a logistic regression model in this decentralized context.&lt;/p>
&lt;p>The setting of the experiment is:&lt;/p>
&lt;ul>
&lt;li>UCI Housing dataset (binarized version)&lt;/li>
&lt;li>Standardized features and normalized data point (to have unit L2 norm and Lipschitz property of the logistic loss)&lt;/li>
&lt;li>Train/test split of 80% uniformly at random&lt;/li>
&lt;li>Training set split between $n = 2000$ users (each user has a local dataset of size $8$)&lt;/li>
&lt;/ul>
&lt;p>The experiment compares three settings for Stochastic Gradient Descent with perturbation:&lt;/p>
&lt;ul>
&lt;li>Centralized DP-SGD, requiring a trusted curator&lt;/li>
&lt;li>Local DP-SGD, corresponding to Algorithm 4 with LDP method&lt;/li>
&lt;li>Network DP-SGD, corresponding to Algorithm 4 with Network DP method, the one of interest&lt;/li>
&lt;/ul>
&lt;br/>
&lt;p>We must run the &lt;em>main.py&lt;/em> file of folder &lt;em>fig2&lt;/em> with Python to display the results.&lt;/p>
&lt;p>It is possible to use the command &lt;em>python main.py &amp;ndash;help&lt;/em> to show the list of parameters that can be tuned to modify the context of the experiment (the default ones are for $\varepsilon = 10$ and $\varepsilon = 1$):&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/options.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>I had some issues to run this program with my settings (same as for the first experiment).&lt;/p>
&lt;ol>
&lt;li>The &lt;em>typer&lt;/em> module was missing therefore I had to install it : &lt;em>pip install typer&lt;/em>&lt;/li>
&lt;li>The &lt;em>_intercept_dot&lt;/em> function from &lt;em>sklearn.linear_model._logistic&lt;/em> couldn&amp;rsquo;t be found either. By checking the &lt;em>sklearn.linear_model.LogisticRegression&lt;/em> (which is the public class corresponding to the import here), this function doesn&amp;rsquo;t appear. I wanted to change it with the &lt;em>intercept_&lt;/em> attribute but it didn&amp;rsquo;t fit either. Then by checking the usage of this function in the case, it seemed that it computes a dot product between the model parameters and the input data, taking into account whether an intercept term is included. Therefore I tried to manually code this functionality but unfortunately it didn&amp;rsquo;t give coherent results compared to the paper.&lt;/li>
&lt;/ol>
&lt;p>Here are the original results from the paper:&lt;/p>
&lt;p>&lt;img
src="./images/Sarah_Abbana/results2.png"
alt=""
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>Here, although the number of contributions per user doesn&amp;rsquo;t align with the optimal regime for network DP, the observed privacy amplification surpasses theoretical expectations. By numerically determining the minimum noise level required for theoretical proofs, they demonstrated that Network DP-SGD achieves a privacy-utility trade-off comparable to Centralized DP-SGD across various privacy levels, showcasing significant privacy amplification benefits over Local DP-SGD, especially in scenarios with fewer iterations than typically recommended.&lt;/p>
&lt;br/>
&lt;h1 id="section-6">&lt;h1 style="font-size: 20px;">Perspectives&lt;/h1>&lt;/h1>
&lt;p>The work presented suggests numerous avenues for exploration. Generalizations to diverse graph structures, incorporating dynamic topologies to reinforce resilience against collusion, and investigating decentralized models beyond our current scope are key directions. Exploring the potential of multiple tokens traversing the graph simultaneously and delving into randomized gossip algorithms offer promising avenues for advancing privacy-preserving techniques. Finally, probing the theoretical limits of network DP and exploring scenarios where users trust nearby peers more could provide insights into refining privacy mechanisms.&lt;/p>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/privacy-amplification/</guid><pubDate>Wed, 27 Mar 2024 12:05:50 +0100</pubDate></item><item><title>Robust or Fair</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/robust-or-fair/</link><description>&lt;h1 style="font-size: 36px;">To be Robust or to be Fair: Towards Fairness in Adversarial Training&lt;/h1>
&lt;h1 style="font-size: 24px;">Authors: Maryem Hajji &amp; Cément Teulier&lt;/h1>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0">Abstract&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Initial Analysis&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#section-2.1">Previous Studies&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2.2">Theoretical Demonstration&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-3">Model&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#section-3.1">Fairness Requirements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3.2">Practical Algorithms&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-4">Experimentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="section-0">Abstract&lt;/h2>
&lt;p>This blog post retraces the study conducted in the &lt;a href="http://proceedings.mlr.press/v139/xu21b.html">paper&lt;/a> &amp;ldquo;To be Robust or to be Fair: Towards Fairness in Adversarial Training&amp;rdquo; and written by Han Xu, Xiaorui Liu, Yaxin Li, Yaxin Li, Anil K. Jain and Jiliang Tang.&lt;/p>
&lt;p>Their study is based on a simple observation: while adversarial training has been shown to improve model&amp;rsquo;s robustness, it also introduces several performances disparities among different data groups.&lt;/p>
&lt;p>To address this issue, the authors present the Fair-Robust-Learning (FRL) framework that aims to reduce such unfairness.&lt;/p>
&lt;h2 id="section-1">Introduction&lt;/h2>
&lt;p>Nowadays, Machine Learning algorithms and Artificial Intelligence are becoming more and more omnipresent in all kinds of jobs. If many of these models are developed to replace human tasks, it is of key importance that they do not reproduce the same mistakes. In fact, human decision making can sometimes be considered &amp;ldquo;unfair&amp;rdquo;, a trait that must not be present in Machine Learning. But as we push our models to be as precise as possible, one question stands out: can we find the good balance between accuracy and equity ?&lt;/p>
&lt;p>Diving into this topic, we focus our study on adversarial training algorithms.
Indeed, it has been shown that there is a significant issue in adversarial training for deep neural networks: while such training boosts the model&amp;rsquo;s defenses against adversarial attacks, it unfortunately leads to significant differences in how well the model performs across various types of data.
For instance, detailed observations on CIFAR-10 dataset show a non-negligeable difference in the model&amp;rsquo;s performance between &amp;ldquo;car&amp;rdquo; and &amp;ldquo;cat&amp;rdquo; classes (details of this example in our section 1.1).&lt;/p>
&lt;p>This phenomenon raises concern on concrete topics like the safety of autonomous driving vehicules or facial recognition while also creating ethical problems by discriminating certain classes.
To put a word on it, the authors have identified this issue as the &lt;strong>robust-fairness&lt;/strong> problem of adversarial training.&lt;/p>
&lt;h2 id="section-2">1. Initial Analysis&lt;/h2>
&lt;p>We recall here the previous studies conducted by the authors that allowed them to identify the existence of the robust-fairness problem.&lt;/p>
&lt;h2 id="section-2.1">1.1 Previous Studies&lt;/h2>
&lt;p>For their first analysis, the authors have decided to study algorithms like the PGD ( Projected Gradient Descent) adversarial training and TRADES ( Theoretically Principled Trade-off between Robustness and Accuracy for Deep Learning ) on the CIFAR-10 dataset.
The investigation is made using a PreAct-ResNet18 model structure under specific adversarial attack constraints.
The results they obtained are as follows:&lt;/p>
&lt;p>&lt;img
src="./images/Hajji_Teulier/cat_car.png"
alt="Paper Initial Results"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>As we can see, natural training maintains a consistent standard error across classes and a consistent robust error rate when faced with 8/255 PGD attacks. However, in the case of adversarial training, some huge disparities appear. Going back to our introduction&amp;rsquo;s example with &amp;ldquo;cats&amp;rdquo; and &amp;ldquo;cars&amp;rdquo;, we observe that the standard and robust errors for &amp;ldquo;car&amp;rdquo; class ( respectively 6% and 34% ) are significantly lower than those of the &amp;ldquo;cat&amp;rdquo; class ( respectively 33% and 82% ). The results on the TRADES, altough not depicted here, also show some great disparities between certain classes.&lt;/p>
&lt;p>To support this graphical study, the authors also present statistical evidence of this phenomenom throughout metrics like the Standard Deviation (SD) or the Normalized SD (NSD) of class-wide error. Once again, these metrics reveal that adversarial training indeed results in greater disparities across classes in both standard and robust performance compared to natural training.&lt;/p>
&lt;h3 id="potential-causes">Potential Causes&lt;/h3>
&lt;p>While the authors succeeded in identifying the problem of fairness, they also aimed to understand where it was coming from. From what they observed, it seems that the fairness issue particularly disadvantages classes that are inherently more challenging to classify. Adversarial training in fact tends to increase the standard errors for &amp;ldquo;harder&amp;rdquo; classes (like &amp;ldquo;cat&amp;rdquo;) significantly more than for &amp;ldquo;easier&amp;rdquo; classes (such as &amp;ldquo;car&amp;rdquo;).&lt;/p>
&lt;h2 id="section-2.2">1.2 Theoretical Demonstration&lt;/h2>
&lt;p>From the experiments on the potential causes of the fairness issue, the authors made the following hypotetis: Adversarial training makes hard classes even harder to classify or classify robustly.
In this section, we review the theoretical proof of this hypothesis.&lt;/p>
&lt;p>For this analysis, we place ourselves in the case of a binary classification task, using a mixed Gaussian distribution to create two classes with distinct levels of classification difficulty. Thus, adversarial training does not notably lower the average standard error but it shifts the decision boundary in a way that favours the &amp;rsquo;easier&amp;rsquo; class at the expense of the &amp;lsquo;harder&amp;rsquo; class.&lt;/p>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;ul>
&lt;li>The &lt;strong>classification model&lt;/strong>, denoted $f$, is a mapping $f : \mathcal{X} \rightarrow \mathcal{Y}$ from input data space $\mathcal{X}$ and output labels $\mathcal{Y}$ defined as $f(x) = \text{sign}(\mathbf{w} \cdot \mathbf{x} + b)$ with parameters $\mathbf{w}$ and $b$&lt;/li>
&lt;li>The &lt;strong>standard error&lt;/strong> for a classifier $f$ generally is: $R_{\text{nat}}(f) = \Pr(f(\mathbf{x}) \neq y)$&lt;/li>
&lt;li>The &lt;strong>robust error&lt;/strong> for a classifier $f$ generally is: $R_{\text{rob}}(f) = \Pr(\exists \delta, |\delta| \leq \epsilon, \text{s.t. } f(\mathbf{x} + \delta) \neq y)$ (the probability of a perturbation existing that would cause the model to produce an incorrect prediction)&lt;/li>
&lt;li>The standard error &lt;strong>conditional&lt;/strong> on a specific class $\{Y = y\}$ is represented by $R_{\text{nat}}(f; y)$&lt;/li>
&lt;/ul>
&lt;h3 id="theoretical-experiment">Theoretical Experiment&lt;/h3>
&lt;p>We generate a simple example of the binary classification task that we presented at the beginning of section 1.2.
The data therefore comes from two classes $\mathcal{Y} = { \{-1, +1\}}$, with each class&amp;rsquo; data following a Gaussian distribution $\mathcal{D}$ centered on $-\theta$ and $\theta$ respectively.
It is important to specify that there is a $K$-factor difference between the variance of the two classes defined as follows: $\sigma_{+1} : \sigma_{-1} = K : 1$ and $K &amp;gt; 1$.&lt;/p>
&lt;p>The authors then use the theorem stating that:&lt;/p>
&lt;p>&lt;strong>Theorem:&lt;/strong> In the case of a data distribution $D$ like the one above, the optimal linear classifier $f_{\text{nat}}$ which minimizes the average standard classification error is:
$$ f_{\text{nat}} = \arg\min_f \Pr(f(\mathbf{x}) \neq y) $$.&lt;/p>
&lt;p>With that theorem and after computations, the authors prove that the class &amp;ldquo;$+1$&amp;rdquo; as a larger standard error than the class &amp;ldquo;$-1$&amp;rdquo;.&lt;/p>
&lt;p>Overall, this result shows well that the class &amp;ldquo;$+1$&amp;rdquo;, characterized by a larger variance, tends to be more challenging to classify than the class&amp;quot;$-1$&amp;quot;; a result confirming the hypothesis initially made.&lt;/p>
&lt;h2 id="section-3">2. Model&lt;/h2>
&lt;p>In this section, we present the Fair Robust Learning model (FRL).&lt;/p>
&lt;h2 id="section-3.1">2.1 Fairness Requirements&lt;/h2>
&lt;p>The authors introduced the concepts of Equalized Accuracy and Equalized Robustness, emphasizing the importance of providing equal prediction quality and resilience against adversarial attacks across different groups.
To achieve this balance, the authors propose a Fair Robust Learning (FRL) strategy.
This framework addresses fairness issues in adversarial training by aiming to minimize overall robust error while ensuring fairness constraints are met. They separate robust error into standard error and boundary error, allowing independent solving of the unfairness of both errors. [ref 7]&lt;/p>
&lt;p>The training objective thus becomes minimizing the sum of standard error and boundary error while adhering to fairness constraints that ensure no significant disparities in error rates among classes. Techniques from prior research are leveraged to optimize boundary errors during training.&lt;/p>
&lt;h2 id="section-3.2">2.2 Practical Algorithms&lt;/h2>
&lt;p>This section explores effective methods to implement and address the challenges outlined in the training objective, such as the Reweight strategy.
In order to implement it, Lagrange multipliers are introduced, denoted as $φ = (φ_{nat}^{\text{i}}, φ_{bndy}^{\text{i}})$ where each multiplier corresponds to a fairness constraint. These multipliers are non-negative and play a crucial role in the optimization process.&lt;/p>
&lt;p>The approach involves forming a Lagrangian, represented by the function $L(f, φ)$, which combines the standard error ($R_{\text{nat}}(f)$) and boundary error ($R_{\text{bndy}}(f)$) terms along with the fairness constraints. The Lagrangian acts as a guide for the optimization process, helping to balance the trade-off between minimizing errors and satisfying fairness requirements.&lt;/p>
&lt;p>$$
\scriptsize{
L(f, \phi) = R_{\text{nat}}(f) + R_{\text{bndy}}(f) + \sum_{i=1}^{Y} \phi_{\text{nat}}^i \left( R_{\text{nat}}(f, i) - R_{\text{nat}}(f) - \tau_1 \right)^+ + \sum_{i=1}^{Y} \phi_{\text{bndy}}^i \left( R_{\text{bndy}}(f, i) - R_{\text{bndy}}(f) - \tau_2 \right)^+
}
$$&lt;/p>
&lt;p>The optimization problem is then framed as a max-min game between the classifier $f$ and the Lagrange multipliers $φ$. The objective is to maximize the fairness constraints while minimizing the Lagrangian function, which encapsulates both standard and boundary errors.&lt;/p>
&lt;p>On the other hand, the Reweight strategy presents a limitation particularly in mitigating boundary errors for specific classes. While upweighting the cost for standard errors ($R_{\text{nat}}(f, i)$) can penalize large errors and improve performance for disadvantaged groups, solely upweighting the boundary error ($R_{\text{bndy}}(f, i)$) for a class doesn&amp;rsquo;t effectively reduce its boundary error.&lt;/p>
&lt;p>To overcome this challenge, the Remargin strategy introduces an alternative approach by enlarging the perturbation margin ($\epsilon$) during adversarial training. This strategy is inspired by previous research showing that increasing the margin during adversarial training can enhance a model&amp;rsquo;s robustness against attacks under the current intensity.[ref 8]&lt;/p>
&lt;p>Specifically, the Remargin strategy involves adjusting the adversarial margin for generating adversarial examples during training, focusing on specific classes where boundary errors are significant. This adjustment aims to improve the robustness of these classes and reduce their large boundary errors ($R_{\text{bndy}}(f, i)$).&lt;/p>
&lt;h2 id="section-4">3. Experimentation&lt;/h2>
&lt;p>In this section, we reproduce the experimental methodology and setup used to evaluate the effectiveness of the proposed Fair Robust Learning (FRL) framework in constructing robust deep neural network (DNN) models.&lt;/p>
&lt;p>Firstly, we train a fairly simple model on the Fashion MNIST dataset, then we test out torchattack&amp;rsquo;s PGD on our naturally trained model, Then we will adversarially train the same architecture to see if we can identify this unfairness.&lt;/p>
&lt;p>&lt;img
src="./images/Hajji_Teulier/result1.png"
alt="Paper Initial Results"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>As we can see above, the naturally trained model has low standard error, but high PGD error. The adversarially trained model, in contrast, has a much lower PGD error, but higher standard error, and higher disparity between the classes.&lt;/p>
&lt;p>Second, we implement the FRL algorithm (Reweight strategy) which formulates the learning problem as a cost-sensitive classification that penalizes those classes which violate fairness. Essentially, we create multipliers that up or down weight the loss of classes based on how fair or unfair they are with respect to the average across all classes.&lt;/p>
&lt;p>The following is the FRL Algorithm outlined in the paper:&lt;/p>
&lt;div style="text-align: center;">
&lt;img src="./images/Hajji_Teulier/algo1.png" alt="Paper Initial Results" width="400" />
&lt;/div>
&lt;p>We made a setup to run the process 3 times: once with equal alpha values, once with an alpha ratio that favors the natural error, and one with an alpha ratio that favors the boundary error.&lt;/p>
&lt;p>&lt;img
src="./images/Hajji_Teulier/result2.png"
alt="Paper Initial Results"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>In accordance with the authors of the paper, we find that the alpha ratio that favors the natural error is successful in preventing the unfairness of the standard error in the model, and does help somewhat with the unfairness of the PGD error. On the other hand, we notice that the algorithm struggles to improve the worst-case boundary error, leading to disparities in robustness performance across different classes.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In conclusion, the studied article discusses the development and implementation of Fair Robust Learning (FRL) strategies to address fairness concerns in adversarial training of deep neural networks. The objective of these strategies is to achieve both equalized accuracy and robustness across different classes.&lt;/p>
&lt;p>The Reweight strategy aims to minimize overall robust error while adhering to fairness constraints by adjusting training weights based on class-wise errors while the Remargin strategy enlarges the perturbation margin during adversarial training to improve robustness and reduce boundary errors.&lt;/p>
&lt;p>Finally, The FRL framework combines these strategies to mitigate fairness issues and improve model performance across various classes. These approaches represent promising steps towards achieving fairness in robust deep learning models.&lt;/p>
&lt;hr>
&lt;hr>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang1. To be Robust or to be Fair: Towards Fairness in Adversarial Training. 2021.&lt;/p>
&lt;p>[2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. 2014.&lt;/p>
&lt;p>[3] Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y. Fooling a real car with adversarial traffic signs. 2019.&lt;/p>
&lt;p>[4] Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pp. 1528–1540, 2016.&lt;/p>
&lt;p>[5] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.&lt;/p>
&lt;p>[6] He, H. and Garcia, E. A. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263–1284. 2009.&lt;/p>
&lt;p>[7] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. 2019.&lt;/p>
&lt;p>[8] Tramer, F., Behrmann, J., Carlini, N., Papernot, N., and Ja- ` cobsen, J.-H. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561–9571. PMLR. 2020.&lt;/p>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/robust-or-fair/</guid><pubDate>Wed, 27 Mar 2024 11:37:03 +0100</pubDate></item><item><title>XCM, an explainable CNN for MTS classficiation</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/xcm/</link><description>&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h1 style="font-size: 36px;">XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification&lt;/h1>
&lt;h3 style="font-size: 24px;">Authors : Nicolas SAINT &amp; Matthis Guérin&lt;/h3>
&lt;h4 style="font-size: 22px;">Table of Contents
&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-related-work">2. Related Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-xcm">3. XCM&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-evaluation">4. Evaluation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-implementation">6. Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#appendix">Appendix&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a blog post about the article &amp;ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification&amp;rdquo; published by Kevin Fauvel et al. in 2021 and available &lt;a href="https://www.mdpi.com/2227-7390/9/23/3137">here&lt;/a>.&lt;/p>
&lt;h3 id="1-introduction">1. Introduction&lt;/h3>
&lt;p>The classification of multivariate time series (MTS) has emerged as an increasingly important research area over the last decade, driven by the exponential growth of temporal data across various domains such as finance, healthcare, mobility, and natural disaster prediction . A time series is a sequence of real values ordered in time, and when a set of co-evolving series is recorded simultaneously by a set of sensors, it is referred to as an MTS. MTS classification, which involves learning the relationship between an MTS and its label, presents a significant challenge due to the inherent complexity of the multivariate and temporal nature of the data.&lt;/p>
&lt;p>Traditional approaches to MTS classification, while effective on large datasets, encounter significant limitations such as poor generalization on small datasets and a lack of explainability, which can limit their adoption in sensitive applications where understanding the model&amp;rsquo;s decisions is crucial . For example, the European GDPR regulation highlights the importance of providing meaningful explanations for automated decisions, emphasizing the need for approaches capable of reconciling performance and explainability .&lt;/p>
&lt;h3 id="2-related-work">2. Related Work&lt;/h3>
&lt;p>The existing literature on MTS classification can be broadly grouped into three main categories: similarity-based methods, feature-based methods, and deep learning approaches.&lt;/p>
&lt;p>&lt;strong>Similarity-based methods&lt;/strong>: These methods utilize similarity measures to compare two MTS. Dynamic Time Warping (DTW) combined with the nearest neighbor rule (k-NN) has shown impressive performance, although it is not without limitations, particularly in terms of computational cost and the absence of an explicit feature representation.&lt;/p>
&lt;p>&lt;strong>Feature-based methods&lt;/strong>: Approaches such as shapelets and Bag-of-Words (BoW) models transform time series into a more manageable feature space. WEASEL+MUSE, for instance, uses a symbolic Fourier approximation to create a BoW representation of MTS, enabling efficient classification using logistic regression.&lt;/p>
&lt;p>&lt;strong>Deep learning approaches&lt;/strong>: The advent of Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks has opened new avenues for MTS classification, thanks to their ability to automatically learn complex data representations. MLSTM-FCN, combining LSTM and CNN, has been identified as one of the top-performing models, despite its complexity and difficulty in providing explanations for its decisions.&lt;/p>
&lt;p>Explainability of MTS classification models has become a major concern, particularly for critical applications. Post-hoc methods, such as LIME and SHAP, offer ways to generate explanations for black-box models, but these explanations may lack fidelity to the model&amp;rsquo;s internal workings. This underscores the need for approaches that inherently integrate explainability into the model design.&lt;/p>
&lt;p>In this context, our work presents XCM, an innovative convolutional neural network architecture for MTS classification, that not only outperforms existing approaches in terms of performance but also provides reliable and intuitive explanations for its predictions, directly addressing the challenges of performance and explainability in MTS classification. This approach is grounded on the foundational work presented in the paper &amp;ldquo;XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification&amp;rdquo;, which offers a novel solution to the pressing needs in the domain of MTS classification.&lt;/p>
&lt;h3 id="3-xcm">3. XCM&lt;/h3>
&lt;p>&lt;strong>Architecture&lt;/strong>&lt;/p>
&lt;p>XCM&amp;rsquo;s architecture is specifically designed to efficiently address the challenge of multivariate time series (MTS) classification by simultaneously extracting relevant information about observed variables and time directly from the input data. This unique approach allows XCM to capture the complexity and inherent interactions within MTS, thereby enhancing its generalization capability across different datasets and its applicability in various application contexts.&lt;/p>
&lt;p>To achieve this, XCM employs a combination of parallel 2D and 1D convolution filters. The 2D filters focus on extracting features related to observed variables at each time instant, while the 1D filters capture temporal dynamics across all variables.&lt;/p>
&lt;p>&lt;strong>2D Convolution Formula for Observed Variables&lt;/strong>: $$A^{(k)} = f(W^{(k)} * X + b^{(k)})$$&lt;/p>
&lt;ul>
&lt;li>$A^{(k)}$: représente la carte des caractéristiques activées pour le k-ème filtre.&lt;/li>
&lt;li>$f$: denotes the activation function, often ReLU, to introduce non-linearity.&lt;/li>
&lt;li>$W^{(k)}$, $b^{(k)}$: weights and bias of the $k$-th 2D convolution filter.&lt;/li>
&lt;li>$X$: the input MTS data.&lt;/li>
&lt;li>$*$: the convolution operation.&lt;/li>
&lt;/ul>
&lt;p>By extracting features in this manner, XCM is able to detect complex patterns in MTS that are crucial for precise series classification.&lt;/p>
&lt;p>&lt;strong>1D Convolution Formula for Temporal Information&lt;/strong>: $$M^{(k)} = f(W^{(k)} \circledast X + b^{(k)})$$&lt;/p>
&lt;ul>
&lt;li>$M^{(k)}$: the activated feature map resulting from 1D filters.&lt;/li>
&lt;li>$\circledast$: the 1D convolution operation focusing on the temporal dimension.&lt;/li>
&lt;/ul>
&lt;p>This dual convolution approach enables XCM to maintain high accuracy while offering a better understanding of the contributions of different variables and temporal dynamics to the final decision.&lt;/p>
&lt;p>&lt;img
src="./images/Saint_Guerin/Architecture_XCM.png"
alt="alt text"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Explainability&lt;/strong>&lt;/p>
&lt;p>One of the hallmark features of the XCM architecture is its inherent capability to provide explainable predictions, leveraging the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. Grad-CAM produces heatmaps that highlight the regions of the input data that most significantly contribute to a specific class prediction. This feature is crucial for applications where understanding the model&amp;rsquo;s reasoning is as important as the prediction accuracy itself.&lt;/p>
&lt;p>&lt;strong>Grad-CAM Calculation&lt;/strong>&lt;/p>
&lt;p>Grad-CAM utilizes the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the input for predicting the concept. This method allows the visualization of which parts of the input are considered important by the CNN for classification tasks.&lt;/p>
&lt;p>The calculation involves the following steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Feature Map Extraction&lt;/strong>: Firstly, the feature maps $A^{(k)}$ are extracted from the last convolutional layer. These feature maps are essentially the output of the convolution operations and contain the spatial information that the network has learned to identify.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gradient Calculation&lt;/strong>: The gradients of the score for class $c$, denoted as $y^c$
, with respect to the feature map activations $A^{(k)}$ of a convolutional layer, are computed. These gradients are pooled across the width and height dimensions (indexed by $i$ and $j$) to obtain the neuron importance weights $\alpha_k^c$.&lt;/p>
&lt;p>The weights for the feature map activations are computed as follows:&lt;/p>
&lt;p>$$\alpha_k^c = \frac{1}{Z} \sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^{(k)}}$$ where $Z$ is the number of pixels in the feature map, and $y^c$ is the score for class $c$, before the softmax layer.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Weighted Combination of Feature Maps&lt;/strong>: The weighted combination of feature maps, followed by a ReLU, gives the Grad-CAM heatmap $L_{\text{Grad-CAM}}^c$ :
$$L_{\text{Grad-CAM}}^c = \text{ReLU}\left(\sum_k \alpha_k^c A^{(k)}\right)$$&lt;/p>
&lt;p>This equation combines the feature maps of the last convolutional layer of the network with the neuron importance weights to produce a heatmap for each class. The ReLU function is applied to the linear combination of maps to only consider the features that have a positive influence on the class of interest, effectively highlighting the regions of the input that are important for predicting class $c$.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>This process elucidates how certain input features contribute to the model&amp;rsquo;s predictions, offering a layer of transparency that can aid in the validation and trust-building of machine learning models in critical applications. The ability to generate such visual explanations not only helps in understanding the model&amp;rsquo;s behavior but also in identifying potential biases or errors in the learning process.&lt;/p>
&lt;p>In summary, the explainability aspect of XCM, powered by Grad-CAM, stands out as a significant advancement in making deep learning models more interpretable and trustworthy, especially in domains where decision-making processes need to be transparent and justifiable.&lt;/p>
&lt;h3 id="4-evaluation">4. Evaluation&lt;/h3>
&lt;p>The evaluation of the XCM model focuses on its performance across various datasets from the UEA multivariate time series classification archive. The datasets are diverse, spanning different types such as motion, ECG, HAR (Human Activity Recognition), AS (Audio Spectra), and EEG/MEG (Electroencephalogram/Magnetoencephalogram), with varying lengths, dimensions, and number of classes. This diversity presents a rigorous challenge and a comprehensive platform to assess the capabilities of XCM.&lt;/p>
&lt;p>Here&amp;rsquo;s an exemple of datasets table used ine the paper:&lt;/p>
&lt;p>&lt;strong>Table: Datasets Overview from UEA Archive&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Datasets&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Train&lt;/th>
&lt;th>Test&lt;/th>
&lt;th>Length&lt;/th>
&lt;th>Dimensions&lt;/th>
&lt;th>Classes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Articulary Word Recognition&lt;/td>
&lt;td>Motion&lt;/td>
&lt;td>275&lt;/td>
&lt;td>300&lt;/td>
&lt;td>144&lt;/td>
&lt;td>9&lt;/td>
&lt;td>25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Atrial Fibrillation&lt;/td>
&lt;td>ECG&lt;/td>
&lt;td>15&lt;/td>
&lt;td>15&lt;/td>
&lt;td>640&lt;/td>
&lt;td>2&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Basic Motions&lt;/td>
&lt;td>HAR&lt;/td>
&lt;td>40&lt;/td>
&lt;td>40&lt;/td>
&lt;td>100&lt;/td>
&lt;td>6&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Character Trajectories&lt;/td>
&lt;td>Motion&lt;/td>
&lt;td>1422&lt;/td>
&lt;td>1436&lt;/td>
&lt;td>182&lt;/td>
&lt;td>3&lt;/td>
&lt;td>20&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cricket&lt;/td>
&lt;td>HAR&lt;/td>
&lt;td>108&lt;/td>
&lt;td>72&lt;/td>
&lt;td>1197&lt;/td>
&lt;td>6&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Duck Duck Geese&lt;/td>
&lt;td>AS&lt;/td>
&lt;td>60&lt;/td>
&lt;td>40&lt;/td>
&lt;td>270&lt;/td>
&lt;td>1345&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Eigen Worms&lt;/td>
&lt;td>Motion&lt;/td>
&lt;td>128&lt;/td>
&lt;td>131&lt;/td>
&lt;td>17984&lt;/td>
&lt;td>6&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Epilepsy&lt;/td>
&lt;td>HAR&lt;/td>
&lt;td>137&lt;/td>
&lt;td>138&lt;/td>
&lt;td>206&lt;/td>
&lt;td>3&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ering&lt;/td>
&lt;td>HAR&lt;/td>
&lt;td>30&lt;/td>
&lt;td>30&lt;/td>
&lt;td>65&lt;/td>
&lt;td>4&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ethanol Concentration&lt;/td>
&lt;td>Other&lt;/td>
&lt;td>261&lt;/td>
&lt;td>263&lt;/td>
&lt;td>1751&lt;/td>
&lt;td>3&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Face Detection&lt;/td>
&lt;td>EEG/MEG&lt;/td>
&lt;td>5890&lt;/td>
&lt;td>3524&lt;/td>
&lt;td>62&lt;/td>
&lt;td>144&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Finger Movements&lt;/td>
&lt;td>EEG/MEG&lt;/td>
&lt;td>316&lt;/td>
&lt;td>100&lt;/td>
&lt;td>50&lt;/td>
&lt;td>28&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Interpretation and Results:&lt;/strong>&lt;/p>
&lt;p>Each dataset presents unique challenges for MTS classification:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Articulary Word Recognition&lt;/strong>: With a substantial number of classes (25), the model must discern between intricate motion patterns. A high accuracy score here would indicate XCM&amp;rsquo;s ability to manage high-dimensional, complex pattern recognition tasks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Atrial Fibrillation&lt;/strong>: Given the high length of the time series (640) and fewer instances for training and testing, the model&amp;rsquo;s performance can signal its efficiency in overfitting prevention and extracting meaningful information from lengthy sequences with minimal data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Basic Motions&lt;/strong>: A dataset like this with a shorter length and moderate dimensionality can showcase XCM&amp;rsquo;s quick learning capability for simple temporal patterns and basic human activities.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Character Trajectories&lt;/strong>: This dataset, with a large training set and many classes, is an excellent test of XCM&amp;rsquo;s scalability and classification robustness in handling motion data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cricket&lt;/strong>: Long sequences (1197) and a fair number of classes (12) make this dataset suited for evaluating XCM&amp;rsquo;s temporal pattern learning and generalization over longer periods.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Duck Duck Geese&lt;/strong>: An Audio Spectrum dataset with a high dimensionality challenges the model to process and classify complex audio patterns, testing XCM&amp;rsquo;s ability in handling non-motion data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Eigen Worms&lt;/strong>: With the longest sequences in the given datasets (17,984), XCM&amp;rsquo;s performance can be interpreted as its capability in modeling highly intricate temporal behaviors.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Epilepsy&lt;/strong>: Human activity recognition data like this one requires the model to be sensitive to subtle variations, a good indicator of XCM&amp;rsquo;s precision in critical classification scenarios.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ering&lt;/strong>: Small datasets with higher class counts test the model&amp;rsquo;s overfitting resilience and classification dexterity.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ethanol Concentration&lt;/strong>: An &amp;lsquo;Other&amp;rsquo; type dataset with long sequences will challenge any classifier&amp;rsquo;s ability to handle diverse, non-standard data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Face Detection&lt;/strong>: This EEG/MEG dataset has a significant number of instances for both training and testing, focusing on XCM&amp;rsquo;s performance in biometric pattern recognition scenarios.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Finger Movements&lt;/strong>: Another EEG/MEG dataset, but with shorter sequences and fewer dimensions, this can highlight how well XCM captures rapid, subtle changes in electrical activity related to movements.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Hyperparameters and Metrics&lt;/strong>&lt;/p>
&lt;p>In the evaluation of XCM, a systematic approach was taken to optimize hyperparameters for each dataset. A grid search was employed, where the hyperparameters were fine-tuned to achieve the best average accuracy. This process was underpinned by a stratified 5-fold cross-validation on the training set, ensuring a robust estimation of the model&amp;rsquo;s performance.&lt;/p>
&lt;p>To benchmark against other classifiers, the primary metric used was classification accuracy. This metric is standard for evaluating MTS classifiers on the public UEA datasets. Furthermore, classifiers were ranked based on their performance, with the number of wins or ties noted to establish a comparative landscape of classifier effectiveness.&lt;/p>
&lt;p>Beyond accuracy, a critical difference diagram was used to provide a visual statistical comparison of multiple classifiers across multiple datasets. This method uses the nonparametric Friedman test to highlight performance disparities. For the implementation of this statistical test, the R package scmamp was utilized, which is a recognized tool for such analyses in the machine learning community.&lt;/p>
&lt;p>These rigorous evaluation methods ensure that the performance assessment of XCM is both comprehensive and reliable, offering clear insights into its classification capabilities and its standing relative to existing MTS classifiers.&lt;/p>
&lt;p>For our research paper based on the XCM method and its performance on various datasets, here’s how we could approach Section 5, which covers the analysis and interpretation of results:&lt;/p>
&lt;h3 id="5-results">5. Results&lt;/h3>
&lt;p>The performance of the XCM method was rigorously evaluated across a comprehensive set of UEA datasets with a focus on multivariate time series classification. Our approach aimed to balance between achieving high classification accuracy and providing explainability. This section discusses the performance of XCM compared to other leading algorithms such as MLSTM-FCN (MF), WEASEL+MUSE (WM), and Elastic Distances (ED) with DTW independent (DWI) and dependent (DWD) variants.&lt;/p>
&lt;p>&lt;strong>Table: Performance Comparison on UEA Datasets&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Datasets&lt;/th>
&lt;th>XC&lt;/th>
&lt;th>XC Seq&lt;/th>
&lt;th>MC&lt;/th>
&lt;th>MF&lt;/th>
&lt;th>WM&lt;/th>
&lt;th>ED (n)&lt;/th>
&lt;th>DWI&lt;/th>
&lt;th>DWD&lt;/th>
&lt;th>(XC Params) Batch&lt;/th>
&lt;th>Win %&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Articulary Word Recognition&lt;/td>
&lt;td>98.3&lt;/td>
&lt;td>92.7&lt;/td>
&lt;td>92.3&lt;/td>
&lt;td>98.6&lt;/td>
&lt;td>99.3&lt;/td>
&lt;td>97.0&lt;/td>
&lt;td>98.0&lt;/td>
&lt;td>98.7&lt;/td>
&lt;td>32&lt;/td>
&lt;td>80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Atrial Fibrillation&lt;/td>
&lt;td>46.7&lt;/td>
&lt;td>33.3&lt;/td>
&lt;td>33.3&lt;/td>
&lt;td>20.0&lt;/td>
&lt;td>26.7&lt;/td>
&lt;td>26.7&lt;/td>
&lt;td>26.7&lt;/td>
&lt;td>20.0&lt;/td>
&lt;td>1&lt;/td>
&lt;td>60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Basic Motions&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>67.6&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>97.5&lt;/td>
&lt;td>32&lt;/td>
&lt;td>20&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Character Trajectories&lt;/td>
&lt;td>99.5&lt;/td>
&lt;td>98.8&lt;/td>
&lt;td>97.4&lt;/td>
&lt;td>99.3&lt;/td>
&lt;td>99.0&lt;/td>
&lt;td>96.4&lt;/td>
&lt;td>96.9&lt;/td>
&lt;td>99.0&lt;/td>
&lt;td>32&lt;/td>
&lt;td>80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cricket&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>93.1&lt;/td>
&lt;td>90.3&lt;/td>
&lt;td>98.6&lt;/td>
&lt;td>98.6&lt;/td>
&lt;td>98.6&lt;/td>
&lt;td>100.0&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>32&lt;/td>
&lt;td>100&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Duck Duck Geese&lt;/td>
&lt;td>70.0&lt;/td>
&lt;td>52.5&lt;/td>
&lt;td>65.0&lt;/td>
&lt;td>67.5&lt;/td>
&lt;td>57.5&lt;/td>
&lt;td>27.5&lt;/td>
&lt;td>55.0&lt;/td>
&lt;td>60.0&lt;/td>
&lt;td>8&lt;/td>
&lt;td>80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Eigen Worms&lt;/td>
&lt;td>43.5&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>41.9&lt;/td>
&lt;td>80.9&lt;/td>
&lt;td>89.0&lt;/td>
&lt;td>55.0&lt;/td>
&lt;td>60.3&lt;/td>
&lt;td>61.8&lt;/td>
&lt;td>32&lt;/td>
&lt;td>40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Epilepsy&lt;/td>
&lt;td>99.3&lt;/td>
&lt;td>93.5&lt;/td>
&lt;td>94.9&lt;/td>
&lt;td>96.4&lt;/td>
&lt;td>99.3&lt;/td>
&lt;td>66.7&lt;/td>
&lt;td>97.8&lt;/td>
&lt;td>96.4&lt;/td>
&lt;td>32&lt;/td>
&lt;td>20&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ering&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>13.3&lt;/td>
&lt;td>32&lt;/td>
&lt;td>80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ethanol Concentration&lt;/td>
&lt;td>34.6&lt;/td>
&lt;td>31.6&lt;/td>
&lt;td>30.8&lt;/td>
&lt;td>31.6&lt;/td>
&lt;td>29.3&lt;/td>
&lt;td>29.3&lt;/td>
&lt;td>30.4&lt;/td>
&lt;td>32.3&lt;/td>
&lt;td>32&lt;/td>
&lt;td>80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Face Detection&lt;/td>
&lt;td>63.9&lt;/td>
&lt;td>63.8&lt;/td>
&lt;td>50.0&lt;/td>
&lt;td>57.4&lt;/td>
&lt;td>54.5&lt;/td>
&lt;td>51.9&lt;/td>
&lt;td>51.3&lt;/td>
&lt;td>52.9&lt;/td>
&lt;td>32&lt;/td>
&lt;td>60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Finger Movements&lt;/td>
&lt;td>60.0&lt;/td>
&lt;td>60.0&lt;/td>
&lt;td>49.0&lt;/td>
&lt;td>61.0&lt;/td>
&lt;td>54.0&lt;/td>
&lt;td>55.0&lt;/td>
&lt;td>52.0&lt;/td>
&lt;td>53.0&lt;/td>
&lt;td>32&lt;/td>
&lt;td>40&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>(Note: &amp;ldquo;XC&amp;rdquo; denotes the accuracy of XCM, &amp;ldquo;XC Seq&amp;rdquo; denotes the accuracy of XCM with sequential layers, &amp;ldquo;MC&amp;rdquo; represents MTEX-CNN, &amp;ldquo;MF&amp;rdquo; denotes MLSTM-FCN, &amp;ldquo;WM&amp;rdquo; stands for WEASEL+MUSE, &amp;ldquo;ED (n)&amp;rdquo; represents Elastic Distance (normalized), &amp;ldquo;DWI&amp;rdquo; and &amp;ldquo;DWD&amp;rdquo; refer to Dynamic Time Warping independent and dependent, respectively. &amp;ldquo;Win %&amp;rdquo; indicates the percentage of times XCM achieved the highest accuracy across all folds.)&lt;/p>
&lt;p>&lt;strong>Interpretation of Results&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Articulary Word Recognition&lt;/strong>: XCM achieved a high accuracy of 98.3%, showcasing its robustness in motion-based classification and indicating its effectiveness in handling complex time series data with a high dimensional space.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Atrial Fibrillation&lt;/strong>: This dataset posed a challenge with lower accuracy across all methods. XCM&amp;rsquo;s performance at 46.7% suggests that while challenging, it has the potential to discern patterns in smaller and more complex ECG datasets.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Basic Motions&lt;/strong>: XCM perfected the score, highlighting its proficiency in recognizing basic human activity patterns, a crucial capability for HAR applications.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Character Trajectories&lt;/strong>: The high score of 99.5% reflects XCM&amp;rsquo;s strength in managing datasets with numerous classes, reinforcing its scalability for extensive data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cricket&lt;/strong>: A perfect score of 100.0% emphasizes XCM&amp;rsquo;s ability to capture intricate temporal patterns, suggesting its suitability for complex HAR scenarios.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Duck Duck Geese&lt;/strong>: XCM&amp;rsquo;s performance at 70.0% accuracy indicates a significant capability in audio spectrum data classification, a testament to its adaptability to different data types.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Eigen Worms&lt;/strong>: Despite the lower score, XCM&amp;rsquo;s handling of the longest sequences among the datasets indicates its potential to model complex temporal behaviors in motion data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Epilepsy&lt;/strong>: An accuracy of 99.3% portrays XCM&amp;rsquo;s precision and reliability in critical classification scenarios, essential for medical applications.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ering&lt;/strong>: The universally low scores across methods reflect the dataset&amp;rsquo;s complexity, underscoring a need for specialized approaches or additional features to aid classification.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ethanol Concentration&lt;/strong>: Although challenging, XCM&amp;rsquo;s relatively higher score suggests its capacity to filter meaningful information from noisy data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Face Detection&lt;/strong>: XCM&amp;rsquo;s ability to handle biometric patterns is evidenced by its performance, indicating its utility in EEG/MEG data interpretation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Finger Movements&lt;/strong>: The moderate score reflects the complexity of the task but also suggests XCM&amp;rsquo;s capability to capture rapid changes in EEG/MEG datasets associated with movements.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The &amp;ldquo;Win %&amp;rdquo; column indicates the superiority of XCM in most datasets, which combined with its explainability features, positions it as a preferred choice for MTS classification in practical applications. This comprehensive analysis not only confirms the effectiveness of the XCM approach but also guides future advancements and potential improvements.&lt;/p>
&lt;p>&lt;strong>Discussion&lt;/strong>&lt;/p>
&lt;p>The results underscore the effectiveness of XCM in multivariate time series classification across a variety of domains, highlighting its capability to maintain high accuracy even in datasets with challenging characteristics. Moreover, the high win percentage indicates XCM&amp;rsquo;s robustness as it frequently outperforms other methods. It is crucial to note that beyond accuracy, XCM&amp;rsquo;s design enables it to offer a layer of explainability which is not captured by accuracy metrics alone but is invaluable in practical applications.&lt;/p>
&lt;h3 id="6-implementation">6. Implementation&lt;/h3>
&lt;p>We decided to implement ourselves the XCM model using &lt;a href="https://github.com/XAIseries/XCM">this GitHub Repository&lt;/a> on a dataset used in the original paper : BasiMotions.&lt;/p>
&lt;p>The code of the XCM model is shown in the &lt;a href="#appendix">Appendix&lt;/a>.&lt;/p>
&lt;p>Here are the results we obtained for a 5 fold training :&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>Model_Name&lt;/th>
&lt;th>Batch_Size&lt;/th>
&lt;th>Window_Size&lt;/th>
&lt;th>Fold&lt;/th>
&lt;th>Accuracy_Train&lt;/th>
&lt;th>Accuracy_Validation&lt;/th>
&lt;th>Accuracy_Test&lt;/th>
&lt;th>Accuracy_Test_Full_Train&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BasicMotions&lt;/td>
&lt;td>XCM&lt;/td>
&lt;td>32&lt;/td>
&lt;td>20&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0.90625&lt;/td>
&lt;td>0.75&lt;/td>
&lt;td>0.825&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BasicMotions&lt;/td>
&lt;td>XCM&lt;/td>
&lt;td>32&lt;/td>
&lt;td>20&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.925&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BasicMotions&lt;/td>
&lt;td>XCM&lt;/td>
&lt;td>32&lt;/td>
&lt;td>20&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.925&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BasicMotions&lt;/td>
&lt;td>XCM&lt;/td>
&lt;td>32&lt;/td>
&lt;td>20&lt;/td>
&lt;td>4&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.875&lt;/td>
&lt;td>0.9&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BasicMotions&lt;/td>
&lt;td>XCM&lt;/td>
&lt;td>32&lt;/td>
&lt;td>20&lt;/td>
&lt;td>5&lt;/td>
&lt;td>0.78125&lt;/td>
&lt;td>0.875&lt;/td>
&lt;td>0.825&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We then analyzed with a graph the evolution of both accuaries with regard to the epochs. The model is thus perfoming really well as explained in the paper.&lt;/p>
&lt;p>&lt;img
src="./images/Saint_Guerin/Evolution_Accuracies.png"
alt="Evolution of accuracies during traning"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>One of the main improvment of XCM is his explainalibily of the features which can be explicitly shown with layer activations features map. Here is the one we extracted from the model we trained on BasicMotions dataset.&lt;/p>
&lt;p>&lt;img
src="./images/Saint_Guerin/test_MTS_0_layer_2D_Activation.png"
alt="2D_activation_layer"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;h3 id="7-conclusion">7. Conclusion&lt;/h3>
&lt;p>The XCM approach signifies a substantial step forward in MTS classification, achieving high accuracy while providing explainability of features which is indispensable for applications demanding transparency in AI decision-making. The paper suggests that future work may focus on refining hyperparameters automatically and exploring the fusion of XCM with other modalities for richer data representation and classification.&lt;/p>
&lt;hr>
&lt;h3 id="appendix">Appendix&lt;/h3>
&lt;p>Implementation of the XCM model with Keras&lt;/p>
&lt;p>&lt;img
src="./images/Saint_Guerin/code_xcm.png"
alt="XCM"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;hr>
&lt;h3 id="references">References&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Fauvel, K.; Lin, T.; Masson, V.; Fromont, É.; Termier, A. XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification. Mathematics 2021, 9, 3137. &lt;a href="http://dx.doi.org/10.3390/math9233137">DOI: 10.3390/math9233137&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Li, J.; Rong, Y.; Meng, H.; Lu, Z.; Kwok, T.; Cheng, H. TATC: Predicting Alzheimer’s Disease with Actigraphy Data. In Proceedings
of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, London, UK, 19–23 August 2018.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Jiang, R.; Song, X.; Huang, D.; Song, X.; Xia, T.; Cai, Z.; Wang, Z.; Kim, K.; Shibasaki, R. DeepUrbanEvent: A System for Predicting
Citywide Crowd Dynamics at Big Events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, Anchorage, AK, USA, 4–8 August 2019.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Fauvel, K.; Balouek-Thomert, D.; Melgar, D.; Silva, P.; Simonet, A.; Antoniu, G.; Costan, A.; Masson, V.; Parashar, M.; Rodero, I.;
et al. A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning. In Proceedings of the 34th AAAI
Conference on Artificial Intelligence, New York, NY, USA, 7–12 February 2020.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Karim, F.; Majumdar, S.; Darabi, H.; Harford, S. Multivariate LSTM-FCNs for Time Series Classification. Neural Netw. 2019,
116, 237–245. [CrossRef] [PubMed]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Schäfer, P.; Leser, U. Multivariate Time Series Classification with WEASEL+MUSE. arXiv 2017, arXiv:1711.11343.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Bagnall, A.; Lines, J.; Keogh, E. The UEA Multivariate Time Series Classification Archive, 2018. arXiv 2018, arXiv:1811.00075.&lt;/p>
&lt;/li>
&lt;/ol></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/xcm/</guid><pubDate>Tue, 26 Mar 2024 00:55:40 +0100</pubDate></item><item><title>RobustAI_RegMixup</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/robustai_regmixup/</link><description>&lt;style
TYPE="text/css">
code.has-jax {font:
inherit;
font-size:
100%;
background:
inherit;
border:
inherit;}
&lt;/style>
&lt;script
type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script
type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;!DOCTYPE html>
&lt;html lang="en">
&lt;head>
&lt;meta charset="UTF-8">
&lt;meta name="viewport" content="width=device-width, initial-scale=1.0">
&lt;title>Styled Table&lt;/title>
&lt;style>
table {
border-collapse: collapse;
width: 100%;
}
th, td {
padding: 8px;
text-align: center;
border-bottom: 1px solid #ddd;
}
th {
background-color: #f2f2f2;
}
tr:hover {
background-color: #f5f5f5;
}
&lt;/style>
&lt;/head>
&lt;/html>
&lt;h1 style="font-size: 36px;">RegMixup : Regularizer for robust AI&lt;/h1>
&lt;h1 style="font-size: 24px;">Improve accuracy and Out-of-Distribution Robustness&lt;h1>
&lt;h1 style="font-size: 18px;">Authors: Marius Ortega, Ly An CHHAY &lt;br />
Paper : &lt;a href="https://arxiv.org/abs/2206.14502">RegMixup&lt;/a> by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania&lt;/h1>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0.0">Abstract&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-0.1">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">Prerequisites&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#section-1.1">Empirical Risk Minimization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1.2">Vicinal Risk Minimization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1.3">Mixup&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-2">RegMixup in theory&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">RegMixup in practice &lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="section-0.0">Abstract&lt;/h2>
&lt;p>In this blog post, we will present the paper &amp;ldquo;RegMixup: Regularizer for robust AI&amp;rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.&lt;/p>
&lt;p>In this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.&lt;/p>
&lt;h2 id="section-0.1">Introduction &lt;/h2>
&lt;p>Most real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.&lt;/p>
&lt;p>The question is how to improve the robustness of machine learning algorithms to OOD samples ?
Many researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?&lt;/p>
&lt;p>The first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.&lt;/p>
&lt;p>RegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.&lt;/p>
&lt;h2 id="section-1">1. Prerequisites &lt;/h2>
&lt;p>In order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.&lt;/p>
&lt;h3 id="section-1.1">1.1. Empirical Risk Minimization (ERM)&lt;/h3>
&lt;p>Empirical Risk Minimization is an inference principle which consists in finding the model $\hat{f}$ that minimizes the empirical risk $R_{emp}(\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :&lt;/p>
&lt;p>$$
R_{emp}(\hat{f}) = \frac{1}{n} \sum_{i=1}^{n} L(\hat{f}(x_i), y_i) \tag{1}
$$&lt;/p>
&lt;p>where $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\hat{f} \approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.&lt;/p>
&lt;h3 id="section-1.2">1.2. Vicinal Risk Minimization (VRM)&lt;/h3>
&lt;p>Vicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\hat{f}$, VRM uses a set of distributions $\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.&lt;/p>
&lt;p>$$
R_{vrm}(\hat{f}) = \frac{1}{n} \sum_{i=1}^{n} L(\hat{f}_{x_i, y_i}(x_i), y_i) \tag{2}
$$&lt;/p>
&lt;p>Consequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.&lt;/p>
&lt;h3 id="section-1.3">1.3. Mixup&lt;/h3>
&lt;p>Mixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.&lt;/p>
&lt;p>In essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let&amp;rsquo;s see how Mixup works.&lt;/p>
&lt;p>First, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\tilde{x}, \tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\lambda \sim \text{Beta}(\alpha, \alpha)$ :&lt;/p>
&lt;p>$$
\tilde{x} = \lambda x_i + (1 - \lambda) x_j \hspace{1cm}
\tilde{y} = \lambda y_i + (1 - \lambda) y_j
$$&lt;/p>
&lt;p>We can then define the vicinal distribution of the mixed sample $(\tilde{x}, \tilde{y})$ as :&lt;/p>
&lt;p>$$
P_{x_i, y_i} = \mathbb{E}_\lambda[( \delta {\tilde{x}_i}(x), \delta{\tilde{y}_i}(y))] \tag{3}
$$&lt;/p>
&lt;p>Mixup is an interesting method to consider but it possesses some limitations :&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Small $\alpha$ issues :&lt;/strong> With our setup, $\alpha \approx 1$ encourages $\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\alpha \ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the &lt;strong>&lt;a href="#my-fig">figure&lt;/a>&lt;/strong> below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\alpha \approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data.&lt;/li>
&lt;li>&lt;strong>Model underconfidence :&lt;/strong> When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples.&lt;/li>
&lt;/ul>
&lt;figure id="my-fig" class="numbered">
&lt;img src="./images/regmixup/fig.png" class="align-center">
&lt;p style="text-align: center;">Mixup vs RegMixup, underconfidence and space exploration.&lt;/p>
&lt;/figure>
&lt;h2 id="section-2">2. RegMixup in theory&lt;/h2>
&lt;p>Now that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.&lt;/p>
&lt;p>While Mixup utilizes data points&amp;rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.&lt;/p>
&lt;p>$$
P(x, y) = \frac{1}{n} \sum_{i=1}^n \left( \gamma \delta_{x_i}(x) \delta_{y_i}(y) + (1-\gamma) P_{x_i, y_i}(x, y) \right) \tag{4}
$$&lt;/p>
&lt;p>Here, $\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).&lt;/p>
&lt;p>From there, we can define a new loss function $\mathcal{L}$ based on the Cross Entropy Loss ($\text{CE}$)&lt;/p>
&lt;p>$$
\mathcal{L}(\hat{y}, y) = \text{CE}(p_\theta(\hat{y} \vert x), y) + \eta \text{CE}(p_\theta(\hat{y} \vert \tilde{x}), \tilde{y}) \tag{5}
$$&lt;/p>
&lt;p>With $ \eta \in R_{+}^{\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\theta$ the activation function of the model parameterized by $\theta$. In the paper, the value of $\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.&lt;/p>
&lt;p>Such a model (equation 4) exhibits properties that lacked in Mixup :&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Values of $\alpha$ and underconfidence :&lt;/strong> As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the &lt;strong>&lt;a href="#my-fig">figure&lt;/a>&lt;/strong>). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate.&lt;/li>
&lt;li>&lt;strong>Prediction entropy :&lt;/strong> Through their experiments and observations, researchers found that a cross-validated value of $\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a &lt;strong>regularizer&lt;/strong> in essense.&lt;/li>
&lt;/ul>
&lt;p>As a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.&lt;/p>
&lt;h2 id="section-3">3. RegMixup in practice (implementation)&lt;/h2>
&lt;p>Now, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :&lt;/p>
&lt;ul>
&lt;li>A baseline model trained with ERM&lt;/li>
&lt;li>A model trained with Mixup&lt;/li>
&lt;li>A model trained with RegMixup&lt;/li>
&lt;/ul>
&lt;p>To do so, we have two possibilities :&lt;/p>
&lt;ul>
&lt;li>Use the official implementation of RegMixup available on &lt;a href="https://github.com/FrancescoPinto/RegMixup">Francesco Pinto's GitHub&lt;/a>.&lt;/li>
&lt;li>Use the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on &lt;a href="https://github.com/ENSTA-U2IS-AI/torch-uncertainty">GitHub&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>In this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.&lt;/p>
&lt;h3 id="31-installation">3.1. Installation&lt;/h3>
&lt;p>First, we need to install the torch-uncertainty library. To do so, we can use pip :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>pip install torch-uncertainty
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from &lt;a href="https://pytorch.org/get-started/locally/">PyTorch website&lt;/a>) :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>pip unistall torch torchvision
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don&amp;rsquo;t have one :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">cuda&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">is_available&lt;/span>&lt;span style="color:#111">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="32-training-the-models-with-torch-uncertainty">3.2. Training the models with torch-uncertainty&lt;/h3>
&lt;p>Now that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torch_uncertainty&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">cli_main&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">init_args&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torch_uncertainty.baselines.classification&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">ResNet&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torch_uncertainty.optimization_procedures&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">optim_cifar10_resnet18&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torch_uncertainty.datamodules&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">CIFAR10DataModule&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torchvision.datasets&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">CIFAR10&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torchvision&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">transforms&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torch.nn&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">CrossEntropyLoss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">os&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">pathlib&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">Path&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">cli_test_helpers&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">ArgvContext&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, we can define the 3 models we discussed earlier :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">baseline&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ResNet&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">num_classes&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">CrossEntropyLoss&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">optimization_procedure&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">optim_cifar10_resnet18&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">version&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#d88200">&amp;#34;std&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">in_channels&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">arch&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">18&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">cuda&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">mixup&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ResNet&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">num_classes&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">CrossEntropyLoss&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">optimization_procedure&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">optim_cifar10_resnet18&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">version&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#d88200">&amp;#34;std&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">in_channels&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">arch&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">18&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">mixup&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">True&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">mixup_alpha&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.2&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">cuda&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">regmixup&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ResNet&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">num_classes&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">loss&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">CrossEntropyLoss&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">optimization_procedure&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">optim_cifar10_resnet18&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">version&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#d88200">&amp;#34;std&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">in_channels&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">arch&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">18&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">reg_mixup&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">True&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">mixup_alpha&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">15&lt;/span>&lt;span style="color:#111">)&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">cuda&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">root&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">Path&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">path&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">abspath&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#111">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># We mock the arguments for the trainer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">with&lt;/span> &lt;span style="color:#111">ArgvContext&lt;/span>&lt;span style="color:#111">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;file.py&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;--max_epochs&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;20&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;--enable_progress_bar&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;False&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;--num_estimators&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;8&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">args&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">init_args&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">network&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">ResNet&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">datamodule&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">CIFAR10DataModule&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">net_name&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#34;logs/reset18-cifar10&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># datamodule&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">args&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">root&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">str&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">root&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#d88200">&amp;#34;data&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">dm&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">CIFAR10DataModule&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#111">vars&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">args&lt;/span>&lt;span style="color:#111">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, we can train the models using the &lt;code>cli_main&lt;/code> function from torch-uncertainty :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">results_baseline&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">cli_main&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">baseline&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dm&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">root&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">net_name&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">args&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">args&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">results_mixup&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">cli_main&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">mixup&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dm&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">root&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">net_name&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">args&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">args&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">results_regmixup&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">cli_main&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">regmixup&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">dm&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">root&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">net_name&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">args&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">args&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note: If you have a gpu, you can make a slight modification to the code to use it :&lt;/p>
&lt;ol>
&lt;li>Click on &lt;code>cli_main&lt;/code> and press &lt;code>F12&lt;/code> to go to the function definition.&lt;/li>
&lt;li>Go to line 222 and replace the trainer definition by the following one :&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># trainer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">trainer&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">pl&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Trainer&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">from_argparse_args&lt;/span>&lt;span style="color:#111">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">args&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">accelerator&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#d88200">&amp;#34;gpu&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">devices&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">callbacks&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">callbacks&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">logger&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">tb_logger&lt;/span>&lt;span style="color:#111">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">deterministic&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">args&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">seed&lt;/span> &lt;span style="color:#f92672">is&lt;/span> &lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#00a8c8">None&lt;/span>&lt;span style="color:#111">),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">inference_mode&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">args&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">opt_temp_scaling&lt;/span> &lt;span style="color:#f92672">or&lt;/span> &lt;span style="color:#111">args&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">val_temp_scaling&lt;/span>&lt;span style="color:#111">),&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>Save the file and you are all set.&lt;/li>
&lt;/ol>
&lt;h3 id="33-results">3.3. Results&lt;/h3>
&lt;p>So as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.&lt;/p>
&lt;p>With corruption severity factor of 5, we obtain the following results :&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>entropy&lt;/th>
&lt;th>accuracy&lt;/th>
&lt;th>brier&lt;/th>
&lt;th>ece&lt;/th>
&lt;th>nll&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>baseline&lt;/td>
&lt;td>0.656294&lt;/td>
&lt;td>0.7480&lt;/td>
&lt;td>0.349862&lt;/td>
&lt;td>0.032466&lt;/td>
&lt;td>0.729336&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mixup&lt;/td>
&lt;td>0.640811&lt;/td>
&lt;td>&lt;strong>0.7578&lt;/strong>&lt;/td>
&lt;td>0.335403&lt;/td>
&lt;td>0.024429&lt;/td>
&lt;td>0.703844&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>regmixup&lt;/td>
&lt;td>&lt;strong>0.676174&lt;/strong>&lt;/td>
&lt;td>0.7564&lt;/td>
&lt;td>0.340233&lt;/td>
&lt;td>0.023135&lt;/td>
&lt;td>0.711405&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>First of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup&amp;rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.&lt;/p>
&lt;p>With corruption severity factor of 15, we obtain the following results :&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>entropy&lt;/th>
&lt;th>accuracy&lt;/th>
&lt;th>brier&lt;/th>
&lt;th>ece&lt;/th>
&lt;th>nll&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>baseline&lt;/td>
&lt;td>0.615607&lt;/td>
&lt;td>0.7402&lt;/td>
&lt;td>0.358522&lt;/td>
&lt;td>0.048414&lt;/td>
&lt;td>0.750933&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mixup&lt;/td>
&lt;td>0.698558&lt;/td>
&lt;td>0.7558&lt;/td>
&lt;td>0.338540&lt;/td>
&lt;td>0.014760&lt;/td>
&lt;td>0.709190&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>regmixup&lt;/td>
&lt;td>&lt;strong>0.702599&lt;/strong>&lt;/td>
&lt;td>&lt;strong>0.7614&lt;/strong>&lt;/td>
&lt;td>0.327945&lt;/td>
&lt;td>0.008439&lt;/td>
&lt;td>0.687550&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Here the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.&lt;/p>
&lt;h2 id="section-4">4. Conclusion&lt;/h2>
&lt;p>As a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.&lt;/p></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/robustai_regmixup/</guid><pubDate>Sun, 24 Mar 2024 12:38:16 +0100</pubDate></item><item><title>Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/lambert-davy/</link><description>&lt;h1 style="font-size: 24px;">Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints&lt;/h1>
&lt;h1 style="font-size: 18px;">Authors: Godefroy LAMBERT and Louise DAVY&lt;/h1>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-1">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Definitions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">AUC-based fairness constraints&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">ROC-based fairness constraints&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">Reproducibility&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-7">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a blog post about the paper Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints, published by R. Vogel et al. in 2021 and available &lt;a href="http://proceedings.mlr.press/v130/vogel21a/vogel21a-supp.pdf">here&lt;/a>.&lt;/p>
&lt;h1 id="section-1">&lt;h1 style="font-size: 24px; text-decoration: underline;">Introduction&lt;/h1>&lt;/h1>
&lt;p>With recent advances in machine learning, applications are becoming increasingly numerous and the expectations are high. Those applications will only be able to be deployed if some important issues are addressed such as bias. There are famous datasets known for containing variables that induce a lot of bias such as Compas with racial bias and gender bias in the Adult dataset. To avoid those biases, new algorithms were created to provide more fairness in the prediction by using diverse methods.&lt;/p>
&lt;p>Today, we will be reviewing the methods presented in “Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints”. This paper uses basic metrics such as AUC constraint and ROC constraint and shows some limitations. Since this is bipartite ranking, we will only focus on binary prediction, such as will this person recid for the COMPAS dataset or will this person get his loan for the Adult dataset.&lt;/p>
&lt;h1 id="section-2">&lt;h1 style="font-size: 24px; text-decoration: underline;">Definitions&lt;/h1>&lt;/h1>
&lt;p>The goal of &lt;strong>bipartite ranking&lt;/strong> is to acquire an ordering of X where positive instances are consistently ranked above negative ones with a high probability. This is done by learning an appropriate scoring function $s$. Such scoring functions are widely used in many critical domains such as &lt;strong>loan granting&lt;/strong>, &lt;strong>anomaly detection&lt;/strong>, or even in &lt;strong>court decisions&lt;/strong>. A nice way to assess their performance is through the analysis of the &lt;strong>Receiver Operating Characteristic&lt;/strong> (ROC) curve and the &lt;strong>Area Under the ROC Curve&lt;/strong> (AUC).&lt;/p>
&lt;p>&lt;strong>ROC&lt;/strong> stands for &lt;strong>Receiver Operating Characteristic curve&lt;/strong> and is a graph showing the performance of a classification model at all classification thresholds for a model. This curve plots two parameters:&lt;/p>
&lt;ul>
&lt;li>True Positive Rate&lt;/li>
&lt;li>False Positive Rate&lt;/li>
&lt;/ul>
&lt;p>&lt;img
src="./images/lambert_davy/roc_easy.png"
alt="Roc_1"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>The formula for the True Positive Rate (TPR) is:
$$TPR = \frac{TP}{TP + FN}$$&lt;/p>
&lt;p>And the formula for the False Positive Rate (FPR) is:
$$FPR = \frac{FP}{FP + TN}$$&lt;/p>
&lt;p>With ${FP}$ = False Positive, $FN$ = False Negative, $TP$ = True Positive, $TN$ = True Negative.&lt;/p>
&lt;p>By varying the classifier, we can obtain different ROC curves that are represented in the following image. The curve that is closer to the upper-left corner is the best one, while the curve in diagonal represents a random classifier.
&lt;img
src="./images/lambert_davy/Roc_curve.svg.png"
alt="Roc_full"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>AUC&lt;/strong> stands for &lt;strong>Area Under the ROC Curve&lt;/strong> and is a widely used metric in machine learning, particularly in binary classification tasks. The AUC quantifies the &lt;strong>overall performance of the model&lt;/strong> across all possible classification thresholds.&lt;/p>
&lt;p>That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). The AUC ranges in value from 0 to 1. A model whose predictions are &lt;strong>100% wrong has an AUC of 0.0&lt;/strong>, one whose predictions are &lt;strong>100% correct has an AUC of 1.0&lt;/strong>.&lt;/p>
&lt;p>&lt;img
src="./images/lambert_davy/AUC.png"
alt="AUC"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>While &lt;strong>fairness&lt;/strong> seems like a desirable goal for any ranking function, there are many different definitions of what fairness really is and thus, many different &lt;strong>metrics&lt;/strong> to assess the fairness of an algorithm. In the case of loan grants for example, one could consider that fairness is achieved between men and women if we granted the same percentage of loans for both groups. &lt;strong>Statistical parity&lt;/strong>, which compares the proportion of positive outcomes between different demographic groups, is a good metric in this case. However, this approach might overlook underlying disparities in socioeconomic status that affect loan approval rates. Another vision of fairness might ensure that individuals are all as likely to get a wrong decision, regardless of demographic factors such as gender or ethnicity. In this case, &lt;strong>parity of mistreatment&lt;/strong> would be a good metric, as it ensures that the proportion of errors is the same for all demographic groups. However, this considers that all errors are the same, which means that one group could have a high false positive rate and another a high false negative rate. The authors thus decided to choose &lt;strong>parity in false positive rates&lt;/strong> and/or &lt;strong>parity in false negative rates&lt;/strong>.&lt;/p>
&lt;h1 id="section-3">&lt;h1 style="font-size: 24px; text-decoration: underline;">AUC-based fairness constraints&lt;/h1>&lt;/h1>
&lt;p>This first approach is based on the AUC, it will help us to highlight the limitations of this metric which motivated the authors to introduce another approach based on ROC constraints.&lt;/p>
&lt;p>Precise example of AUC based constraints presented in the paper are the intra-group pairwise AUC fairness (Beutel et al., 2019), Background Negative Subgroup Positive (BNSP) AUC fairness (Borkan et al., 2019), the inter-group pairwise AUC fairness (Kallus and Zhou, 2019). The first one require the ranking performance to be equal within groups, the second one enforces that positive instances from either group have the same probability of being ranked higher than a negative example and the last one imposes that the positives of a group can be distinguished from the negatives of the other group as effectively for both groups. Those 3 AUC based constraints are only a part of the many constraints that exist.&lt;/p>
&lt;p>The paper introduces a new framework to generalize all relevant AUC-based constraint as a &lt;strong>linear combination of 5 relevant elementary constraints&lt;/strong> noted $C_1$ to $C_5$.&lt;/p>
&lt;p>The value of |$ C_ {1} $(s)| (resp. |$ C_ {2} $(s)|) quantifies the &lt;strong>resemblance of the distribution&lt;/strong> of the negatives (resp. positives) between the &lt;strong>two sensitive attributes&lt;/strong>.&lt;/p>
&lt;p>$ C_ {1} $(s) = $ AUC_ {{H_S^{(0)}} ,{H_S^{(1)}}} $ - $\frac{1}{2}$&lt;/p>
&lt;p>$ C_ {2} $(s) = $\frac{1}{2}$ - $ AUC_ {{G_S^{(0)}} ,{G_S^{(1)}}} $&lt;/p>
&lt;p>The values of $ C_ {3} $(s), $ C_ {4} $(s) and $ C_ {5} $(s) measure the &lt;strong>difference
in ability of a score to discriminate&lt;/strong> between positives and negatives for any two pairs of sensitive attributes.&lt;/p>
&lt;p>$ C_ {3} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $&lt;/p>
&lt;p>$ C_ {4} $(s) = $ AUC_ {{H_S^{(0)}} ,{G_S^{(1)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $&lt;/p>
&lt;p>$ C_ {5} $(s) = $ AUC_ {{H_S^{(1)}} ,{G_S^{(0)}}} $ - $ AUC_ {{H_S^{(1)}} ,{G_S^{(1)}}} $&lt;/p>
&lt;p>The family of fairness constraints considered is then the set of linear combinations of the $C_l(s)$ = 0:&lt;/p>
&lt;p>\begin{align*}
% $C_l(s)$ = 0
C_Γ(s): Γ^T C(s) =
\sum_{l=1}^{5} {Γ_l}{C_l}(s) = 0
\end{align*}&lt;/p>
&lt;p>Where $Γ$ = $(Γ_1, &amp;hellip; Γ_5)^T$.&lt;/p>
&lt;p>The objective function is thus defined as follows :&lt;/p>
&lt;p>\begin{align}
\label{eq:auc_general_problem}
\textstyle\max_{s\in\mathcal{S}} \quad AUC_{H_s,G_s} - \lambda
|\Gamma^\top
C
(s)|,
\end{align}
where $\lambda\ge 0$ is a hyperparameter balancing ranking performance
and fairness.&lt;/p>
&lt;p>The paper focuses on a special case of fairness, the &lt;strong>intra-group pairwise AUC fairness&lt;/strong>. This was to be more concise. In this example, the objective function becomes:&lt;/p>
&lt;p>$$
L_\lambda(s) = AUC_{H_s,G_s} - \lambda | AUC_{H_s^{(0)}, G_s^{(0)}} - AUC_{H_s^{(1)}, G_s^{(1)} } |
$$&lt;/p>
&lt;p>&lt;strong>&lt;u> Issues of AUC-Based constraint:&lt;/u>&lt;/strong>&lt;/p>
&lt;p>Fairness using AUC-based constraints defined by the equality between two AUC’s only quantify a stochastic order between distributions, not the equality between these distributions, and would lead to some unfair result, for a group or for the other group.&lt;/p>
&lt;p>The authors conducted experiments with the credit-risk dataset and found that creditworthy individuals from both groups had equal chances of being ranked higher than a &amp;ldquo;bad borrower.&amp;rdquo; However, employing high thresholds (which represent low probabilities of default on approved loans) would result in unfair outcomes for one group.&lt;/p>
&lt;h1 id="section-4">&lt;h1 style="font-size: 24px; text-decoration: underline;">ROC-based fairness constraints&lt;/h1>&lt;/h1>
&lt;p>A richer approach is then to use &lt;strong>pointwised ROC-based fairness constraints&lt;/strong>. Ideally, we would want to enforce the equality of all score distributions between both groups (i.e., identical ROC curves). This would satisfy all AUC-based fairness constraints previously mentioned. However, this condition is so restrictive that it will most likely lead to a significant drop in performances. As a result, the authors propose to satisfy this constraint on only a &lt;strong>finite number of points&lt;/strong>. They were indeed able to prove that this was sufficient to ensure maximum fairness for a fixed false positive or false negative $\alpha$.&lt;/p>
&lt;p>As a result, the objective function becomes :&lt;/p>
&lt;p>\begin{align*}
% L_\Lambda(s) =
AUC_{H_s,G_s} &amp;amp;-
\sum_{k=1}^{m_H} \lambda_H^{(k)} \big| \Delta_{H,\alpha_H^{
(k)}}(s) \big|
- \sum_{k=1}^{m_G} \lambda_G^{(k)} \big| \Delta_{G,\alpha_G^{(k)}}(s) \big|,
\end{align*}&lt;/p>
&lt;p>Where $\Delta_{H,\alpha_H^{(k)}}(s)$ and $\Delta_{G,\alpha_G^{(k)}}(s)$ represent the deviations between the positive (resp. negative) inter-group ROCs and the identity function:&lt;/p>
&lt;p>$$
\Delta_{G, \alpha}(s) = ROC_{G^{(0)}_s, G^{(1)}_s}(\alpha) - \alpha
$$&lt;/p>
&lt;p>$$
\Delta_{H, \alpha}(s) = ROC_{H^{(0)}_s,H^{(1)}_s}(\alpha) - \alpha
$$&lt;/p>
&lt;p>In practice, the objective function is slightly modified to be able to maximise it. The authors applied a classic smooth surrogate relaxations of the AUCs or ROCs based on a logistic function. They also removed the absolute values and, instead, relied on some parameters to ensure positive values.&lt;/p>
&lt;h1 id="section-5">&lt;h1 style="font-size: 24px; text-decoration: underline;">Results&lt;/h1>&lt;/h1>
&lt;p>The authors tested out their results on two datasets : &lt;strong>Compas&lt;/strong> and &lt;strong>Adult&lt;/strong>. Both are widely used when it comes to fairness. Indeed, they are known to be biased against race (for Compas) and gender (for both). Compas is a recidivism prediction dataset, whereas Adult predicts whether income exceeds $50K/yr based on census data. The results reported in the next figure show that the ROC-based method achieves its goal of mitigating the differences between favoured and unfavoured groups with limited drop in performances (the AUC went from 0.72 to 0.70 on the Compas dataset and from 0.91 to 0.87 on the Adult dataset). Indeed, the blue ROC curve, which is the ROC curve of the unfavoured group (Afro-American people for the Compas Dataset and women for the Adult Dataset), is brought closer to the green ROC curve (the ROC curve of the favoured group).&lt;/p>
&lt;p>&lt;img
src="./images/lambert_davy/main_text_inkscape_all_rocs_no_train_new.svg"
alt="AUC"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;h1 id="section-6">&lt;h1 style="font-size: 24px; text-decoration: underline;">Reproducibility&lt;/h1>&lt;/h1>
&lt;p>We were able to run the provided code without too much trouble on WSL2. The only modification we had to make was to change the calls for python in the sh files. We replace &lt;code>python&lt;/code> with &lt;code>python3&lt;/code>. However, as mentionned in the cide, the experiments were very long to run (several days) and we were not able to run the &lt;code>generate_all_figures.sh&lt;/code> script fully as it made our computers crash. Still, we were able to get some of the figures found in the paper (see below) by launching some scripts separately.&lt;/p>
&lt;p>&lt;img
src="./images/lambert_davy/dist.png"
alt="dist"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;img
src="./images/lambert_davy/roc.png"
alt="roc_gen"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>Here are two figure generated for the toy 1 dataset, one for the distribution of the scores and one for the ROC curve.&lt;/p>
&lt;h1 id="section-7">&lt;h1 style="font-size: 24px; text-decoration: underline;">Conclusion&lt;/h1>&lt;/h1>
&lt;p>The paper &amp;ldquo;Learning Fair Scoring Functions: Bipartite Ranking under ROC-based Fairness Constraints&amp;rdquo; underscores the growing importance of fairness in machine learning applications. It shows the limits of AUC-based fairness constraints for their inability to ensure equality between distributions, potentially leading to unfair outcomes. In contrast, ROC-based fairness constraints offer a richer approach by enforcing equality of score distributions between groups, albeit with some performance trade-offs. The paper tests the method on typical fairness datasets, but it is also possible to apply it to reel use cases. &amp;ldquo;A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization&amp;rdquo;, for example, explores the possibility to apply ROC-based methods for similarity learning, such as face recognition.&lt;/p>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/lambert-davy/</guid><pubDate>Sat, 23 Mar 2024 19:39:13 +0100</pubDate></item><item><title>Label-Free Explainability</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/label-free-explainability/</link><description>&lt;h1 style="font-size: 36px;">Label-Free Explainability for Unsupervised Models&lt;/h1>
&lt;h1 style="font-size: 18px;">Authors: &lt;a href="https://github.com/Valentinahxu">Valentina Hu &lt;/a> and &lt;a href="https://github.com/selmazrg"> Selma Zarga&lt;/a>&lt;/h1>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0">Incentives&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Feature Importance &lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">Example Importance&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">Experiment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a blog post about the paper Label-Free Explainability for Unsupervised Models, published by J. Crabbé et al. in 2022 and available &lt;a href="https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf">here&lt;/a>.&lt;/p>
&lt;h2 id="section-0">Why do we need explainability ?&lt;/h2>
&lt;p>Machine learning models are becoming increasingly capable of making advanced predictions. While models like linear regression are relatively easy to understand and explain, more complex models, often called &lt;strong>&amp;ldquo;black boxes&amp;rdquo;&lt;/strong> due to their complexity, present challenges in explaining how they make predictions. These models can be problematic in highstakes applications such as healthcare, finance, and justice, where it&amp;rsquo;s crucial to justify decision-making. Additionally, in case of errors, it&amp;rsquo;s important to understand the origin in order to address and correct them.&lt;/p>
&lt;center>
&lt;p>&amp;ldquo;&lt;strong>Explainability is the cornerstone of trust in black box models; without it, they remain inscrutable and unreliable.&lt;/strong>&amp;rdquo; - &lt;em>Yoshua Bengio&lt;/em>&lt;/p>
&lt;/center>
&lt;p>To tackle this challenge, the field of Explainable Artificial Intelligence (XAI) has emerged, offering various methods to enhance &lt;strong>model transparency&lt;/strong>. &lt;strong>Post-Hoc explainability&lt;/strong> methods exist, which intervene after the model has generated its results, enabling users to comprehend the reasoning behind specific decisions or predictions. These methods supplement the predictions of black box models with diverse explanations of how they arrive at their predictions.&lt;/p>
&lt;p>&lt;img
src="./images/explainability/Black-Box.png"
alt="XAI explainability"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;h2 id="section-1">I. Introduction&lt;/h2>
&lt;p>The entire post focuses on the quest for explainability of unsupervised models. In these models, no labels are assigned to the data, making understanding the model even more complicated due to the absence of explicit guidance on what the model is learning. In the supervised setting, users know the meaning of the black-box output they are trying to interpret. However, this clarity is not always available in machine learning. Therefore, elucidating concepts such as feature importance and example importance provides insights into why the model makes certain decisions or identifies specific patterns in the data.&lt;/p>
&lt;p>A recent research conducted by Crabbé and van der Schaar in 2022 explores the explainability of unsupervised models. They have developed two new methods to explain these complex models without labels. The first method highlights important features in the data, while the second identifies training examples that have the biggest impact on the model&amp;rsquo;s construction of representations. In this post, we will attempt to explain these two methods.&lt;/p>
&lt;h2 id="section-2">II. Feature Importance&lt;/h2>
&lt;p>Feature importance aims to explain how the model arrives at its prediction for a given input by assigning an importance scores to each feature (or attribute) of the input. This helps understand which features have the most influence on the model&amp;rsquo;s predictions. This method is developed based on a linear reasoning that is extended to label-free settings.&lt;/p>
&lt;p>Given a model $( f : \mathcal{X} \rightarrow \mathcal{Y} )$, which maps an input space $( \mathcal{X} \subset \mathbb{R}^{d_X} )$ to an output space $( \mathcal{Y} \subset \mathbb{R}^{d_Y} )$. Where, $( d_X )$ and $( d_Y )$ is the dimensions of the input and output spaces.&lt;/p>
&lt;p>In the traditionnal method, the process requires selecting one component $( f_j(x) )$ of the model&amp;rsquo;s output to compute the importance score for each feature $( i )$, denoted as $( a_i(f, x) )$. The selection is based on the ground-truth label, and $( j )$ corresponds to the class predicted with the highest probability.&lt;/p>
&lt;p>To understand how the label-free feature importance method works, let&amp;rsquo;s start by looking at the labeled case:&lt;/p>
&lt;p>&lt;strong>1. Labelled Feature Importance&lt;/strong>&lt;/p>
&lt;p>Authors introduces an alternative approach to calculate feature importance scores. The method proposes to combine the importance scores of different components of the model&amp;rsquo;s output by weighting them with the associated class probabilities. For each component of the model&amp;rsquo;s output, we multiply the importance score of the corresponding feature by the probability of that component.&lt;/p>
&lt;p>These weighted importance scores are then combined to obtain the final importance score of each feature.&lt;/p>
&lt;p>Let $a_i(f_j;x)$ be the importance score of feature $x_i$ calculated with respect to the component $f_j$ of the model&amp;rsquo;s output. The method proposes to calculate the importance score $b_i(f;x)$ for feature $x_i$ as follows:&lt;/p>
&lt;p>$b_i(f;x) = \sum_{j=1}^{d_Y} f_j(x) \times a_i(f_j,x)$&lt;/p>
&lt;p>Here, $f_j(x)$ represents the probability of class $j$, and $a_i(f_j;x)$ is the importance score of feature $x_i$ for class $j$.&lt;/p>
&lt;p>Hovewer when the class probabilities are balanced, this method accounts for the contribution of each class to the feature importance score, rather than focusing only on the class with the highest probability, which is the usual practice.&lt;/p>
&lt;p>This method proves to be efficient predominantly when the significance scores exhibit linearity in relation to the model. To facilitate a streamlined computation of weighted importance scores, another method is to introduce an auxiliary function, denoted as $(g_x)$ :&lt;/p>
&lt;p>$\ g_x(z) = \sum_{j=1}^{d_Y} f_j(x) \cdot f_j(z) $&lt;/p>
&lt;p>With the function $(g_x)$, it becomes feasible to calculate the weighted importance score, $(b_i(f, x))$, for each feature $(i)$, by merely employing $(g_x)$. This technique significantly simplifies the computational process, obviating the need to calculate $(d_Y \times d_X)$ importance scores. Such a calculation becomes impractically cumbersome with the escalation of the number of classes, $(d_Y)$. With this trick, we can compute the weighted importance score by only calling the auxiliary function.&lt;/p>
&lt;p>We can see that in the labeled case, the method is quite clear. A similar reasoning is used in the label-free setting. Now, let&amp;rsquo;s move on to the label-free setting.&lt;/p>
&lt;p>&lt;strong>2. Label-Free Feature Importance&lt;/strong>&lt;/p>
&lt;p>In the context of the unlabelled setting, we consider a latent space $H$ of dimension $d_H$ where a black-box model $f : X \rightarrow H$ is given. The goal is to assign an importance score to each feature of the input $x$, even if the dimensions of the latent space have no clear relations with the labels.&lt;/p>
&lt;p>A similar weighting formula for importance scores is used, where the components $f_j(x)$ do not correspond to probabilities but to neuron activations. The weighted sum is considered as a inner product in the latent space.&lt;/p>
&lt;p>The method is developed using linear feature importance functions, and it retains the completeness property, meaning that the sum of importance scores equals the black-box prediction up to a baseline constant.&lt;/p>
&lt;p>Here is how the method operates:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Presentation of the Latent Space:&lt;/strong> We consider a latent space $H$ of dimension $d_H$ where each input $x$ is mapped by the black-box model $f$ to obtain a representation $h = f(x)$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Assignment of Importance Scores:&lt;/strong> The objective is to assign an importance score $b_i(f; x)$ to each feature $x_i$ of $x$. Unlike in the previous setting, where we had probabilities associated with each component, here, we do not have a clear method to choose a particular component $f_j$ in the latent space. Therefore, we use a similar approach to the one described previously.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Calculation of Importance Scores:&lt;/strong> We use a weighting method where the importance score is given by $b_i(f; x) = a_i(\sum_{j=1}^{d_H} f_j(x) \cdot f_j(x))$. The individual components $f_j(x)$ do not correspond to probabilities in this case; they generally correspond to neuron activation functions. Inactive neurons will have a corresponding component that vanishes ($f_j(x) = 0$), meaning they will not contribute to the weighted sum, while more activated neurons will contribute more.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Completeness:&lt;/strong> An important property shared by many feature importance methods is completeness. This means that the sum of importance scores equals the black-box prediction up to a baseline constant. This establishes a connection between importance scores and black-box predictions.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This method proposes an extension of linear feature importance methods to the unlabelled setting by defining an auxiliary scalar function $g_x$ that encapsulates the black-box function $f$. This extension is achieved by using a function $g_x$ that computes the inner product between the representation $f(x)$ and the representation $f(\tilde{x})$ for all $\tilde{x}$ in the input space.&lt;/p>
&lt;h2 id="section-3">III. Example Importance&lt;/h2>
&lt;p>In this section, we explain the approach to extending example importance methods to the label-free setting. Given that example importance methods vary significantly, they are separated into two families: loss-based and representation-based methods. The extension to the label-free setting differs for these two families, so we discuss them separately in distinct subsections.&lt;/p>
&lt;p>&lt;strong>1. Loss-Based Example Importance&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Supervised Setting&lt;/strong>&lt;/p>
&lt;p>In supervised learning, loss-based example importance methods determine how important each training example is by assessing the impact of its removal on the model&amp;rsquo;s performance on test data. This is measured by the change in the loss function, which quantifies how well the model&amp;rsquo;s predictions match the true data.&lt;/p>
&lt;p>Mathematically, let $z$ represent the data of an example required to evaluate the loss, typically corresponding to a pair $(x, y)$ in supervised settings. The loss function $L(z; \theta)$ is optimized over a parameter space $\Theta$ to train the model. When an example $z_n$ is removed from the training set $D_{\text{train}}$, it results in a parameter shift $\theta_n - \theta&amp;rsquo;_{-n}$, impacting the loss $L(z; \theta&amp;rsquo;)$ on a test example $z$. This loss shift provides a meaningful measure of example importance.&lt;/p>
&lt;p>To estimate the loss shift without retraining the model, methods like the influence function and checkpoint evaluation are employed. For example, Koh &amp;amp; Liang (2017) propose using the influence function:&lt;/p>
&lt;p>&lt;em>Influence Function Formula&lt;/em>&lt;/p>
&lt;p>\begin{equation} \langle
\delta_{\theta}^{n} L(z; \theta&amp;rsquo;) \approx \frac{1}{N} \langle {\nabla L(z; \theta_{*})}, H^{-1} {\nabla L(z_{n}; \theta_{*}&amp;rsquo;)} \rangle_{\theta} &lt;br>
\end{equation}&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$( \nabla_{\theta} L(z, \theta^*) )$ is the gradient of the loss with respect to the parameters for the test example.&lt;/li>
&lt;li>$( H_{\theta^*} )$ is the Hessian matrix.&lt;/li>
&lt;li>$( \nabla_{\theta} L(z^n, \theta^*) )$ is the gradient of the loss for the removed training example.&lt;/li>
&lt;li>$( \langle \cdot, \cdot \rangle_{\theta} )$ denotes the inner product in the parameter space.&lt;/li>
&lt;li>$( N )$ is the number of training examples.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Label-Free Setting&lt;/strong>&lt;/p>
&lt;p>In a label-free setting, the models are trained without explicit labels. Instead, they use a label-free loss function, which typically tries to capture the structure of the data itself rather than fitting to specific target labels.&lt;/p>
&lt;p>In the context of autoencoders, determining the importance of a training example can be tricky due to the loss function used during training (uses the encoder and decoder). When we are only interested in the encoder part and it is not sufficient to only use the model&amp;rsquo;s loss function as this also include the influence of the decoder.&lt;/p>
&lt;p>To address this, we decompose the parameter space into relevant and irrelevant components. The proposed method computes the example importance scores by considering only the relevant parameters. The model to interpret, denoted as $f_r$, is parametrized only by the relevant parameters $\theta_r$.&lt;/p>
&lt;p>This motivates the definition of Label-Free Loss-Based Example Importance:&lt;/p>
&lt;p>\begin{equation}
c_n(f_r; x) = \theta_n L(x; \theta&amp;rsquo;)
\end{equation}&lt;/p>
&lt;p>Label-Free Loss-Based Example Importance score $( c_n(f_{\theta_r}, x) )$ measures the impact of removing a training example $( x_n )$ from the training set on the learned latent representation $( f_{\theta_r}(x) )$ of a test example $( x )$. It uses $( \delta_{\theta_r} L )$ to denote the part of the loss shift that is only due to changes in the relevant parameters $(( \theta_r ))$.&lt;/p>
&lt;p>This definition extends any loss-based example importance method to the label-free setting, where the unsupervised loss $L$ is used to fit the model, and the gradients with respect to the parameters of the encoder are computed.&lt;/p>
&lt;p>&lt;strong>2. Representation-Based Example Importance&lt;/strong>&lt;/p>
&lt;p>Representation-based example importance methods analyze the latent representations of examples to assign importance scores.&lt;/p>
&lt;p>&lt;strong>Supervised Setting&lt;/strong>&lt;/p>
&lt;p>These methods quantify the affinity between a test example and the training set examples based on their latent representations. For instance, in a model $f_l \circ f_e: X \rightarrow Y$, where $f_e: X \rightarrow H$ maps inputs to latent representations and $f_l: H \rightarrow Y$ maps representations to labels, representation-based methods reconstruct the test example&amp;rsquo;s latent representation using training representations. The reconstruction involves assigning weights to training representations, typically based on nearest neighbors or learned weights. For example, using a kernel function $\mathcal{K}$:&lt;/p>
&lt;p>\begin{equation}
w_n(x) = \frac{1}{|KNN(x)|} \sum_{n&amp;rsquo; \in KNN(x)} \mathcal{K}(\text{fe}(x_n), \text{fe}(x))
\end{equation}&lt;/p>
&lt;p>&lt;strong>Label-Free Setting&lt;/strong>&lt;/p>
&lt;p>Rrepresentation-based methods remain valid by replacing supervised representation maps with unsupervised ones. Hence, no additional modifications are needed.&lt;/p>
&lt;h2 id="section-5">IV. Experiments: Evaluation and Results&lt;/h2>
&lt;h3 id="section-111">Consistency Checks&lt;/h3>
&lt;p>Now, we are verifying the consistency of results obtained from different methods of assessing feature and example importance using the MNIST dataset.&lt;/p>
&lt;p>In MNIST, important features are the pixels of the images, and various methods can be employed to evaluate their importance. To assess feature importance, we can measure the impact of selectively removing the most important pixels on the latent representation constructed by the encoder, as described in the previous example. By comparing the results of different methods of importance assessment, such as perturbing the most important pixels according to various importance measures, we can check if the same pixels are identified as important and if their removal consistently affects the latent representation.&lt;/p>
&lt;p>&lt;em>We rerun the tests provided in the &lt;a href="https://github.com/JonathanCrabbe/Label-Free-XAI">GitHub repository&lt;/a>:&lt;/em>&lt;/p>
&lt;p>On the MNIST dataset, we perturb the most important pixels and observe how this perturbation affects the quality or relevance of the latent representation generated by the encoder.
Here we can see the result of the experiment :&lt;/p>
&lt;p>&lt;img
src="./images/explainability/mnist_consistency_features.png"
alt="XAI explainability"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>The results obtained from the representation shift curves as a function of the percentage of perturbed pixels demonstrate the effectiveness of Feature Importance methods on the MNIST dataset.&lt;/p>
&lt;p>We observe that Feature Importance methods such as Gradient Shap and Integrated Gradients show a significant increase in representation shift when the most important pixels are perturbed. This indicates that these methods successfully identify the most relevant pixels for constructing the latent representation. However, after perturbing approximately 20% of the most important pixels, we notice a stabilization of the representation shift, suggesting that adding additional perturbations does not necessarily lead to a significant increase in impact on the latent representation.&lt;/p>
&lt;p>On the other hand, the Saliency method appears to be less effective, with an almost linear representation shift curve, suggesting that it fails to selectively identify the most important pixels for the latent representation.&lt;/p>
&lt;p>Overall, this confirms the effectiveness of Feature Importance methods, particularly Integrated Gradients.&lt;/p>
&lt;p>Similarly, to evaluate the importance of examples in MNIST, we select training examples that have a significant influence on predicting the latent representation of test examples. By comparing the results obtained with different methods of assessing example importance, we can verify if the same examples are identified as important and if their relevance is consistent with the model&amp;rsquo;s predictions.&lt;/p>
&lt;p>&lt;img
src="./images/explainability/example.png"
alt="XAI explainability"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>For all example importance methods, we observe a decrease in similarity rates, with a consistent trend across all curves.&lt;/p>
&lt;p>This observation highlights that the similarity rate is significantly higher among the most similar examples compared to the least similar examples, confirming the effectiveness of label-free importance scores cn(fe; x) in identifying training examples related to the test example we wish to explain.&lt;/p>
&lt;p>In summary, these results affirm the capability of label-free importance scores in effectively selecting relevant training examples and distinguishing between similar and dissimilar examples.&lt;/p>
&lt;h2 id="section-6">V. Conclusion&lt;/h2>
&lt;p>In this post you learned about label-free explainability a new framework developped by Crabbé and van der Schaar in 2022, wich extend linear feature importance and example importance
methods to the unsupervised setting with a focus on the MNIST dataset.&lt;/p>
&lt;h3 id="references">References&lt;/h3>
&lt;ol>
&lt;li>Crabbé, J. &amp;amp; van der Schaar, M.. (2022). Label-Free Explainability for Unsupervised Models. &lt;i>Proceedings of the 39th International Conference on Machine Learning&lt;/i>, in &lt;i>Proceedings of Machine Learning Research&lt;/i> 162:4391-4420 Available from &lt;a href="https://proceedings.mlr.press/v162/crabbe22a.html">https://proceedings.mlr.press/v162/crabbe22a.html&lt;/a>.&lt;/li>
&lt;/ol>
&lt;style>
.highlight {
background-color: red;
}
.highlight-on-hover:hover {
background-color: yellow;
}
/* Quiz form styles */
.quiz-form {
max-width: 500px;
margin: auto;
padding: 20px;
border: 1px solid #ccc;
border-radius: 5px;
background-color: #f9f9f9;
}
.quiz-question {
margin-bottom: 20px;
}
.quiz-options label {
display: block;
margin-bottom: 10px;
}
.quiz-submit {
background-color: #4caf50;
color: white;
padding: 10px 20px;
border: none;
border-radius: 5px;
cursor: pointer;
}
.quiz-submit:hover {
background-color: #45a049;
}
/* Quiz results styles */
.quiz-results {
margin-top: 20px;
font-weight: bold;
}
.quiz-options label {
display: block;
margin-bottom: 10px;
}
.quiz-options label.correct {
color: green;
}
.quiz-options label.incorrect {
color: red;
}
a[name]:hover {
background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */
text-decoration: none; /* Optionally remove underline on hover */
}
&lt;/style>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/label-free-explainability/</guid><pubDate>Sun, 17 Mar 2024 15:31:34 +0100</pubDate></item><item><title>Adversarially Reweighted Learning</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/adversarially_reweighted_learning/</link><description>&lt;h1 style="font-size: 36px;">Fairness without Demographics through Adversarially Reweighted Learning&lt;/h1>
&lt;h1 style="font-size: 24px;">Authors: Pierre Fihey &amp; Guerlain Messin&lt;/h1>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0">Fairness issues in ML and AI&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">The privacy of demographic’s data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">The Adversarial Reweighted Learning Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">An Hypothesis: Protected Groups are Correlated with Both Features and Labels&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">Computational identifiability of protected groups&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">The Rawlsian Max-Min Fairness principle&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">The ARL objective&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-7">The Model Architecture&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-8">Results analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-9">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a blog post about the paper Fairness without Demographics through Adversarially Reweighted Learning, published by P. Lahoti et al. in 2020 and available &lt;a href="https://dl.acm.org/doi/abs/10.5555/3495724.3495786">here&lt;/a>.&lt;/p>
&lt;h2 id="section-0">Fairness issues in ML and AI&lt;/h2>
&lt;p>As Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.&lt;/p>
&lt;p>Such biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms&amp;rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments.
Machine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.&lt;/p>
&lt;p>The root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.&lt;/p>
&lt;h2 id="section-1">The privacy of demographic’s data&lt;/h2>
&lt;p>Strict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.&lt;/p>
&lt;p>In this blog, we&amp;rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google&amp;rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?&lt;/p>
&lt;h2 id="section-2">The Adversarial Reweighted Learning Model&lt;/h2>
&lt;h3 id="section-3">An Hypothesis: Protected Groups are Correlated with Both Features and Labels&lt;/h3>
&lt;p>While access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.&lt;/p>
&lt;p>The authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.&lt;/p>
&lt;p>&lt;img
src="./images/Fihey_Messin/Identifying_Groups.png"
alt="Identifying Groups"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>This assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google&amp;rsquo;s research team is based to outperform previous work.&lt;/p>
&lt;h3 id="section-4">Computational identifiability of protected groups&lt;/h3>
&lt;p>Computational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:&lt;/p>
&lt;p>For a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \times Y \rightarrow \text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \in S$.&lt;/p>
&lt;p>This function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.&lt;/p>
&lt;h3 id="section-5">The Rawlsian Max-Min Fairness principle&lt;/h3>
&lt;p>In philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:&lt;/p>
&lt;p>&lt;strong>Definition (Rawslan Max-Min Fairness):&lt;/strong> Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility.
$$h^* = argmax_{h \in H} min_{s \in S} U_{D_s}(h)$$&lt;/p>
&lt;p>The Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged. This is what will enable our model to obtain truly relevant results, and we&amp;rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.&lt;/p>
&lt;h3 id="section-6">The ARL objective&lt;/h3>
&lt;p>To adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally.
The aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:&lt;/p>
&lt;p>$$J(\theta, \lambda) := min_{\theta} max_{\lambda} \sum_{s \in S} \lambda_s L_{D_s}(h)$$
$$= min_{\theta} max_{\lambda} \sum_{i=0}^{n} \lambda_{s_i} l(h(x_i), y_i)$$&lt;/p>
&lt;p>With $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.&lt;/p>
&lt;h3 id="section-7">The Model Architecture&lt;/h3>
&lt;p>As previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.&lt;/p>
&lt;p>The learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.&lt;/p>
&lt;p>The adversary learns a function mapping $f_\phi : X \times Y \rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\lambda_\phi : f_\phi \rightarrow \mathbb{R}$ so as to maximize the expected loss.&lt;/p>
&lt;p>The learner then adjusts itself to minimize the adversarial loss:
$$J(\theta, \phi) = min_{\theta} max_{\phi} \sum_{i=1}^{n} \lambda_{\phi}(x_i, y_i) \cdot l_{ce}(h_\theta(x_i), y_i)$$&lt;/p>
&lt;p>To ensure that the loss function is well defined, it&amp;rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.&lt;/p>
&lt;p>$$\lambda_{\phi}(x_i, y_i) = 1 + n \cdot \frac{f_{\phi}(x_i, y_i)}{\sum_{i=1}^{n} f_{\phi}(x_i, y_i)}$$&lt;/p>
&lt;p>The authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.&lt;/p>
&lt;p>&lt;img
src="./images/Fihey_Messin/ARL_Computational_Graph.png"
alt="ARL Computational Graph"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;h2 id="section-8">Results analysis&lt;/h2>
&lt;p>This section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.&lt;/p>
&lt;h3 id="reproducibility">Reproducibility&lt;/h3>
&lt;p>We first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.&amp;rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.&lt;/p>
&lt;h3 id="replicability">Replicability&lt;/h3>
&lt;p>We replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.&amp;rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.&amp;rsquo;s findings, highlighting the robustness of the ARL model across different implementations.&lt;/p>
&lt;h3 id="significance-evaluation">Significance Evaluation&lt;/h3>
&lt;p>We conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.&lt;/p>
&lt;h2 id="section-9">Conclusion&lt;/h2>
&lt;p>In this study, we critically examined the paper &amp;ldquo;Fairness without Demographics through Adversarially Reweighted Learning&amp;rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.&amp;rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.&lt;/p>
&lt;hr>
&lt;hr>
&lt;h2 id="annexes">Annexes&lt;/h2>
&lt;h3 id="references">References&lt;/h3>
&lt;p>[1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., &amp;amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.&lt;/p>
&lt;p>[2] Veale, M., &amp;amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data &amp;amp; Society, 4(2), 2053951717743530.&lt;/p>
&lt;p>[3] Hanley, J. A., &amp;amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.&lt;/p>
&lt;p>[4] Hanley, J. A., &amp;amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.&lt;/p>
&lt;p>[5] Dua, D., &amp;amp; Graff, C. (2019). UCI machine learning repository.&lt;/p>
&lt;p>[6] Kim, M. P., Ghorbani, A., &amp;amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).&lt;/p>
&lt;p>[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., &amp;hellip; &amp;amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.&lt;/p>
&lt;p>[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., &amp;hellip; &amp;amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).&lt;/p>
&lt;p>[9] Kamishima, T., Akaho, S., &amp;amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.&lt;/p>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/adversarially_reweighted_learning/</guid><pubDate>Mon, 04 Mar 2024 18:35:12 +0100</pubDate></item><item><title>Packed Ensembles</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/packed-ensembles/</link><description>&lt;script
type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script
type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;div style="text-align:center;">
This is a blog post about the paper Packed-Ensembles for Efficient Uncertainty Estimation, published by O. Laurent et al. in 2023 and available [here](https://openreview.net/pdf?id=XXTyv1zD9zD).
&lt;h3 id="authors-cynthia-obeid-and-elie-nakad">&lt;strong>Authors&lt;/strong>: Cynthia Obeid and Elie Nakad&lt;/h3>
&lt;h1>Introduction&lt;/h1>
&lt;/div>
The document "Packed-Ensembles for Efficient Uncertainty Estimation" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems.
&lt;div style="text-align:center;">
&lt;h1>Presentation of the model&lt;/h1>
&lt;/div>
&lt;p>&lt;strong>Packed-Ensembles&lt;/strong>&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/fig1.jpg" alt="The base network and Packed-Ensembles" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>The base network and Packed-Ensembles&lt;/i>&lt;/p>
&lt;p>Packed-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.&lt;/p>
&lt;p>&lt;strong>Benefits of Packed-Ensembles&lt;/strong>&lt;/p>
&lt;p>Packed-Ensembles offer several benefits over traditional ensemble methods, including:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Efficiency:&lt;/strong> Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Accuracy:&lt;/strong> Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Calibration:&lt;/strong> Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Out-of-distribution (OOD) detection:&lt;/strong> Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Comparison to other ensemble methods&lt;/strong>&lt;/p>
&lt;p>The paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.&lt;/p>
&lt;div style="text-align:center;">
&lt;h1>Packed-Ensembles: A Technique for Efficient Neural Network Ensembles&lt;/h1>
&lt;/div>
&lt;p>Packed-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.&lt;/p>
&lt;p>&lt;strong>Understanding Convolutional Layers and Grouped Convolutions:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Convolutional Layers:&lt;/strong> These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows:&lt;/li>
&lt;/ul>
&lt;p>$z^{(j+1)}(c,:,:) = (h^j \otimes \omega^j)(c,:,:) = \sum_{k=0}^{C_{j}-1} \omega^j(c, k,:,:) \star h^j(k,:,:)$&lt;/p>
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>$c$&lt;/strong> represents the channel index&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>$h^j$&lt;/strong> denotes the input feature map&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>$ω^j$&lt;/strong> represents the weight tensor (kernel)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>$⋆$&lt;/strong> denotes the 2D cross-correlation operator&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Grouped Convolutions:&lt;/strong> This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating &lt;strong>independent subnetworks&lt;/strong>. The mathematical formulation for grouped convolutions is given by:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$
z^{(j+1)}(c,:,:) = \left( h^j \otimes \omega^j_{\gamma} \right) (c,:,:) = \sum_{k=0}^{\frac{C_{j}}{\gamma}-1} \omega^j_{\gamma} (c, k,:,:) \star h^j \left( k + \left\lfloor \frac{c}{C_{j+1}/\gamma} \right\rfloor \frac{C_{j}}{\gamma}, :,:\right)
$$&lt;/p>
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>$γ$&lt;/strong> represents the number of groups&lt;/li>
&lt;li>&lt;strong>$C_{j+1}$&lt;/strong> and &lt;strong>$C_j$&lt;/strong> denote the number of output and input channels, respectively.&lt;/li>
&lt;/ul>
&lt;p>The formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask &lt;strong>$\text{mask}_{m}^j$&lt;/strong>
&lt;strong>$\in \{{ 0, 1 \}}^{C_{j+1} \times C_j \times s_j^2}$&lt;/strong> with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\text{mask}_{m}^j$ is either 0 or 1.&lt;/p>
&lt;p>The condition &lt;strong>$\text{mask}_{m}^j(k, l, :, :) = 1$&lt;/strong> happens only if $\left\lfloor \frac{l}{C_{j}/\gamma} \right\rfloor = \left\lfloor \frac{k}{C_{j+1}/\gamma} \right\rfloor$ for each group $m \in [|0, \gamma - 1 |]$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Complete Mask and Convolution:&lt;/strong>
&lt;ul>
&lt;li>$\text{mask}^j = \sum_{m=0}^{{\gamma}-1}\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\text{mask}^j$ for layer $j$.&lt;/li>
&lt;li>$z^{j+1} = h^j \otimes (ω^j ◦ \text{mask}^j)$: This rewrites the grouped convolution operation. Here:
&lt;ul>
&lt;li>$z^{j+1}$: Output feature map of the layer.&lt;/li>
&lt;li>$h^j$: Input feature map.&lt;/li>
&lt;li>$ω^j$: Convolution weights for layer &lt;code>j&lt;/code>.&lt;/li>
&lt;li>$\otimes$: Denotes convolution operation.&lt;/li>
&lt;li>$◦$: Denotes Hadamard product (element-wise multiplication).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>In simpler terms:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Grouped convolution divides the input channels and weights into groups.&lt;/li>
&lt;li>A separate mask is created for each group, ensuring elements within a group are aligned.&lt;/li>
&lt;li>These masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group.&lt;/li>
&lt;li>The final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask.&lt;/li>
&lt;/ul>
&lt;div style="text-align:center;">
&lt;h1>Background on Deep Ensembles&lt;/h1>
&lt;/div>
&lt;p>This section delves into Deep Ensembles (DE), a technique for image classification tasks.&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/fig2.png" alt="Deep Ensembles" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>Deep Ensembles&lt;/i>&lt;/p>
&lt;p>&lt;strong>Setting the Scene&lt;/strong>&lt;/p>
&lt;p>We have a dataset $D$ containing pairs of images and their corresponding labels:&lt;/p>
&lt;ul>
&lt;li>$x_i$: Represents an image sample with dimensions $C0 \times H0 \times W0$ (likely referring to color channels, height, and width).&lt;/li>
&lt;li>$y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes).&lt;/li>
&lt;/ul>
&lt;p>The dataset is assumed to be drawn from a joint distribution $P(X, Y)$.&lt;/p>
&lt;p>A neural network $f_\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\theta$.&lt;/p>
&lt;ul>
&lt;li>$\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Traditional Approach:&lt;/strong>&lt;/p>
&lt;p>The model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.&lt;/p>
&lt;p>&lt;strong>Introducing Deep Ensembles&lt;/strong>&lt;/p>
&lt;p>DE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).&lt;/p>
&lt;p>The ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:&lt;/p>
&lt;p>$$
P(y_i|x_i, D) = M^{-1} \sum_{m=0}^{M-1} P(y_i|x_i, \theta_m)
$$&lt;/p>
&lt;p>This essentially combines the outputs of multiple networks to create a more robust prediction.&lt;/p>
&lt;p>In simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.&lt;/p>
&lt;p>&lt;strong>Building Packed-Ensembles:&lt;/strong>&lt;/p>
&lt;p>Packed-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here&amp;rsquo;s how it works:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Subnetworks:&lt;/strong> The ensemble is formed by creating &lt;strong>$M$&lt;/strong> smaller subnetworks within the main network architecture. These subnetworks share the same structure but have &lt;strong>independent parameters&lt;/strong> due to the use of grouped convolutions.&lt;/li>
&lt;li>&lt;strong>Hyperparameters:&lt;/strong> Packed-Ensembles are defined by three hyperparameters:
&lt;ul>
&lt;li>&lt;strong>$α$ (alpha):&lt;/strong> expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters).&lt;/li>
&lt;li>&lt;strong>$M$:&lt;/strong> number of subnetworks in the ensemble (represents the ensemble size).&lt;/li>
&lt;li>&lt;strong>$γ$ (gamma):&lt;/strong> number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Mathematical Implementation:&lt;/strong>&lt;/p>
&lt;p>The output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:&lt;/p>
&lt;p>$$
\hat{y} = M^{-1} \sum_{m=0}^{M-1} P(y|\theta_a^m, x) \quad \text{with} \quad \theta_a^m = ({\omega_j^{\alpha} \circ \text{mask}_{m}^j})_j
$$&lt;/p>
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>$\hat{y}$&lt;/strong> represents the ensemble&amp;rsquo;s predicted label&lt;/li>
&lt;li>&lt;strong>$P(y|θ_a^m, x)$&lt;/strong> denotes the probability of class &lt;strong>$y$&lt;/strong> given the input &lt;strong>$x$&lt;/strong> and the parameters &lt;strong>$θ_a^m$&lt;/strong> of the &lt;strong>$m-th$&lt;/strong> subnetwork&lt;/li>
&lt;li>&lt;strong>$\theta_a^m = ({\omega_j^{\alpha} \circ \text{mask}_{m}^j})_j$&lt;/strong> represents the parameters of the &lt;strong>$m-th$&lt;/strong> subnetwork, obtained by applying element-wise multiplication (&lt;strong>$∘$&lt;/strong>) between the expanded weights (&lt;strong>$\omega_j^{\alpha}$&lt;/strong>) and the group mask (&lt;strong>$\text{mask}_{m}$&lt;/strong>) for each layer &lt;strong>$j$&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Implementation&lt;/strong>&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/fig4.png" alt="Equivalent architectures for Packed-Ensembles" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>Equivalent architectures for Packed-Ensembles&lt;/i>&lt;/p>
&lt;p>The authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/fig5.png" alt="subnetwork mask" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>Diagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer&lt;/i>&lt;/p>
&lt;div style="text-align:center;">
&lt;h1>Experiments&lt;/h1>
&lt;/div>
&lt;p>The experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Datasets:&lt;/strong> CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels.&lt;/li>
&lt;li>&lt;strong>Architectures:&lt;/strong> PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles.&lt;/li>
&lt;li>&lt;strong>Metrics:&lt;/strong> Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used.&lt;/li>
&lt;li>&lt;strong>Implementation Details:&lt;/strong> Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift.&lt;/li>
&lt;li>&lt;strong>Code:&lt;/strong> PyTorch-Lightning framework is used for implementation.&lt;/li>
&lt;/ul>
&lt;div style="text-align:center;">
&lt;h1>Results&lt;/h1>
&lt;/div>
&lt;p>The experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CIFAR-10/100:&lt;/strong>
&lt;ul>
&lt;li>PE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet).&lt;/li>
&lt;li>Smaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>ImageNet:&lt;/strong>
&lt;ul>
&lt;li>PE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models.&lt;/li>
&lt;li>PE achieves better accuracy with a reasonable increase in training and inference cost.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>These results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/fig3.png" alt="ResNet50 performance" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>Packed-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100&lt;/i>&lt;/p>
&lt;div style="text-align:center;">
&lt;h1>Ethics&lt;/h1>
&lt;/div>
&lt;p>This section emphasizes the ethical considerations of the research. Here are the key points:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Goal:&lt;/strong> This research proposes a method to improve uncertainty estimation in deep learning models.&lt;/li>
&lt;li>&lt;strong>Limitations:&lt;/strong> The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it&amp;rsquo;s not ready for such applications.&lt;/li>
&lt;li>&lt;strong>Concerns:&lt;/strong> The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like:
&lt;ul>
&lt;li>Unknown situations&lt;/li>
&lt;li>Corner cases (uncommon but important situations)&lt;/li>
&lt;li>Adversarial attacks (attempts to intentionally mislead the model)&lt;/li>
&lt;li>Potential biases in the model&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Overall:&lt;/strong> The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems.&lt;/li>
&lt;/ul>
&lt;div style="text-align:center;">
&lt;h1>Reproducibility: Packed-Ensemble on CIFAR-10&lt;/h1>
&lt;/div>
&lt;p>We attempted to reproduce the experiment outlined in the tutorial available at &lt;a href="https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html">https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html&lt;/a> which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Data Loading and Preprocessing:&lt;/strong> Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images.&lt;/li>
&lt;li>&lt;strong>Packed-Ensemble Definition:&lt;/strong> Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture.&lt;/li>
&lt;li>&lt;strong>Loss Function and Optimizer:&lt;/strong> Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training.&lt;/li>
&lt;li>&lt;strong>Training:&lt;/strong> Training the Packed-Ensemble model on the CIFAR-10 training data.&lt;/li>
&lt;li>&lt;strong>Testing and Evaluation:&lt;/strong> Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Experimental Runs and Observations:&lt;/strong>&lt;/p>
&lt;p>Test 1:&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/Result1.png" alt="First result" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>GroundTruth: cat ship ship plane&lt;/i>&lt;/p>
&lt;p>The predicted labels are: cat ship ship ship&lt;/p>
&lt;p>Test 2:&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/Result2.png" alt="Second result" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>GroundTruth: dog bird horse bird&lt;/i>&lt;/p>
&lt;p>The predicted labels are: dog frog car dog&lt;/p>
&lt;p>Test 3:&lt;/p>
&lt;div style="text-align:center;">
&lt;img src="./images/Result3.png" alt="Third result" style="display:block; margin:auto;">
&lt;/div>
&lt;p style="text-align:center;">&lt;i>GroundTruth: dog truck plane car &lt;/i>&lt;/p>
&lt;p>The predicted labels are: dog horse ship truck&lt;/p>
&lt;p>&lt;strong>Challenges and Limitations:&lt;/strong>&lt;/p>
&lt;p>A significant limitation of the tutorial is the lack of guidance on evaluating the model&amp;rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it&amp;rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.&lt;/p></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/packed-ensembles/</guid><pubDate>Tue, 27 Feb 2024 15:05:20 +0100</pubDate></item><item><title>A Framework to Learn with Interpretation</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/a-framework-to-learn-with-interpretation/</link><description>&lt;hr>&lt;/hr>
&lt;style
TYPE="text/css">
&lt;p>code.has-jax {font:
inherit;
font-size:
100%;
background:
inherit;
border:
inherit;}&lt;/p>
&lt;p>&lt;/style>&lt;/p>
&lt;script
type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script
type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h1 style="font-size: 36px;">A Framework to Learn with Interpretation&lt;/h1>
&lt;p>&lt;strong>Authors: Maroun ABOU BOUTROS, Mohamad EL OSMAN&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Article: &lt;a href="https://arxiv.org/abs/2010.09345">A Framework to Learn with Interpretation&lt;/a> by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc&lt;/strong>&lt;/p>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-1">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Learning a classifier and an interpreter&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#section-2.1">Design of FLINT&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2.2">Interpretation in FLINT&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2.3">Learning by imposing interpretability properties&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-3">Understanding encoded concepts in FLINT&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">Reproducing the experiments&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#section-4.1">Global interpretation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4.2">Local interpretation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-5">Subjective evaluation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">Specialization of FLINT to post-hoc interpretability&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-7">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="section-1">1 Introduction&lt;/h1>
&lt;p>In this blog post, we’ll explore FLINT, a framework introduced in the paper titled “A Framework to Learn with Interpretation” by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc, available on the following &lt;a href="https://arxiv.org/abs/2010.09345">link&lt;/a>, addressing the crucial need for interpretability in machine learning as complex predictive models become more prevalent in fields like law, healthcare, and defense. Interpretability, synonymous with explainability, provides insights into a model’s decision-making process. Two main approaches, post-hoc methods and “interpretable by design” methods, tackle the challenge of interpreting models, each with its pros and cons. A new approach, Supervised Learning with Interpretation (SLI), jointly learns a predictive model and an interpreter model. FLINT, specifically designed for deep neural network classifiers, introduces a novel interpreter network architecture promoting local and global interpretability. It also proposes a criterion for concise and diverse attribute functions, enhancing interpretability. We’ll delve into the architecture of FLINT and how it works to give explainable predictions, and we will reproduce some experiments done in the experimental section of the article and evaluate their outputs to study FLINT&amp;rsquo;s performance. And finally, we will present a specialization of FLINT for post-hoc interpretability.&lt;/p>
&lt;h1 id="section-2">2 Learning a classifier and its interpreter with FLINT&lt;/h1>
&lt;p>The paper introduces Supervised Learning with Interpretation (SLI), a new task aimed at incorporating interpretability alongside prediction in machine learning models. In SLI, a separate model, called an interpreter, is employed to interpret the predictions made by the primary predictive model. The task involves minimizing a combined loss function consisting of prediction error and interpretability objectives. The paper focuses on addressing SLI within the context of deep neural networks for multi-class classification tasks. It proposes a framework called Framework to Learn with INTerpretation (FLINT), which utilizes a specialized architecture for the interpreter model, distinguishes between local and global interpretations, and introduces corresponding penalties in the loss function to achieve the desired interpretability.&lt;br>
So for a dataset $S$ and a given model $f \in F$ where $F$ is a class of classifiers (here neural networks) and an interpreter model $g \in G_f$ where $G_f$ is a family of models, the SLI problem is presented by:
$$
\arg{\min_{f \in F, g \in G_f}{L_{pred}(f, S) + L_{int}(f, g, S)}}
$$
Where $L_{pred}(f, S)$ denotes a loss term related to prediction error and $L_{int}(f, g, S)$ measures the ability of $g$ to provide interpretations of predictions by $f$.&lt;/p>
&lt;h2 id="section-2.1">2.1 Design of FLINT&lt;/h2>
&lt;p>&lt;img
src="./images/FLINT/FLINT_design.png"
alt="design of FLINT"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>In FLINT, depicted in the image above, both a prediction model ($f$) and an interpreter model ($g$) are used. The input to FLINT is a vector $x \in X$, where $X = \mathbb{R}^d$, and the output is a vector $y \in Y$, where $Y$ is defined as the set of one-hot encoding vectors with binary components of size $C$ (the number of classes to predict). The prediction model $f$ is structured as a deep neural network with $l$ hidden layers, represented as $f = f_{l+1} \circ f_l \circ \ldots \circ f_1$. Each $f_k$ represents a hidden layer mapping from $R^{d_{k-1}}$ to $R^{d_k}$. To interpret the outputs of $f$, we randomly select a subset of $T$ hidden layers, indexed by $I=\{i_1, i_2, \ldots, i_T\}$, and concatenate their outputs to form a new vector $f_I(x) \in \mathbb{R}^D$, where $D = \sum_{t=1}^T d_{i_t}$. This vector is then fed into a neural network $\Psi$ to produce an output vector $\Phi(x) = \Psi(f_I(x)) = (\phi_1(x), &amp;hellip;, \phi_J (x)) \in \mathbb{R}^J$, representing an attribute dictionary comprising functions $\phi_j: X \rightarrow \mathbb{R}^+$, where $\phi_j(x)$ captures the activation of a high-level attribute or a &amp;ldquo;concept&amp;rdquo; over $X$. Finally, $g$ computes the composition of the attribute dictionnary with an interpretable function $h: R^J \rightarrow Y$.
$$
\forall x \in X, g(x) = h(\Phi(x))
$$
For now we take $h(x) = softmax(W^T \Phi(x))$ but $h$ can be any interpretable function (like a decision tree for example).&lt;/p>
&lt;p>Note that $d$ in the image is a decoder network that takes $\Phi(x)$ and reconstructs the input $x$. This decoder is used for training and its purpose will be detailed later on in section 2.3.&lt;/p>
&lt;h2 id="section-2.2">2.2 Interpretation in FLINT&lt;/h2>
&lt;p>With the interpreter defined, let&amp;rsquo;s clarify its role and interpretability objectives within FLINT. Interpretation serves as an additional task alongside prediction. We&amp;rsquo;re interested in two types: global interpretation, which aids in understanding which attribute functions contribute to predicting a class, and local interpretation, which pinpoints the attribute functions involved in predicting a specific sample.&lt;/p>
&lt;p>To interpret a local prediction $f(x)$, it&amp;rsquo;s crucial that the interpreter&amp;rsquo;s output $g(x)$ aligns with $f(x)$. Any discrepancy prompts analysis of conflicting data, potentially raising concerns about the prediction&amp;rsquo;s confidence.&lt;/p>
&lt;p>To establish local and global interpretation, we rely on attribute relevance. Given an interpreter with parameters $\Theta_g = (\theta_\Psi, \theta_h)$ and an input $x$, an attribute $\phi_j$&amp;rsquo;s relevance is defined concerning the prediction $g(x) = f(x) = \hat{y}$. The attribute&amp;rsquo;s contribution to the unnormalized score of class $\hat{y}$ is $\alpha_{j, \hat{y}, x} = \phi_j(x) \cdot w_{j, \hat{y}}$, where $w_{j, \hat{y}}$ is the coefficient associated with this class. Relevance score $r_{j, x}$ is computed by normalizing $\alpha$ as $r_{j, x} = \frac{\alpha_{j, \hat{y}, x}}{\max_i |\alpha_{i, \hat{y}, x}|}$. An attribute $\phi_j$ is considered relevant for a local prediction if it&amp;rsquo;s both activated and effectively used in the linear model.&lt;/p>
&lt;p>Attribute relevance extends to its overall importance in predicting any class $c$. This is achieved by averaging relevance scores from local interpretations over a random subset or the entirety of the training set $S$ where the predicted class is $c$. Thus, $r_{j, c} = \frac{1}{|S_c|} \sum_{x \in S_c} r_{j, x}$, where $S_c = \{x \in S \mid \hat{y} = c\}$.&lt;/p>
&lt;p>Now, let&amp;rsquo;s introduce the local and global interpretations the interpreter will provide:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Global interpretation ($G(g, f)$) identifies class-attribute pairs $(c, \phi_j)$ where the global relevance $r_{j, c}$ exceeds a threshold $\frac{1}{\tau}$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Local interpretation ($L(x, g, f)$) for a sample $x$ includes attribute functions $\phi_j$ with local relevance $r_{j, x}$ surpassing $\frac{1}{\tau}$. These definitions don&amp;rsquo;t assess interpretation quality directly.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="section-2.3">2.3 Learning by imposing interpretability properties&lt;/h2>
&lt;p>For learning, the paper defines certain penalties to minimize, where each one aims to enforce a certain desirable property:&lt;/p>
&lt;p>&lt;em>&lt;strong>Fidelity to output:&lt;/strong>&lt;/em> The output of $g(x)=h(\Psi(f_I(x)))$ should be close to $f(x)$ for any x. This can be imposed through a cross-entropy loss:
$$
L_{of}(f, g, S) = - \sum_{x \in S} h(\Psi(f_I(x)))^T \log(f(x))
$$&lt;/p>
&lt;p>&lt;em>&lt;strong>Conciseness and Diversity of Interpretations:&lt;/strong>&lt;/em> We aim for concise local interpretations, containing only essential attributes per sample, promoting clearer understanding and capturing high-level concepts. Simultaneously, we seek diverse interpretations across samples to prevent attribute functions from being class-exclusive. To achieve this, the paper proposes that we leverage entropy (defined for a vector as $\mathcal{E}(v) = - \sum_i p_i \log(p_i)$), which quantifies uncertainty in real vectors. Conciseness is fostered by minimizing the entropy of the interpreter&amp;rsquo;s output, $\Phi(x) = \Psi(f_I(x))$, while diversity is encouraged by maximizing the entropy of the average $\Psi(f_I(x))$ over a mini-batch. This approach promotes sparse and varied coding of $f_I(x)$, enhancing interpretability. However, as entropy-based losses lack attribute activation constraints, leading to suboptimal optimization, we also minimize the $l_1$ norm of $\Psi(f_I(x))$ with hyperparameter $\eta$. Although $l_1$-regularization commonly encourages sparsity, the experiments done in the paper show that entropy-based methods are more effective.
$$
L_{cd}(f, g, S) = -\mathcal{E}(\frac{1}{\lvert S \lvert} \sum_{x \in S} \Psi(f_I(x))) + \sum_{x \in S} \mathcal{E}(\Psi(f_I(x))) + \sum_{x \in S} \eta \lVert \Psi(f_I(x)) \lVert_1
$$&lt;/p>
&lt;p>&lt;em>&lt;strong>Fidelity to input:&lt;/strong>&lt;/em> In order to promote the representation of intricate patterns associated with the input within $\Phi(x)$, a decoder network $d : \mathbb{R}^J \rightarrow X$ is employed. This network is designed to take the attribute dictionary $\Phi(x)=\Psi(f_I(x))$ as input and reconstruct the original input $x$.
$$
L_{if}(f, g, d, S) = \sum_{x \in S} (d(\Psi(f_I(x))) - x)^2
$$&lt;/p>
&lt;p>Given the proposed loss terms, the loss for the interpretability model writes as follows:
$$
L_{int}(f, g, d, S) = \beta L_{of}(f, g, S) + \gamma L_{if}(f, g, d, S) + \delta L_{cd}(f, g, S)
$$
Where $\beta, \gamma, \delta$ are non-negative hyperparameters. the total loss to be minimized $L = L_{pred} + L_{int}$, where the prediction loss, $L_{pred}$, is the well-know cross entropy loss (since this a classification problem).&lt;/p>
&lt;h1 id="section-3">3 Understanding encoded concepts in FLINT&lt;/h1>
&lt;p>Once the predictor and interpreter networks are jointly learned, interpretation can be conducted at both global and local levels . A critical aspect highlighted by the authors is understanding the concepts encoded by each individual attribute function ​$\phi_j$ . Focusing on image classification, the authors propose representing an encoded concept as a collection of visual patterns in the input space that strongly activate $\phi_j$ . They present a pipeline for generating visualizations for both global and local interpretation, adapting various existing tools .&lt;/p>
&lt;p>For global interpretation visualization, the authors propose starting by selecting a small subset of training samples from a given class c that maximally activate ​$\phi_j$ . This subset, referred to as Maximum Activating Samples (MAS), is denoted as $MAS(c , ​\phi_j , l)$ where $l$ is the subset size (set as 3 in their experiments). However, while MAS provides some insight into the encoded concept, further analysis is required to understand the specific aspects of these samples that cause ​$\phi_j$ activation. To achieve this, the authors propose utilizing a modified version of activation maximization called Activation Maximization with Partial Initialization (AM+PI). This technique aims to synthesize input that maximally activates ​$\phi_j$ by optimizing a common activation maximization objective, initialized with a low-intensity version of the sample from MAS.&lt;/p>
&lt;p>For local analysis, given any test sample $x_{0}$ , its local interpretation $L(x_{0},f,g)$ can be determined, representing the relevant attribute functions . To visualize a relevant attribute ​$\phi_j$, the authors suggest repeating the AM+PI procedure with initialization using a low-intensity version of $x_{0}$ to enhance the concept detected by ​$\phi_j$ in $x_{0}$ .&lt;/p>
&lt;h2 id="section-4">4 Reproducing the experiments&lt;/h2>
&lt;p>In the experimental section of the article, several experiments were conducted to do a quantitative evaluation of FLINT&amp;rsquo;s performance compared to other state-of-the-art models designed for interpretability, such as SENN and PrototypeDNN. Additionally, FLINT was compared to LIME and VIBI to evaluate the fidelity of its interpretations, measuring the proportion of samples where the predictions of a model and its interpreter agree. Across these tests, FLINT consistently outperformed the other models, demonstrating its reliability and effectiveness.&lt;/p>
&lt;p>However, in this blog post we will specifically focus on reproducing the experiments in the article related to FLINT&amp;rsquo;s explainability, that aim to do a qualitative analysis of it. To achieve a thorough understanding of the model and its operational dynamics across prevalent datasets, we replicated the study by cloning the project from the GitHub repository referenced in the article (&lt;a href="https://github.com/jayneelparekh/FLINT">repo link&lt;/a>). Our experimentation involved the CIFAR10 and QuickDRAW datasets, employing a ResNet18-based network for both. For the QuickDRAW dataset, we utilized J=24 attributes, while for the CIFAR10 dataset, we used J=36 attributes.&lt;/p>
&lt;p>The instructions provided in the GitHub repository for executing the model are clear, and the model runs flawlessly. We have the option to either train the model ourselves or download the pre-trained models. Furthermore, there is a well-detailed Python notebook named &amp;ldquo;FLINT demo.ipynb&amp;rdquo;, which contains code for visualizing data, such as attribute relevance scores for each class and local interpretations for data samples. We will execute FLINT on test images and take a look at how interpretability is done with FLINT in this section.&lt;/p>
&lt;h3 id="section-4.1">4.1 Global interpretation&lt;/h3>
&lt;p>In the article, the authors explore global interpretation using a figure similar to the one provided below which was reproduced from the notebook, and which illustrates the generated global relevances $r_{j,c}$ for all class-attribute pairs in the QuickDraw dataset.&lt;/p>
&lt;!-- ![Global class-attribute relevances](/images/FLINT/Global_class_attribute_QuickDRAW.png) -->
&lt;div style="text-align:center;">
&lt;img src="./images/FLINT/Global_class_attribute_QuickDRAW.png" alt="Image" width="300" height="200">
&lt;/div>
&lt;p>Additionally, by running the model on the CIFAR10 and QuickDRAW dataset we got visual outputs representative of class-attribute pair analyses for both datasets. These outputs served as pivotal tools in elucidating interrelations and facilitating comparative assessments between attributes and classes. We present below two figures derived from the resultant class-attribute pair analyses for each of the 2 datasets. The class-attribute pairs shown are different from the examples shown in the paper.&lt;/p>
&lt;p>&lt;img
src="./images/FLINT/Class_attribute_pair_CIFAR10.png"
alt="Class-attribute pair analysis on dataset CIFAR10"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;em>Caption: Class-attribute pair analysis on dataset CIFAR10&lt;/em>&lt;/p>
&lt;p>&lt;img
src="./images/FLINT/Class_attribute_pair_QuickDraw.png"
alt="Class-attribute pair analysis on dataset QuickDraw"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;em>Caption: Class-attribute pair analysis on dataset QuickDraw&lt;/em>&lt;/p>
&lt;p>We focus on class-attribute pairs with high relevance, showcasing examples in the provided figure above . For each pair, we examine Maximum Activating Samples (MAS) alongside their corresponding Activation Maximization with Partial Initialization (AM+PI) outputs.&lt;/p>
&lt;p>MAS analysis alone provides valuable insights into the encoded concept. For instance, on QuickDRAW dataset, attribute $\phi_{16}$ relevant for class &amp;lsquo;Banana&amp;rsquo; activates the curve shape of the banana. However, AM+PI outputs offer deeper insights by elucidating which parts of the input activate an attribute function more clearly. And on CIFAR10 dataset , attribute $\phi_{12}$ activates for &amp;lsquo;Deer&amp;rsquo; class , but the specific focus of the attribute remains ambiguous. The outputs of the AM+PI method indicate that attribute $\phi_{12}$ predominantly highlights the area encompassing the legs and the horns of the deer, characterized as the most prominently enhanced regions.&lt;/p>
&lt;h3 id="section-4.2">4.2 Local interpretation&lt;/h3>
&lt;p>Similarly to the article, we explored local interpretation through the figure provided below which was generated in the notebook, which showcases visualizations for 4 test samples of the QuickDRAW dataset. Both predictor $f$ and interpreter $g$ accurately predict the true class in all cases, for the first 2 it&amp;rsquo;s &amp;ldquo;Cat&amp;rdquo; and the last 2 it&amp;rsquo;s &amp;ldquo;Banana&amp;rdquo;. For each case, they highlighted the top 3 relevant attributes to the prediction along with their relevances and corresponding AM+PI outputs.&lt;/p>
&lt;p>&lt;img
src="./images/FLINT/Local_interpretations.jpg"
alt="Local interpretations for test samples"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>Analysis of the AM+PI outputs reveals that attribute functions generally activate for patterns corresponding to the same concept inferred during global analysis. This consistency is evident for attribute functions present in the previous figures. Additionaly, by looking at the figure showing the relevance of class-attribute pairs in section 4.1 for the QuickDRAW dataset we observe that the 3 most important features for each class in the local interpretations are also those having the highest relevence for these classes. For example for the &amp;ldquo;Banana&amp;rdquo; class, $\phi_{16}$, which activates the curve shape, is by far the most important feature for identifying this class by looking at both the local interpretations and the class-attribute relevences. While for the &amp;ldquo;Cat&amp;rdquo; class, it seems that the most important features are in order $\phi_{23}$, $\phi_1$ and $\phi_{19}$ when looking at both the local interpretations and the class-attribute relevences.&lt;/p>
&lt;h2 id="section-5">5 Subjective evaluation&lt;/h2>
&lt;p>In the article, a subjective evaluation survey with 20 respondents using the QuickDraw dataset to assess FLINT&amp;rsquo;s interpretability is conducted. The authors selected 10 attributes covering 17 class-attribute pairs and presented visualizations (3 MAS and AM+PI outputs) along with textual descriptions for each attribute to the respondents. They were asked to indicate their level of agreement with the association between the descriptions and the patterns in the visualizations using predefined choices.&lt;/p>
&lt;p>Descriptions were manually generated, including 40% incorrect ones to ensure informed responses. Results showed that for correct descriptions, 77.5% of respondents agreed, 10.0% were unsure, and 12.5% disagreed. For incorrect descriptions, 83.7% disagreed, 7.5% were unsure, and 8.8% agreed. These results affirm that the concepts encoded in FLINT&amp;rsquo;s learned attributes are understandable to humans.&lt;/p>
&lt;h1 id="section-6">6 Specialization of FLINT to post-hoc interpretability&lt;/h1>
&lt;p>FLINT primarily aims for interpretability by design, but the authors of the article propose that it can also be adapted to provide post-hoc interpretations when a classifier $\hat{f}$ is already available. Post-hoc interpretation learning, a special case of SLI, involves building an interpreter for $\hat{f}$ by minimizing a certain objective function. Specifically, Given a classifier
$\hat{f} \in F$ and a training set $S$, the goal is to build an interpreter of $\hat{f}$ by solving:
$$
\text{arg} \min_{g \in G_{f}} L_{int}(\hat{f}, g, S)
$$
Where $g(x)=h(\Phi(\hat{f_I} (x)))$ for a given set of $I$ hidden layers and an attribute dictionnary of size $J$. The learning is performed the same as before but we only keep the parameters $\theta_\Psi$, $\theta_h$ and $\theta_d$. We fix $\theta_\hat{f}$ and remove $L_{pred}$ from the training loss $L$.&lt;/p>
&lt;p>There are experimental results in the article and in the supplements that are not mentionned here that demonstrate the effectiveness of post-hoc interpretation within FLINT, showing that even without fine-tuning the internal layers of the classifier, meaningful interpretations can be generated with high fidelity.&lt;/p>
&lt;h1 id="section-7">7 Conclusion&lt;/h1>
&lt;p>In conclusion, FLINT offers a robust framework for enhancing the interpretability of machine learning models, particularly deep neural networks, in critical domains like healthcare, law, and defense. By jointly learning predictor and interpreter models, FLINT addresses the challenge of providing both global and local interpretations of model predictions. Through carefully designed loss functions, FLINT ensures fidelity to input and output, promotes concise and diverse interpretations, and facilitates the representation of intricate patterns associated with input data. Reproducing experiments on datasets such as CIFAR10 and QuickDRAW showcases FLINT&amp;rsquo;s effectiveness in providing interpretable insights into model predictions. Subjective evaluations affirm the understandability of FLINT&amp;rsquo;s learned attributes, reinforcing its potential for real-world applications. Moreover, FLINT&amp;rsquo;s adaptability for post-hoc interpretability underscores its versatility, enabling meaningful interpretations without extensive modification of the underlying classifier. Overall, FLINT emerges as a valuable tool for fostering transparency and trust in complex machine learning models, contributing to the development of interpretable AI systems across various domains.&lt;/p></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/a-framework-to-learn-with-interpretation/</guid><pubDate>Tue, 13 Feb 2024 16:56:04 +0100</pubDate></item><item><title>NTK-SAP: IMPROVING NEURAL NETWORK PRUNING BY ALIGNING TRAINING DYNAMICS</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/ntk-sap/</link><description>&lt;p>This is a blog post about the paper NTK-SAP: Improving neural network pruning by aligning training dynamics, published by Y. Wang et al. in 2023 and available &lt;a href="https://openreview.net/pdf?id=-5EWhW_4qWP">here&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Introduction:&lt;/strong>&lt;/p>
&lt;p>In a world increasingly driven by demand for data and computational resources, the narrative of artificial intelligence has been one of abundance: more data, more power, more precision. Yet, nestled within this grand tale, lies a quieter narrative - one that champions the concept of achieving more with less—Frugal AI.&lt;/p>
&lt;p>Imagine a craftsman from a bygone era, working in a workshop filled with natural light. Instead of an overwhelming array of tools, he possesses only a few, each worn and refined by years of careful use. With these simple instruments, he creates works of unexpected beauty, demonstrating that the value lies not in the abundance of resources, but in the skill and wisdom with which they are used.
Frugal AI embodies this craftsman’s spirit in the digital age. It does not revel in the excesses of computational power or data. Instead, it thrives in constraint, finding clever pathways through the limitations, optimizing algorithms not just for performance, but for efficiency and accessibility.&lt;/p>
&lt;p>In the quest for efficiency, neural network pruning has emerged as a foundation of Frugal AI principles. Just as craftsmen meticulously select and refine their tools, neural network pruning systematically removes redundant, non-critical components from a network, optimizing its performance without compromising its functionality.&lt;/p>
&lt;p>&lt;strong>Neural network pruning&lt;/strong>&lt;/p>
&lt;p>Neural network pruning stems from the recognition that many models, especially deep learning networks, are often over-parameterized. This means they contain more parameters than are necessary for effective learning or inference. In the context of Frugal AI, this over-parameterization is analogous to an artist&amp;rsquo;s studio cluttered with unused tools and materials, which, rather than aiding, only serve to overwhelm and complicate. The act of pruning, therefore, can be seen as an effort to streamline and refine. It&amp;rsquo;s about identifying and removing the &amp;rsquo;excess&amp;rsquo; in the network—those weights and connections that contribute little to the output. This not only reduces the computational load, making the network faster and more energy-efficient, but also often improves its generalization ability, making the model less prone to overfitting and more adaptable to different tasks or datasets.&lt;/p>
&lt;p>&lt;strong>Pruning Methods:&lt;/strong>&lt;/p>
&lt;p>Pruning methodologies come in various forms, each tailored to specific needs and objectives. These methodologies can be categorized into three main types: &lt;strong>post-hoc pruning&lt;/strong>, &lt;strong>pruning during training&lt;/strong>, and &lt;strong>foresight pruning&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>Post-hoc Pruning:&lt;/strong> This technique trims neural networks after training, typically requiring multiple train-prune-retrain cycles. It utilizes various metrics, like magnitude and Hessian values, to determine which weights to eliminate, primarily aiming to reduce inference time.&lt;/p>
&lt;p>&lt;strong>Pruning During Training:&lt;/strong> This approach involves gradually removing connections within a neural network as it trains, employing regularization or trainable masks. It aims to save training time but doesn&amp;rsquo;t necessarily reduce memory costs.&lt;/p>
&lt;p>&lt;strong>Foresight Pruning:&lt;/strong> This strategy prunes networks before training begins to prevent unnecessary computational waste. It seeks to address issues like layer collapse collapse at high sparsity levels. Recent advancements aim to overcome the limitations of early pruning methods by incorporating more informed strategies, such as meta-gradients.&lt;/p>
&lt;p>&lt;strong>Foresight pruning methods - saliency score:&lt;/strong>&lt;/p>
&lt;p>Foresight pruning methods optimize neural network structures by identifying and removing less important connections, reducing computational complexity while maintaining performance. At the heart of these methods lies the loss function, which serves as the guiding metric for evaluating the network&amp;rsquo;s performance on a given dataset and determining which connections to prune. Given the complexity of directly solving the loss function, an indirect method is employed. Each potential connection within the network is assigned a &amp;ldquo;saliency score,&amp;rdquo; reflecting its influence on the loss function. This score is computed by assessing how changes in the connection impact the loss function, scaled by the initial weight value. Essentially, connections with higher saliency scores, indicating greater impact on the loss function, are retained, while those with lower scores are pruned. This systematic approach ensures that the network remains efficient while preserving its effectiveness in solving tasks.&lt;/p>
&lt;p>Key pruning methods such as &lt;strong>SNIP&lt;/strong>, &lt;strong>Iterative SNIP&lt;/strong>, &lt;strong>GraSP&lt;/strong>, and &lt;strong>Synflow&lt;/strong>, introduce specific saliency measures to assess the importance of connections:&lt;/p>
&lt;p>&lt;strong>1. SNIP&lt;/strong> calculates saliency as $S_{\text{SNIP}}(m&amp;rsquo;) = \left|\frac{\partial L}{\partial \theta}\odot \theta\right|$, focusing on the impact of each connection on the loss. SNIP&amp;rsquo;s saliency score is the difference in the loss function before and after pruning a connection.&lt;/p>
&lt;p>&lt;strong>2. Iterative SNIP&lt;/strong> repeats the process of SNIP multiple times for a refined pruning.&lt;/p>
&lt;p>&lt;strong>3. GraSP&lt;/strong> employs the Hessian-gradient product to identify connections important for preserving gradient flow, with saliency defined as $S_{\text{GraSP}}(m&amp;rsquo;) = -\left[H(\theta \odot m&amp;rsquo;; D)\frac{\partial L}{\partial \theta}\right] \odot \theta$.&lt;/p>
&lt;p>&lt;strong>4. Synflow&lt;/strong> uses $S_{\text{Synflow}}(m&amp;rsquo;) = \left|\theta\right| \odot \left|\frac{\partial L}{\partial \theta}\right|$ as a data-agnostic measure, emphasizing connections&amp;rsquo; overall contribution to the network&amp;rsquo;s output irrespective of the dataset.&lt;/p>
&lt;p>Each method&amp;rsquo;s saliency score guides the pruning process by ranking the connections based on their calculated importance to only keep the top-ranked connections - the most salient ones. Therefore, the overall idea is to start with a complex network, score each connection by importance, and keep only the most important connections. This results in a simpler network that is cheaper to train and run but still capable of learning effectively from the data.&lt;/p>
&lt;p>&lt;strong>Neural Tangent Kernel (NTK):&lt;/strong>&lt;/p>
&lt;p>In recent studies, there has been significant exploration into optimizing neural networks on a global scale. One notable area of focus involves leveraging the neural tangent kernel (NTK) to gain deeper insights into how gradient descent functions within extensive deep neural networks. The NTK spectrum provides valuable information about convergence patterns. Remarkably, researchers have observed that the NTK remains consistent throughout training in sufficiently large DNNs. This suggests that the NTK spectrum could serve as a comprehensive measure for understanding training dynamics.&lt;/p>
&lt;p>&lt;strong>Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP):&lt;/strong>&lt;/p>
&lt;p>Consequently, a novel pruning approach has emerged: selectively removing connections that exert minimal influence on the NTK spectrum.&lt;/p>
&lt;p>In order to implement this conceptual pruning methods, there are a few considerations:&lt;/p>
&lt;p>&lt;strong>1. Metric Selection:&lt;/strong> Due to the complexity and time required to calculate the full range of eigenvalues (the eigenspectrum) of the Neural Tangent Kernel, the nuclear norm—essentially the sum of these eigenvalues—is used instead as a scalar to summarize the characteristics of the eigenspectrum.&lt;/p>
&lt;p>&lt;strong>2. Choosing the Right NTK Matrix:&lt;/strong>&lt;/p>
&lt;p>We can distinguish between wo types of NTK matrices:&lt;/p>
&lt;ul>
&lt;li>Fixed-Weight NTK: Related to the network&amp;rsquo;s initial setup.&lt;/li>
&lt;li>Analytic NTK: A theoretical model assuming a network of infinite size&lt;/li>
&lt;/ul>
&lt;p>However, since calculating the Analytic NTK is highly resource-intensive, the researchers use a practical workaround. They approximate the Analytic NTK by averaging multiple Fixed-Weight NTKs from various initial setups, balancing computational efficiency with accuracy.&lt;/p>
&lt;p>&lt;strong>3. Computational Efficiency:&lt;/strong> To manage computation costs, there is a technique known as the &amp;ldquo;new-input-new-weight&amp;rdquo; (NINW) method. This approach involves changing the network&amp;rsquo;s weights for each new set of input data. By doing this, they can efficiently evaluate the properties of the Neural Tangent Kernel (NTK) across different scenarios without significantly adding to the computational load.&lt;/p>
&lt;p>Based on these considerations, Wang and colleagues have developed an innovative approach called &lt;strong>Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP)&lt;/strong>.&lt;/p>
&lt;p>NTK-SAP leverages the NTK spectrum for efficient foresight pruning by using multi-sampling to predict pruning outcomes and ensure accuracy. It also incorporates the Novel Iterative Network Weighting (NINW) technique to reduce computation costs. This method streamlines neural networks by preemptively removing less impactful parts, optimizing both the pruning process and the network&amp;rsquo;s performance with minimal resource expenditure.&lt;/p>
&lt;p>NTK-SAP follows the following implementation:&lt;/p>
&lt;p>&lt;img
src="./images/Adrien_Elia/algo.png"
alt="algorithm"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Calculation of NTK-SAP Saliency Score:&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1. Finite Approximation Approach&lt;/strong>&lt;/p>
&lt;p>The NTK-SAP method introduces a finite approximation expression to calculate a saliency score S-NTK-SA, which leverages the pruning dataset to approximate the entire training set. This foresight pruning approach identifies and prunes weights with the lowest saliency scores.&lt;/p>
&lt;p>Saliency score based on a fixed-weight Neural Tangent Kernel:&lt;/p>
&lt;p>$$S_{\text{NTK-SAP}}(m^j) = \left| \frac{\partial}{\partial m_j} \mathbb{E}_{\Delta\theta \sim \mathcal{N}(0, \epsilon I)} \left[ \left| f(\mathbf{X}_D; \theta_0 \odot m) - f(\mathbf{X}_D; (\theta_0 + \Delta\theta) \odot m) \right|_2^2 \right] \right|$$&lt;/p>
&lt;p>&lt;strong>2. Multi-Sampling Approach:&lt;/strong>&lt;/p>
&lt;p>While a single fixed-weight-NTK provides an approximation of the analytic NTK, averaging over multiple fixed-weight-NTKs offers a closer approximation to the expected behavior of the analytic NTK. This method entails sampling several independent weight configurations and averaging their fixed-weight-NTKs to better understand the parameter space and the anticipated performance of pruned networks.&lt;/p>
&lt;p>A stabilized version of the saliency score, S-NTK-SAP(mj) is introduced and incorporates the average of fixed-weight-NTKs computed across multiple random weight configurations, to assess the impact of pruning. Unlike most existing foresight pruning scores, which are dependent on specific weight configurations, this proposed saliency score is weight-agnostic; it primarily reflects the structure of the mask applied for pruning rather than the weights themselves. This distinction highlights the score&amp;rsquo;s focus on the inherent characteristics of the pruning method over the variability of weight initializations.&lt;/p>
&lt;p>&lt;strong>3. New-input-new-weight (NINW) trick:&lt;/strong>&lt;/p>
&lt;p>To reconcile the theoretical aspirations with practical viability, NTK-SAP leverage the &amp;rsquo;new-input-new-weight&amp;rsquo; (NINW) trick. This technique estimates the expected behavior of pruned networks by utilizing a new set of weights for each mini-batch of input data. This approach ensures that the pruning algorithm remains computationally feasible, allowing for the real-world application without prohibitive resource demands.&lt;/p>
&lt;p>&lt;strong>4. Random Input Trick:&lt;/strong>&lt;/p>
&lt;p>NTK-SAP relies on another trick that consists in replacing the pruning set with random inputs. This allows to approximate the network&amp;rsquo;s behavior without depending on real data, thus highlighting NTK-SAP&amp;rsquo;s ability to adapt to any dataset without requiring specific adjustments or optimization.&lt;/p>
&lt;p>$$S_{\text{NTK-SAP}}(m^j) = \left| \frac{\partial}{\partial m_j} \frac{1}{|D|} \sum_{i=1}^{|D|} \left[ \left| f\left(Z_i; \theta_{0,i} \odot m\right) - f\left(Z_i; \left(\theta_{0,i} + \Delta\theta_i\right) \odot m\right) \right|_2^2 \right] \right|$$&lt;/p>
&lt;p>&lt;strong>Experimental validation:&lt;/strong>&lt;/p>
&lt;p>Experiments were performed on CIFAR-10, CIFAR-100, and Tiny-ImageNet data sets to validate NTK-SAP&amp;rsquo;s superiority across various sparsity levels. Particularly noteworthy is its robust performance at extreme sparsity ratios, where traditional methods falter. These results underscore the efficacy of our multi-sampling strategy and the practical utility of the NINW trick.&lt;/p>
&lt;p>&lt;img
src="./images/Adrien_Elia/performance_curves.png"
alt="performance_curves"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>Extending the analysis to the more challenging ImageNet dataset, NTK-SAP consistently outperforms baseline pruning methods, including SNIP and GraSP, especially at high sparsity levels. This success highlights NTK-SAP&amp;rsquo;s scalability and its potential to facilitate efficient neural network training on large-scale datasets.&lt;/p>
&lt;p>&lt;img
src="./images/Adrien_Elia/performance_table.png"
alt="performance_table"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>&lt;strong>Reproductive experiments:&lt;/strong>&lt;/p>
&lt;p>To ensure reproducibility, begin by installing the required packages:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>pip install -r requirements.txt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, to run NTK-SAP with the default dataset and parameters using the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>python main.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The default parameters are as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--dataset&lt;/code>: Mnist&lt;/li>
&lt;li>&lt;code>--model-class&lt;/code>: default&lt;/li>
&lt;li>&lt;code>--model&lt;/code>: fc&lt;/li>
&lt;li>&lt;code>--pruner&lt;/code>: rand&lt;/li>
&lt;li>&lt;code>--prune-batch-size&lt;/code>: 256&lt;/li>
&lt;li>&lt;code>--compression&lt;/code>: 0.0&lt;/li>
&lt;li>&lt;code>--prune-train-mode&lt;/code>: False&lt;/li>
&lt;li>&lt;code>--prune-epochs&lt;/code>: 1&lt;/li>
&lt;li>&lt;code>--ntksap_R&lt;/code>: 1&lt;/li>
&lt;li>&lt;code>--ntk_epsilon&lt;/code>: 0.01&lt;/li>
&lt;/ul>
&lt;p>For experimenting with different parameters, proceed with the desired adjustments.&lt;/p>
&lt;p>&lt;strong>1. Experiment NTK-SAP with Cifar100 dataset, a 0.01 perturbation hyper-parameter&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>python main.py --dataset cifar100 --ntksap_epsilon 0.01
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Train results:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: center">&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">train_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">test_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top1_accuracy&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top5_accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Init.&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.607083&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.00&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pre-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.607083&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.00&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Post-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.607083&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.00&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Final&lt;/td>
&lt;td style="text-align: center">10&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.337817&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.421804&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">17.91&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">45.41&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>2. Experiment NTK-SAP with Cifar100 dataset and a 0.02 perturbation hyper-parameter&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>python main.py --dataset cifar100 --ntksap_epsilon 0.02
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Train results:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: center">&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">train_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">test_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top1_accuracy&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top5_accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Init.&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.607163&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.02&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.72&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pre-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.607163&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.02&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.72&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Post-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.607163&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.02&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.72&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Final&lt;/td>
&lt;td style="text-align: center">10&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.341863&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.460254&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">17.74&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">43.78&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>3. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 3&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>python main.py --dataset cifar100 --prune-epochs &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Train results:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: center">&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">train_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">test_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top1_accuracy&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top5_accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Init.&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.606948&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">0.96&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">5.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pre-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.606948&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">0.96&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">5.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Post-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.606948&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">0.96&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">5.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Final&lt;/td>
&lt;td style="text-align: center">10&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.337061&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.448972&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">18.09&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">43.97&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>4. Experiment NTK-SAP with Cifar100 dataset and a number of iterations of 7&lt;/strong>&lt;/p>
&lt;p>Train results:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: center">&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">train_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">test_loss&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top1_accuracy&lt;/th>
&lt;th style="text-align: center">   &lt;/th>
&lt;th style="text-align: center">top5_accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Init.&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.606786&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.01&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pre-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.606786&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.01&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Post-Prune&lt;/td>
&lt;td style="text-align: center">0&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">NaN&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.606786&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">1.01&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">4.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Final&lt;/td>
&lt;td style="text-align: center">10&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.335409&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">3.397401&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">18.93&lt;/td>
&lt;td style="text-align: center">   &lt;/td>
&lt;td style="text-align: center">44.89&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Analysis from experiments:&lt;/strong>&lt;/p>
&lt;p>&lt;strong>1. Dataset Adaptability:&lt;/strong>&lt;/p>
&lt;p>The study demonstrated NTK-SAP as being data-free. This quality allows pruned networks developed via these methods to be seamlessly adapted to various datasets without requiring additional data, highlighting their versatility and efficiency.&lt;/p>
&lt;p>&lt;strong>2. Robustness across hyper-parameter variations:&lt;/strong>&lt;/p>
&lt;p>The robustness of NTK-SAPP is evident through its consistent performance across varying perturbation hyper-parameters (ϵ) in experiments conducted on the Cifar100 dataset. When the perturbation hyper-parameter is set to 0.01, the model exhibits stable behavior throughout training and pruning phases, yielding a final top-1 accuracy of 17.91% and a top-5 accuracy of 45.41%. Similarly, when the perturbation hyper-parameter is increased to 0.02, the model maintains its stability, with minimal fluctuations observed in performance metrics compared to the unperturbed model. Both pre-prune and post-prune stages demonstrate resilience to perturbations, showcasing nearly identical results to the unperturbed model. This consistency across different perturbation levels underscores the robustness of NTK-SAPP, making it a reliable choice for tasks where stability under varying conditions is crucial.&lt;/p>
&lt;p>&lt;strong>3. Fewer iterations for small datasets:&lt;/strong>&lt;/p>
&lt;p>An exploration into how the number of iterations (T) affects performance across datasets reveals that for smaller datasets, reducing T slightly impacts outcomes, suggesting that computational efficiency can be achieved without significantly compromising results.&lt;/p>
&lt;p>&lt;strong>Conclusion:&lt;/strong>&lt;/p>
&lt;p>In conclusion, NTK-SAP stands as a pivotal advancement in the realm of neural network pruning, showcasing its efficacy across diverse datasets and network architectures. By pruning at initialization, it eliminates the necessity for post-training methods and mask training. Moreover, by leveraging NTK theory, it addresses the oversight of training dynamics post-pruning, enabling iterative pruning without data dependency. NTK-SAP effectively bridges the theoretical underpinnings of optimization with practical neural network training, thus pushing the boundaries of frugal neural networks.&lt;/p>
&lt;p>While NTK-SAP represents a significant leap forward, it also unveils several avenues for future exploration. Subsequent research could delve into alternative spectral measures or extend the methodology to other forms of network optimization.&lt;/p>
&lt;p>In essence, NTK-SAP not only signifies a crucial stride towards more efficient and theoretically grounded neural network pruning but also sets the stage for future innovations in enhancing network frugality.
&lt;br>&lt;br>&lt;br>
By Elia Lejzerowicz and Adrien Oleksiak.&lt;/p>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/ntk-sap/</guid><pubDate>Wed, 07 Feb 2024 16:07:10 +0100</pubDate></item><item><title>Do Perceptually Aligned Gradients imply Robustness?</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/robustness-and-pag-the-converse/</link><description>&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h1 style="font-size: 36px;">Robustness and Perceptually Aligned Gradients : does the converse stand ?&lt;/h1>
&lt;h3 style="font-size: 24px;">Author: Yohann Zerbib&lt;/h3>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">Adversarial Attacks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Perceptually Aligned Gradients&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">Experiment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">To go further&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a blog post about the paper Do Perceptually Aligned Gradients Imply Robustness?, published by R. Ganz et al. in 2023 and available &lt;a href="https://openreview.net/pdf?id=W6topEXC2-v">here&lt;/a>.&lt;/p>
&lt;h2 id="section-0">Introduction&lt;/h2>
&lt;p>In the context of image recognition in Machine Learning, one could quickly realize that building &lt;em>robust&lt;/em> models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against &lt;em>&lt;strong>adversarials attacks&lt;/strong>&lt;/em>, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily &lt;strong>interpretable by humans&lt;/strong>, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?&lt;/p>
&lt;h2 id="section-1">Adversarial attacks&lt;/h2>
&lt;p>But before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.&lt;/p>
&lt;p>Adversarial attacks refer to a class of techniques in machine learning where &lt;strong>intentionally crafted input data&lt;/strong> is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model&amp;rsquo;s decision-making process, taking advantage of the model&amp;rsquo;s sensitivity to small changes in input data that might be &lt;strong>imperceptible to humans&lt;/strong>.
They are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.&lt;/p>
&lt;p>Concretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a &lt;em>&lt;strong>stop sign as speed limit sign&lt;/strong>&lt;/em>.&lt;/p>
&lt;p>&lt;img
src="./images/Yohann_Zerbib/stop.png"
alt="stop"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>(Eykholt et al. [1])&lt;/p>
&lt;p>Now, let&amp;rsquo;s dive a bit deeper to understand how these errors happen.
Several points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the &lt;strong>loss is steep&lt;/strong>. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a &lt;strong>small change&lt;/strong> of the input can cause &lt;strong>abrupt shifts&lt;/strong> in the decision process of our model. This effect increases with the dimensionnality (quality of pictures&amp;hellip;) and therefore will still be relevant with time.&lt;/p>
&lt;p>The basic modelisation of an attack would be the following. Let&amp;rsquo;s consider :&lt;/p>
&lt;ul>
&lt;li>a model $f\ :\ \mathcal{X} \ \rightarrow \ \mathcal{Y}$&lt;/li>
&lt;li>the input to pertub : $x \in \mathcal{X}$&lt;/li>
&lt;li>a potential target label : $t \in \mathcal{Y}$&lt;/li>
&lt;li>a small perturbation : $\eta$&lt;/li>
&lt;/ul>
&lt;p>Then, mathematically, the attacker would try to have something that verifies $f(x + \eta) = t$ (or any other label than $f(x)$ for an untargeted attack).&lt;/p>
&lt;p>Now, as one can imagine, it is possible to compute attacking models related to this framework. Let&amp;rsquo;s understand two well-knowns algorithms that follow this goal.&lt;/p>
&lt;h3 id="fast-gradient-sign-method-fgsm-">Fast Gradient Sign Method (FGSM) :&lt;/h3>
&lt;p>This method can be &lt;em>&lt;strong>targeted&lt;/strong>&lt;/em> or &lt;em>&lt;strong>untargeted&lt;/strong>&lt;/em>. Let&amp;rsquo;s study the targeted one. The algorithm is the following [3]:
One compute the perturbation $\eta \ =\ \epsilon \ \cdotp \ sign( \ \nabla x\ L( x,\ t) \ )$ where $\epsilon$ is the perturbation size. Then, one would have $x&amp;rsquo;\ =\ x\ −\ \eta $ such that we remain espilon close from $x$ and that $f(x&amp;rsquo;) = t$.
The perturbation has to remain small to ensure it will be undetected by human&amp;rsquo;s perception.&lt;/p>
&lt;p>But, at this point, one question arises : how can we be sure that $x&amp;rsquo;$ is still close to $x$? How can we be sure that we have $||x\ −\ x&amp;rsquo;||_{p} \ \leq \ \epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>$L_{2 }$ norm : This norm captures the &lt;strong>global quantity of changes&lt;/strong>. It is the euclidean distance.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$L_{\infty }$ : This norm captures the &lt;strong>maximum change&lt;/strong> in the vector.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>So, we have several ways to have a level of control over the changed features.&lt;/p>
&lt;p>Now that the first intuition for attack is understood, one should take a rapid look at &lt;strong>PGD&lt;/strong> (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.&lt;/p>
&lt;p>The algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we&amp;rsquo;re trying to reach a specific target, we actually want to move in the &lt;strong>opposite direction&lt;/strong>, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don&amp;rsquo;t overshoot or undershoot our target.
After taking a step, the perturbation is &lt;em>projected&lt;/em> back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range.
This process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).&lt;/p>
&lt;p>However, our role here is not to learn how to create the best attacks, but more to learn how to &lt;em>&lt;strong>defend&lt;/strong>&lt;/em> them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks.
Then, it all comes down to this optimization problem :&lt;/p>
&lt;p>$\min_{\theta }$ $\mathbb{E}_{(x, y)} $ [A] where&lt;/p>
&lt;p>A = $(\max_{\eta \leqslant \epsilon }$ $L( f_{\theta}( x\ +\ \eta ) ,\ y))$&lt;/p>
&lt;p>This is more or less an optimization problem to solve with $\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the &lt;strong>best trade-off&lt;/strong> on a model.&lt;/p>
&lt;h2 id="section-2">Perceptually Aligned gradients&lt;/h2>
&lt;p>Finally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have &lt;em>perceptually aligned gradients&lt;/em>.
Here, one should understand &amp;ldquo;gradient&amp;rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are &lt;strong>perceptually relevant&lt;/strong>. In other words, the gradients &lt;em>make sense&lt;/em> from a human perspective.&lt;/p>
&lt;p>&lt;img
src="./images/Yohann_Zerbib/pagdemo.png"
alt="demopag"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>Here an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a &lt;em>ghost&lt;/em> information.&lt;/p>
&lt;p>Now, is it a Bidirectional Connection ? Let&amp;rsquo;s try to have some hints about it.&lt;/p>
&lt;p>The first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.&lt;/p>
&lt;p>Then, it is shown that models with aligneds gradients can be considered as robust.&lt;/p>
&lt;p>Finally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.&lt;/p>
&lt;p>&lt;strong>1. Algorithm of the Model&lt;/strong>&lt;/p>
&lt;p>To disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>the classical cross-entropy loss from the usual categorization problem framework,&lt;/p>
&lt;/li>
&lt;li>
&lt;p>an auxiliary loss on the input-gradients, differentiable.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Then, our global loss function would look like this :&lt;/p>
&lt;p>$L( x,\ y) \ =LCE\ ( f_{\theta }( x) ,\ y) \ + \lambda\sum_{y_{t} =1}^{C}L_{cos}( \nabla_{x}f_{\theta }(x)_{y_t},\ g( x,\ y_t))$&lt;/p>
&lt;p>It is similar to training with a regularization part ($\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).&lt;/p>
&lt;p>This does not use robust model of any sort, on the hypothesis that we have &lt;strong>ground-true PAG&lt;/strong> in the input. This is a &lt;strong>strong hypothesis&lt;/strong>, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.&lt;/p>
&lt;p>After minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.&lt;/p>
&lt;p>&lt;strong>2. Creation of Perceptually Aligned Gradients&lt;/strong>&lt;/p>
&lt;p>As we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are &lt;strong>approximated&lt;/strong>. Firstly, let&amp;rsquo;s consider the heuristics to understand what happens.&lt;/p>
&lt;p>With this objective in mind, we follow a straightforward assumption: the gradient $g( x,\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\ y_t) = r_{y_t} - x$&lt;/p>
&lt;p>&lt;img
src="./images/Yohann_Zerbib/target.png"
alt="target"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>To implement this heuristic, three setups are provided.&lt;/p>
&lt;p>$\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.&lt;/p>
&lt;p>$\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.&lt;/p>
&lt;p>$\textbf{Nearest Neighbor (NN):}$ For each image $x$ and each target class$\ y_{t} \ \in \ {{1,\ 2\ .\ .\ .\ ,\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image&amp;rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define
$r( x,\ y_{t}) \ \ =\ \underset{ \begin{array}{l}
\widehat{x\ } \in \ D_{y_{t}} \ s.t.\ \hat{x} =x
\end{array}}{\arg\min} ||x\ −\ \hat{x} ||_{2}{}$&lt;/p>
&lt;p>where $ D_{y_{t}}$
is the set of sample images with class $y_t$.&lt;/p>
&lt;p>Now, the more theoretical approach is provided thanks to score-based gradients. Authors have used &lt;strong>Denoising Diffusion Probabilistic Models&lt;/strong> (DDPMs), to generate approximations of PAG.&lt;/p>
&lt;p>Let&amp;rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution&lt;/p>
&lt;p>$(p_t({x_{t})})_{t=1}^{T}$.&lt;/p>
&lt;p>An iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\nabla_{x_t} \log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\nabla_{x_t} \log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes&amp;rsquo; formula.&lt;/p>
&lt;p>\begin{equation}
\nabla_{x_t} \log p(x_t|y) = \nabla_{x_t} \log p(y|x_t) + \nabla_{x_t} \log p(x_t),
\end{equation}&lt;/p>
&lt;p>which results in&lt;/p>
&lt;p>\begin{equation}
\nabla_{x_t} \log p(y|x_t) = \nabla_{x_t} \log p(x_t|y) - \nabla_{x_t} \log p(x_t).
\end{equation}&lt;/p>
&lt;p>This formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\log p(y|x) \approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of &amp;ldquo;ground-truth&amp;rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).&lt;/p>
&lt;p>To understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That&amp;rsquo;s how you obtain your set of ground-truth gradients related to the input images.&lt;/p>
&lt;p>At this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.&lt;/p>
&lt;h2 id="section-3">Experiment&lt;/h2>
&lt;p>Now, let&amp;rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains &lt;strong>three mods&lt;/strong> (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.&lt;/p>
&lt;p>The code is available at this &lt;a href="https://github.com/YohannZe/responsible-ai-datascience-ipParis.github.io.git">link&lt;/a>.&lt;/p>
&lt;p>To this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.&lt;/p>
&lt;p>As expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?&lt;/p>
&lt;p>Let&amp;rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600.
How can this be explained ? One should observe the decision boundaries.&lt;/p>
&lt;p>&lt;img
src="./images/Yohann_Zerbib/vanilla_l2_toy.png"
alt="vanillal2"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>This is what is obtained for the regular neural network with cross-entropy Loss.&lt;/p>
&lt;p>&lt;img
src="./images/Yohann_Zerbib/nn_l2_toy.png"
alt="nnl2"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>Here is the result obtained for the particular neural network with the new loss.&lt;/p>
&lt;p>What one should notice is the decision boundaries. The vanilla neural network provides manifolds that really &lt;em>&lt;strong>stick&lt;/strong>&lt;/em> to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).&lt;/p>
&lt;p>However, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a &lt;strong>much greater margin&lt;/strong> of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.&lt;/p>
&lt;p>Another possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.&lt;/p>
&lt;h2 id="section-4">To go further&lt;/h2>
&lt;p>What&amp;rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.&lt;/p>
&lt;p>PAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).&lt;/p>
&lt;p>But, the question is not yet answered : &lt;em>&lt;strong>Do Perceptually Aligned Gradients imply Robustness?&lt;/strong>&lt;/em>&lt;/p>
&lt;p>And that&amp;rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.&lt;/p>
&lt;p>&lt;img
src="./images/Yohann_Zerbib/regu.png"
alt="regu"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>As one can see, the robustness increases with the increase of the regularization hyperparameter. The more the &lt;em>ghost&lt;/em> features of the target class are visible (even if it not always comprehensible), the more the model is robust.&lt;/p>
&lt;p>So, it seems that yes, models with &lt;strong>PAG would be more robust&lt;/strong>.&lt;/p>
&lt;h2 id="section-5">Conclusion&lt;/h2>
&lt;p>To draw a conclusion, this paper has empirically shown that &lt;strong>PAG lead to more robustness&lt;/strong> in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it &lt;strong>ouperforms Adversarially Training&lt;/strong> and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.&lt;/p>
&lt;h2 id="section-6">References&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>EYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ganz, R., Kawar, B., &amp;amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Goodfellow, I. J., Shlens, J., &amp;amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp;amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.&lt;/p>
&lt;/li>
&lt;/ol></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/robustness-and-pag-the-converse/</guid><pubDate>Wed, 07 Feb 2024 16:06:43 +0100</pubDate></item><item><title>To update or not to update? Neurons at equilibrium in deep models</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/neq/</link><description>&lt;h1 style="font-size: 36px;">To update or not to update? Neurons at equilibrium in deep models
&lt;/h1>
&lt;h1 style="font-size: 24px;">Author: Alexis WINTER Augustin CREUSILLET&lt;/h1>
&lt;h1 id="table-of-content">Table of content&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">NEq&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Experiments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">Reproducibility&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a blog post about the paper To update or not to update? Neurons at equilibrium in deep models, published by A. Bgragagnolo et al. in 2022 and available [here]https://proceedings.neurips.cc/paper_files/paper/2022/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf).&lt;/p>
&lt;h2 id="section-0">Introduction&lt;/h2>
&lt;h3 id="background">Background&lt;/h3>
&lt;p>Recent advances in &lt;strong>deep learning&lt;/strong> have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for &lt;strong>computational power&lt;/strong> has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both &lt;strong>affordability and environmental sustainability&lt;/strong>, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.&lt;/p>
&lt;p>This paper tries to focus on the overall behavior of neurons, leveraging the notion of &lt;strong>neuronal equilibrium (NEq)&lt;/strong>. When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.&lt;/p>
&lt;h3 id="related-works">Related works&lt;/h3>
&lt;h4 id="pruning-strategies">Pruning strategies&lt;/h4>
&lt;p>Pruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to &lt;strong>improve efficiency and reduce computational complexity&lt;/strong>. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.&lt;/p>
&lt;p>Despite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically &lt;strong>do not alleviate the computational complexity associated with training neural networks&lt;/strong>. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.&lt;/p>
&lt;h4 id="lottery-ticket-hypothesis">Lottery ticket hypothesis&lt;/h4>
&lt;p>The &lt;strong>lottery ticket hypothesis&lt;/strong> is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or &amp;ldquo;winning tickets,&amp;rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even &lt;strong>surpass the performance of the original dense network&lt;/strong>.&lt;/p>
&lt;p>The hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.&lt;/p>
&lt;p>The significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these &lt;strong>winning tickets&lt;/strong> and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.&lt;/p>
&lt;h2 id="section-1">NEq&lt;/h2>
&lt;h3 id="neuronal-equilibrium">Neuronal equilibrium&lt;/h3>
&lt;p>The concept of &lt;strong>neuronal equilibrium&lt;/strong> aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.&lt;/p>
&lt;p>To assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\Xi_{val}$ as follows:&lt;/p>
&lt;img src="./images/images_Winter_Creusillet/neq_formula.png" width="300"/>
&lt;p>The neuron $i$-th reaches the equilibrium when $(\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium we need to detect when :&lt;/p>
&lt;p>$$\lim_{t\rightarrow \infty} \phi_{i}^t = k,$$&lt;/p>
&lt;p>Since it is not trivial to assess this statment we prefer to work with variations of $(\phi_{i})_t$ that can be defined as :&lt;/p>
&lt;p>\begin{equation}
v_{\Delta \phi_i}^t = \Delta \phi_i^t - \mu_{eq} v_{\Delta \phi_i}^{t-1},
\end{equation}&lt;/p>
&lt;p>With $\mu_{eq}$ the momentum coefficient.&lt;/p>
&lt;p>This only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\Delta \phi_i^t \rightarrow 0$$&lt;/p>
&lt;p>Since we want to track the evolution of $\Delta \phi_i^t$ over time we introduce the &lt;strong>velocity of the variations&lt;/strong>:
$$
v_{\Delta \phi_i}^t = \Delta \phi_i^t - \mu_{eq} v_{\Delta \phi_i}^{t-1},
$$&lt;/p>
&lt;p>With $\mu_{eq}$ the momentum coefficient.&lt;/p>
&lt;p>Rewrited:&lt;/p>
&lt;p>&lt;img
src="./images/images_Winter_Creusillet/momentum_coef.png"
alt="creusilet/winter"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>We need to have $$\mu_{eq} \in [0; 0.5]$$ to prevent the velocity from exploding.&lt;/p>
&lt;p>Finally we can set the condition for the neuron to be at the equilibrium as:
\begin{equation}
\left| v_{\Delta \phi}^t \right | &amp;lt; \varepsilon,~~~~~\varepsilon \geq 0.
\end{equation}&lt;/p>
&lt;p>It is important to know that this relation might not hold for all $t$ since there could be an instant $t&amp;rsquo; &amp;lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.&lt;/p>
&lt;h3 id="training-scheme">Training scheme&lt;/h3>
&lt;p>The training scheme can be presented according to this scheme:&lt;/p>
&lt;p>&lt;img
src="./images/images_Winter_Creusillet/prunedbackprop-scheme_full-1.png"
alt="creusilet/winter"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>At the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:&lt;/p>
&lt;ul>
&lt;li>An epoch of training is made for all trainable neurons on the training set.&lt;/li>
&lt;li>The training either stops due to the end of training criterion being met or continues to the next step.&lt;/li>
&lt;li>The velocity of the similarities is evaluated for every neuron.&lt;/li>
&lt;li>The set of trainable neurons is determined for the next step according to the equilibrium criterion.&lt;/li>
&lt;/ul>
&lt;p>Comparing with regular training, we can see two more hyper-parameters:&lt;/p>
&lt;ul>
&lt;li>$\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities.&lt;/li>
&lt;li>$\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities.&lt;/li>
&lt;/ul>
&lt;h2 id="section-2">Experiments&lt;/h2>
&lt;h3 id="sgd-vs-adam">SGD vs Adam&lt;/h3>
&lt;p>&lt;img
src="./images/images_Winter_Creusillet/sgd_vs_adam.png"
alt="adam/sgd"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>The authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.&lt;/p>
&lt;p>In the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.&lt;/p>
&lt;p>As training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that &lt;strong>Adam brings the network towards this equilibrium faster than SGD&lt;/strong>, but also note that in this specific task, &lt;strong>SGD achieves a slightly higher final accuracy than Adam&lt;/strong>. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD&amp;rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.&lt;/p>
&lt;p>The experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.&lt;/p>
&lt;h3 id="distribution-of-phi--choice-of-µ_eq">Distribution of $\phi$ &amp;amp; choice of $µ_{eq}$&lt;/h3>
&lt;p>&lt;img
src="./images/images_Winter_Creusillet/mu-line-1.png"
alt="creusilet/winter"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>The paper also discusses the distribution of $\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.&lt;/p>
&lt;p>The parameter $\phi$ measures the &lt;strong>cosine similarity between the outputs of a particular neuron at two consecutive training epochs&lt;/strong>, over the validation set. It is used to determine if a neuron&amp;rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\phi$ equals 1, it indicates that the neuron&amp;rsquo;s output is stable across the epochs, signifying it has reached equilibrium.&lt;/p>
&lt;p>The paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\phi$ values across epochs, and $v_{∆\phi}$, which measures the velocity of this change considering a &lt;strong>momentum coefficient $µ_{eq}$&lt;/strong>. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.&lt;/p>
&lt;p>By examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper&amp;rsquo;s Figure 5, which shows the distribution of $\phi$, $∆\phi$, and $v_{∆\phi}$ for a ResNet-32 model trained on CIFAR-10.&lt;/p>
&lt;p>In summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally &amp;ldquo;unfreeze&amp;rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.&lt;/p>
&lt;h3 id="impact-of-the-validation-set-size-and-ε">Impact of the validation set size and ε&lt;/h3>
&lt;p>The authors found that the size of the validation set &lt;strong>does not significantly impact the performance of the model&lt;/strong>. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.&lt;/p>
&lt;p>When examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, &lt;strong>striking a balance between model performance and computational efficiency&lt;/strong>.&lt;/p>
&lt;h2 id="section-3">Results&lt;/h2>
&lt;h2 id="section-4">Reproducibility&lt;/h2>
&lt;p>Using the author&amp;rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from &lt;strong>limited computational resources&lt;/strong>. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.&lt;/p>
&lt;h3 id="experiment">Experiment&lt;/h3>
&lt;p>This experiment aims to replicate the section 4.1.1 &amp;ldquo;SGD vs Adam&amp;rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>python3 train_classification.py --amp&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --arch&lt;span style="color:#f92672">=&lt;/span>resnet32-cifar --batch-size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">100&lt;/span> --dataset&lt;span style="color:#f92672">=&lt;/span>cifar10 --device&lt;span style="color:#f92672">=&lt;/span>cuda --epochs&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">250&lt;/span> --eps&lt;span style="color:#f92672">=&lt;/span>0.001 --lr&lt;span style="color:#f92672">=&lt;/span>0.1 --momentum&lt;span style="color:#f92672">=&lt;/span>0.9 --optim&lt;span style="color:#f92672">=&lt;/span>sgd --val-size&lt;span style="color:#f92672">=&lt;/span>0.01 --velocity-mu&lt;span style="color:#f92672">=&lt;/span>0.5 --weight-decay&lt;span style="color:#f92672">=&lt;/span>0.0005
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named &lt;strong>Weights &amp;amp; Biases (wandb)&lt;/strong> to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.&lt;/p>
&lt;p>&lt;img
src="./images/images_Winter_Creusillet/frozen_sgd_vs_adam1.png"
alt="creusilet/winter"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;img
src="./images/images_Winter_Creusillet/accuracy_sgd_vs_adam1.png"
alt="creusilet/winter"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>As expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.&lt;/p>
&lt;h2 id="section-5">Conclusion&lt;/h2>
&lt;p>From the initial problem of &lt;strong>computational resources saving&lt;/strong>, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, &lt;strong>NEq produces a new knowledge&lt;/strong> that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and &lt;strong>might lead to new training strategies&lt;/strong>.&lt;/p>
&lt;p>One possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.&lt;/p>
&lt;h2 id="section-6">References&lt;/h2>
&lt;ol>
&lt;li>Bragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022.&lt;/li>
&lt;li>Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017.&lt;/li>
&lt;li>J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019.&lt;/li>
&lt;/ol>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/neq/</guid><pubDate>Wed, 07 Feb 2024 15:55:14 +0100</pubDate></item><item><title>Optimal Transport Based Adversarial Patch Attacks</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/optimal_transport_based_adversarial_patch/</link><description>&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\\[','\\]']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;h1 id="authors">Authors:&lt;/h1>
&lt;ul>
&lt;li>Mohammed Jawhar&lt;/li>
&lt;li>Aymane Rahmoune&lt;/li>
&lt;/ul>
&lt;h3 id="paper--optimal-transport-based-adversarial-based-patch-to-leverage-large-scale-attack-transferability">Paper : &lt;a href="https://openreview.net/forum?id=nZP10evtkV">Optimal Transport Based Adversarial Based Patch To Leverage Large Scale Attack Transferability&lt;/a>&lt;/h3>
&lt;h1 id="table-of-contents-">Table of contents :&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">Understanding Adversarial Patch Attacks&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#subsection-11">Decision boundary based&lt;/a>&lt;/li>
&lt;li>&lt;a href="#subsection-12">Feature point based&lt;/a>&lt;/li>
&lt;li>&lt;a href="#subsection-13">Distribution based&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-2">Transferability&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">Optimal Transport&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">Experiments&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#subsection-41">Experimental Setup&lt;/a>&lt;/li>
&lt;li>&lt;a href="#subsection-42">Results and Findings&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#subsection-421">Digital Experiments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#subsection-422">Hybrid Experiments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#subsection-423">Physical Experiments&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#section-5">Reproducibility&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="section-0">Introduction&lt;/h2>
&lt;p>Imagine you&amp;rsquo;re showing a picture to a friend, asking them to guess who&amp;rsquo;s in it, then sticking a tiny, almost invisible sticker on that photo. For some reason, this sticker makes your friend completely unable to recognize who&amp;rsquo;s in the picture. This might sound like magic, but something similar can happen with Computer Vision models designed to capture an image content, either through a classification, a segmentation or even a generation task. These AI programs can be vulnerable to such tricks, that we call technically, Adversarial Patch Attacks.&lt;/p>
&lt;p>As AI becomes increasingly integrated into various aspects of our lives, including critical applications like passport security systems, autonomous vehicles, traffic sign detection, and surgical assistance; the reliability, trustworthiness, and performance of these systems under all conditions became of prime importance. This has led to a growing interest in the area of Robust AI, which focuses on enhancing the safety and security of AI technologies by improving their resilience to adverse conditions and digital threats. Within this domain, the study of Attacks and Defense ways plays a pivotal role.&lt;/p>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/road_scene.png" alt="Road_scene">
&lt;/p>
&lt;p>While these attacks might not seem like a big deal, nor dangerous in this context, the consequences can be severe in critical scenarios - take for example an autonomous vehicle failing to recognize a stop sign, hurting potentially a pedestrian. In this blog we will explore a new approach used for developping such adversarial patch attacks, based on Optimal Transport, as outlined in the paper &lt;em>&lt;strong>Optimal Transport Based Adversarial Patch To Leverage Large Scale Attack Transferability&lt;/strong>&lt;/em>. We will try to follow the same structure as in the paper to make the reading easier for you, but with much more simplicity.&lt;/p>
&lt;h2 id="section-1">Understanding Adversarial Attacks&lt;/h2>
&lt;p>First thing first, let us redefine some previously mentionned concepts, while making them into context.&lt;/p>
&lt;p>As deep neural networks keep getting better, developers are working hard to make sure they are trustworthy and reliable. This means constantly testing them to see how well they can handle different challenges, quantifying their robustness, and developping some robustification methods. In the context of image classification for instance, one way to do this is by designing adversarial attacks, which consists of a perturbation or noise, sometimes invisible patterns added to the input images in order to confuse the model and make it misclassify them, causing a huge drop in the accuracy.&lt;/p>
&lt;p>&lt;strong>Adversarial Patch Attacks&lt;/strong> are a specific type that consists of altering only a small part(patch) of the input, either physically or digitally by inserting a crafted &amp;ldquo;sticker&amp;rdquo;. These attacks happen to be more threatful as they can be easily applied in real life, they do not require modification of the entire image, and they can fool multiple, vastly different models with the same crafted patch. This last property is called &lt;strong>transferability&lt;/strong> and aims to test these engineered adversarial patches on various target models, beyond the original one used for learning, even if the two models(source and target) have been trained on different data or use different architectures, to evaluate the attack&amp;rsquo;s efficacy, and measure the models robustness.&lt;/p>
&lt;p>Despite the fact that crafting adversarial patch attacks is mainly based around maximizing the classification error through a gradient ascent, we can differenciate between three distinct approaches:&lt;/p>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/APA_strategies.png" alt="APA_strategies">
&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Decision boundaries based :&lt;/strong> &lt;a name="subsection-11">&lt;/a> Which is the most applied approach in previous works and litterature. It focuses on pushing the image&amp;rsquo;s representation in the neural network&amp;rsquo;s &lt;strong>decision&lt;/strong> space, across the decision boundary, making the network perceive it as belonging to a different, probability maximized class.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>To simplify this approach, imagine a group of fans attempting to sneak into a VIP section at a concert by dressing in a fancy way, like known VIP guests(targeted class). The idea is to blend in so well that they are indistinguishable from actual VIPs to the security guards (the ML model). Despite the simplicity and goodness of this strategy, it has some drawbacks :&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It is highly dependant on the model on which the attack is based, which makes it not really transferable: The success of this method hinges on the security&amp;rsquo;s lack of detail. If they are controlled by another security gard who is very familiar with the actual VIPs, the disguises will fail.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The patch may push the corrupted image representations into unknown regions of the representation space: In their attempt to mimic the VIPs, there&amp;rsquo;s a risk that their disguises might be so overdone that they don&amp;rsquo;t resemble any actual VIPs, pushing them to have a weird unique look. Hence, they end up in a no-man&amp;rsquo;s-land, not fitting in with either the regular attendees or the VIPs.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Feature point based :&lt;/strong> &lt;a name="subsection-12">&lt;/a>Instead of crossing a decision boundary, this strategy aims to modify the input so its representation in the &lt;strong>feature space&lt;/strong> matches the one of a target point belonging to a different class. This is like fine-tuning the attack to match a specific &amp;ldquo;signature&amp;rdquo; that the model associates to a specific point.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Revisiting our concert analogy, consider the fans now opting to mimic a specific celebrity known to be attending the concert, assuming that matching this one high-profile individual&amp;rsquo;s appearance will guarantee them entry. Although it seems more precise and effective than the first approach, this strategy has a significan drawback :&lt;/p>
&lt;ul>
&lt;li>It depends heavily on the targeted point selection, this later may be not representative of all instances in the target class : For instance, if the celebrity is known for a distinctive but uncommon style or if it&amp;rsquo;s unusual for such celebrities to attend such events, their attempt to copy him might not match what the security team usually expects from VIP guests.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Distribution based :&lt;/strong> &lt;a name="subsection-13">&lt;/a>This new approach implemented in the paper we are analyzing , is based on Optimal Transport theory, and aims to alter the overall feature distribution ofa set of input images belonging to a specific class, to resemble another class&amp;rsquo;s distribution, reducing the gap between them in the &lt;strong>feature space&lt;/strong>. It is more sophisticated than the previous ones as it exploits the fundamental way neural networks process and classify images based on learned distributions.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>This time, the group studies a wide variety of guests behaviors and appearances to craft a new, ambiguous look that doesn&amp;rsquo;t specifically mimic any single guest type, nor disguise blindly in a &amp;ldquo;VIP&amp;rdquo; style, but instead blends into the overall crowd, avoiding easy detection.&lt;/p>
&lt;ul>
&lt;li>The main advantage of this approach is that it allows a better transferability between models, enhancing the performance in the blackbox configuration, as it is independant of the classifier&amp;rsquo;s decision boundary , and the choice of a specific target point. Furthermore it captures the useful characteristics (features) from an input in a more universal way.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="section-2">Why do we need transferability ?&lt;/h2>
&lt;p>You surely noticed that we mentionned the transferability term many times in the last section, showing that is an essential property for designing such attacks, but why do we focus so much to make our patch transferable through many models? Well, it is like discovering a master key for many locks : It enables bad actors to compromise and confuse an AI system using a crafted patch they made without knowing anything about that system(architecture, training,&amp;hellip;).&lt;/p>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/transferability_diagram.png" alt="transferability_diagram">
&lt;/p>
&lt;p>This ability to create a &amp;lsquo;one-size-fits-all&amp;rsquo; adversarial patch allows to challenge many models, making it more difficult to develop defense mechanisms, and fostering the development of more robust AI systems. Unfortunately, this important property, which confronts the real-world variability of target systems, whose specific architectures or training details are often unknown, was not achieved strongly by previously developped Adversarial attacks; it was studied only by some specialized Adversarial Patch Attacks models(GAP, LaVan, PS-GAN) and gave very modest rsults, being evaluated on dated, non state of the art models Other models (TTP, M3D, Inkawhich et al.) conducted some experiments to measure the transferability of ivisible adversarial attacks and gave promizing results, but they didn&amp;rsquo;t focus i their work on patch attacks transferability.&lt;/p>
&lt;h2 id="section-3">Diving into Optimal Transport theory&lt;/h2>
&lt;p>The method introduced in this paper represents a remarkable success, as it bridges the gap between transferability studies of invisible adversarial examples and adversarial patch attacks, and provides a trade-off between an efficient non complex patch designing approach, and an exceptional transferability among many advanced state-of-the-art models. The key reason for this success lies in the inherent capabilities of &lt;strong>optimal transport&lt;/strong> to measure the distance between two distributions. Particularly, the loss optimized in this method is relevant, as it can be used when the distributions do not overlap, and the theory behind it is intuitive. It is based mainly on the &lt;strong>Wasserstain distance&lt;/strong> defined as :&lt;/p>
&lt;p>$$W_{p}^p(\mu,\nu) = \inf_{\pi \in \Pi(\mu,\nu)} \int_{\mathbb{R}^d \times \mathbb{R}^d} ||x - y||^p d\pi(x, y)$$&lt;/p>
&lt;!DOCTYPE html>
&lt;html>
&lt;head>
&lt;title>MathJax Visualization Example&lt;/title>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\\[','\\]']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;/head>
&lt;body>
&lt;p>or its more computationnaly efficient Sliced version, which compares the two distributions by computing the expected Wasserstein distance between their one-dimensional linear projections :
$$SW_{p}^p(\mu,\nu) = \int_{S^{d-1}} W_{p}^p(\theta_{\#}^{*}\mu, \theta_{\#}^{*}\nu) d\sigma(\theta)$$&lt;/p>
&lt;p>
Where $\mu$ and $\nu$ are two propbability distributions on $\mathbb{R}^d$, $||.||$ the euclidean norm, $\pi$ is a transport plan between $\mu$ and $\nu$, and $ \theta_{\#}^{*} \mu $ and $ \theta_{\#}^{*} \nu $ the push-forward by $\theta^{*}(x)=&lt;\theta, x>$ of $\mu$ and $\nu$ respectively.
&lt;/p>
&lt;/body>
&lt;/html>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/Sliced_wasserstain.png" alt="Sliced Wasserstain">
&lt;br>
&lt;em>This image is taken and adapted from the &lt;a href="https://theses.hal.science/tel-03533097/document">Sliced-Wasserstein distance for large-scale machine learning: theory, methodology and extensions&lt;/a> paper.&lt;/em>
&lt;/p>
&lt;!-- $$SW_{p}(\mu,\nu) = \int_{S^{d-1}} W_{p}(\theta_{\#}\mu, \theta_{\#}\nu) d\sigma(\theta)$$-->
&lt;p>To delve more into the mathematical details, let us explore how Optimal Transport, specifically the Wasserstein distance, is employed to craft effective adversarial examples:
In the context of image classification, we consider the standard notation where a set of image-label pairs $(x_i, y_i)$ is drawn from a joint distribution of random variables $X$ and $Y$. The images $X$ are typically multi-dimensional arrays representing the height, width, and color channels of an image (e.g., a colored $256 \times 256$ pixel image would have $h = 256$, $w = 256$, and $c = 3$). Meanwhile, $Y$ is a set of discrete labels that classify these images (e.g., &amp;lsquo;cat&amp;rsquo;, &amp;lsquo;dog&amp;rsquo;, etc.). Within a given encoder-decoder neural network $F$, designed to predict these labels, the encoder function $f$ compresses the raw image data $X$ throughout each pooling layer into a feature space $S^{(l)}$, capturing essential patterns.&lt;/p>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/Optimal transport.png" alt="Optimal Transport">
&lt;/p>
&lt;p>The Wasserstein distance $W_p$, calculated between the distributions of these feature spaces, reflects how much &amp;ldquo;effort&amp;rdquo; it would take to transform the distribution of features from one class into another. In the case of the proposed method, crafting the patch consits of minimizing the transformation cost (distance)of the features distribution from a corrupted &amp;ldquo;true&amp;rdquo; class into a &amp;ldquo;target&amp;rdquo; adversarial class across multiple layers. This can be formulated as follows:&lt;/p>
&lt;!DOCTYPE html>
&lt;html>
&lt;head>
&lt;title>MathJax Visualization Example&lt;/title>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\\[','\\]']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;/head>
&lt;body>
&lt;p>
$$\delta^* = \arg \min_{\delta} \mathbb{E}_X \left[ \sum_{l \in \mathcal{L}} OT(\mu_{X_{\delta}}^{(l)}, \nu_y^{(l)}) \right]$$&lt;/p>
&lt;p>
Where $OT$ is the optimal transport distance (Wasserstein or Sliced Wasserstein), $\mu_{X_{\delta}}^{(l)}$ is the feature distribution of images with the patch and $\nu_y^{(l)}$ is the target feature distribution for the incorrect class.&lt;/p>
&lt;p>This can be further enhanced by adding a regularization term to ensure that the patches are effective under various conditions, and can be physically realisable. The problem becomes as follows :&lt;/p>
&lt;p>
$$\delta^* = \arg \min_{\delta} \mathbb{E}_{X, t\sim \tau, e\sim E} \left[ \sum_{l \in \mathcal{L}} OT(\mu_{A(\delta, X, e, t)}^{(l)}, \nu_y^{(l)}) + TV(\delta)\right]$$
where TV is the total variation loss discouraging high-frequency patterns.&lt;/p>
&lt;/body>
&lt;/html>
&lt;h2 id="section-4">Experiments&lt;/h2>
&lt;h3 id="experimental-setup">Experimental setup &lt;a name="subsection-41">&lt;/a>&lt;/h3>
&lt;p>To confirm the theoretical results and assumptions, several experiments were conducted under different conditions and settings. For the sake of simplicity, we will not delve into the exhaustive details of the experimental setup, procedures, and results. In summary:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The experiments aimed to evaluate the impact and transferability of the proposed adversarial patch - referred to as $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ - across a range of models.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ performance was benchmarked against other adversarial patch attack (APA) methods such as GAP, LaVAN, TNT, TTP, M3D, and others.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The source and target models chosen for this analysis were regrouped into six categories based on their architecture: CNNs-V1, CNNs-V2, ENet, CNext(ConvNext), DeiT, and Swin.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Tested patches were randomly placed to the side of images, in order to avoid occluding the object of interest and replicate more closely the real world conditions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Targeted success rate (tSuc)&lt;/strong> metric was used for evaluating transferability. It consists of the percentage of instances where the network, when presented with an image containing the adversarial patch, incorrectly classifies the image as the attacker&amp;rsquo;s intended target class, out of the total number of attempts.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="results-and-findings-">Results and Findings : &lt;a name="subsection-42">&lt;/a>&lt;/h3>
&lt;p>The experiments are structured into three main categories:&lt;/p>
&lt;h4 id="digital-experiments-">Digital experiments : &lt;a name="subsection-421">&lt;/a>&lt;/h4>
&lt;h5 id="simple-configuration-">Simple configuration :&lt;/h5>
&lt;p>In this configuration, the patches efficacy was tested in a purely digital environment, using images from the ImageNet-1K dataset, which was used also for training. Patches were first designed to attack one of the source models, then tested on other target models to measure the attacking transferability. The table below summarizes for each APA method, the best transferring attack performance achieved :&lt;/p>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/Digital_transferability.png" alt="Digital Transferability">
&lt;/p>
&lt;p>As expected through the novelty of $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, it shows the highest transferability capacity(mean, min and max) and outperforms all the other methods. Additionaly, we can make the following observations:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Networks trained with older training recipes (CNNs-v1) seem more vulnerable to attacks, unlike tansformers and models trained with new training recipes (scheduler, augmenting training data like RandAug and Mixup, &amp;hellip;) which appear to be more robust.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For all APA methods, patches learned using Swin or CNext are more universal as they can transfer uniformly to multiple models.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In general, baseline methods tend to overfit and fail to generate patches that effectively transfer to complex architectures like CNext and Swin models, even if these patches are developed using the same category of models.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Methods based on feature space optimization, including L2 and the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ approach, demonstrate improved transferability and are less likely to overfit.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h5 id="robustified-configuration-">Robustified configuration :&lt;/h5>
&lt;p>In the second configuration of the digital experiments, the same procedures were reapplied. However this time, the methods learn on Swin, and transfer to a robustified version, by Local Gradients Smoothing (LGS) - a defense mechanism smoothing salient regions in images before passing them to the network - , of the six model categories.&lt;/p>
&lt;p>Similarly, $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ outperforms significantly all other methods as we can see in the following table :&lt;/p>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/Digital_robustified.png" alt="Digital robustified">
&lt;/p>
&lt;h4 id="hybrid-experiments">Hybrid experiments: &lt;a name="subsection-422">&lt;/a>&lt;/h4>
&lt;p>In order to simulate real-world applications more closely, the hybrid experiments conducted within this section involved printing adversarial patches trained with Swin, placing them in physical environments, capturing the images, and then digitally analyzing the results, for simple, and robustified models.&lt;/p>
&lt;p>The table below shows the criticality of the $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ giving very large tSuc in comparison with the other methods, for all settings:&lt;/p>
&lt;p align="center">
&lt;img src="./images/image_optimal_transport_patch/Digital_robustified.png" alt="Digital robustified">
&lt;/p>
&lt;h4 id="physical-experiments">Physical experiments: &lt;a name="subsection-423">&lt;/a>&lt;/h4>
&lt;p>In this last experiments category, we get closer to the real world situations, by recording a video of some ImageNet-1K objects (banana, cup , keyboard) while moving a designed patch in the set. This aims to quantify the severity of each attack, for realistic scenarios (as the example provided above about the autonomous vehicule not detecting the stop sign while driving due to an adversarial patch designed without knowing the AI system at all).&lt;/p>
&lt;p>All APA methods failed to transfer properly on all architectures except for L2 with a modest tSuc(9.3%) and $(W_2^2)^{(1)} / (SW_2^2)_{500}^{(1)}$ that gave much better results (23.4% and 29.3%)&lt;/p>
&lt;h2 id="section-5">Reproducibility&lt;/h2>
&lt;p>In this section, we wanted to reproduce some of the experiments conducted in the paper to validate the results and the findings. However, by exploring the code provided with the paper, and analyzing the python files, we found that it is not well documented, and the structure is not very clear, which makes it difficult to understand and reproduce the complex experiments involving transferability evaluation. Furthermore, given that the paper is based on the ImageNet dataset, which is very large and requires a lot of computational resources, we were not able to run the experiments on our local machines, as we do not have access to a powerful GPU cluster. Consequently, we opted for the CIFAR-10 dataset, which is smaller and more manageable. Despite this adjustment, we still faced some issues Specifically, the model is built from scratch without an available pre-trained, and there are missing components, notably the function required to extract feature vectors from each layer of the target models. To address these challenges and make the reproduction process easier, we decided to develop the missing feature extraction function as an enhancement, and save the obtained results into files(in the same way it was done in the code), to be able to apply the optimal transport method and craft the adversarial patches later as perspectives&lt;/p>
&lt;p>Here are the code that we developed :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torchvision.models&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">models&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torchvision.transforms&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">transforms&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">torch.utils.data&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">DataLoader&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">torchvision.datasets&lt;/span> &lt;span style="color:#00a8c8">as&lt;/span> &lt;span style="color:#111">datasets&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">get_loader&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">:&lt;/span> &lt;span style="color:#111">str&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">split&lt;/span>&lt;span style="color:#111">:&lt;/span> &lt;span style="color:#111">str&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">batch_size&lt;/span>&lt;span style="color:#111">:&lt;/span> &lt;span style="color:#111">int&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#f92672">-&amp;gt;&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">utils&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">data&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">DataLoader&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;&amp;#34;&amp;#34;Return a DataLoader object for a given dataset and split.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">return&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">utils&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">data&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">DataLoader&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">get_dataset&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">dataset&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">split&lt;/span>&lt;span style="color:#111">),&lt;/span> &lt;span style="color:#111">batch_size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">batch_size&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">shuffle&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">True&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">extract_features&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">data_loader&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">list_models&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#d88200">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> Extracts features from each layer of the pre-trained models provided in the list_models
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> by applying average pooling, and saves the extracted features into files.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d88200"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">model_name&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">list_models&lt;/span>&lt;span style="color:#111">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">model&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">models&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">__dict__&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#111">model_name&lt;/span>&lt;span style="color:#111">](&lt;/span>&lt;span style="color:#111">pretrained&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#00a8c8">True&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">model&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">eval&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">layer_name&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">layer&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">model&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">named_children&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">if&lt;/span> &lt;span style="color:#111">isinstance&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">layer&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Sequential&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">layer_features&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#111">i&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">input&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">target&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#111">enumerate&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">data_loader&lt;/span>&lt;span style="color:#111">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">with&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">no_grad&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">output&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">layer&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">input&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">output&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">nn&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">functional&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">adaptive_avg_pool2d&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">output&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">output&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">output&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">view&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">output&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">size&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#111">),&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">layer_features&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">append&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">output&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">layer_features&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">cat&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">layer_features&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">torch&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">save&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">layer_features&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;./data/CIFAR/all_images_feature/&lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">layer_name&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">/&lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">model_name&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">.pt&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Apply the function extract_features to some targeted models&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">list_models&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#34;resnet18&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;vgg19&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;convnext_tiny&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;swin_t&amp;#34;&lt;/span>&lt;span style="color:#111">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">data_loader&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">get_loader&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#34;CIFAR10&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;train&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">batch_size&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">64&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">extract_features&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">data_loader&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">list_models&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="section-6">Conclusion&lt;/h2>
&lt;p>In conclusion, our exploration of the paper &lt;em>&lt;strong>OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY&lt;/strong>&lt;/em>, revealed an innovative and promizing technique that uses Optimal Transport to make adversarial patches more effectively fool different models. This method, focusing on altering image feature distributions to match a target distribution from another class, has proven to be both theoretically sound and practically successful. It significantly outperforms current state of the art methods in creating patches that can be highly transferable between models and potentially very harmful, showing great promise for both advancements in the field and potential challenges in security applications.&lt;/p>
&lt;h2 id="references">References:&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://openreview.net/forum?id=nZP10evtkV">OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://theses.hal.science/tel-03533097/document">Sliced-Wasserstein distance for large-scale machine learning : theory, methodology and extensions&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1803.00567.pdf">Computational Optimal Transport&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf">Feature Space Perturbations Yield More Transferable Adversarial Examples&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/optimal_transport_based_adversarial_patch/</guid><pubDate>Sat, 03 Feb 2024 22:22:36 +0100</pubDate></item><item><title>Statistical Minimax Rates Under Privacy</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/statistical_minimax_rates_under_privacy/</link><description>&lt;h1 style="font-size: 36px;">Estimating Privacy in Data Science: A Comprehensive Guide&lt;/h1>
&lt;h1 style="font-size: 24px;">Author: Antoine Klein &lt;a href="https://github.com/AntoineTSP">Github Link&lt;/a>&lt;/h1>
&lt;h1 id="table-of-contents">Table of Contents&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#section-0">Incentives&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-1">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-2">Definition&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-3">Theory&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-4">The case of multinomial estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-5">The case of density estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-6">Experiment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-7">Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-8">Quizz&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="section-0">Why do we care about privacy ?&lt;/h2>
&lt;p>Imagine, you&amp;rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you&amp;rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you &lt;em>struggle&lt;/em>. Not because you don&amp;rsquo;t know the answer, but because you&amp;rsquo;re faced with an &lt;strong>ethical dilemma&lt;/strong>: transparency towards the state versus protection of personal data.&lt;br>
$$\text{In short, transparency goes against your privacy. }$$&lt;/p>
&lt;p>This stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you &lt;strong>underestimate&lt;/strong> your answers. On a wider scale, this leads to a &lt;strong>suffrage bias&lt;/strong> and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:&lt;br>
&lt;strong>&amp;ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response&amp;rdquo;&lt;/strong>&lt;/p>
&lt;p>This situation presented a trusted agent, in that he wasn&amp;rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it&amp;rsquo;s also an attack on yours: as the guarantor of your data, it&amp;rsquo;s now at the mercy of the attacker. The problem here is &lt;strong>not to have protected yourself against a malicious agent&lt;/strong>.&lt;/p>
&lt;p>Admittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes&amp;hellip;One area for improvement is quite simply to better &lt;strong>certify usage&lt;/strong> by means of cyber protection labels and leads to such a norm to achieve trust:
&lt;img
src="./images/Antoine_Klein/Umbrella.png"
alt="Data Privacy2"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>In this blog, we propose to tackle this problem from a completely different angle: &lt;strong>how to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data&lt;/strong>. We&amp;rsquo;ll also use minimax bounds to answer the question: &lt;strong>for a given privacy criterion, what&amp;rsquo;s the loss in terms of estimation?&lt;/strong> (fundamental trade-offs between privacy and convergence rate)&lt;/p>
&lt;h2 id="section-1">Scientific introduction&lt;/h2>
&lt;p>Our blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that &lt;strong>response randomization achieves optimal convergence&lt;/strong> in the case of multinomial estimation, and then that this process can be generalized to any &lt;em>nonparametric distribution estimation&lt;/em>. To this end, we will introduce the notion of &lt;strong>local differential privacy&lt;/strong> as well as the &lt;strong>minimax theory&lt;/strong> for obtaining optimal limits. All this will shed light on the &lt;strong>trade-off between privacy and estimation rates&lt;/strong>. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.&lt;/p>
&lt;h2 id="section-2">Some key definitions&lt;/h2>
&lt;p>Let assume that you want to make private $X_1 , &amp;hellip; , X_n \in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a &lt;strong>markov kernel&lt;/strong> that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.&lt;/p>
&lt;p>The privacy mechanism is to be said &lt;strong>non interactive&lt;/strong> if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is &lt;strong>memory less&lt;/strong>. If not, the mechnism is said to be interactive.&lt;/p>
&lt;p>In the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.&lt;/p>
&lt;p>$Z_i$ is said to be &lt;strong>α-local-differentially private&lt;/strong> for the original data $X_i$ if $$sup(\frac{Q(Z | X_i = x)}{Q(Z | X_i = x&amp;rsquo;)} | x, x&amp;rsquo; ∈ X) ≤ exp(α)$$.&lt;/p>
&lt;p>An intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more &lt;strong>difficult it is to distinguish&lt;/strong> the distribution of Z conditional on two different X data.&lt;/p>
&lt;h2 id="section-3">Theoretical results&lt;/h2>
&lt;h3 id="section-4">The case of multinomial estimation&lt;/h3>
&lt;p>In this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$.
This problem is a special case of multinomial estimation, where &lt;code>θ&lt;/code> is now a multidimensional parameter that is amenable to simplex probability. $∆&lt;em>d := (θ ∈ ℝ&lt;/em>+ |∑θ_j = 1)$.&lt;/p>
&lt;p>&lt;a name="Recall">&lt;/a>&lt;/p>
&lt;p>&lt;strong>Theorem :&lt;/strong> Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\alpha\in [0,1]$:
$$C_1 min(1, \frac{1}{\sqrt{n\alpha^2}}, \frac{d}{n\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \frac{d}{n\alpha^2})$$ and
$$C_1 min(1,\frac{1}{\sqrt{n\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\frac{d}{\sqrt{n\alpha^2}})$$.&lt;/p>
&lt;p>&lt;strong>Recall from standard statistics:&lt;/strong> For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that:
$$E[|θ_{hat} - θ|^2] ≤ \frac{C_3}{n}$$&lt;/p>
&lt;p>In others term, providing α-local-differentially privacy &lt;strong>causes a reduction&lt;/strong> in the effective sample size of a factor $\frac{\alpha^2}{d}$ for best situations. It thus means that the &lt;strong>asymptotically rate of convergences remains unchanged&lt;/strong> which is a really good news !&lt;/p>
&lt;h4 id="practical-strategies">Practical strategies&lt;/h4>
&lt;p>The paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#section-10">Randomized responses&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-11">Laplace Noise (beyond paper)&lt;/a>&lt;/li>
&lt;/ul>
&lt;h5 id="section-10">Randomized responses&lt;/h5>
&lt;p>The &lt;em>intuition&lt;/em> of this section is the following : &lt;strong>to not allow the statistician to retrieve your personnal data&lt;/strong> in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn&amp;rsquo;t know what was the result of the coin, &lt;strong>he can&amp;rsquo;t distinguish&lt;/strong> if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.&lt;/p>
&lt;p>For the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :&lt;/p>
&lt;p>$$[Z]_j = x_j \text{ with probability } \frac{e^\frac{\alpha}{2}} {1 + e^\frac{\alpha}{2}}$$
$$[Z]_j = 1 - x_j \text{ with probability } \frac{1}{1 + e^\frac{\alpha}{2}}$$&lt;/p>
&lt;p>Such a mechanism achieves &lt;em>α-local-differentially privacy&lt;/em> because one can show that :&lt;/p>
&lt;p>$$\frac{Q(Z = z | x)}{Q(Z = z | x&amp;rsquo;)} = e^\frac{\alpha}{2}(||z - x||_1 - ||z - x&amp;rsquo;||_1) \in [e^{-\alpha}, e^\alpha]$$ which is the criteria given above.&lt;/p>
&lt;p>With the notation as $1_d=[1, 1, 1, &amp;hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :&lt;/p>
&lt;p>$$E[Z | x] = \frac{e^\frac{\alpha}{2} - 1}{e^\frac{\alpha}{2} + 1} * x + \frac{1}{1 + e^\frac{\alpha}{2}}1_d$$&lt;/p>
&lt;p>This leads to the natural moment-estimator :&lt;/p>
&lt;p>$$θ_{hat} = \frac{1}{n} ∑_{i=1}^{n} \frac{Z_i - 1_d}{1 + e^\frac{\alpha}{2}} * \frac{e^\frac{\alpha}{2} + 1}{e^\frac{\alpha}{2} - 1}$$&lt;/p>
&lt;p>One can also show that it verifies :&lt;/p>
&lt;p>$$E[ ||θ_{hat}- θ||_2] ≤ \frac{d}{n} * \frac{(e^\frac{\alpha}{2} + 1)^2}{(e^\frac{\alpha}{2} - 1)^2} &amp;lt; \frac{C_3}{nα^2}$$ which is the announced result.&lt;/p>
&lt;h5 id="section-11">Laplace Noise (beyond paper)&lt;/h5>
&lt;p>Instead of saying the truth with some probability, one may think of &lt;strong>adding noise&lt;/strong> to the answer so that the statistician can&amp;rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is &lt;strong>not covered in the paper&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>Definition:&lt;/strong> A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:&lt;br>
$$f(x|μ, b) = \frac{1}{2b} * exp(\frac{-|x - μ|}{b})$$&lt;/p>
&lt;p>A visualisation for differents parameters is given below. We can see that Laplace distribution is a &lt;strong>shaper verson of the gaussian distribution&lt;/strong> :
&lt;img
src="./images/Antoine_Klein/Laplace.png"
alt="Laplace"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>The trick is to use such a noise. Let assume $X_i \in [-M,M]$ and construct the private mechanism as follow:&lt;br>
$$Z_i = X_i + \sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).&lt;/p>
&lt;p>One can show that :&lt;/p>
&lt;p>$$\frac{Q(Z = z | x)}{Q(Z = z | x&amp;rsquo;)} \leq e^{\frac{1}{\sigma} * |x - x&amp;rsquo;|} \leq e^{\frac{2M}{\sigma}}$$&lt;/p>
&lt;p>Thus, with the choice of $\sigma = \frac{2M}{\alpha}$, &lt;strong>it verifies α-local-differentially privacy&lt;/strong>. The proposed estimator is the following :&lt;br>
$$\hat{Z} = \bar{X} + \frac{2M}{\alpha} \bar{W}$$&lt;/p>
&lt;p>One can show that it is an unbiaised estimator that achieves the optimal rates:&lt;br>
$$E[\hat{Z}] = E[X]$$&lt;br>
$$V[\hat{Z}] = \frac{V(X)}{n} + \frac{4M^2}{n\alpha^2} V[\bar{W}] = \frac{V(X)}{n} + \frac{8M^2}{n\alpha^2}$$
$$E[ |\hat{Z}- X|^2] \leq \frac{C_3}{n\alpha^2}.$$&lt;/p>
&lt;p>This is &lt;strong>exactly the optimal rates&lt;/strong>, quite outstanding !&lt;/p>
&lt;h3 id="section-5">The case of density estimation&lt;/h3>
&lt;p>One accurate question that can raise is : &lt;strong>what about others distribution ?&lt;/strong> Is privacy more costly in general cases ? What is the trade-off ?&lt;/p>
&lt;p>To answer this question, let&amp;rsquo;s precise the problem.&lt;/p>
&lt;p>We want to estimate in a non-parametric way a 1D-density function &lt;code>f&lt;/code> belonging to one of theses classes :&lt;br>
-&lt;strong>Hölder Class (β, L):&lt;/strong> $\text{For all }x, y \in \mathbb{R} \text{ and } m \leq \beta, \quad \left| f^{(m)}(x) - f^{(m)}(y) \right| \leq L \left| x - y \right|^{\beta - m}$&lt;br>
-&lt;strong>Sobolev Class:&lt;/strong> $F_{\beta}[C] := \left( f \in L^2([0, 1]) , \middle| , f = \sum_{j=1}^{\infty} \theta_j \phi_j \text{ such that } \sum_{j=1}^{\infty} j^{2\beta} \phi_j^2 \leq C^2 \right)$&lt;/p>
&lt;p>In a intuitition way, those two classes express that &lt;code>f&lt;/code> is &lt;strong>smooth enough&lt;/strong> to admits Lipschitz constant to its derivative so that it doesn&amp;rsquo;t &amp;ldquo;vary&amp;rdquo; locally too much.&lt;/p>
&lt;h4 id="theorem">Theorem&lt;/h4>
&lt;h5 id="without-privacy">Without privacy&lt;/h5>
&lt;p>One can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:&lt;br>
$$\text{MSE}(\hat{f} - f) \leq C_1 \cdot n^{-\frac{2\beta}{1+2\beta}}$$ with the estimator&lt;br>
$$\hat{f}(x) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{h} K\left(\frac{x - X_i}{h}\right) \text{with } h = C_2 \cdot n^{-\frac{1}{2\beta+1}}$$&lt;/p>
&lt;p>In the case of d-multidimensionnal density &lt;code>f&lt;/code>, the optimal rate is :&lt;br>
$$\text{MSE}(\hat{f} - f) \leq C_4 \cdot n^{-\frac{2\beta}{d+ 2\beta}}$$ with the estimator&lt;br>
$$\hat{f}(x) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{h^d} K^d\left(\frac{x-X_i}{h}\right) \quad \text{with} \quad h = C_5 \cdot n^{-\frac{1}{2\beta + d}}$$&lt;/p>
&lt;p>This illustrates once again the &lt;strong>curse of dimensionnality&lt;/strong>.&lt;/p>
&lt;h5 id="with-privacy">With privacy&lt;/h5>
&lt;p>Let assume that &lt;code>f&lt;/code> bellongs to one of the two classes with &lt;code>β&lt;/code> as smoothness parameter.&lt;br>
Then, the optimal α-local-differentially private optimal rate is :&lt;br>
$$\text{MSE}(\hat{f} - f) \leq C_1 \cdot (n\alpha^2)^{-\frac{2\beta}{2\beta+2}}.$$&lt;/p>
&lt;p>One may observe &lt;strong>two pessimistic news&lt;/strong>:&lt;br>
-The rate is &lt;strong>affected by a factor&lt;/strong> of $\alpha^2$ as for the multinomial estimation&lt;br>
-More damageable: the &lt;strong>rate is slower&lt;/strong> in term of &lt;code>n&lt;/code> unlike the previous problem which make privacy in this case &lt;strong>more costly&lt;/strong>.&lt;/p>
&lt;h5 id="practical-strategies-1">Practical strategies&lt;/h5>
&lt;p>Eventhough this rate is pessimistic and proves that &lt;strong>privacy comes at a cost&lt;/strong>, it remains to illustrates how can we achieves this best but not great rate.
For this end, once again, two strategies are possible.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#section-12">Randomized responses&lt;/a>&lt;/li>
&lt;li>&lt;a href="#section-13">Laplace Noise (beyond paper)&lt;/a>&lt;/li>
&lt;/ul>
&lt;h5 id="section-12">Randomized responses&lt;/h5>
&lt;p>This is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of &lt;code>α&lt;/code>.
As it is not the most comprehensive and straightforward method, &lt;strong>we prefer to dive in depth into the second one; uncovered in the paper&lt;/strong>.&lt;/p>
&lt;h5 id="section-13">Laplace Noise (beyond paper)&lt;/h5>
&lt;p>Let assume that $X_i \in [0,M]$ almost surely. We note $G_j = [\frac{j-1}{K},\quad \frac{j}{K}]$ the bin of length $\frac{1}{K}$.&lt;/p>
&lt;p>We consider the histogramm estimator:
$$\hat{f}(x) = \frac{K}{n} \sum_{j=1}^{K} \sum_{i=1}^{n} 1_{X_i \in G_j} \cdot 1_{x \in G_j}.$$&lt;/p>
&lt;p>We now construct the private mechanism as follow:&lt;br>
$$Z_i = \left[1_{X_i \in G_1} + \frac{2}{\alpha} W_1, \ldots, 1_{X_i \in G_K} + \frac{2}{\alpha} W_K\right]$$&lt;/p>
&lt;p>In an intuitive way, we add a Laplace noise realisation for each bin.&lt;/p>
&lt;p>This guarantees α-local-differentially privacy as :
$$\frac{Q(Z = z | x)}{Q(Z = z | x&amp;rsquo;)} \leq \exp\left(\frac{\alpha}{2} \sum_{j=1}^{K} |1_{x \in G_j} - 1_{x&amp;rsquo; \in G_j}| \right) \leq \exp\left(\frac{\alpha}{2} \cdot 2\right).$$&lt;/p>
&lt;p>This leads to the α-local-differentially private estimator :&lt;br>
$$f_{\text{private_estimate}} = \hat{f} + \frac{2K}{n\alpha} \sum_{j=1}^{K} W_j$$&lt;/p>
&lt;p>The biais is the same as the unprivate case as :&lt;br>
$$E[f_{\text{private_estimate}}] = E[\hat{f}] + 0 .$$&lt;/p>
&lt;p>One may prove that if f bellongs to the β-Hölder Class:&lt;br>
$$Biais(f_{\text{private_estimate}}, f) \leq C_1 * K^{-\beta}$$&lt;/p>
&lt;p>Meanwhile, $$V[f_{\text{private_estimate}}] \leq \frac{C_2}{n} + \frac{4K^2}{\alpha^2} \frac{V[W]}{n}$$, such that in total :&lt;br>
$$\text{MSE}(f_{\text{private_estimate}} - f) \leq C_1 K^{-2\beta} + \frac{C_2}{n} + \frac{C_3 K^2}{n\alpha^2}.$$
Minimizing over K (hyperparameters) leads to : $K = C_4 \cdot (n\alpha^2)^{-\frac{1}{2\beta+2}}$ and thus to:&lt;br>
$$\text{MSE}(f_{\text{private_estimate}} - f) \leq C_5 \cdot (n\alpha^2)^{-\frac{2\beta}{2\beta + 2}}$$, which is the expected bound.&lt;/p>
&lt;hr>
&lt;h2 id="section-6">Experiment: Illustration of the Minimax privacy rate&lt;/h2>
&lt;h3 id="section-111">Overview&lt;/h3>
&lt;p>The aim of this section is to &lt;strong>provide illustrations of the theoretical results&lt;/strong> set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.&lt;/p>
&lt;p>For the sake of &lt;strong>reproducibility and transparency&lt;/strong>, the source code can be found in the notebook at this: &lt;a href="https://github.com/AntoineTSP/responsible-ai-datascience-ipParis.github.io.git">Github link&lt;/a>.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Data Preparation&lt;/strong>: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects.&lt;/li>
&lt;/ol>
&lt;p>More precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\alpha).$&lt;br>
As for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$&lt;/p>
&lt;ol start="2">
&lt;li>
&lt;p>&lt;strong>Privacy Metric Calculation&lt;/strong>: We will look at the use case of estimating the mean of a distribution.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Evaluation&lt;/strong>: The results will be compared in terms of Mean Square Error (MSE).&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>In terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:&lt;/p>
&lt;p>&lt;img
src="./images/Antoine_Klein/Private_distribution.png"
alt="Data Privacy2"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>As expected, the greater the desired privacy (low $\alpha$), &lt;strong>the more spread out&lt;/strong> the distribution of observed data.&lt;/p>
&lt;p>When it comes to estimating the true average from private data, we obtain the following figure:&lt;/p>
&lt;p>&lt;img
src="./images/Antoine_Klein/Estimated_mean.png"
alt="Data Privacy2"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>This figure illustrates two major points:&lt;br>
-The first is that whatever the level of privacy, we have an &lt;strong>unbiased estimator&lt;/strong> of the mean. It&amp;rsquo;s a beautiful property, empirically verified !&lt;br>
-The second is that, unfortunately, the greater the privacy (low alpha), &lt;strong>the greater the variance&lt;/strong> of this estimator.&lt;/p>
&lt;p>We recall our main theorem demonstrated above &lt;a href="#Recall" style="background-color: yellow; padding: 2px 5px; border-radius: 3px;">Previous theorem&lt;/a> :&lt;br>
&lt;strong>Theorem&lt;/strong> : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\alpha\in [0,1]$:
$$C_1 min(1, \frac{1}{\sqrt{n\alpha^2}}, \frac{d}{n\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \frac{d}{n\alpha^2})$$&lt;/p>
&lt;p>We now want to &lt;strong>compare the theoretical optimal rate with empirical results&lt;/strong>. To do this, we distinguish two situations:&lt;br>
-The first is with &lt;strong>fixed alpha&lt;/strong>, and determines the MSE as a function of the number of samples n. This leads to these empirical results:&lt;/p>
&lt;p>&lt;img
src="./images/Antoine_Klein/Minimax_rate_n.png"
alt="Data Privacy2"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>The dotted line represents the regime of the theoretical bound of the form $n \rightarrow \frac{C1}{n}$ . This is the shape of the empirical curves!&lt;/p>
&lt;p>-The second has a &lt;strong>fixed n&lt;/strong> and determines the MSE as a function of alpha. This leads to these empirical results:&lt;/p>
&lt;p>&lt;img
src="./images/Antoine_Klein/Minimax_rate_alpha.png"
alt="Data Privacy2"
loading="lazy"
decoding="async"
class="full-width"
/>
&lt;/p>
&lt;p>The dotted line represents the regime of the theoretical bound of the form $\alpha \rightarrow \frac{C1}{\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!&lt;/p>
&lt;h3 id="section-7">Conclusion&lt;/h3>
&lt;p>From a problem rooted in an &lt;strong>ethical dilemma&lt;/strong> (privacy versus completeness and transparency), we have looked at the &lt;strong>cost of guaranteeing&lt;/strong> one at the expense of the other, to better sketch out desirable situations.&lt;br>
This has enabled us to develop theoretical results in terms of &lt;strong>minimax rates&lt;/strong>. There is indeed a &lt;strong>trade-off&lt;/strong> between these criteria, which is even more costly in the case of non-parametric density estimation.&lt;br>
Finally, we have compared these theoretical limits with empirical results, which &lt;strong>confirm the conformity of the statements&lt;/strong>.&lt;br>
The aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following &lt;strong>quiz&lt;/strong> to ensure his or her understanding.&lt;/p>
&lt;h1 id="section-8">Quizz&lt;/h1>
&lt;p>To test yourself abour privacy:&lt;/p>
&lt;form id="quiz-form" class="quiz-form">
&lt;div class="quiz-question">
&lt;p>What is privacy?&lt;/p>
&lt;div class="quiz-options">
&lt;label>
&lt;input type="radio" name="question1" value="1">
Avoid asking questions that can raise private information
&lt;/label>
&lt;label>
&lt;input type="radio" name="question1" value="2">
A mechanism that prevents other agent to retrieve personnal information in your answer
&lt;/label>
&lt;label>
&lt;input type="radio" name="question1" value="3">
An ethical-washing trend
&lt;/label>
&lt;/div>
&lt;p>Which situation is α-local-differentially privacy?&lt;/p>
&lt;div class="quiz-options">
&lt;label>
&lt;input type="radio" name="question2" value="1">
sup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} >= exp(α)
&lt;/label>
&lt;label>
&lt;input type="radio" name="question2" value="2">
You tell the truth half the time, you lie otherwise.
&lt;/label>
&lt;label>
&lt;input type="radio" name="question2" value="3">
Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1)
&lt;/label>
&lt;/div>
&lt;p>What is the privacy cost in term of optimal rate ?&lt;/p>
&lt;div class="quiz-options">
&lt;label>
&lt;input type="radio" name="question3" value="1">
Multinomial estimation: A factor α^2/d
&lt;/label>
&lt;label>
&lt;input type="radio" name="question3" value="2">
Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2))
&lt;/label>
&lt;label>
&lt;input type="radio" name="question3" value="3">
We loose nothing, that's the surprising finding of the paper
&lt;/label>
&lt;/div>
&lt;/div>
&lt;!-- Add more quiz questions as needed -->
&lt;button type="submit" class="quiz-submit">Submit&lt;/button>
&lt;/form>
&lt;div id="quiz-results" class="quiz-results">&lt;/div>
&lt;script>
// Define quiz questions and correct answers
const quizQuestions = [
{
question: "What is privacy?",
answer: "2"
},
//Add more quiz questions as needed
{
question: "Which situation is α-local-differentially privacy?",
answer: "3"
},
//Add more quiz questions as needed
{
question: "What is the privacy cost in term of optimal rate ?",
answer: "1"
}
];
// Handle form submission
document.getElementById('quiz-form').addEventListener('submit', function(event) {
event.preventDefault();
// Calculate quiz score
let score = 0;
quizQuestions.forEach(question => {
const selectedAnswer = document.querySelector(`input[name="question${quizQuestions.indexOf(question) + 1}"]:checked`);
if (selectedAnswer) {
if (selectedAnswer.value.toLowerCase() === question.answer) {
score++;
selectedAnswer.parentElement.classList.add('correct');
} else {
selectedAnswer.parentElement.classList.add('incorrect');
}
}
});
// Display quiz results
const quizResults = document.getElementById('quiz-results');
quizResults.innerHTML = `&lt;p>You scored ${score} out of ${quizQuestions.length}.&lt;/p>`;
});
&lt;/script>
&lt;hr>
&lt;hr>
&lt;h2 id="annexes">Annexes&lt;/h2>
&lt;h3 id="references">References&lt;/h3>
&lt;ol>
&lt;li>Warner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830.&lt;/li>
&lt;li>John C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013)&lt;/li>
&lt;li>Dwork, C., &amp;amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407.&lt;/li>
&lt;li>Narayanan, A., &amp;amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE.&lt;/li>
&lt;/ol>
&lt;script>
function highlight(text) {
var inputText = document.getElementById("markdown-content");
var innerHTML = inputText.innerHTML;
var index = innerHTML.indexOf(text);
if (index >= 0) {
innerHTML = innerHTML.substring(0,index) + "&lt;span class='highlight'>" + innerHTML.substring(index,index+text.length) + "&lt;/span>" + innerHTML.substring(index + text.length);
inputText.innerHTML = innerHTML;
}
}
highlight("Estimating Privacy in Data Science");
&lt;/script>
&lt;hr>
&lt;script>
function displayInput() {
var inputValue = document.getElementById("inputField").value;
document.getElementById("output").innerText = "You typed: " + inputValue;
}
&lt;/script>
&lt;style>
.highlight {
background-color: red;
}
.highlight-on-hover:hover {
background-color: yellow;
}
/* Quiz form styles */
.quiz-form {
max-width: 500px;
margin: auto;
padding: 20px;
border: 1px solid #ccc;
border-radius: 5px;
background-color: #f9f9f9;
}
.quiz-question {
margin-bottom: 20px;
}
.quiz-options label {
display: block;
margin-bottom: 10px;
}
.quiz-submit {
background-color: #4caf50;
color: white;
padding: 10px 20px;
border: none;
border-radius: 5px;
cursor: pointer;
}
.quiz-submit:hover {
background-color: #45a049;
}
/* Quiz results styles */
.quiz-results {
margin-top: 20px;
font-weight: bold;
}
.quiz-options label {
display: block;
margin-bottom: 10px;
}
.quiz-options label.correct {
color: green;
}
.quiz-options label.incorrect {
color: red;
}
a[name]:hover {
background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */
text-decoration: none; /* Optionally remove underline on hover */
}
&lt;/style>
&lt;style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
&lt;/style>
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/statistical_minimax_rates_under_privacy/</guid><pubDate>Wed, 31 Jan 2024 17:22:02 +0100</pubDate></item><item><title>Measuring the Transferability of Pre-trained Models: a link with Neural Collapse Distances on Target Datasets</title><link>https://responsible-ai-datascience-ipParis.github.io/posts/transferability/</link><description>&lt;script
type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
}
});
MathJax.Hub.Queue(function() {
var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i &lt; all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
&lt;/script>
&lt;script
type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full">&lt;/script>
&lt;p>&lt;strong>Authors&lt;/strong> : Marion Chadal and Julie Massé&lt;/p>
&lt;p>This blog post discusses the paper &amp;ldquo;How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability&amp;rdquo; &lt;a href="#ref1">[1]&lt;/a>. It provides an explanation of it so that you can understand the usefulness of measuring transferability, and a reproduction of the authors&amp;rsquo; experiment so that you can better visualize their methodology.&lt;/p>
&lt;h1 id="pre-trained-models-and-fine-tuning">Pre-trained models and fine-tuning&lt;/h1>
&lt;p>Pre-trained models are currently one of the most active fields in Machine Learning. They can be found in a wide range of applications, from image recognition and natural language processing to autonomous driving and medical diagnosis. These models are &amp;ldquo;pre-trained&amp;rdquo; on massive datasets, most of the time encompassing millions of examples across diverse domains. The training process leverages Deep Learning algorithms and can take weeks or even months, utilizing powerful computing resources to iteratively adjust the model&amp;rsquo;s parameters until it achieves high accuracy on the training data.&lt;/p>
&lt;p>The first purpose of pre-training is to enable the model to learn a broad understanding of the world, capturing intricate patterns, relationships, and features that are not easily discernible. This extensive learning phase allows the model to develop a deep amount of knowledge, which it can then apply to more specific tasks through a process known as fine-tuning.&lt;/p>
&lt;p>What is &lt;strong>fine-tuning&lt;/strong>? It consists in adapting a general-purpose model to perform well on a specific task. This adaptation allows the model to fine-tune its learned features to better align with the nuances of the new task, enhancing its accuracy and performance. Whether it&amp;rsquo;s identifying specific types of objects in images, understanding the subtleties of natural language in a particular context, or diagnosing medical conditions from scans, fine-tuning enables pre-trained models to become specialized tools capable of tackling a wide range of applications.&lt;/p>
&lt;p>Fine-tuning begins with a pre-trained model—a model that has already learned a vast array of features and patterns from a comprehensive dataset, often spanning millions of examples. This model, equipped with a deep understanding of various data representations, serves as a robust starting point. The fine-tuning process then adapts this model to a specific task by continuing the training process on a smaller, task-specific dataset. This additional training phase is typically shorter and requires significantly fewer data and computational resources than training a model from scratch, as the model already possesses a foundational knowledge base.&lt;/p>
&lt;p align="center">
&lt;img src="./images/ChadalMasse/schema.png" width="600"/>
&lt;/p>
&lt;p>One of the key aspects of fine-tuning is its efficiency in data utilization. Since the model has already learned general features and patterns, the fine-tuning process can achieve high performance with relatively small datasets. This characteristic is particularly valuable in domains where collecting large amounts of labeled data is challenging or expensive.&lt;/p>
&lt;p>Training from scratch is the complete opposite of fine-tuned pre-trained models, as it involves starting with randomly initialized parameters and requires a substantial dataset specific to the task at hand, along with considerable computational resources and time to achieve comparable performance to a fine-tuned pre-trained model. While training from scratch can be beneficial in certain scenarios where highly specialized knowledge is required or when a suitable pre-trained model is not available, the efficiency and effectiveness of leveraging pre-trained models are nowadays undeniable.&lt;/p>
&lt;h1 id="transferability">Transferability&lt;/h1>
&lt;p>Transferability caracterizes the &lt;em>ability of pre-trained models to run on downstream tasks without performing fine-tuning, but achieving comparable results&lt;/em>. Models that exhibit &lt;strong>high transferability&lt;/strong> are those that have learned &lt;strong>generalizable features&lt;/strong> during pre-training—features that are not overly specific to the training data but that capture universal patterns or structures present across different datasets and domains.&lt;/p>
&lt;p>Beside, transferability arises as an attempt of improvement in &lt;strong>scalable AI&lt;/strong>, as it enables researchers and practitioners to build upon existing knowledge without reinventing the wheel for every new task. This characteristic is especially crucial in our current case where data is abundant, but labeled data is scarce or expensive to obtain. Transferable models can leverage unlabeled data from similar domains, or even entirely different domains, to achieve impressive results with minimal effort.&lt;/p>
&lt;p>Moreover, the pursuit of enhancing transferability has led to innovations in model architecture, training strategies, and domain adaptation techniques. &lt;strong>Few-shot learning&lt;/strong> for instance, where models learn from a very small amount of labeled data, and zero-shot learning, where models apply their knowledge to tasks they have not explicitly been trained on.&lt;/p>
&lt;p>The concept of transferability also intersects with &lt;strong>ethical AI&lt;/strong> development, as it encourages the use of more generalizable models that can perform equitably across diverse datasets and demographics, reducing the risk of biased or unfair outcomes.&lt;/p>
&lt;h1 id="why-measuring-transferability">Why measuring transferability?&lt;/h1>
&lt;p>Fine-tuning pre-trained models works as follows. First, you &lt;strong>pick a downstream task&lt;/strong>, for which you have at your disposal several pre-trained models candidates. You want to compare their performances to pick the best one on test set, with the &lt;strong>optimal fine-tuning configuration&lt;/strong>. Then, you have to fine-tune each of them. Even if the dataset to train on is smaller, thanks to fine-tuning, you have to repeat it for all your models candidates, and one does not want that, as it can quickly become &lt;strong>computationnally expensive&lt;/strong>.&lt;/p>
&lt;p>Transferability estimation arises as a solution to anticipate and avoid unnecessary fine-tuning, by &lt;strong>ranking the performances of pre-trained models&lt;/strong> on a downstream task without any fine-tuning. Having a &lt;strong>benchmark on the pre-trained models&amp;rsquo; transferability&lt;/strong> would allow you to pick the relevant ones for your own downstream task.&lt;/p>
&lt;p align="center">
&lt;img src="./images/ChadalMasse/machine-learning-life-cycle.png" width="250" height="250"/>
&lt;/p>
&lt;p>This measure is also in line with &lt;strong>frugality in AI&lt;/strong>, which means using limited resources at every step of the Machine Learning lifecycle, while maintaining an acceptable accuracy. This frugality is especially relevant for small and medium-sized enterprises (SMEs) or startups, which may not have the vast computational resources that larger corporations possess. Transferable models democratize access to advanced AI capabilities, enabling these smaller entities to innovate and compete effectively. Frugality in AI also speaks to the broader goal of creating models that are not only powerful but also lean and efficient. Models with high transferability can achieve excellent performance across multiple tasks using significantly less data and fewer computational resources. This efficiency reduces the carbon footprint of training models and makes AI more accessible to a wider range of users and applications.&lt;/p>
&lt;h1 id="neural-collapse">Neural Collapse&lt;/h1>
&lt;p>Neural Collapse happens when training beyond 0 training error, i.e training error is at 0 while pushing training loss approaching 0 even further down. Imagine training a deep neural network on a dataset for a classification task. As the training process nears its end—particularly when the model is trained to a point of perfect or near-perfect classification accuracy on the training data. Intuitively, one would expect a highly overfitted and noisy model. Instead, a remarkable simplification occurs in the way the model represents the data, as it was shown in &lt;a href="#ref2">[2]&lt;/a>. This training approach offers better &lt;strong>generalization&lt;/strong> performance, better &lt;strong>robustness&lt;/strong>, and better &lt;strong>interpretability&lt;/strong>.&lt;/p>
&lt;p>Neural Collapse is characterized by three distinct proxies:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Within-Class Variability Collapse:&lt;/strong> for any given class, the feature vectors of all samples converge to a singular point or a tightly compact cluster in the high-dimensional feature space. This collapsing effect reduces the within-class variance to near zero, meaning that all samples of a class are represented almost identically from the model&amp;rsquo;s perspective ;&lt;/li>
&lt;li>&lt;strong>Simplex Encoded Label Interpolation (SELI) geometry:&lt;/strong> measures the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. The higher the rank, the smaller the difference, the closer to Neural Collapse ;&lt;/li>
&lt;li>&lt;strong>Nearest Center Classifier:&lt;/strong> ensures that the means of the collapsed points for different classes are maximally separated in the feature space.&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s look at this visual example of neural collapse :&lt;/p>
&lt;p align="center">
&lt;img src="./images/ChadalMasse/neural_collapse.gif" width="250" height="250"/>
&lt;/p>
&lt;p>Where :&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>Green Balls&lt;/strong> represent the coordinates of a simplex equiangular tight frame (ETF).&lt;/li>
&lt;li>The &lt;strong>Red Lines&lt;/strong> represent the Final Layer Classifier. The direction of the sticks indicates the orientation of its decision boundaries, while the ball-end represents the centroid in the feature space used for classification.&lt;/li>
&lt;li>The &lt;strong>Blue Lines&lt;/strong> represent the class means of the activations in the last hidden layer. The sticks show the variance around these means.&lt;/li>
&lt;li>The &lt;strong>Small Blue Balls&lt;/strong> represent the last hidden layer activations. It shows how data points from each class are distributed around the class means, forming tight clusters.&lt;/li>
&lt;/ul>
&lt;p>Initially these elements are all scattered, but as training progresses and neuronal collapse occurs, at each epoch, they move and converged gradually as shown in the GIF.&lt;/p>
&lt;h1 id="why-choosing-neural-collapse-proxies">Why choosing Neural Collapse proxies?&lt;/h1>
&lt;p>Let&amp;rsquo;s go back to imagining you have to perform a downstream task, and to do so you have to measure transferability between pre-trained models candidates. The three Neural Collapse proxies were previously defined, but we did not mention yet the three model&amp;rsquo;s aspects that are crucial to evaluate when choosing one:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Generalization:&lt;/strong> through Within-Class Variability Collapse, we gain insight into a model&amp;rsquo;s ability to generalize ;&lt;/li>
&lt;li>&lt;strong>Interpretability:&lt;/strong> the convergence toward SELI geometry not only enhances the model&amp;rsquo;s interpretability but also its alignment with optimal data representation structures. This alignment signifies a model&amp;rsquo;s capacity to distill and encode information in a way that mirrors the inherent structure of the data itself ;&lt;/li>
&lt;li>&lt;strong>Robustness:&lt;/strong> the Nearest Center Classifier proxy underscores a model&amp;rsquo;s robustness. By ensuring that class means are well-separated, the model demonstrates resilience against noise and variability in data.&lt;/li>
&lt;/ul>
&lt;p>Authors in &lt;a href="#ref3">[3]&lt;/a> demonstrate &lt;strong>both theoretically and empirically&lt;/strong> that Neural Collapse not only generalizes to new samples from the same classes seen during training but also, and more crucially, to entirely new classes. Also, a more recent research &lt;a href="#ref4">[4]&lt;/a> proposes a fine-tuning method based on Neural Collapse that achieves even better performance while reducing fine-tuning parameters by at least &lt;strong>70%&lt;/strong> !&lt;/p>
&lt;h1 id="the-ncti">The NCTI&lt;/h1>
&lt;p>Given these promising results, the authors developed a transferability estimation metric : the Neural Collapse Transferability Index (NCTI). This metric measures the proximity between the current state of a pre-trained model and its final fine-tuning stage on target, using the three neural collapse proxies defined above : Within-Class Variability Collapse, SELI geometry and Nearest Center Classifier. For each of them, a score is established : $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$. These three scores are then grouped together using normalization to prevent one score from dominating due to different scales. The final transferability estimation metric is obtained by adding the normalized scores:&lt;/p>
&lt;p>$$ S^m_{total} = S^m_{vc}(H^m) + S^m_{seli}(H^m) + S^{m}_{ncc}(H^m) $$&lt;/p>
&lt;p>Where $H_m$ is the feature extracted by the $m$-th pre-trained model (after ranking a set of $M$ pre-trained models).&lt;/p>
&lt;p>The higher the score $S^m_{total}$, the better the transferability of the model for target dataset.&lt;/p>
&lt;p>Let&amp;rsquo;s detail the scores $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$:&lt;/p>
&lt;h3 id="within-class-variability-collapse">Within-Class Variability Collapse&lt;/h3>
&lt;p>The authors noticed that larger singular values indicate higher within-class variability because the features within the class exhibit significant variation from the mean, which is desirable for effective feature representation. But since singular value decomposition (SVD) is computationally expensive for large matrices, the nuclear norm which calculates the sum of singular values in a less expensive way was used. Additionally, as feature spaces are high dimensionnal, noise may appear and affect the calculation of variability. Therefore, instead of using the feature matrix $H^m_c$ directly, the classwise logits $Z^m_c$ are substituted to calculate the feature variability.&lt;/p>
&lt;p>Thus, the score $S_{vc}$ is calculated as follow :&lt;/p>
&lt;p>$$ S^m_{vc}(H^m) = - \sum_{c=1}^{C} ||Z^m_c||_* $$&lt;/p>
&lt;p>Where $Z^m_c$ denotes the logits of the $c$-th class extracted by the $m$-th model.&lt;/p>
&lt;p>The higher the score $S_{vc}$, the higher the within-class variability, which means that the pre-trained model is closer to the final fine-tuning stage.&lt;/p>
&lt;h3 id="seli-geometry">SELI geometry&lt;/h3>
&lt;p>SELI geometry is a concept proposed in &lt;a href="#ref6">[6]&lt;/a> as a generalized geometric structure version of the simplex equiangular tight frame (ETF). ETF is defined in the context of the phenomenon of neuronal collapse, but it is limited to balanced datasets. In contrast, SELI extends this concept to both balanced and unbalanced datasets. Difference between the two geometries is shown in the figure below :&lt;/p>
&lt;div style="display: flex; justify-content: center; align-items: center;">
&lt;img src="./images/ChadalMasse/geometry.png" alt="Image 1" style="width: 49%; max-width: 100%; height: auto;">
&lt;img src="./images/ChadalMasse/neural_network.png" alt="Image 2" style="width: 49%; max-width: 100%; height: auto;">
&lt;/div>
&lt;p>Embeddings $H$ (in blue) and classifiers $W$ (in red) follow the SELI geometry if :&lt;/p>
&lt;p>$$ W^T W \alpha V \Lambda V^T, H^T H \alpha U \Lambda U^T \text{and} W^T H \alpha \hat{Z} $$&lt;/p>
&lt;p>Where $\hat{Z} = V \Lambda U^T$ is the SEL matrix &lt;a href="#ref6">[6]&lt;/a>. $U$ and $V$ denote the left and right singular vector matrix of $\hat{Z}$. $\Lambda$ represents the diagonal singular value matrix.&lt;/p>
&lt;p>A method to assess the SELI geometry structure involves computing the difference between the logits $Z^m$ extracted from the pre-trained model and the optimal logits $\hat{Z}$. However, obtaining $Z^m$ directly without fine-tuning on the target dataset is time-consuming. Therefore, features $H^m$ of the model are extracted and their difference is measured to form the SELI structure. The complexity of achieving the optimal logits $\hat{Z}$ through features $H_m$ is approximated via the nuclear norm.&lt;/p>
&lt;p>Thus, the score $S^m_{seli}$ is calculated as :&lt;/p>
&lt;p>$$S^m_{seli}(H^m) = ||H^m||_*$$&lt;/p>
&lt;p>The higher the score $S^m_{seli}$ the higher the rank of the feature matrix $H_m$, making $Z$ closer to a full rank matrix.&lt;/p>
&lt;h3 id="nearest-center-classifier">Nearest Center Classifier&lt;/h3>
&lt;p>First, the posterior probability $P(y = c|h)$ for each class $c$ is calculated using Bayes&amp;rsquo; Rule:&lt;/p>
&lt;p>$$ \log P(y = c|h) = \frac{1}{2}(h_i - \mu_c)^T \Sigma (h_j - \mu_c) + \log P(y = c) $$&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$\mu_c$ is the mean vector for class $c$.&lt;/li>
&lt;li>$\Sigma$ is the covariance matrix.&lt;/li>
&lt;li>$P(y = c)$ is the prior probability of class $c$.&lt;/li>
&lt;li>$h$ is the feature vector extracted by the pre-trained model.&lt;/li>
&lt;/ul>
&lt;p>Next, the softmax function is applied to obtain the normalized posterior probability $z^m_{i,c}$ for each class $c$ of the $i$-th sample:&lt;/p>
&lt;p>$$ z^m_{i,c} = \frac{\exp(\log P(y = c|h^m_i))}{\Sigma ^C_{k=1} \exp(\log P(y = k|h^m_i))} $$&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$C$ is the number of classes.&lt;/li>
&lt;li>$h^m_i$ is the feature vector of the $i$-th sample extracted by the m-th pre-trained model.&lt;/li>
&lt;/ul>
&lt;p>Finally, the score $S^m_{ncc}$ is computed as the average of the dot product of the normalized posterior probabilities $z^m_i$ and the ground truth labels $y_i$ for all samples:&lt;/p>
&lt;p>$$ S^m_{ncc}(H^m) = \frac{1}{N} \Sigma ^N_{i=1} z^m_i \cdot y_i $$&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$N$ is the number of samples.&lt;/li>
&lt;li>$y_i$ is the ground truth label of the $i$-th sample (in one-hot encoding).&lt;/li>
&lt;/ul>
&lt;p>The higher the score $S^{m}_{ncc}(H^m)$, the smaller the deviation to the nearest optimal centroid classifier and therefore the greater the transferability to the target dataset.&lt;/p>
&lt;h1 id="numerical-experiment">Numerical Experiment&lt;/h1>
&lt;p>To reproduce their experiment, the authors&amp;rsquo; code available on a &lt;a href="https://github.com/BUserName/NCTI/tree/main">Github&lt;/a> repository was used. A first encountered issue was the required &lt;code>torch&lt;/code> and &lt;code>torchvision&lt;/code> versions, which are quite old, and thus not always available to install, which was the case here. Fortunately, the most recent versions were compatible with the code. A &lt;code>requirements.txt&lt;/code> file would have been welcome.&lt;/p>
&lt;p>A second issue is that there are remaining personal paths in some scripts, which should be replaced by downloading paths to PyTorch source models. As a consequence, the loading method from &lt;code>torch&lt;/code> should also be replaced.&lt;/p>
&lt;p>Other issues considering the datasets loading remained unsolved.&lt;/p>
&lt;p>After these modifications, it is possible to run the authors&amp;rsquo; experiments on the CIFAR10 dataset for the group of supervised pre-trained models. Consisting of 60 000 32x32 colour images in 10 classes, this dataset is broadly used in benchmarks for image classification. 12 pre-trained models were ran on CIFAR10 to establish a ranking based on their performances in terms of NCTI available below.&lt;/p>
&lt;table style="width:100%; border-collapse: collapse;" border="1">
&lt;thead>
&lt;tr>
&lt;th style="text-align:left; padding: 8px;">Model&lt;/th>
&lt;th style="text-align:left; padding: 8px;">NCTI Score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="padding: 8px;">ResNet152&lt;/td>
&lt;td style="padding: 8px;">2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">ResNet101&lt;/td>
&lt;td style="padding: 8px;">1.799&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">DenseNet201&lt;/td>
&lt;td style="padding: 8px;">1.434&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">DenseNet169&lt;/td>
&lt;td style="padding: 8px;">1.146&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">ResNet34&lt;/td>
&lt;td style="padding: 8px;">0.757&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">ResNet50&lt;/td>
&lt;td style="padding: 8px;">0.709&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">DenseNet121&lt;/td>
&lt;td style="padding: 8px;">0.655&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">MnasNet1_0&lt;/td>
&lt;td style="padding: 8px;">0.031&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">GoogleNet&lt;/td>
&lt;td style="padding: 8px;">-0.251&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">MobileNetV2&lt;/td>
&lt;td style="padding: 8px;">-0.444&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="padding: 8px;">InceptionV3&lt;/td>
&lt;td style="padding: 8px;">-0.732&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Results show that the deepest architectures offer the best NCTI scores. The depth of a network is closely related to its ability to learn and represent complex features and patterns from the training data, which contributes to a model&amp;rsquo;s superior transferability. The different performances between ResNet and DenseNet could be attributed to the way DenseNet connects each layer to every other layer in a feed-forward fashion, which, while efficient in parameter use and reducing overfitting, may not capture as complex a feature hierarchy as ResNet. Models like MnasNet, MobileNetV2, and InceptionV3, designed for efficiency and speed with a compromise on depth, understandably score lower in transferability, as reflected by their NCTI scores.&lt;/p>
&lt;p>Then, we evaluated the transferability of the supervised pre-trained models, in terms of weighted Kendall&amp;rsquo; τ, and obtained the exact same result as the one presented in the paper: &lt;strong>0.843&lt;/strong>.&lt;/p>
&lt;p>It was not possible for us to run the experiment on the group of self-supervised pre-trained models as the authors&amp;rsquo; code included personal paths, and we were not able to find them online.&lt;/p>
&lt;p>A Github repository with all the necessary modifications from the original code is at your disposal &lt;a href="https://github.com/marionchadal/NCTI">here&lt;/a>.&lt;/p>
&lt;h1 id="what-about-source-features">What about source features?&lt;/h1>
&lt;p>Through extensive testing, authors have identified that two specific attributes related to neural collapse, observed in the source features, consistently predicted the model&amp;rsquo;s performance on new tasks. These attributes were the diversity within data categories and the compactness of category representations. Remarkably, models showing higher within-category diversity and more compact category representations in their source features tended to adapt better to new tasks. On the other hand, SELI did not consistently correlate with transferability.&lt;/p>
&lt;h1 id="challenges">Challenges&lt;/h1>
&lt;p>Authors did experiments on the effectiveness of each individual component in NCTI. They used the three terms individually and removed them one at a time from the full system, and it turned out that for supervised learning, the NCTI without NCC achieved the best weighted Kendall&amp;rsquo; τ. Instead of having normalized the three NCTI components equally, it could have been interesting to tune hyperparameters.
Moreover, the current implementation and validation of NCTI are confined to image classification tasks, suggesting its applicability may be limited to similar types of problems. Future work could extend the method&amp;rsquo;s applicability to a broader range of tasks beyond classification, such as detection or segmentation​​. Pre-trained language models could also be considered to measure their transferability based on Neural Collapse. For example, the Fair Collapse (FaCe) method &lt;a href="#ref7">[7]&lt;/a> considers both Computer Vision and Natural Language Processing tasks, using different proxies of Neural Collapse than NCTI, and producing a slightly less good τ on the CIFAR-10 dataset (0.81).&lt;/p>
&lt;h1 id="takeaways">Takeaways&lt;/h1>
&lt;p>Key points to remember are :&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Calculating model transferability and choosing the optimal pre-trained model is important for reasons of computational cost, environmental impact, and overall performance.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The authors have developed a new metric, the &lt;strong>Neural Collapse informed Transferability Index (NCTI)&lt;/strong>, which is based on the concept of &lt;strong>neural collapse&lt;/strong> and measures &lt;em>the gap between the current feature geometry and the geometry at the terminal stage after hypothetical fine-tuning on the downstream task.&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The NCTI metric integrates three aspects equally: &lt;strong>SELI geometry&lt;/strong>, &lt;strong>within-class variability&lt;/strong>, and &lt;strong>nearest center classifier&lt;/strong>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This method is &lt;em>light to compute&lt;/em>, enabling rapid evaluation of model transferability.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Empirical results demonstrate that &lt;em>the ranking of model transferability has a very strong correlation with the ground truth ranking&lt;/em> and &lt;strong>compares with state-of-the-art methods&lt;/strong>, highlighting its effectiveness in selecting pre-trained models for specific tasks.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In summary, the development of metrics such as NCTI is crucial for optimizing the use of pre-trained models, considering both performance and associated costs in real-world applications.&lt;/p>
&lt;h1 id="references">References&lt;/h1>
&lt;p>&lt;a id="ref1">&lt;/a>1. Z. Wang Y.Luo, L.Zheng, Z.Huang, M.Baktashmotlagh (2023), How far pre-trained models are from neural collapse on the target dataset informs their transferabilityWang, ICCV.&lt;/p>
&lt;p>&lt;a id="ref2">&lt;/a>2. V. Papyan,1 , X. Y. Hanb,1 , and D.L. Donoho (2020), Prevalence of neural collapse during the terminal phase of deep learning training, National Academy of Sciences.&lt;/p>
&lt;p>&lt;a id="ref3">&lt;/a>3. Galanti, T., György, A., &amp;amp; Hutter, M. (2021). On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121.&lt;/p>
&lt;p>&lt;a id="ref4">&lt;/a>4. Li, X., Liu, S., Zhou, J., Lu, X., Fernandez-Granda, C., Zhu, Z., &amp;amp; Qu, Q. (2022). Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206.&lt;/p>
&lt;p>&lt;a id="ref5">&lt;/a>5. Vignesh Kothapalli, (2023). Neural Collapse: A Review on Modelling Principles and Generalization. arXiv preprint arXiv:2206.04041.&lt;/p>
&lt;p>&lt;a id="ref6">&lt;/a>6. Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. (2022). Imbalance trouble: Revisiting neural-collapse
geometry. arXiv preprint arXiv:2208.05512.&lt;/p>
&lt;p>&lt;a id="ref7">&lt;/a>7. Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang. (2023). Unleashing the power of neural collapse for transferability estimation. arXiv preprint arXiv:2310.05754v1.&lt;/p>
&lt;hr>&lt;/hr>
&lt;p>Start writing here !&lt;/p></description><author>Students from M2 Data Science IP Paris</author><guid>https://responsible-ai-datascience-ipParis.github.io/posts/transferability/</guid><pubDate>Mon, 08 Jan 2024 11:26:03 +0100</pubDate></item></channel></rss>